[{
  "id" : "78vedq",
  "name" : "data_sentinel2",
  "description" : null,
  "code" : "# Data Preparation for Sentinel 2\n\nprint(\"Not ready yet..Prepare sentinel 2 into .csv\")\n\nprint('test')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "mxpyqt",
  "name" : "model_creation_lstm",
  "description" : "python",
  "code" : "# Create LSTM model\n\nprint(\"Create LSTM\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rauqsh",
  "name" : "model_creation_ghostnet",
  "description" : "python",
  "code" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "u7xh2p",
  "name" : "data_integration",
  "description" : null,
  "code" : "\"\"\"\nThis script reads three CSV files into Dask DataFrames, performs data type conversion,\nand merges them into a final Dask DataFrame. The merged data is then saved to a CSV file.\n\nThe three CSV files include climatology data, training-ready SNOTEL data, and training-ready\nterrain data, each with latitude ('lat'), longitude ('lon'), and date ('date') columns.\n\nAttributes:\n    file_path1 (str): File path of the climatology data CSV file.\n    file_path2 (str): File path of the training-ready SNOTEL data CSV file.\n    file_path3 (str): File path of the training-ready terrain data CSV file.\n\nFunctions:\n    small_function: Reads, processes, and merges the CSV files and saves the result to a CSV file.\n\"\"\"\n\nimport dask.dataframe as dd\nfrom snowcast_utils import homedir, work_dir, train_start_date, train_end_date, fsca_dir\nimport os\n\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\n# Define the file paths of the three CSV files\nfile_path1 = '/home/chetana/gridmet_test_run/climatology_data.csv'\nfile_path2 = '/home/chetana/gridmet_test_run/training_ready_snotel_data.csv'\nfile_path3 = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\n\ndef small_function():\n    \"\"\"\n    Reads each CSV file into a Dask DataFrame, performs data type conversion for latitude and longitude,\n    merges the DataFrames based on specific columns, and saves the merged Dask DataFrame to a CSV file.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Read each CSV file into a Dask DataFrame\n    df1 = dd.read_csv(file_path1)\n    df2 = dd.read_csv(file_path2)\n    df3 = dd.read_csv(file_path3)\n\n    # Perform data type conversion for latitude and longitude columns\n    df1['lat'] = df1['lat'].astype(float)\n    df1['lon'] = df1['lon'].astype(float)\n    df2['lat'] = df2['lat'].astype(float)\n    df2['lon'] = df2['lon'].astype(float)\n    df3['lat'] = df3['lat'].astype(float)\n    df3['lon'] = df3['lon'].astype(float)\n\n    # Merge the first two DataFrames based on 'lat', 'lon', and 'date'\n    merged_df1 = dd.merge(df1, df2, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'Date'])\n\n    # Merge the third DataFrame based on 'lat' and 'lon'\n    merged_df2 = dd.merge(merged_df1, df3, on=['lat', 'lon'])\n\n    # Save the merged Dask DataFrame directly to a CSV file\n    merged_df2.to_csv('/home/chetana/gridmet_test_run/model_training_data.csv', index=False, single_file=True)\n\n# Uncomment the line below to execute the function\n# small_function()\n\ndef merge_all_files_for_training_points(training_points_csv, final_final_output_file):\n    training_points_file_name = os.path.basename(training_points_csv)\n\n    terrain_file = f\"{work_dir}/{training_points_file_name}_dem.csv\"\n    gridmet_file = f\"{work_dir}/{training_points_file_name}_gridmet_training.csv\"\n    amsr_file = f\"{work_dir}/{training_points_file_name}_amsr_training.csv\"\n    fsca_file = f\"{fsca_dir}/{training_points_file_name}_fsca_training.csv\"\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    # ground_truth = dd.read_csv(all_station_obs_file, blocksize=chunk_size)\n    # print(\"ground_truth.columns = \", ground_truth.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    # ground_truth = ground_truth.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    # print(\"start to merge amsr and ground_truth\")\n    # merged_df = dd.merge(amsr, ground_truth, on=['lat', 'lon', 'date'], how='outer')\n    # merged_df = merged_df.drop_duplicates(keep='first')\n    # output_file = os.path.join(working_dir, f\"{final_output_name}_ground_truth.csv\")\n    # merged_df.to_csv(output_file, single_file=True, index=False)\n    # print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge amsr and gridmet\")\n    merged_df = dd.merge(amsr, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    # output_file = os.path.join(work_dir, f\"{final_output_name}_gridmet.csv\")\n    # merged_df.to_csv(output_file, single_file=True, index=False)\n    # print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    # output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    # merged_df.to_csv(output_file, single_file=True, index=False)\n    # print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    # output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    # merged_df.to_csv(output_file, single_file=True, index=False)\n    # print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    # output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(final_final_output_file, single_file=True, index=False)\n    print(f'Merge completed. {final_final_output_file}')\n\ndef cleanup_dataframe(training_points_csv, final_final_output_file):\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \n\nif __name__ == \"__main__\":\n    training_points_csv = f\"{work_dir}/salt_pepper_points_for_training.csv\"\n    final_final_output_file = f\"{training_points_csv}_all_training.csv\"\n    merge_all_files_for_training_points(training_points_csv, final_final_output_file)\n    cleanup_dataframe(training_points_csv, final_final_output_file)\n    sort_training_data(final_final_output_file, f\"{final_final_output_file}_sorted.csv\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "e8k4wq",
  "name" : "model_train_validate",
  "description" : null,
  "code" : "\"\"\"\nThis script trains and validates machine learning models for hole analysis.\n\nAttributes:\n    RandomForestHole (class): A class for training and using a Random Forest model.\n    XGBoostHole (class): A class for training and using an XGBoost model.\n    ETHole (class): A class for training and using an Extra Trees model.\n\nFunctions:\n    main(): The main function that trains and validates machine learning models for hole analysis.\n\"\"\"\n\nfrom model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\nfrom model_creation_et import ETHole\n\ndef main():\n    print(\"Train Models\")\n\n    # Choose the machine learning models to train (e.g., RandomForestHole, XGBoostHole, ETHole)\n    worm_holes = [ETHole()]\n\n    for hole in worm_holes:\n        # Perform preprocessing for the selected model\n        hole.preprocessing()\n        print(hole.train_x.shape)\n        print(hole.train_y.shape)\n        \n        # Train the machine learning model\n        hole.train()\n        \n        # Test the trained model\n        hole.test()\n        \n        # Evaluate the model's performance\n        hole.evaluate()\n        \n        # Save the trained model\n        hole.save()\n\n    print(\"Finished training and validating all the models.\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "h1qp9v",
  "name" : "model_predict",
  "description" : null,
  "code" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, model_dir, plot_dir, output_dir, month_to_season, test_start_date, test_end_date, process_dates_in_range\nimport os\nimport random\nimport string\nimport shutil\nfrom model_creation_et import selected_columns\nfrom datetime import datetime, timedelta\n# from interpret_model_results import explain_predictions\n\nimport shap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport traceback\n\nCOLUMN_NAME_MAPPER = {'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n#                          'Elevation': 'elevation',\n#                          'Slope': 'Slope',\n#                          'Aspect': 'Aspect',\n#                          'Curvature': 'Curvature',\n#                          'Northness': 'Northness',\n#                          'Eastness': 'Eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n#                          'relative_humidity_rmin': '',\n#                          'cumulative_rmin',\n#                          'mean_vapor_pressure_deficit', \n#                          'cumulative_vpd', \n#                          'wind_speed',\n#                          'cumulative_vs', \n#                          'relative_humidity_rmax', 'cumulative_rmax',\n\n# 'precipitation_amount', 'cumulative_pr', 'air_temperature_tmmx',\n\n# 'cumulative_tmmx', 'potential_evapotranspiration', 'cumulative_etr',\n\n# 'air_temperature_tmmn', 'cumulative_tmmn', 'x', 'y', 'elevation',\n\n# 'slope', 'aspect', 'curvature', 'northness', 'eastness', 'AMSR_SWE',\n\n# 'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag',\n}\n\nCOLUMN_LOOK_BACK = [\n    'mean_vapor_pressure_deficit',\n    'wind_speed', \n    'precipitation_amount', \n    'potential_evapotranspiration',\n    'air_temperature_tmmn',\n    'air_temperature_tmmx',\n    'relative_humidity_rmin',\n    'relative_humidity_rmax',\n    'SWE',\n    'fsca',\n]\n\nCOLUMN_UNCHANGED = [\n    'Aspect', \n    'Elevation', \n    'Curvature', \n    'Northness', \n    'Flag', \n    'x', \n    'Eastness', \n    'water_year', \n    'Slope', \n    'lc_prop3', \n    'y',\n    'snodas_mask',\n]\n\ndef generate_random_string(length):\n    # Define the characters that can be used in the random string\n    characters = string.ascii_letters + string.digits  # You can customize this to include other characters if needed\n\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n\n    return random_string\n  \n\ndef load_model(model_path):\n    \"\"\"\n    Load a machine learning model from a file.\n\n    Args:\n        model_path (str): Path to the saved model file.\n\n    Returns:\n        model: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file containing the data.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the loaded data.\n    \"\"\"\n    return pd.read_csv(file_path)\n\n\n# 'SWE', 'relative_humidity_rmin', 'potential_evapotranspiration',\n#     'air_temperature_tmmx', 'relative_humidity_rmax',\n#     'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed',\n#     'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'fsca',\n#     'Slope', 'SWE_1', 'air_temperature_tmmn_1',\n#     'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n#     'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n#     'air_temperature_tmmx_1', 'wind_speed_1', 'fsca_1', 'SWE_2',\n#     'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n#     'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n#     'relative_humidity_rmin_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n#     'fsca_2', 'SWE_3', 'air_temperature_tmmn_3',\n#     'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n#     'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n#     'air_temperature_tmmx_3', 'wind_speed_3', 'fsca_3', 'SWE_4',\n#     'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n#     'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n#     'relative_humidity_rmin_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n#     'fsca_4', 'SWE_5', 'air_temperature_tmmn_5',\n#     'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n#     'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n#     'air_temperature_tmmx_5', 'wind_speed_5', 'fsca_5', 'SWE_6',\n#     'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n#     'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n#     'relative_humidity_rmin_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n#     'fsca_6', 'SWE_7', 'air_temperature_tmmn_7',\n#     'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n#     'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n#     'air_temperature_tmmx_7', 'wind_speed_7', 'fsca_7', 'water_year'\n\ndef preprocess_data_with_history():\n    pass\n\ndef preprocess_chunk(chunk, day_offset):\n    \"\"\"\n    Load, clean, and rename columns for a specific day.\n\n    Args:\n        file_path (str): Path to the CSV file.\n        day_offset (int): Day offset (0 for current day, 1 for one day ago, etc.).\n\n    Returns:\n        pd.DataFrame: Processed DataFrame for the specific day.\n    \"\"\"\n    if \"date.1\" in chunk.columns:\n        chunk = chunk.drop([\"date.1\"], axis=1)\n    chunk.replace('--', pd.NA, inplace=True)\n    chunk.rename(columns=COLUMN_NAME_MAPPER, inplace=True)\n    chunk['date'] = pd.to_datetime(chunk['date'])\n\n    # print(\"Before drop: \", chunk.columns)\n    if day_offset != 0:\n        chunk.drop(COLUMN_UNCHANGED+[\"date\"], axis=1, inplace=True)\n        for col in COLUMN_LOOK_BACK:\n            chunk.rename(\n                columns={col: f\"{col}_{day_offset}\"}, inplace=True\n            )\n\n    # print(\"After drop: \", chunk.columns)\n    return chunk\n\ndef preprocess_data(target_date, is_model_input: bool = True):\n    \"\"\"\n    Preprocess the input data for model prediction.\n\n    Args:\n        target_date (str): Target date in the format 'YYYY-MM-DD'.\n        is_model_input (bool): Flag to specify if the data is for model input.\n\n    Returns:\n        pd.DataFrame: Preprocessed data ready for prediction.\n    \"\"\"\n    \n    # Initialize a list to store all data including past 7 days\n    all_data = []\n\n    # Process the current day\n    # current_day_path = f'{work_dir}/testing_all_ready_{target_date}.csv'\n    # current_day_data = process_day_data(current_day_path, 0)\n    # all_data.append(current_day_data)\n\n    # Process the past 7 days\n    target_date_dt = pd.to_datetime(target_date)\n    for i in range(0, 7):\n        past_date = (target_date_dt - pd.Timedelta(days=i)).strftime('%Y-%m-%d')\n        past_data_path = f'{work_dir}/testing_all_ready_{past_date}.csv_snodas_mask.csv'\n        past_day_data = process_day_data(past_data_path, i)\n        all_data.append(past_day_data)\n\n    # Merge all data on 'date', 'lat', and 'lon'\n    merged_data = all_data[0]\n    for additional_data in all_data[1:]:\n        merged_data = merged_data.merge(additional_data, on=['date', 'lat', 'lon'], how='outer')\n\n    if is_model_input:\n        if \"swe_value\" in selected_columns:\n            selected_columns.remove(\"swe_value\")\n        desired_order = selected_columns + ['lat', 'lon']\n\n        merged_data = merged_data[desired_order]\n        merged_data = merged_data.reindex(columns=desired_order)\n\n        # print(\"Reorganized columns: \", merged_data.columns)\n\n    # print(merged_data.head())\n\n    return merged_data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Predict Snow Water Equivalent (SWE) values using a pre-trained model.\n\n    This function takes in a machine learning model and a DataFrame containing \n    meteorological and geospatial data, preprocesses the data by handling missing \n    values and dropping unnecessary columns, and applies the model to predict SWE values. \n    The predicted SWE values are then added to the original DataFrame as a new column \n    called 'predicted_swe'.\n\n    Args:\n        model (object): A pre-trained machine learning model with a `predict` method.\n        data (pd.DataFrame): A pandas DataFrame containing input data for prediction.\n            It is expected to have columns including 'lat', 'lon', and other relevant \n            features for the model.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with an additional column 'predicted_swe' \n        containing the predicted SWE values.\n    \"\"\"\n    data = data.fillna(-1)\n    input_data = data\n    input_data = data.drop([\"lat\", \"lon\"], axis=1)\n\n    print(\"Assign -1 to fsca column..\")\n    # original_input_data = input_data.copy()\n    # input_data.loc[input_data['fsca'] > 100, 'fsca'] = -1 \n    for column in input_data.columns:\n        if 'fsca' in column.lower() or 'swe' in column.lower():  # Adjust to case-insensitive match\n            input_data.loc[input_data[column] > 100, column] = -1\n\n    #input_data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin',], axis=1)\n    #scaler = StandardScaler()\n\n    # Fit the scaler on the training data and transform both training and testing data\n    #input_data_scaled = scaler.fit_transform(input_data)\n    print(\"Start to predict\", input_data.shape)\n    predictions = model.predict(input_data)\n    input_data['predicted_swe'] = predictions\n    input_data['lat'] = data['lat']\n    input_data['lon'] = data['lon']\n\n    # print(\"Explain the prediction: \")\n    # explain_predictions(model, input_data, input_data.columns, f\"{output_dir}/explain_ai.csv\", f\"{plot_dir}\")\n    return input_data\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge predicted SWE data with the original data.\n\n    Args:\n        original_data (pd.DataFrame): Original input data.\n        predicted_data (pd.DataFrame): Dataframe with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged dataframe.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    if \"date\" not in predicted_data:\n    \tpredicted_data[\"date\"] = test_start_date\n    # new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    # print(\"original_data.columns: \", original_data.columns)\n    # print(\"predicted_data.columns: \", predicted_data.columns)\n    # print(\"new prediction statistics: \", predicted_data[\"predicted_swe\"].describe())\n    #merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(predicted_data, on=['lat', 'lon'], how='left')\n    # print(\"first merged df: \", merged_df.columns)\n\n    # merged_df.loc[merged_df['fsca'] == -1, 'predicted_swe'] = 0\n    # merged_df.loc[merged_df['fsca'] == 239, 'predicted_swe'] = 0\n    # merged_df.loc[merged_df['fsca'] == 225, 'predicted_swe'] = 0\n    #merged_df.loc[merged_df['cumulative_fsca'] == 0, 'predicted_swe'] = 0\n    # merged_df.loc[merged_df['fsca'] <= 0, 'predicted_swe'] = 0\n\n    # if predicted value is minus, assign 0\n    merged_df.loc[merged_df['predicted_swe'] < 0, 'predicted_swe'] = 0\n    \n    merged_df.loc[merged_df['air_temperature_tmmx'].isnull(), \n                  'predicted_swe'] = 0\n\n    merged_df.loc[merged_df['lc_prop3'] == 3, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['lc_prop3'] == 255, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['lc_prop3'] == 27, 'predicted_swe'] = 0\n\n    return merged_df\n\n\ndef predict_in_batches(\n    target_date: str, \n    output_path: str = None, \n    batch_size: int = 100000\n):\n    \"\"\"\n    Predict snow water equivalent (SWE) in batches by processing 7 days' data chunk by chunk.\n\n    Args:\n        target_date (str): Target date in the format 'YYYY-MM-DD'.\n        output_path (str): Path to save the prediction results.\n        batch_size (int): Size of each chunk to process.\n\n    Returns:\n        None\n    \"\"\"\n    # height = 666\n    # width = 694\n    model_path = f'{model_dir}/wormhole_ETHole_latest.joblib'\n    print(f\"Using model: {model_path}\")\n\n    if output_path is None:\n        output_path = f'{output_dir}/test_data_predicted_latest_{target_date}.csv_snodas_mask.csv'\n\n    if os.path.exists(output_path):\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    # Load the model\n    model = load_model(model_path)\n\n    # Initialize file readers for each of the 7 days\n    target_date_dt = pd.to_datetime(target_date)\n    day_file_iters = []\n\n    for day_offset in range(8):\n        day_date = (target_date_dt - pd.Timedelta(days=day_offset)).strftime('%Y-%m-%d')\n        day_file_path = f'{work_dir}/testing_all_ready_{day_date}.csv_snodas_mask.csv'\n\n        try:\n            print(\"Loading batches from \", day_file_path)\n            day_file_iters.append(pd.read_csv(day_file_path, chunksize=batch_size))\n        except FileNotFoundError:\n            print(f\"File not found: {day_file_path}. Skipping this day.\")\n            day_file_iters.append(None)\n\n    # Process chunks\n    chunk_idx = 0\n    while True:\n        chunk_list = []\n        for day_idx, file_iter in enumerate(day_file_iters):\n            if file_iter is None:\n                continue\n            try:\n                chunk = next(file_iter)\n                print(f\"Read chunk {chunk_idx + 1} from day {day_idx + 1}\")\n                preprocessed_chunk = preprocess_chunk(chunk, day_offset=day_idx)\n                chunk_list.append(preprocessed_chunk)\n            except StopIteration:\n                print(f\"No more chunks for day {day_idx + 1}\")\n                continue\n\n        # If no more chunks for all days, break\n        if not chunk_list:\n            print(\"All chunks are processed\")\n            break\n\n        # Merge all chunks on 'date', 'lat', and 'lon'\n        merged_input = chunk_list[0]\n        for additional_chunk in chunk_list[1:]:\n            merged_input = merged_input.merge(additional_chunk, on=['lat', 'lon'], how='outer')\n\n        if len(merged_input) != len(chunk_list[0]):\n            raise ValueError(\n                f\"Row number mismatch: merged_input has {len(merged_input)} rows, \"\n                f\"but chunk_list[0] has {len(chunk_list[0])} rows. Ensure data alignment.\"\n            )\n\n        # print(\"merged_input.columns = \", merged_input.columns)\n\n        # Reorganize columns for model input\n        if \"swe_value\" in selected_columns:\n            selected_columns.remove(\"swe_value\")\n        desired_order = selected_columns + ['lat', 'lon']\n        used_input = merged_input[desired_order].reindex(columns=desired_order)\n        unused_input = merged_input[[\"lc_prop3\", \"lat\", \"lon\", \"date\"]]\n\n        # Predict on the merged input\n        predictions = predict_swe(model, used_input)\n        # print(f\"Predicted {len(predictions)} rows for chunk {chunk_idx + 1}\")\n\n        # Merge predictions with input\n        predictions_merged = merge_data(unused_input, predictions)\n\n        # Save predictions to output file incrementally\n        if chunk_idx == 0:\n            predictions_merged.to_csv(output_path, index=False, mode='w')\n        else:\n            predictions_merged.to_csv(output_path, index=False, mode='a', header=False)\n\n        chunk_idx += 1\n\n    print(f\"Prediction completed. Results saved to {output_path}\")\n\ndef predict_for_date(current_date, force: bool = False):\n    \"\"\"\n    Example callback function to predict SWE for a specific date.\n\n    Args:\n        current_date (datetime): The date to process.\n        force (bool): Whether to force processing even if conditions aren't met.\n    \"\"\"\n    current_date_str = current_date.strftime(\"%Y-%m-%d\")\n    print(f\">>>>>\\nPredicting SWE for day {current_date_str}\")\n    # Replace this with actual prediction logic\n    predict_in_batches(target_date=current_date_str,)\n\nif __name__ == \"__main__\":\n\tprocess_dates_in_range(\n        start_date=test_start_date,\n        end_date=test_end_date,\n        days_look_back=0,\n        # start_date=\"2025-01-14\",\n        # end_date=\"2025-01-14\",\n        callback=predict_for_date,\n        force = True\n    )\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "urd0nk",
  "name" : "data_terrainFeatures",
  "description" : null,
  "code" : "# Load dependencies\nimport geopandas as gpd\nimport json\nimport geojson\nfrom pystac_client import Client\nimport planetary_computer\nimport xarray\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyproj import Proj, transform\nimport os\nimport sys, traceback\nimport requests\nfrom snowcast_utils import homedir, snowcast_github_dir, work_dir, \\\nsupplement_point_for_correction_file, data_dir, gridcells_file, \\\nstations_file, all_training_points_with_station_and_non_station_file, \\\nall_training_points_with_snotel_ghcnd_file, gridcells_outfile, stations_outfile\n\n# # user-defined paths for data-access\n# data_dir = f'{snowcast_github_dir}data/'\n# gridcells_file = data_dir+'snowcast_provided/grid_cells_eval.geojson'\n# #stations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\n# stations_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n# supplement_point_for_correction_file = f\"{work_dir}/salt_pepper_points_for_training.csv\"\n# #stations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\n# all_training_points_with_station_and_non_station_file = f\"{work_dir}/all_training_points_in_westus.csv\"\n# all_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\n# gridcells_outfile = data_dir+'terrain/gridcells_terrainData_eval.csv'\n# #stations_outfile = f\"{work_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv\"\n# stations_outfile = f\"{work_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv\"\n\ndef get_planetary_client():\n  #requests.get('https://planetarycomputer.microsoft.com/api/stac/v1')\n\n  # setup client for handshaking and data-access\n  print(\"setup planetary computer client\")\n  client = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",ignore_conformance=True)\n  \n  return client\n\ndef prepareGridCellTerrain():\n  client = get_planetary_client()\n  # Load metadata\n  gridcellsGPD = gpd.read_file(gridcells_file)\n  gridcells = geojson.load(open(gridcells_file))\n  stations = pd.read_csv(stations_file)\n\n  # instantiate output panda dataframes\n  df_gridcells = df = pd.DataFrame(columns=(\n    \"Longitude [deg]\",\"Latitude [deg]\",\n    \"Elevation [m]\",\"Aspect [deg]\",\n    \"Curvature [ratio]\",\"Slope [deg]\",\n    \"Eastness [unitCirc.]\",\"Northness [unitCirc.]\"))\n  # instantiate output panda dataframes\n  # Calculate gridcell characteristics using Copernicus DEM data\n  print(\"Prepare GridCell Terrain data\")\n  for idx,cell in enumerate(gridcells['features']):\n      print(\"Processing grid \", idx)\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Polygon\", \"coordinates\":cell['geometry']['coordinates']},\n      )\n      items = list(search.get_items())\n      print(\"==> Searched items: \", len(items))\n\n      cropped_data = None\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              #xarray.open_rasterio(signed_asset.href)\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n      except:\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n\n      # calculate lat/long of center of gridcell\n      longitude = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      latitude = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      print(\"reproject data to EPSG:32612\")\n      # reproject the cropped dem data\n      cropped_data = cropped_data.rio.reproject(\"EPSG:32612\")\n\n      # Mean elevation of gridcell\n      mean_elev = cropped_data.mean().values\n      print(\"Elevation: \", mean_elev)\n\n      # Calculate directional components\n      aspect = xrspatial.aspect(cropped_data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(\"Aspect: \", mean_aspect)\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n      print(\"Eastness: \", mean_eastness)\n      print(\"Northness: \", mean_northness)\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(cropped_data)\n      mean_curvature = curvature.mean().values\n      print(\"Curvature: \", mean_curvature)\n\n      # Calculate mean slope\n      slope = xrspatial.slope(cropped_data)\n      mean_slope = slope.mean().values\n      print(\"Slope: \", mean_slope)\n\n      # Fill pandas dataframe\n      df_gridcells.loc[idx] = [longitude,latitude,\n                               mean_elev,mean_aspect,\n                               mean_curvature,mean_slope,\n                               mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n      #     df_gridcells.to_csv(gridcells_outfile)\n\n  # Save output data into csv format\n  df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n  df_gridcells.to_csv(gridcells_outfile)\n\ndef prepareStationTerrain():\n  client = get_planetary_client()\n  \n  df_station = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                     \"Elevation [m]\",\"Elevation_30 [m]\",\"Elevation_1000 [m]\",\n                                     \"Aspect_30 [deg]\",\"Aspect_1000 [deg]\",\n                                     \"Curvature_30 [ratio]\",\"Curvature_1000 [ratio]\",\n                                     \"Slope_30 [deg]\",\"Slope_1000 [deg]\",\n                                     \"Eastness_30 [unitCirc.]\",\"Northness_30 [unitCirc.]\",\n                                     \"Eastness_1000 [unitCirc.]\",\"Northness_1000 [unitCirc.]\"))\n  \n  stations_df = pd.read_csv(stations_file)\n  print(stations_df.head())\n  # Calculate terrain characteristics of stations, and surrounding regions using COP 30\n  for idx,station in stations_df.iterrows():\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\n            \"type\": \"Point\", \n            \"coordinates\": [\n              stations_df['lon'],\n              stations_df['lat']\n            ]\n          },\n      )\n      items = list(search.get_items())\n      print(f\"Returned {len(items)} items\")\n\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-stations_df['lon'])\n          ydiff = np.abs(data.y-stations_df['lat'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n      except:\n          traceback.print_exc(file=sys.stdout)\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-stations_df['lon'])\n          ydiff = np.abs(data.y-stations_df['lat'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n\n      # Reproject the station data to better include only 1000m surrounding area\n      inProj = Proj(init='epsg:4326')\n      outProj = Proj(init='epsg:32612')\n      new_x,new_y = transform(inProj,outProj,\n                              stations_df['lon'],\n                              stations_df['lat'])\n\n      # Calculate elevation of station and surroundings\n      mean_elevation = data.mean().values\n      elevation = data.sel(x=new_x,y=new_y,method='nearest')\n      print(elevation.values)\n\n      # Calcuate directional components\n      aspect = xrspatial.aspect(data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      #print(mean_aspect)\n      aspect = aspect.sel(x=new_x,y=new_y,method='nearest')\n      #print(aspect.values)\n      eastness = np.cos(aspect*(np.pi/180))\n      northness = np.sin(aspect*(np.pi/180))\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(data)\n      mean_curvature = curvature.mean().values\n      curvature = curvature.sel(x=new_x,y=new_y,method='nearest')\n      print(curvature.values)\n\n      # Calculate slope\n      slope = xrspatial.slope(data)\n      mean_slope = slope.mean().values\n      slope = slope.sel(x=new_x,y=new_y,method='nearest')\n      print(slope.values)\n\n      # Fill pandas dataframe\n      df_station.loc[idx] = [stations_df['lon'],\n                             stations_df['lat'],\n                             station['elevation_m'],\n                             elevation.values,mean_elevation,\n                             aspect.values,mean_aspect,\n                             curvature.values,mean_curvature,\n                             slope.values,mean_slope,\n                             eastness.values,northness.values,\n                             mean_eastness,mean_northness]\n\n  # Save output data into CSV format\n  df_station.set_index(stations_df['station_name'][0:idx+1],inplace=True)\n  df_station.to_csv(stations_outfile)\n\n\ndef add_more_points_to_the_gridcells():\n  # check how many points are in the current grid_cell json\n  station_cell_mapping = f\"{work_dir}/station_cell_mapping.csv\"\n  current_grid_df = pd.read_csv(station_cell_mapping)\n  \n  print(current_grid_df.columns)\n  print(current_grid_df.shape)\n  \n  western_us_coords = f'{work_dir}/dem_file.tif.csv'\n  dem_df = pd.read_csv(western_us_coords)\n  print(dem_df.head())\n  print(dem_df.shape)\n  filtered_df = dem_df[dem_df['Elevation'] > 20]  # choose samples from points higher than 20 meters\n\n  # Randomly choose 700 rows from the filtered DataFrame\n  random_rows = filtered_df.sample(n=700)\n  random_rows = random_rows[[\"Latitude\", \"Longitude\"]]\n  random_rows.rename(columns={\n    'Latitude': 'lat', \n    'Longitude': 'lon'\n  }, inplace=True)\n  previous_cells = current_grid_df[[\"lat\", \"lon\"]]\n  result_df = previous_cells.append(random_rows, ignore_index=True)\n  print(result_df.shape)\n  result_df.to_csv(f\"{work_dir}/new_training_points_with_random_dem_locations.csv\")\n  print(f\"New training points are saved to {work_dir}/new_training_points_with_random_dem_locations.csv\")\n  \n  \n  \n  # find the random points that are on land from the dem.json\n  \n  # merge the grid_cell.json with the new dem points into a new grid_cell.json\n  \ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n    #print(\"lat_diff = \", lat_diff)\n    #print(\"lon_diff = \", lon_diff)\n\n    #lat_idx = np.argmin(lat_diff)\n    #lon_idx = np.argmin(lon_diff)\n    # Find the indices corresponding to the minimum differences\n    #lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n    row_idx = np.argmin(lat_diff + lon_diff)\n\n    return row_idx\n  \n  \ndef read_terrain_from_dem_csv(input_csv, output_csv):\n  western_us_coords = f'{work_dir}/dem_all.csv'\n  western_df = pd.read_csv(western_us_coords)\n  print(\"western_df.head() = \", western_df.head())\n  \n  stations_file_df = pd.read_csv(input_csv)\n  print(\"stations_file_df.head() = \", stations_file_df.head())\n  \n  def find_closest_dem_row(row, western_df):\n    #print(row)\n    row_idx = find_closest_index(\n      row[\"latitude\"],\n      row[\"longitude\"],\n      western_df[\"Latitude\"], \n      western_df[\"Longitude\"]\n    )\n    #print(\"row_idx = \", row_idx)\n    dem_row = western_df.iloc[row_idx]\n    #print(\"dem_row = \", dem_row)\n    new_row = pd.concat([row, dem_row], axis=0)\n    #print(\"result_series = \", new_row)\n    #exit(1)\n    return new_row\n  \n  stations_file_df = stations_file_df.apply(find_closest_dem_row, args=(western_df,), axis=1)\n  stations_file_df.to_csv(output_csv, index=False)\n  print(f\"New elevation csv is aved to {output_csv}\")\n  \n\nif __name__ == \"__main__\":\n  try:\n    #prepareGridCellTerrain()\n    #prepareStationTerrain()\n    \n    #add_more_points_to_the_gridcells()\n    # read_terrain_from_dem_csv(all_training_points_with_snotel_ghcnd_file, stations_outfile)\n    read_terrain_from_dem_csv(supplement_point_for_correction_file, f\"{work_dir}/salt_pepper_dem.csv\")\n  except:\n    traceback.print_exc(file=sys.stdout)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "525l8q",
  "name" : "data_gee_modis_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport eeauth as e\n\n#exit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize(e.creds())\nexcept Exception as e:\n    # the following is for the server\n    #service_account = 'eartheginegcloud@earthengine58.iam.gserviceaccount.com'\n#creds = ee.ServiceAccountCredentials(\n    #service_account, '/home/chetana/bhargavi-creds.json')\n    #ee.Initialize(creds)\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n\n#org_name = 'sentinel1'\n#product_name = 'COPERNICUS/S1_GRD'\n#var_name = 'VV'\n#column_name = 's1_grd_vv'\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n  \t  single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/modis/{column_name}_{current_cell_id}.csv\"\n\n  \t  if os.path.exists(single_csv_file):\n  \t    print(\"exists skipping..\")\n  \t    continue\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate('2013-01-01','2017-12-31')\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df1 = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  viirs2 = ee.ImageCollection(product_name).filterDate('2018-01-01','2021-12-31')\n  \t  poi_reduced_imgs2 = viirs2.map(poi_mean)\n  \t  nested_list2 = poi_reduced_imgs2.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df2 = pd.DataFrame(nested_list2.getInfo(), columns=['date',column_name])\n      \n\n  \t  df = pd.concat([df1, df2])\n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7temiv",
  "name" : "data_gee_sentinel1_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import work_dir\n\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{work_dir}/testing_points.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'lat', 'lon'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      #current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      #print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{work_dir}/{org_name}_{column_name}_{ind}.csv\"\n\n#       if os.path.exists(single_csv_file):\n#           print(\"exists skipping..\")\n#           continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      #poi = ee.Geometry.Point(longitude, latitude)\n      viirs = ee.ImageCollection(product_name).filterDate('2017-10-01','2018-07-01').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      #df['cell_id'] = current_cell_id\n      df['lat'] = latitude\n      df['lon'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nprint(all_cell_df.head())\nprint(all_cell_df[\"s1_grd_vv\"].describe())\nall_cell_df.to_csv(f\"{work_dir}/Sentinel1_Testing.csv\")\nprint(\"The Sentinel 1 is downloaded successfully. \")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rmxece",
  "name" : "data_associate_station_grid_cell",
  "description" : null,
  "code" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom snowcast_utils import work_dir, read_json_file\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n\n\nif __name__ == \"__main__\":\n  \n    # pd.set_option('display.max_columns', None)\n\n    # read the grid geometry file\n    homedir = os.path.expanduser('~')\n    print(homedir)\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n    # read grid cell\n    gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\n    model_dir = f\"{github_dir}/model/\"\n    training_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\n    testing_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\n    train_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\n    ground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n    station_list_file = f\"{work_dir}/training_snotel_station_list_elevation.csv\"\n\n    ready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\n    result_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\n    station_locations = read_json_file(f'{work_dir}/snotelStations.json')\n    # print(station_locations)\n\n    result_df = pd.DataFrame(columns=['station_name', 'elevation', 'lat', 'lon'])\n    for station in station_locations:\n        print(f'station {station[\"name\"]} completed.')\n\n        location_name = station['name']\n        location_triplet = station['triplet']\n        location_elevation = station['elevation']\n        location_station_lat = station['location']['lat']\n        location_station_long = station['location']['lng']\n        new_df = pd.DataFrame([{\n            'station_name': location_name,\n            'elevation': location_elevation,\n            'lat': location_station_lat,\n            'lon': location_station_long\n        }])\n        result_df = pd.concat([result_df, new_df], axis=0, ignore_index=True)\n             \n              \n    print(result_df)\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(station_list_file, index=False)\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "illwc1",
  "name" : "data_gee_modis_real_time",
  "description" : null,
  "code" : "import os\nimport pprint\n\n# import gdal\nimport subprocess\nfrom datetime import datetime, timedelta\n\n# set up your credentials using\n# echo 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\n\nmodis_download_dir = \"/home/chetana/modis_download_folder/\"\nmodis_downloaded_data = modis_download_dir + \"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\ngeo_tiff = modis_download_dir + \"geo-tiff/\"\nvrt_file_dir = modis_download_dir + \"vrt_files/\"\ndir_path = os.path.dirname(os.path.realpath(__file__))\nprint(dir_path)\n\ntile_list = ['h09v04', 'h10v04', 'h11v04', 'h08v04', 'h08v05', 'h09v05', 'h10v05', 'h07v06', 'h08v06', 'h09v06']\n\n\ndef get_files(directory):\n    \"\"\"\n    Get a list of files in a directory and its subdirectories.\n\n    Args:\n        directory (str): The directory to search for files.\n\n    Returns:\n        dict: A dictionary where keys are subdirectory names and values are lists of file paths.\n    \"\"\"\n    file_directory = list()\n    complete_directory_structure = dict()\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            file_directory.append(file_path)\n            complete_directory_structure[str(dirpath).rsplit('/')[-1]] = file_directory\n\n    return complete_directory_structure\n\n\ndef get_latest_date():\n    \"\"\"\n    Retrieve the latest date from the MODIS data website.\n\n    Returns:\n        datetime: The latest date as a datetime object.\n    \"\"\"\n    all_rows = get_web_row_data()\n\n    latest_date = None\n    for row in all_rows:\n        try:\n            new_date = datetime.strptime(row.text[:-1], '%Y.%m.%d')\n            if latest_date is None or latest_date < new_date:\n                latest_date = new_date\n        except:\n            continue\n\n    print(\"Find the latest date: \", latest_date.strftime(\"%Y.%m.%d\"))\n    second_latest_date = latest_date - timedelta(days=8)\n    return second_latest_date\n\n\ndef get_web_row_data():\n    \"\"\"\n    Fetch and parse the MODIS data website content.\n\n    Returns:\n        list: A list of rows from the website's table.\n    \"\"\"\n    try:\n        from BeautifulSoup import BeautifulSoup\n    except ImportError:\n        from bs4 import BeautifulSoup\n    modis_list_url = \"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\n    print(\"Source / Product: \" + modis_list_url)\n    if os.path.exists(\"index.html\"):\n        os.remove(\"index.html\")\n    subprocess.run(\n        f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies '\n        f'--no-check-certificate --auth-no-challenge=on -np -e robots=off {modis_list_url}',\n        shell=True, stderr=subprocess.PIPE)\n    index_file = open('index.html', 'r')\n    webContent = index_file.read()\n    parsed_html = BeautifulSoup(webContent, \"html.parser\")\n    all_rows = parsed_html.body.findAll('td', attrs={'class': 'indexcolname'})\n    return all_rows\n\n\ndef download_recent_modis(date=None):\n    \"\"\"\n    Download recent MODIS data.\n\n    Args:\n        date (datetime, optional): A specific date to download. Defaults to None.\n    \"\"\"\n    if date:\n        latest_date_str = date.strftime(\"%Y.%m.%d\")\n    else:\n        latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n    for tile in tile_list:\n        download_cmd = f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies ' \\\n                       f'--no-check-certificate --auth-no-challenge=on -r --reject \"i' \\\n                       f'ndex.html*\" -P {modis_download_dir} -np -e robots=off ' \\\n                       f'https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/ -A \"*{tile}*.hdf\" --quiet'\n        # print(download_cmd)\n        p = subprocess.run(download_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        print(\"Downloading tile, \", tile, \" with status code \", \"OK\" if p.returncode == 0 else p.returncode)\n\n\n# def merge_wrap_tif_into_western_us_tif():\n#     latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n#     # traverse the folder and find the new download files\n#     for filename in os.listdir(f\"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/\"):\n#         f = os.path.join(directory, filename)\n#         # checking if it is a file\n#         if os.path.isfile(f):\n#             print(f)\n# merge_wrap_tif_into_western_us_tif()\n\ndef hdf_tif_cvt(resource_path, destination_path):\n    \"\"\"\n    Convert HDF files to GeoTIFF format.\n\n    Args:\n        resource_path (str): The path to the source HDF file.\n        destination_path (str): The path to save the converted GeoTIFF file.\n    \"\"\"\n    if not os.path.isfile(resource_path):\n        raise Exception(\"HDF file not found\")\n\n    max_snow_extent_path = destination_path + \"maximum_snow_extent/\"\n    eight_day_snow_cover = destination_path + \"eight_day_snow_cover/\"\n    if not os.path.exists(max_snow_extent_path):\n        os.makedirs(max_snow_extent_path)\n    if not os.path.exists(eight_day_snow_cover):\n        os.makedirs(eight_day_snow_cover)\n\n    tif_file_name_snow_extent = max_snow_extent_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_name_eight_day = eight_day_snow_cover + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name_snow_extent + '_max_snow_extent' + tif_file_extension\n    eight_day_snow_cover_file_name = tif_file_name_eight_day + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Maximum_Snow_Extent\"\n    eight_day_snow_cover = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Eight_Day_Snow_Cover\"\n\n    subprocess.run(f\"gdal_translate {maximum_snow_extent} {maximum_snow_extent_file_name}\", shell=True)\n    subprocess.run(f\"gdal_translate {eight_day_snow_cover} {eight_day_snow_cover_file_name}\", shell=True)\n\n\ndef combine_geotiff_gdal(vrt_array, destination):\n    \"\"\"\n    Combine GeoTIFF files using GDAL.\n\n    Args:\n        vrt_array (list): A list of GeoTIFF file paths to combine.\n        destination (str): The path to save the combined VRT and GeoTIFF files.\n    \"\"\"\n    subprocess.run(f\"gdalbuildvrt {destination} {' '.join(vrt_array)}\", shell=True)\n    tif_name = destination.split('.vrt')[-2] + '.tif'\n    subprocess.run(f\"gdal_translate -of GTiff {destination} {tif_name}\", shell=True)\n\n\ndef hdf_tif_conversion(resource_path, destination_path):\n    \"\"\"\n    Convert HDF files to GeoTIFF format using GDAL.\n\n    Args:\n        resource_path (str): The path to the source HDF file.\n        destination_path (str): The path to save the converted GeoTIFF file.\n    \"\"\"\n    hdf_dataset = gdal.Open(resource_path)\n    if hdf_dataset is None:\n        raise Exception(\"Could not open HDF dataset\")\n\n    maximum_snow_extent = hdf_dataset.GetSubDatasets()[0][0]\n    modis_snow_500m = hdf_dataset.GetSubDatasets()[1][0]\n\n    driver = gdal.GetDriverByName('GTiff')\n\n    tif_file_name = destination_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name + '_max_snow_extent' + tif_file_extension\n    modis_snow_500m_file_name = tif_file_name + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent_dataset = gdal.Open(maximum_snow_extent)\n    modis_snow_500m_dataset = gdal.Open(modis_snow_500m)\n\n    if maximum_snow_extent_dataset is None:\n        raise Exception(\"Could not open maximum_snow_extent dataset\")\n\n    if modis_snow_500m_dataset is None:\n        raise Exception(\"Could not open modis_snow_500m dataset\")\n\n    driver.CreateCopy(maximum_snow_extent_file_name, maximum_snow_extent_dataset, 0)\n    driver.CreateCopy(modis_snow_500m_file_name, modis_snow_500m_dataset, 0)\n\n    print(\"HDF to TIF conversion completed successfully.\")\n\n\ndef download_modis_archive(*, start_date, end_date):\n    \"\"\"\n    Download MODIS data for a specified date range.\n\n    Keyword Args:\n        start_date (datetime): The start date of the date range.\n        end_date (datetime): The end date of the date range.\n    \"\"\"\n    all_archive_dates = list()\n\n    all_rows = get_web_row_data()\n    for r in all_rows:\n        try:\n            all_archive_dates.append(datetime.strptime(r.text.replace('/', ''), '%Y.%m.%d'))\n        except:\n            continue\n\n    for a in all_archive_dates:\n        if start_date <= a <= end_date:\n            download_recent_modis(a)\n\n\ndef step_one_download_modis():\n  \"\"\"\n  Step one of the main workflow: Download recent MODIS data.\n  \"\"\"\n  download_recent_modis()\n                   \ndef step_two_merge_modis_western_us():\n  \"\"\"\n  Step two of the main workflow: Merge MODIS data for the western US.\n  \"\"\"\n  download_modis_archive(start_date=datetime(2022, 1, 1), end_date=datetime(2022, 12, 31))\n\n  files = get_files(modis_downloaded_data)\n  for k, v in get_files(modis_downloaded_data).items():\n\n    conversion_path = modis_download_dir + \"geo-tiff/\" + k + \"/\"\n    if not os.path.exists(conversion_path):\n        os.makedirs(conversion_path)\n    for hdf_file in v:\n        # print(hdf_file.split('/')[-1].split('.hdf')[0], 1)\n        hdf_tif_cvt(hdf_file, conversion_path)\n\n  if not os.path.exists(vrt_file_dir):\n    os.makedirs(vrt_file_dir)\n\n\n  directories = [d for d in os.listdir(geo_tiff) if   os.path.isdir(os.path.join(geo_tiff, d))]\n\n  for d in directories:\n    eight_day_snow_cover = geo_tiff + d + '/eight_day_snow_cover'\n    maximum_snow_extent = geo_tiff + d + '/maximum_snow_extent'\n\n    eight_day_abs_path = list()\n    snow_extent_abs_path = list()\n\n    for file in os.listdir(eight_day_snow_cover):\n        file_path = os.path.abspath(os.path.join(eight_day_snow_cover, file))\n        eight_day_abs_path.append(file_path)\n\n    for file in os.listdir(maximum_snow_extent):\n        file_path = os.path.abspath(os.path.join(maximum_snow_extent, file))\n        snow_extent_abs_path.append(file_path)\n\n    combine_geotiff_gdal(eight_day_abs_path, vrt_file_dir + f\"{d}_eight_day.vrt\")\n    combine_geotiff_gdal(snow_extent_abs_path, vrt_file_dir + f\"{d}_snow_extent.vrt\")\n\n                   \n# main workflow is here:\nstep_one_download_modis()\nstep_two_merge_modis_western_us()\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "sjs5by",
  "name" : "data_gee_sentinel1_real_time",
  "description" : null,
  "code" : "# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernel\n\nfrom all_dependencies import *\nfrom snowcast_utils import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # This must be run in the terminal instead of Geoweaver. Geoweaver doesn't support prompts.\n    ee.Initialize()\n\n# Read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# Read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# Read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\n\nprint(\"submission_format_df shape: \", submission_format_df.shape)\n\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_df = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n# Start_date = \"2022-04-20\" # Test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\nif os.path.exists(final_csv_file):\n    #print(\"exists skipping..\")\n    #exit()\n    os.remove(final_csv_file)\n\nall_cell_df = pd.DataFrame(columns=['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor current_cell_id in submission_format_df.index:\n\n    try:\n        #print(\"collecting \", current_cell_id)\n\n        longitude = all_cell_coords_df['lon'][current_cell_id]\n        latitude = all_cell_coords_df['lat'][current_cell_id]\n\n        # Identify a 500-meter buffer around our Point Of Interest (POI)\n        poi = ee.Geometry.Point(longitude, latitude).buffer(10)\n\n        viirs = ee.ImageCollection(product_name) \\\n            .filterDate(start_date, end_date) \\\n            .filterBounds(poi) \\\n            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n            .select('VV')\n\n        def poi_mean(img):\n            reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n            mean = reducer.get(var_name)\n            return img.set('date', img.date().format()).set(column_name, mean)\n\n        poi_reduced_imgs = viirs.map(poi_mean)\n\n        nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date', column_name]).values().get(0)\n\n        # Don't forget we need to call the callback method \"getInfo\" to retrieve the data\n        df = pd.DataFrame(nested_list.getInfo(), columns=['date', column_name])\n\n        df['date'] = pd.to_datetime(df['date'])\n        df = df.set_index('date')\n\n        df['cell_id'] = current_cell_id\n        df['latitude'] = latitude\n        df['longitude'] = longitude\n\n        df_list = [all_cell_df, df]\n        all_cell_df = pd.concat(df_list)  # Merge into a big dataframe\n\n    except Exception as e:\n\n        #print(e)\n        pass\n\nall_cell_df.to_csv(final_csv_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "y7nb46",
  "name" : "base_hole",
  "description" : null,
  "code" : "'''\nThe wrapper for all the snowcast_wormhole predictors.\n'''\n\nimport os\nimport joblib\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport shutil\nfrom snowcast_utils import homedir, work_dir, code_dir, model_dir\n\n# homedir = os.path.expanduser('~')\n\nclass BaseHole:\n    '''\n    Base class for snowcast_wormhole predictors.\n\n    Attributes:\n        all_ready_file (str): The path to the CSV file containing the data for training.\n        classifier: The machine learning model used for prediction.\n        holename (str): The name of the wormhole class.\n        train_x (numpy.ndarray): The training input data.\n        train_y (numpy.ndarray): The training target data.\n        test_x (numpy.ndarray): The testing input data.\n        test_y (numpy.ndarray): The testing target data.\n        test_y_results (numpy.ndarray): The predicted results on the test data.\n        save_file (str): The path to save the trained model.\n    '''\n\n    training_data_path = f\"{code_dir}/data/ready_for_training/all_ready_new.csv\"\n\n\n    def __init__(self):\n        '''\n        Initializes a new instance of the BaseHole class.\n        '''\n        self.classifier = self.get_model()\n        self.holename = self.__class__.__name__ \n        self.train_x = None\n        self.train_y = None\n        self.test_x = None\n        self.test_y = None\n        self.test_y_results = None\n        self.save_file = None\n        self.latest_model_file = f\"{model_dir}/wormhole_{self.holename}_latest.joblib\"\n    \n    def save(self):\n        '''\n        Save the trained model to a joblib file with a timestamp.\n\n        Returns:\n            None\n        '''\n        now = datetime.now()\n        date_time = now.strftime(\"%Y%d%m%H%M%S\")\n        self.save_file = f\"{model_dir}/wormhole_{self.holename}_{date_time}.joblib\"\n        \n        directory = os.path.dirname(self.save_file)\n        if not os.path.exists(directory):\n          os.makedirs(directory)\n        \n        print(f\"Saving model to {self.save_file}\")\n        joblib.dump(self.classifier, self.save_file)\n        # copy a version to the latest file placeholder\n        \n        shutil.copy(self.save_file, self.latest_model_file)\n        print(f\"a copy of the model is saved to {self.latest_model_file}\")\n  \n    def preprocessing(self):\n        '''\n        Preprocesses the data for training and testing.\n\n        Returns:\n            None\n        '''\n        all_ready_pd = pd.read_csv(self.training_data_path, header=0, index_col=0)\n        print(\"all columns: \", all_ready_pd.columns)\n        all_ready_pd = all_ready_pd[all_cols]\n        all_ready_pd = all_ready_pd.dropna()\n        train, test = train_test_split(all_ready_pd, test_size=0.2)\n        self.train_x, self.train_y = train[input_columns].to_numpy().astype('float'), train[['swe_value']].to_numpy().astype('float')\n        self.test_x, self.test_y = test[input_columns].to_numpy().astype('float'), test[['swe_value']].to_numpy().astype('float')\n  \n    def train(self):\n        '''\n        Trains the machine learning model.\n\n        Returns:\n            None\n        '''\n        self.classifier.fit(self.train_x, self.train_y)\n  \n    def test(self):\n        '''\n        Tests the machine learning model on the testing data.\n\n        Returns:\n            numpy.ndarray: The predicted results on the testing data.\n        '''\n        self.test_y_results = self.classifier.predict(self.test_x)\n        return self.test_y_results\n  \n    def predict(self, input_x):\n        '''\n        Makes predictions using the trained model on new input data.\n\n        Args:\n            input_x (numpy.ndarray): The input data for prediction.\n\n        Returns:\n            numpy.ndarray: The predicted results.\n        '''\n        return self.classifier.predict(input_x)\n  \n    def evaluate(self):\n        '''\n        Evaluates the performance of the machine learning model.\n\n        Returns:\n            None\n        '''\n        pass\n  \n    def get_model(self):\n        '''\n        Get the machine learning model.\n\n        Returns:\n            object: The machine learning model.\n        '''\n        pass\n\n    def load_model(self):\n        \"\"\"\n        Load a machine learning model from a file.\n\n        Args:\n            model_path (str): Path to the saved model file.\n\n        Returns:\n            model: The loaded machine learning model.\n        \"\"\"\n        self.classifier = joblib.load(self.latest_model_file)\n  \n    def post_processing(self):\n        '''\n        Perform post-processing on the model's predictions.\n\n        Returns:\n            None\n        '''\n        pass\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "a8p3n7",
  "name" : "data_gee_gridmet_station_only",
  "description" : null,
  "code" : "import os\nimport glob\nimport re\nimport urllib.request\nfrom datetime import date, datetime\n\nimport pandas as pd\nimport xarray as xr\nfrom pathlib import Path\nimport warnings\nimport dask.dataframe as dd\nfrom dask.delayed import delayed\n\nfrom snowcast_utils import homedir, snowcast_github_dir, work_dir, train_start_date, train_end_date, data_dir, gridcells_file, stations_file, all_training_points_with_station_and_non_station_file, all_training_points_with_snotel_ghcnd_file, gridcells_outfile, stations_outfile, supplement_point_for_correction_file\n\n# Suppress FutureWarnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nstart_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\nend_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n\nyear_list = [start_date.year + i for i in range(end_date.year - start_date.year + 1)]\n\n# working_dir = work_dir\nprint(\"work_dir = \", work_dir)\n#stations = pd.read_csv(f'{work_dir}/station_cell_mapping.csv')\nall_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\ngridmet_save_location = f'{work_dir}/gridmet_climatology'\nfinal_merged_csv = f\"{work_dir}/salt_pepper_point_gridmet_training.csv\"\nvariables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n\ndef get_files_in_directory(gridmet_save_location):\n    f = list()\n    for files in glob.glob(gridmet_save_location + \"/*.nc\"):\n        f.append(files)\n    return f\n\n\ndef download_file(url, save_location):\n    try:\n        print(\"download_file\")\n        with urllib.request.urlopen(url) as response:\n            file_content = response.read()\n        file_name = os.path.basename(url)\n        save_path = os.path.join(save_location, file_name)\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\n\ndef download_gridmet_climatology():\n    folder_name = gridmet_save_location\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    \n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            print(\"downloading\", download_link)\n            if not os.path.exists(os.path.join(folder_name, var + '_' + '%s' % y + '.nc')):\n                download_file(download_link, folder_name)\n\n\ndef process_station(ds, lat, lon):\n    subset_data = ds.sel(lat=lat, lon=lon, method='nearest')\n    subset_data['lat'] = lat\n    subset_data['lon'] = lon\n    converted_df = subset_data.to_dataframe()\n    converted_df = converted_df.reset_index(drop=False)\n    converted_df = converted_df.drop('crs', axis=1)\n    return converted_df\n\ndef get_year_short_var_name_from_path(file_path):\n    # Regular expression to match the variable name and the year\n    pattern = '/([^/]*)_([0-9]{4})\\\\.nc$'\n    match = re.search(pattern, file_path)\n    if match:\n        variable_name = match.group(1)\n        year = match.group(2)\n        return year, variable_name\n    else:\n        print(\"No match found.\")\n        return None, None\n\ndef get_gridmet_variable(gridmet_nc_file_name, target_points_csv_path):\n    print(f\"Reading values from {gridmet_nc_file_name}\")\n    ds = xr.open_dataset(gridmet_nc_file_name)\n    var_name = list(ds.keys())[0]\n    year, short_var_name = get_year_short_var_name_from_path(gridmet_nc_file_name)\n\n    # Step 2: Define the start and end of that year\n    year_start = datetime(int(year), 1, 1)\n    year_end = datetime(int(year), 12, 31)\n\n    # Step 3: Check if there is an overlap between the year and the date range\n    if max(start_date, year_start) <= min(end_date, year_end):\n        print(f\"The year {year} is within the range.\")\n    else:\n        print(f\"The year {year} is not within the range. exiting\")\n        return\n\n    file_name = os.path.basename(target_points_csv_path)\n    csv_file = f'{gridmet_save_location}/{file_name}_{year}_{short_var_name}_gridmet_training.csv'\n    if os.path.exists(csv_file):\n        print(f\"The file '{csv_file}' exists.\")\n        return\n\n    result_data = []\n    # stations = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n    stations = pd.read_csv(target_points_csv_path)\n    for _, row in stations.iterrows():\n        delayed_process_data = delayed(process_station)(ds, row['latitude'], row['longitude'])\n        result_data.append(delayed_process_data)\n\n    print(\"ddf = dd.from_delayed(result_data)\")\n    ddf = dd.from_delayed(result_data)\n    \n    print(\"result_df = ddf.compute()\")\n    result_df = ddf.compute()\n    result_df.to_csv(csv_file, index=False)\n    print(f'Completed extracting data for {gridmet_nc_file_name}')\n\n\ndef parse_var_year_from_path(file_path):\n    # Regular expression to extract the year and variable name\n    regex_pattern = \"_(\\d{4})_(.*?)_gridmet_training\"\n    match = re.search(regex_pattern, file_path)\n    if match:\n        year = match.group(1)\n        variable_name = match.group(2)\n        return year, variable_name\n    else:\n        # print(\"Don't match\")\n        return None, None\n\ndef merge_similar_variables_from_different_years(target_points_file_path):\n    print(f\"Listing files in {gridmet_save_location}\")\n    files = os.listdir(gridmet_save_location)\n    file_groups = {}\n\n    point_file_name = os.path.basename(target_points_file_path)\n\n    for filename in files:\n        year, variable_name = parse_var_year_from_path(filename)\n        if year is None:\n            continue;\n        \n        print(filename)\n        file_groups.setdefault(variable_name, []).append(filename)\n        # base_name, year_ext = os.path.splitext(filename)\n        # parts = base_name.split('_')\n        # print(parts)\n        # print(year_list)\n        # if len(parts) == 4 and parts[3] == \"ghcnd\" and year_ext == '.csv' and parts[1].isdigit() and int(parts[1]) in year_list:\n        #     file_groups.setdefault(parts[0], []).append(filename)\n\n    print(\"file_groups = \", file_groups)\n    for variable_name, file_list in file_groups.items():\n        if len(file_list) > 1:\n            dfs = []\n            for filename in file_list:\n                df = pd.read_csv(os.path.join(gridmet_save_location, filename))\n                dfs.append(df)\n            merged_df = pd.concat(dfs, ignore_index=True)\n            merged_filename = f\"{point_file_name}_{variable_name}_merged.csv\"\n            merged_df.to_csv(\n                os.path.join(gridmet_save_location, merged_filename), \n                index=False\n            )\n            print(f\"Merged {file_list} into {merged_filename}\")\n\n\ndef merge_all_variables_together(target_points_csv_path):\n    merged_df = None\n    file_paths = []\n\n    point_file_name = os.path.basename(target_points_csv_path)\n\n    for filename in os.listdir(gridmet_save_location):\n        if filename.endswith(\"_merged.csv\") and point_file_name in filename:\n            file_paths.append(os.path.join(gridmet_save_location, filename))\n\n    print(\"file_paths = \", file_paths)\n\t\n    rmin_merged_path = os.path.join(gridmet_save_location, f'{point_file_name}_rmin_merged.csv')\n    rmax_merged_path = os.path.join(gridmet_save_location, f'{point_file_name}_rmax_merged.csv')\n    tmmn_merged_path = os.path.join(gridmet_save_location, f'{point_file_name}_tmmn_merged.csv')\n    tmmx_merged_path = os.path.join(gridmet_save_location, f'{point_file_name}_tmmx_merged.csv')\n\n    print(\"saving to \", rmin_merged_path)\n    \n    df_rmin = pd.read_csv(rmin_merged_path)\n    df_rmax = pd.read_csv(rmax_merged_path)\n    df_tmmn = pd.read_csv(tmmn_merged_path)\n    df_tmmx = pd.read_csv(tmmx_merged_path)\n    \n    df_rmin.rename(columns={'relative_humidity': 'relative_humidity_rmin'}, inplace=True)\n    df_rmax.rename(columns={'relative_humidity': 'relative_humidity_rmax'}, inplace=True)\n    df_tmmn.rename(columns={'air_temperature': 'air_temperature_tmmn'}, inplace=True)\n    df_tmmx.rename(columns={'air_temperature': 'air_temperature_tmmx'}, inplace=True)\n    \n    df_rmin.to_csv(rmin_merged_path)\n    df_rmax.to_csv(rmax_merged_path)\n    df_tmmn.to_csv(tmmn_merged_path)\n    df_tmmx.to_csv(tmmx_merged_path)\n    \n    if file_paths:\n        merged_df = pd.read_csv(file_paths[0])\n        for file_path in file_paths[1:]:\n            df = pd.read_csv(file_path)\n            merged_df = pd.concat([merged_df, df], axis=1)\n        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n        final_merged_csv = f\"{target_points_csv_path}_gridmet_training.csv\"\n        merged_df.to_csv(final_merged_csv, index=False)\n        print(f\"all files are saved to {final_merged_csv}\")\n\n\nif __name__ == \"__main__\":\n    \n    download_gridmet_climatology()\n    \n    # mock out as this takes too long\n    nc_files = get_files_in_directory(gridmet_save_location)\n    for nc in nc_files:\n        # should check if the nc file year number is in the year_list\n        get_gridmet_variable(\n            gridmet_nc_file_name = nc, \n            target_points_csv_path = supplement_point_for_correction_file\n        )\n    \n    merge_similar_variables_from_different_years(supplement_point_for_correction_file)\n    merge_all_variables_together(supplement_point_for_correction_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "smsdr0",
  "name" : "data_gee_gridmet_real_time",
  "description" : null,
  "code" : "import os\nimport urllib\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Download the NetCDF files from the Idaho HTTP site daily or for the time period matching the MODIS period.\n# Download site: https://www.northwestknowledge.net/metdata/data/\n\ndownload_source = \"https://www.northwestknowledge.net/metdata/data/\"\ngridmet_download_dir = \"/home/chetana/terrian_data/\"\n\ndef download_gridmet():\n    \"\"\"\n    Download GridMET NetCDF files from the specified source to the download directory.\n\n    This function fetches NetCDF files from the provided website and saves them in the specified download directory.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(gridmet_download_dir):\n        os.makedirs(gridmet_download_dir)\n\n    soup = BeautifulSoup(requests.get(download_source).text, \"html.parser\")\n    tag_links = soup.find_all('a')\n    for t in tag_links:\n        if '.nc' in t.text and not 'eddi' in t.text and not os.path.isfile(gridmet_download_dir + t.get(\"href\")):\n            print(f'Downloading {t.get(\"href\")}')\n            urllib.request.urlretrieve(download_source + t.get('href'), gridmet_download_dir + t.get(\"href\"))\n\ndownload_gridmet()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "4i0sop",
  "name" : "model_creation_xgboost",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the XGBoostHole class, which is used to train and evaluate an Extra Trees Regression model for hole analysis.\n\nAttributes:\n    XGBoostHole (class): A class for training and using an Extra Trees Regression model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n\"\"\"\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass XGBoostHole(RandomForestHole):\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n        return etmodel\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b63prf",
  "name" : "testing_data_integration",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom snowcast_utils import homedir, work_dir, data_dir, test_start_date, test_end_date, process_dates_in_range\nimport sys\nimport numpy as np\nfrom add_snodas_mask_column import add_snodas_mask_column\n\ndef get_water_year(date):\n    if date.month >= 10:  # If the month is October or later\n        return date.year + 1  # Water year starts in the following calendar year\n    else:\n        return date.year\n\ndef merge_all_gridmet_amsr_csv_into_one(target_date, gridmet_csv_folder, dem_all_csv, testing_all_csv, water_mask_csv):\n    \"\"\"\n    Merge all GridMET and AMSR CSV files into one combined CSV file.\n\n    Args:\n        gridmet_csv_folder (str): The folder containing GridMET CSV files.\n        dem_all_csv (str): Path to the DEM (Digital Elevation Model) CSV file.\n        testing_all_csv (str): Path to save the merged CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # List of file paths for the CSV files\n    csv_files = []\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('_cumulative.csv') and target_date in file:\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    all_df = None\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        print(f\"reading {file}\")\n        df = pd.read_csv(file)\n        # print(df.head())\n        # print(\"df.shape:\", df.shape)\n        df = df.apply(pd.to_numeric, errors='coerce')\n        if all_df is None:\n          all_df = df\n        else:\n          #all_df = all_df.merge(df, on=['Latitude', 'Longitude']).drop_duplicates()\n          df = df.drop(columns=['Latitude', 'Longitude'])\n          all_df = pd.concat([all_df, df], axis=1)\n          \n        # print(\"all_df.head() :\", all_df.head())\n        # print(\"all_df.columns\", all_df.columns)\n        # print(\"all_df.shape: \", all_df.shape)\n\n    unique_loc_pairs = all_df[['Latitude', 'Longitude']].drop_duplicates()\n    # print(\"unique_loc_pairs.shape: \", unique_loc_pairs.shape)\n        \n    dem_df = pd.read_csv(f\"{data_dir}/srtm/dem_all.csv\", encoding='utf-8', index_col=False)\n    #all_df = pd.merge(all_df, dem_df, on=['Latitude', 'Longitude']).drop_duplicates()\n    dem_df = dem_df.drop(columns=['Latitude', 'Longitude'])\n    # print(\"dem_df.shape: \", dem_df.shape)\n    all_df = pd.concat([all_df, dem_df], axis=1)\n\n    date = target_date\n    \n    #date = date.replace(\"-\", \".\")\n    amsr_file = f'{data_dir}/amsr_testing/testing_ready_amsr_{date}_cumulative.csv'\n    print(f\"reading {amsr_file}\")\n    amsr_df = pd.read_csv(amsr_file, index_col=False)\n    amsr_df.rename(columns={'gridmet_lat': 'Latitude', 'gridmet_lon': 'Longitude'}, inplace=True)\n    # print(amsr_df.head())\n    # print(\"amsr_df.shape = \", amsr_df.shape)\n    amsr_df = amsr_df.drop(columns=['Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, amsr_df], axis=1)\n    \n    fsca_df = pd.read_csv(f'{data_dir}/fsca/final_output/{target_date}_output.csv')\n    # print(fsca_df.head())\n    # print(\"fsca_df.shape: \", fsca_df.shape)\n    fsca_df = fsca_df.drop(columns=['Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, fsca_df], axis=1)\n\n    water_mask_df = pd.read_csv(water_mask_csv)\n    water_mask_df = water_mask_df.drop(columns=[\"date\", 'Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, water_mask_df], axis=1)\n    \n    # print(\"all columns: \", all_df.columns)\n    # add water year\n    all_df[\"water_year\"] = get_water_year(selected_date)\n    \n    all_df.rename(columns={'date_x': 'date'}, inplace=True)\n    \n    # log10 all the cumulative columns\n    # Get columns with \"cumulative\" in their names\n    for col in all_df.columns:\n        print(\"Checking \", col)\n        if \"cumulative\" in col:\n\t        # Apply log10 transformation to selected columns\n            all_df[col] = np.log10(all_df[col] + 0.1)  # Adding 1 to avoid log(0)\n            print(f\"converted {col} to log10\")\n    \n    # Remove the duplicated date column\n    all_df = all_df.loc[:, ~all_df.columns.duplicated()]\n\n    # Save the merged dataframe to a new CSV file\n    all_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input CSV files are merged to {testing_all_csv}\")\n    print(all_df.columns)\n    print(\"all_df.shape = \", all_df.shape)\n\n\n    # Call the function with appropriate paths\n    # training_csv = f\"{work_dir}/all_points_final_merged_training.csv\"\n    # new_training_csv = f\"{work_dir}/all_points_final_merged_training_snodas_mask.csv\"\n    model_path = f\"/home/chetana/models//SNODASDNNHole_e10_nTrue_20253101055710.model\"\n    add_snodas_mask_column(\n        model_path, \n        testing_all_csv, \n        f\"{testing_all_csv}_snodas_mask.csv\",\n        lat_col_name = \"Latitude\", \n        lon_col_name = \"Longitude\", \n        date_col_name = \"date\"\n    )\n\ndef merge_callback(current_date):\n    # Prepare the cumulative history CSVs for the current date\n    print(\">>>>>\\nGetting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n    current_date_str = current_date.strftime(\"%Y-%m-%d\")\n    test_year = \"2024\"\n\n    merge_all_gridmet_amsr_csv_into_one(\n        current_date_str, \n        f\"{work_dir}/testing_output/\",\n        f\"{data_dir}/srtm/dem_all.csv\",\n        f\"{work_dir}/testing_all_ready_{current_date_str}.csv\",\n        f\"{data_dir}/water_mask/final_output/{test_year}_output.csv\"\n    )\n\nif __name__ == \"__main__\":\n    process_dates_in_range(\n        start_date = test_start_date,\n        end_date = test_end_date,\n        days_look_back=7,\n        callback = merge_callback,\n    )\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "zh38b6",
  "name" : "snowcast_utils",
  "description" : null,
  "code" : "from datetime import date, datetime, timedelta\nimport json\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef create_folders(folder_path):\n    \"\"\"\n    Create all layers of folders if they don't exist.\n    \n    :param folder_path: Path to the folder (string).\n    \"\"\"\n    os.makedirs(folder_path, exist_ok=True)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\n# homedir = \"/media/volume1/swe/data/\"\n# homedir = \"/groups/ESS3/zsun/swe/\"\nprint(homedir)\ngithub_dir = f\"{homedir}/../code/SnowCast\"\ndata_dir = f\"{homedir}/data\"\nwork_dir = f\"{data_dir}/gridmet_test_run\"\nmodel_dir = f\"{homedir}/models/\"\nplot_dir = f\"{homedir}/plots/\"\noutput_dir = f\"{data_dir}/output\"\ncode_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nsnowcast_github_dir = f\"{code_dir}\"\nfsca_dir = f\"{data_dir}/fsca\"\n\ncreate_folders(model_dir)\ncreate_folders(plot_dir)\ncreate_folders(output_dir)\ncreate_folders(fsca_dir)\n\n# user-defined paths for data-access\ngithub_data_dir = f'{snowcast_github_dir}data/'\ngridcells_file = github_data_dir+'snowcast_provided/grid_cells_eval.geojson'\n#stations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\nstations_file = f\"{github_data_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\nsupplement_point_for_correction_file = f\"{work_dir}/salt_pepper_points_for_training.csv\"\n#stations_file = github_data_dir+'snowcast_provided/ground_measures_metadata.csv'\nall_training_points_with_station_and_non_station_file = f\"{github_data_dir}/all_training_points_in_westus.csv\"\nall_training_points_with_snotel_ghcnd_file = f\"{github_data_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\ngridcells_outfile = github_data_dir+'terrain/gridcells_terrainData_eval.csv'\n#stations_outfile = f\"{github_data_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv\"\nstations_outfile = f\"{github_data_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv\"\n\ninterrupt_on_fail = False\n\ntoday = date.today()\n\n# dd/mm/YY\nd1 = today.strftime(\"%Y-%m-%d\")\nprint(\"today date =\", d1)\n\ncumulative_mode = False  # cumulative no long makes sense, we only need the past 7 days of data exists\n\n# -125, 25, -100, 49\nsouthwest_lon = -125.0\nsouthwest_lat = 25.0\nnortheast_lon = -100.0\nnortheast_lat = 49.0\n\n# the training period is three years from 2018 to 2021\ntrain_start_date = \"2002-01-01\"\ntrain_end_date = \"2024-12-31\"\n\ndef get_operation_day():\n    # Check if \"SWE_FORECASTING_DATE\" environment variable is set\n    swe_forecasting_start_date = os.environ.get(\"SWE_FORECASTING_START_DATE\")\n    swe_forecasting_end_date = os.environ.get(\"SWE_FORECASTING_END_DATE\")\n\n    if swe_forecasting_start_date:\n        # Return the date from the environment variable\n        print(swe_forecasting_start_date, swe_forecasting_end_date)\n        return swe_forecasting_start_date, swe_forecasting_end_date\n    else:\n        # Calculate 10 days ago as the start day\n        current_date = datetime.now()\n        start_day = current_date - timedelta(days=7)\n        end_day = current_date - timedelta(days=3)\n\n        # Format dates as strings in \"YYYY-MM-DD\" format\n        start_day_string = start_day.strftime(\"%Y-%m-%d\")\n        end_day_string = end_day.strftime(\"%Y-%m-%d\")\n\n        print(f\"Start day: {start_day_string}, End day: {end_day_string}\")\n        \n        return start_day_string, end_day_string\n\n# Get the start and end dates\n# test_start_date, test_end_date = get_operation_day()\n\ntest_start_date, test_end_date = \"2025-01-01\", \"2025-01-30\"\n\n#test_end_date = d1\nprint(\"test start date: \", test_start_date)\nprint(\"test end date: \", test_end_date)\n\n\n# Define a function to convert the month to season\ndef month_to_season(month):\n    if 3 <= month <= 5:\n        return 1\n    elif 6 <= month <= 8:\n        return 2\n    elif 9 <= month <= 11:\n        return 3\n    else:\n        return 4\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the distance (Euclidean) between two sets of coordinates (lat1, lon1) and (lat2, lon2).\n    \n    Parameters:\n    - lat1 (float): Latitude of the first point.\n    - lon1 (float): Longitude of the first point.\n    - lat2 (float): Latitude of the second point.\n    - lon2 (float): Longitude of the second point.\n    \n    Returns:\n    - float: The Euclidean distance between the two points.\n    \"\"\"\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n\ndef read_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8-sig') as json_file:\n        data = json.load(json_file)\n        return data\n\ndef create_cell_location_csv():\n    \"\"\"\n    Create a CSV file containing cell locations from a GeoJSON file.\n    \"\"\"\n    # read grid cell\n    gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n    all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n    if os.path.exists(all_cell_coords_file):\n        os.remove(all_cell_coords_file)\n\n    grid_coords_df = pd.DataFrame(columns=[\"cell_id\", \"lat\", \"lon\"])\n    print(grid_coords_df.head())\n    gridcells = geojson.load(open(gridcells_file))\n    for idx, cell in enumerate(gridcells['features']):\n        current_cell_id = cell['properties']['cell_id']\n        cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n        cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n        grid_coords_df.loc[len(grid_coords_df.index)] = [current_cell_id, cell_lat, cell_lon]\n\n    # grid_coords_np = grid_coords_df.to_numpy()\n    # print(grid_coords_np.shape)\n    grid_coords_df.to_csv(all_cell_coords_file, index=False)\n    # np.savetxt(all_cell_coords_file, grid_coords_np[:, 1:], delimiter=\",\")\n    # print(grid_coords_np.shape)\n\ndef get_latest_date_from_an_array(arr, date_format):\n    \"\"\"\n    Get the latest date from an array of date strings.\n    \n    Parameters:\n    - arr (list): List of date strings.\n    - date_format (str): Date format for parsing the date strings.\n    \n    Returns:\n    - str: The latest date string.\n    \"\"\"\n    return max(arr, key=lambda x: datetime.strptime(x, date_format))\n\ndef findLastStopDate(target_testing_dir, data_format):\n    \"\"\"\n    Find the last stop date from CSV files in a directory.\n    \n    Parameters:\n    - target_testing_dir (str): Directory containing CSV files.\n    - data_format (str): Date format for parsing the date strings.\n    \n    Returns:\n    - str: The latest stop date.\n    \"\"\"\n    date_list = []\n    for filename in os.listdir(target_testing_dir):\n        f = os.path.join(target_testing_dir, filename)\n        # checking if it is a file\n        if os.path.isfile(f) and \".csv\" in f:\n            pdf = pd.read_csv(f, header=0, index_col=0)\n            date_list = np.concatenate((date_list, pdf.index.unique()))\n    latest_date = get_latest_date_from_an_array(date_list, data_format)\n    print(latest_date)\n    date_time_obj = datetime.strptime(latest_date, data_format)\n    return date_time_obj.strftime(\"%Y-%m-%d\")\n\ndef convert_date_from_1900(day_value):\n    \"\"\"\n    Convert a day value since 1900 to a date string in the format \"YYYY-MM-DD\".\n    \n    Parameters:\n    - day_value (int): Number of days since January 1, 1900.\n    \n    Returns:\n    - str: Date string in \"YYYY-MM-DD\" format.\n    \"\"\"\n    reference_date = datetime(1900, 1, 1)\n    result_date = reference_date + timedelta(days=day_value)\n    return result_date.strftime(\"%Y-%m-%d\")\n\ndef convert_date_to_1900(date_string):\n    \"\"\"\n    Convert a date string in the format \"YYYY-MM-DD\" to a day value since 1900.\n    \n    Parameters:\n    - date_string (str): Date string in \"YYYY-MM-DD\" format.\n    \n    Returns:\n    - int: Number of days since January 1, 1900.\n    \"\"\"\n    input_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n    reference_date = datetime(1900, 1, 1)\n    delta = input_date - reference_date\n    day_value = delta.days\n    return day_value\n\ndef date_to_julian(date_str):\n    \"\"\"\n    Convert a date to Julian date.\n    \"\"\"\n    date_object = datetime.strptime(date_str, \"%Y-%m-%d\")\n    tt = date_object.timetuple()\n    \n\n    # Format the result as 'YYYYDDD'\n    julian_format = str('%d%03d' % (tt.tm_year, tt.tm_yday))\n\n    return julian_format\n\nfrom datetime import datetime, timedelta\nimport os\n\ndef process_dates_in_range(\n    start_date: str,\n    end_date: str,\n    days_look_back: int,\n    callback: callable,\n    **callback_kwargs,\n):\n    \"\"\"\n    Utility function to iterate over a range of dates and execute a callback for each date.\n\n    Args:\n        start_date (str): Start date in \"YYYY-MM-DD\" format.\n        end_date (str): End date in \"YYYY-MM-DD\" format.\n        callback (callable): A function to be executed for each date.\n        **callback_kwargs: Additional arguments to be passed to the callback function.\n    \n    Raises:\n        ValueError: If any errors occurred during the processing of the dates.\n    \"\"\"\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n    if days_look_back is None:\n        days_look_back = 0\n    start_date = start_date - timedelta(days=days_look_back)\n    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n    current_date = start_date\n    failed_dates = []\n\n    while current_date <= end_date:\n        try:\n            print(f\"Processing date: {current_date.strftime('%Y-%m-%d')}\")\n            callback(current_date, **callback_kwargs)\n        except Exception as e:\n            print(f\"Error processing {current_date.strftime('%Y-%m-%d')}: {str(e)}\")\n            failed_dates.append(current_date.strftime('%Y-%m-%d'))\n        current_date += timedelta(days=1)\n\n    if failed_dates and interrupt_on_fail:\n        raise ValueError(f\"Processing failed for the following dates: {', '.join(failed_dates)}\")\n\n\nif __name__ == \"__main__\":\n    print(date_to_julian(test_start_date))\n    #day_index = convert_date_to_1900(test_start_date)\n    #create_cell_location_csv()\n    #findLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\n    #findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1/\", \"%Y-%m-%d %H:%M:%S\")\n    #findLastStopDate(f\"{github_dir}/data/sat_testing/modis/\", \"%Y-%m-%d\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wdh394",
  "name" : "model_create_kehan",
  "description" : null,
  "code" : "\nfrom BaseHole import *\n\nclass KehanModel(BaseHole):\n\t\n  def preprocessing():\n    pass  \n  \n  def train():\n    pass\n  \n  def test():\n    pass",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "p87wh1",
  "name" : "data_snotel_real_time",
  "description" : null,
  "code" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# First Python script in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\n\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n# Example of using the SnotelPointData class\n# snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n# df = snotel_point.get_daily_data(\n#     datetime(2020, 1, 2), datetime(2020, 1, 20),\n#     [snotel_point.ALLOWED_VARIABLES.SWE]\n# )\n# print(df)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ilbqzg",
  "name" : "all_dependencies",
  "description" : null,
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\n#pd.set_option('display.max_columns', None)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "do86ae",
  "name" : "data_WUS_UCLA_SR",
  "description" : null,
  "code" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import date_to_julian, work_dir, data_dir, train_start_date, train_end_date\n\n# change directory before running the code\nos.chdir(f\"{data_dir}/ucla/\")\n\ninput_folder = os.getcwd() + \"/raw/\"\ntemp_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output/\"\nos.makedirs(input_folder, exist_ok=True)\nos.makedirs(temp_folder, exist_ok=True)\nos.makedirs(output_folder, exist_ok=True)\n\n\ndef get_env_var_for_gdalwarp():\n    # if \"PROJ_LIB\" in os.environ:\n    #     os.environ.pop(\"PROJ_LIB\")\n    #     print(f\"Environment variable PROJ_LIB removed.\")\n    os.environ[\"PROJ_LIB\"] = \"/home/chetana/miniconda/share/proj/\"\n\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n  hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n# https://n5eil01u.ecs.nsidc.org/DP6/SNOWEX/WUS_UCLA_SR.001/2020.10.01/WUS_UCLA_SR_v01_N48_0W125_0_agg_16_WY2020_21_SWE_SCA_POST.nc\n  # Specific subdataset name you're interested in\n  target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n  # Create a name for the output file based on the HDF file name and subdataset\n  output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n  output_path = os.path.join(output_folder, output_file_name)\n\n  # student 1 changed something here\n\n  if os.path.exists(output_path):\n    pass\n    #print(f\"The file {output_path} exists. skip.\")\n  else:\n    for subdataset in hdf_ds.GetSubDatasets():\n      # Check if the subdataset is the one we want to convert\n      if target_subdataset_name in subdataset[0]:\n        ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n        # Convert to GeoTIFF\n        gdal.Translate(output_path, ds)\n        ds = None\n        break  # Exit the loop after converting the target subdataset\n\n  hdf_ds = None\n\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n  file_lst = list()\n  for file in os.listdir(folder_path):\n    file_lst.append(file)\n    if file.lower().endswith(\".hdf\"):\n      hdf_file = os.path.join(folder_path, file)\n      convert_hdf_to_geotiff(hdf_file, output_folder)\n      #print(f\"Converted {file} to GeoTIFF\")\n  return file_lst\n\n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['gdal_translate', '-b', '1', '-outsize', '100%', '100%', '-scale', '0', '255', '200', '200', f\"{output_folder}/fsca_template.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_ucla_swe(start_date, end_date):\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{output_folder}/{current_date}__ucla_swe.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"WUS_UCLA_SR\", \n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n\n        # SWE_SCA_POST\n        # Filter results to include only files related to SWE\n        # print(str(results[1]))\n        # swe_results = [\n        #     item for item in results\n        #     if \"SWE_SCA\" in str(item)\n        # ]\n        print(results)\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        # convert_all_hdf_in_folder(input_folder, output_folder)\n        # print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        # merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\n    \nif __name__ == \"__main__\":\n  # download_ucla_swe(train_start_date, train_end_date)\n  download_ucla_swe(\"2001-02-17\", \"2001-02-19\")\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gkhtc0",
  "name" : "data_nsidc_4km_swe",
  "description" : null,
  "code" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nprint(\"station_cell_mapper_file = \", station_cell_mapper_file)\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\nif __name__ == \"__main__\":\n    # call this method to extract the \n    turn_nsidc_nc_to_csv()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "lbd6cp",
  "name" : "model_creation_et",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, homedir, model_dir, plot_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\nimport seaborn as sns\nimport os\nfrom datetime import datetime\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n\n    #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n    #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n    #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n    # training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    # training_data_path = f\"{working_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n    # training_data_path = f\"{working_dir}/all_points_final_merged_training.csv\"\n    # training_data_path = f\"{working_dir}/all_points_final_merged_training_snodas_mask.csv\"\n    training_data_path = f\"{working_dir}/all_points_final_merged_training_snodas_mask_resnet.csv\"\n    # training_data_path = f\"{working_dir}/../snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n\n\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        print(\"preparing training data from csv: \", self.training_data_path)\n        data = pd.read_csv(self.training_data_path)\n        print(\"data.shape = \", data.shape)\n        for column in data.columns:\n            print(column)\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-1, inplace=True)\n        data = data.replace(-999, -1)\n        \n        data = data[(data['swe_value'] != -1)]\n        data = data[(data['swe_value'] != -999)]\n        print(\"After dropping -1: \", data.shape)\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n        \n        # these columns all come from SNOTEL report\n        cumulative_columns = [col for col in data.columns if 'cumulative' in col.lower()]\n\n        precip_history_columns = [col for col in data.columns if 'precipitation_amount' in col.lower()]\n\n        snotel_columns_to_drop = [\n            'station_name', \n            # 'swe_value', \n            'change_in_swe_inch', \n            'snow_depth', \n            'air_temperature_observed_f', \n            # 'precipitation_amount'\n        ] + cumulative_columns + precip_history_columns\n\n        # Drop columns only if they exist in the DataFrame\n        data = data.drop(columns=[col for col in snotel_columns_to_drop if col in data.columns], axis=1)\n\n        print(\"SNOTEL columns dropped successfully!\")\n        print(data.columns)  # Print the remaining columns for verification\n\n        \n\n        X = data.drop('swe_value', axis=1)\n        print('required features after removing swe_value:', X.columns)\n\n        print(\"assign fsca > 100. and SWE > 100 to -1\")\n        # Iterate over columns and check if 'fsca' or 'SWE' is in the column name\n        for column in X.columns:\n            if 'fsca' in column.lower() or 'swe' in column.lower():\n                # Set values > 100 to -1\n                X.loc[X[column] > 100, column] = -1\n\n        y = data['swe_value']\n        print(\"describe the statistics of training input: \", X.describe())\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # don't use the sample weights which are more appropriate for classification problem\n        # Make predictions\n        # predictions = self.classifier.predict(self.train_x)\n\n        # # Calculate absolute errors\n        # errors = np.abs(self.train_y - predictions)\n\n        # # Assign weights based on errors (higher errors get higher weights)\n        # weights = compute_sample_weight('balanced', errors)\n        # self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def evaluate(self):\n        print(\"Starting evaluation...\")\n        \n        # Get predictions and true values\n        print(\"Generating predictions...\")\n        y_pred = self.test()\n        y_test = self.test_y\n\n        # Calculate evaluation metrics\n        print(\"Calculating metrics...\")\n        r2 = r2_score(y_test, y_pred)\n        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n        mae = np.mean(np.abs(y_test - y_pred))\n\n        print(f\"Metrics calculated:\\n - R²: {r2:.4f}\\n - RMSE: {rmse:.4f}\\n - MAE: {mae:.4f}\")\n\n        # Scatter plot for predicted vs actual values\n        print(\"Creating scatter plot...\")\n        plt.figure(figsize=(8, 6))\n        sns.scatterplot(x=y_test, y=y_pred, color=\"blue\", alpha=0.6, s=50)\n        plt.plot(\n            [min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--', lw=2,\n            label=\"1:1 line\"\n        )\n        plt.title(f\"Predicted vs Actual Values (R² = {r2:.2f})\", fontsize=16)\n        plt.xlabel(\"Actual SWE\", fontsize=14)\n        plt.ylabel(\"Predicted SWE\", fontsize=14)\n        plt.xticks(fontsize=12)\n        plt.yticks(fontsize=12)\n        plt.legend(fontsize=12)\n        plt.tight_layout()\n\n        # Save plot\n        plot_path = f\"{plot_dir}/predict_actual_scatter.png\"\n        print(f\"Ensuring plot directory exists at: {plot_dir}\")\n        os.makedirs(plot_dir, exist_ok=True)\n\n        print(f\"Saving scatter plot to: {plot_path}\")\n        plt.savefig(plot_path, dpi=300)\n        print(\"Scatter plot saved successfully.\")\n\n        # Display metrics in console\n        print(\"\\nEvaluation complete.\")\n        # print(f\"Final Metrics:\\n - R²: {r2:.4f}\\n - RMSE: {rmse:.4f}\\n - MAE: {mae:.4f}\")\n\n    def post_processing(self, chosen_columns=None,):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n\n        Parameters:\n        - chosen_columns (list, optional): Columns selected for the model. Defaults to None.\n        \"\"\"\n        self.load_model()\n\n        # Ensure feature names are initialized\n        if not hasattr(self, 'feature_names') or self.feature_names is None:\n            self.feature_names = ['SWE', 'relative_humidity_rmin', 'potential_evapotranspiration',\n                'air_temperature_tmmx', 'relative_humidity_rmax',\n                'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed',\n                'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'fsca',\n                'Slope', 'SWE_1', 'air_temperature_tmmn_1',\n                'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1',\n                'relative_humidity_rmax_1', 'relative_humidity_rmin_1',\n                'air_temperature_tmmx_1', 'wind_speed_1', 'fsca_1', 'SWE_2',\n                'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n                'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n                'relative_humidity_rmin_2', 'air_temperature_tmmx_2', 'wind_speed_2',\n                'fsca_2', 'SWE_3', 'air_temperature_tmmn_3',\n                'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3',\n                'relative_humidity_rmax_3', 'relative_humidity_rmin_3',\n                'air_temperature_tmmx_3', 'wind_speed_3', 'fsca_3', 'SWE_4',\n                'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n                'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n                'relative_humidity_rmin_4', 'air_temperature_tmmx_4', 'wind_speed_4',\n                'fsca_4', 'SWE_5', 'air_temperature_tmmn_5',\n                'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5',\n                'relative_humidity_rmax_5', 'relative_humidity_rmin_5',\n                'air_temperature_tmmx_5', 'wind_speed_5', 'fsca_5', 'SWE_6',\n                'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n                'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n                'relative_humidity_rmin_6', 'air_temperature_tmmx_6', 'wind_speed_6',\n                'fsca_6', 'SWE_7', 'air_temperature_tmmn_7',\n                'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7',\n                'relative_humidity_rmax_7', 'relative_humidity_rmin_7',\n                'air_temperature_tmmx_7', 'wind_speed_7', 'fsca_7', 'water_year']\n\n        # Extract feature importances and sort them\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = np.array(feature_names)[sorted_indices]\n\n        # Set figure size dynamically based on the number of features\n        num_features = len(feature_names)\n        fig_height = max(8, num_features * 0.3)  # Adjust height based on feature count\n\n        plt.figure(figsize=(12, fig_height))\n        plt.barh(range(num_features), sorted_importances, color=\"skyblue\")\n        \n        # Set feature names as y-tick labels\n        plt.yticks(range(num_features), sorted_feature_names, fontsize=10)\n        plt.gca().invert_yaxis()  # Reverse the order of features for better readability\n        plt.xlabel('Feature Importance', fontsize=14)\n        plt.ylabel('Feature', fontsize=14)\n        plt.title('Feature Importance Plot (ET model)', fontsize=16)\n        plt.tight_layout()  # Ensure no overlap between labels and plot\n\n        # Generate a timestamp to append to the filename\n        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n        \n        # Determine the output file name with timestamp\n        if chosen_columns is None:\n            feature_png = f'{plot_dir}/et-model-feature-importance-{timestamp}.png'\n        else:\n            feature_png = f'{plot_dir}/et-model-feature-importance-{len(chosen_columns)}-{timestamp}.png'\n\n        # Save the plot\n        plt.savefig(feature_png)\n        plt.close()  # Close the plot to free memory\n        print(f\"Feature importance plot saved to {feature_png}\")\n\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = ['SWE', 'swe_value', 'relative_humidity_rmin',\n'potential_evapotranspiration', 'air_temperature_tmmx',\n'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect',\n'Curvature', 'Northness', 'Eastness', 'fsca', 'Slope', 'SWE_1',\n'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n'relative_humidity_rmin_1', 'air_temperature_tmmx_1', 'wind_speed_1',\n'fsca_1', 'SWE_2', 'air_temperature_tmmn_2',\n'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2',\n'relative_humidity_rmax_2', 'relative_humidity_rmin_2',\n'air_temperature_tmmx_2', 'wind_speed_2', 'fsca_2', 'SWE_3',\n'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n'relative_humidity_rmin_3', 'air_temperature_tmmx_3', 'wind_speed_3',\n'fsca_3', 'SWE_4', 'air_temperature_tmmn_4',\n'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4',\n'relative_humidity_rmax_4', 'relative_humidity_rmin_4',\n'air_temperature_tmmx_4', 'wind_speed_4', 'fsca_4', 'SWE_5',\n'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n'relative_humidity_rmin_5', 'air_temperature_tmmx_5', 'wind_speed_5',\n'fsca_5', 'SWE_6', 'air_temperature_tmmn_6',\n'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6',\n'relative_humidity_rmax_6', 'relative_humidity_rmin_6',\n'air_temperature_tmmx_6', 'wind_speed_6', 'fsca_6', 'SWE_7',\n'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n'relative_humidity_rmin_7', 'air_temperature_tmmx_7', 'wind_speed_7',\n'fsca_7', 'water_year', 'snodas_mask']\n\n# ['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax', 'cumulative_potential_evapotranspiration', 'cumulative_wind_speed'] \n\nif __name__ == \"__main__\":\n  hole = ETHole()\n#   hole.preprocessing(chosen_columns = selected_columns)\n  hole.preprocessing()\n\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n#   hole.post_processing(chosen_columns = selected_columns)\n  hole.post_processing()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "br9etb",
  "name" : "data_snotel_station_only",
  "description" : null,
  "code" : "import math\nimport json\nimport requests\nimport pandas as pd\nimport csv\nimport io\nimport os\nimport dask\nimport dask.dataframe as dd\nfrom snowcast_utils import work_dir, southwest_lat, southwest_lon, northeast_lat, northeast_lon, train_start_date, train_end_date\n\nworking_dir = work_dir\n\n\ndef download_station_json():\n    # https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\n    output_json_file = f'{working_dir}/all_snotel_cdec_stations.json'\n    if not os.path.exists(output_json_file):\n        # Fetch data from the URL\n        response = requests.get(\"https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\")\n        \n\n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            # Decode the JSON content\n            json_content = response.json()\n\n            # Save the JSON content to a file\n            with open(output_json_file, 'w') as json_file:\n                json.dump(json_content, json_file, indent=2)\n\n            print(f\"Data downloaded and saved to {output_json_file}\")\n        else:\n            print(f\"Failed to download data. Status code: {response.status_code}\")\n    else:\n        print(f\"The file {output_json_file} already exists.\")\n        \n    \n    # read the json file and convert it to csv\n    csv_file_path = f'{working_dir}/all_snotel_cdec_stations.csv'\n    if not os.path.exists(csv_file_path):\n        # Read the JSON file\n        with open(output_json_file, 'r') as json_file:\n            json_content = json.load(json_file)\n\n        # Check the content (print or analyze as needed)\n        #print(\"JSON Content:\")\n        #print(json.dumps(json_content, indent=2))\n\n        # Convert JSON data to a list of dictionaries (assuming JSON is a list of objects)\n        data_list = json_content if isinstance(json_content, list) else [json_content]\n\n        # Get the header from the keys of the first dictionary (assuming consistent structure)\n        header = data_list[0].keys()\n        # Write to CSV file\n        with open(csv_file_path, 'w', newline='') as csv_file:\n            csv_writer = csv.DictWriter(csv_file, fieldnames=header)\n            csv_writer.writeheader()\n            csv_writer.writerows(data_list)\n\n        print(f\"Data converted and saved to {csv_file_path}\")\n    \n    else:\n        print(f\"The csv all snotel/cdec stations exists.\")\n        \n        \n    active_csv_file_path = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv'\n    if not os.path.exists(active_csv_file_path):\n        all_df = pd.read_csv(csv_file_path)\n        print(all_df.head())\n        all_df['endDate'] = pd.to_datetime(all_df['endDate'])\n        print(all_df.shape)\n        end_date = pd.to_datetime('2050-01-01')\n        filtered_df = all_df[all_df['endDate'] > end_date]\n        \n        # Filter rows within the latitude and longitude ranges\n        filtered_df = filtered_df[\n            (filtered_df['latitude'] >= southwest_lat) & (filtered_df['latitude'] <= northeast_lat) &\n            (filtered_df['longitude'] >= southwest_lon) & (filtered_df['longitude'] <= northeast_lon)\n        ]\n\n        # Print the original and filtered DataFrames\n        print(\"Filtered DataFrame:\")\n        print(filtered_df.shape)\n        filtered_df.to_csv(active_csv_file_path, index=False)\n    else:\n        print(f\"The active csv already exists: {active_csv_file_path}\")\n\t\n\ndef read_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8-sig') as json_file:\n        data = json.load(json_file)\n        return data\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n    d_lat = lat2 - lat1\n    d_long = lon2 - lon1\n    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_long / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    distance = 6371 * c  # Earth's radius in kilometers\n    return distance\n\n\ndef find_nearest_location(locations, target_lat, target_lon):\n    n_location = None\n    min_distance = float('inf')\n    for location in locations:\n        lat = location['location']['lat']\n        lon = location['location']['lng']\n        distance = haversine(lat, lon, target_lat, target_lon)\n        if distance < min_distance:\n            min_distance = distance\n            n_location = location\n            return n_location\n\n\ndef csv_to_json(csv_text):\n    lines = csv_text.splitlines()\n    header = lines[0]\n    field_names = header.split(',')\n    json_list = []\n    for line in lines[1:]:\n        values = line.split(',')\n        row_dict = {}\n        for i, field_name in enumerate(field_names):\n            row_dict[field_name] = values[i]\n            json_list.append(row_dict)\n            json_string = json.dumps(json_list)\n            return json_string\n\n\ndef remove_commented_lines(text):\n    lines = text.split(os.linesep)\n    cleaned_lines = []\n    for line in lines:\n        if not line.startswith('#'):\n            cleaned_lines.append(line)\n    cleaned_text = os.linesep.join(cleaned_lines)\n    return cleaned_text\n\n  \ndef get_swe_observations_from_snotel_cdec():\n    new_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n    new_base_df = pd.read_csv(new_base_station_list_file)\n    print(new_base_df.head())\n  \t\n    old_messed_file = f\"{work_dir}/\"\n    csv_file = f'{new_base_station_list_file}_swe_restored_dask_all_vars.csv'\n    start_date = train_start_date\n    end_date = train_end_date\n\t\n    # Create an empty Pandas DataFrame with the desired columns\n    result_df = pd.DataFrame(columns=[\n      'station_name', \n      'date', \n      'lat', \n      'lon', \n      'swe_value', \n      'change_in_swe_inch', \n      'snow_depth', \n      'change_in_swe_inch', \n      'air_temperature_observed_f'\n    ])\n\n    # Function to process each station\n    @dask.delayed\n    def process_station(station):\n        location_name = station['name']\n        location_triplet = station['stationTriplet']\n        location_elevation = station['elevation']\n        location_station_lat = station['latitude']\n        location_station_long = station['longitude']\n\n        url = f\"https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customSingleStationReport/daily/{location_triplet}%7Cid%3D%22%22%7Cname/{start_date},{end_date}%2C0/WTEQ%3A%3Avalue%2CWTEQ%3A%3Adelta%2CSNWD%3A%3Avalue%2CSNWD%3A%3Adelta%2CTOBS%3A%3Avalue\"\n\n        r = requests.get(url)\n        text = remove_commented_lines(r.text)\n        reader = csv.DictReader(io.StringIO(text))\n        json_data = json.loads(json.dumps(list(reader)))\n\n        entries = []\n        \n        for entry in json_data:\n            try:\n              # {'Date': '2021-06-18', 'Snow Water Equivalent (in) Start of Day Values': '', 'Change In Snow Water Equivalent (in)': '', 'Snow Depth (in) Start of Day Values': '', 'Change In Snow Depth (in)': '', 'Air Temperature Observed (degF) Start of Day Values': '70.5'}\n              required_data = {\n                'station_name': location_name,\n                'date': entry['Date'],\n                'lat': location_station_lat, \n                'lon': location_station_long,\n                'swe_value': entry['Snow Water Equivalent (in) Start of Day Values'],\n                'change_in_swe_inch': entry['Change In Snow Water Equivalent (in)'],\n                'snow_depth': entry['Snow Depth (in) Start of Day Values'],\n                'change_in_swe_inch': entry['Change In Snow Depth (in)'],\n                'air_temperature_observed_f': entry['Air Temperature Observed (degF) Start of Day Values']\n              }\n              entries.append(required_data)\n            except Exception as e:\n              print(\"entry = \", entry)\n              raise e\n        return pd.DataFrame(entries)\n\n    # List of delayed computations for each station\n    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n\n    # Compute the delayed results\n    result_lists = dask.compute(*delayed_results)\n\n    # Concatenate the lists into a Pandas DataFrame\n    result_df = pd.concat(result_lists, ignore_index=True)\n\n    # Print the final Pandas DataFrame\n    print(result_df.head())\n\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(csv_file, index=False)\n#     result_df.to_csv(csv_file, index=False)\n\nif __name__ == \"__main__\":\n    download_station_json()\n    get_swe_observations_from_snotel_cdec()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c2xkhz",
  "name" : "model_creation_rf",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the RandomForestHole class, which is used for training and evaluating a Random Forest Regressor model for hole analysis.\n\nAttributes:\n    RandomForestHole (class): A class for training and using a Random Forest Regressor model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Random Forest Regressor model with specified hyperparameters.\n    evaluate(): Evaluates the performance of the trained model and returns metrics such as MAE, MSE, R2, and RMSE.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = os.path.join(homedir, 'Documents', 'GitHub', 'SnowCast')\n\nclass RandomForestHole(BaseHole):\n  \n    def get_model(self):\n        \"\"\"\n        Returns the Random Forest Regressor model with specified hyperparameters.\n\n        Returns:\n            Pipeline: The Random Forest Regressor model wrapped in a scikit-learn pipeline.\n        \"\"\"\n        rfc_pipeline = Pipeline(steps=[\n            ('data_scaling', StandardScaler()),\n            ('model', RandomForestRegressor(max_depth=15,\n                                           min_samples_leaf=0.004,\n                                           min_samples_split=0.008,\n                                           n_estimators=25))\n        ])\n        return rfc_pipeline\n\n    def evaluate(self):\n        \"\"\"\n        Evaluates the performance of the trained model and returns metrics such as MAE, MSE, R2, and RMSE.\n\n        Returns:\n            dict: A dictionary containing MAE, MSE, R2, and RMSE metrics.\n        \"\"\"\n        mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n        mse = metrics.mean_squared_error(self.test_y, self.test_y_results)\n        r2 = metrics.r2_score(self.test_y, self.test_y_results)\n        rmse = math.sqrt(mse)\n\n        print(\"The random forest model performance for testing set\")\n        print(\"--------------------------------------\")\n        print('MAE is {}'.format(mae))\n        print('MSE is {}'.format(mse))\n        print('R2 score is {}'.format(r2))\n        print('RMSE is {}'.format(rmse))\n        return {\"mae\": mae, \"mse\": mse, \"r2\": r2, \"rmse\": rmse}\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "doinnd",
  "name" : "model_creation_pycaret",
  "description" : null,
  "code" : "import pandas as pd\nimport autokeras as ak\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data from a CSV file\ndf = pd.read_csv('/home/chetana/gridmet_test_run/five_years_data.csv')\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for better readability\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the data into features (X) and target variable (y)\nX = df.drop(columns=['swe_value'])\ny = df['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the AutoKeras regressor\nreg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\nreg.fit(X_train, y_train, epochs=10)\n\n# Evaluate the AutoKeras regressor on the test set\npredictions = reg.predict(X_test)\n\n# Calculate and print evaluation metrics\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\nprint('RMSE:', rmse)\nprint('R2 Score:', r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b7a4fu",
  "name" : "model_creation_autokeras",
  "description" : null,
  "code" : "import pandas as pd\nimport autokeras as ak\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom snowcast_utils import work_dir, month_to_season\nfrom datetime import datetime\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\nimport pandas as pd\nimport numpy as np\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n\nworking_dir = work_dir\n\nhomedir = os.path.expanduser('~')\nnow = datetime.now()\ndate_time = now.strftime(\"%Y%d%m%H%M%S\")\n\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\ntraining_data_path = f\"{working_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n\nmodel_save_file = f\"{github_dir}/model/wormhole_autokeras_{date_time}.joblib\"\n\n# Read the data from the CSV file\nprint(f\"start to read data {training_data_path}\")\ndf = pd.read_csv(training_data_path)\n#df.dropna(inplace=True)\n\nprint(df.head())\nprint(df.columns())\n\n# Load and prepare your data\n# data = pd.read_csv('your_dataset.csv')  # Replace with your actual data source\nX = df.drop(columns=['swe_value', 'Date'])  # Replace with your actual target column\ny = df['swe_value']\n\n\n# Define the function to create a diverse Keras model\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            if i == 0:\n                model.add(Dense(num_units, activation='relu', input_shape=(input_shape,)))\n            else:\n                model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu',\n                         input_shape=(input_shape, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True),\n                       input_shape=(input_shape, 1)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Use a small random subset of the data\n    subset_idx = np.random.choice(len(X_train), size=int(0.1 * len(X_train)), replace=False)\n    X_subset = X_train[subset_idx]\n    y_subset = y_train[subset_idx]\n\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_subset, y_subset, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        loss, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    return mae\n\n# Encode categorical features\nfor col in X.select_dtypes(include=['object']).columns:\n    X[col] = LabelEncoder().fit_transform(X[col])\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\n\n# If using CNN, LSTM, or Transformer, add a new dimension for channels\nX_train = np.expand_dims(X_train, axis=-1)\nX_val = np.expand_dims(X_val, axis=-1)\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Train the best model on the full dataset\nbest_model = create_model(study.best_trial)\nif isinstance(best_model, TabNetRegressor):\n    best_model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric=['mae'],\n        max_epochs=100,\n        patience=10,\n        batch_size=256,\n        virtual_batch_size=128,\n        verbose=0\n    )\n    best_model.save_model('best_tabnet_model')\n    print(\"Best TabNet model saved as best_tabnet_model.zip\")\nelse:\n    best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=0)\n    best_model.save(model_save_file)\n    print(f\"Best model saved as {model_save_file}\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gnpbdq",
  "name" : "model_creation_autopytorch",
  "description" : null,
  "code" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport autopytorch as apt\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\n\ntraining_data_path = f\"{working_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n# Load the data from a CSV file\ndf = pd.read_csv(training_data_path)\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for better readability\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the dataset into features (X) and target variable (y)\nX = df.drop('swe_value', axis=1)\ny = df['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Auto-PyTorch configuration\nconfig = apt.AutoNetRegressionConfig()\n\n# Initialize and train the Auto-PyTorch regressor\nreg = apt.AutoNetRegressor(config=config)\nreg.fit(X_train, y_train)\n\n# Evaluate the model\npredictions = reg.predict(X_test)\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\n\n# Print the evaluation metrics\nprint(\"RMSE:\", rmse)\nprint(\"R2 Score:\", r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "oon4sb",
  "name" : "western_us_dem.py",
  "description" : null,
  "code" : "import numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport warnings\nimport rasterio\nimport csv\nfrom rasterio.transform import Affine\nfrom scipy.ndimage import sobel, gaussian_filter\nfrom snowcast_utils import homedir\n\nmile_to_meters = 1609.34\nfeet_to_meters = 0.3048\n\n# Set the warning filter globally to ignore the FutureWarning\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ndef lat_lon_to_pixel(lat, lon, geotransform):\n    \"\"\"\n    Convert latitude and longitude to pixel coordinates using a geotransform.\n\n    Args:\n        lat (float): Latitude.\n        lon (float): Longitude.\n        geotransform (tuple): Geotransform coefficients (e.g., (origin_x, pixel_width, 0, origin_y, 0, pixel_height)).\n\n    Returns:\n        tuple: Pixel coordinates (x, y).\n    \"\"\"\n    x = int((lon - geotransform[0]) / geotransform[1])\n    y = int((lat - geotransform[3]) / geotransform[5])\n    return x, y\n\ndef calculate_slope_aspect_for_single(elevation_data, pixel_size_x, pixel_size_y):\n    \"\"\"\n    Calculate slope and aspect for a single pixel using elevation data.\n\n    Args:\n        elevation_data (array): Elevation data.\n        pixel_size_x (float): Pixel size in the x-direction.\n        pixel_size_y (float): Pixel size in the y-direction.\n\n    Returns:\n        tuple: Slope (in degrees), aspect (in degrees).\n    \"\"\"\n    # Calculate slope using the Sobel operator\n    slope_x = np.gradient(elevation_data, pixel_size_x, axis=1)\n    slope_y = np.gradient(elevation_data, pixel_size_y, axis=0)\n    slope_rad = np.arctan(np.sqrt(slope_x ** 2 + slope_y ** 2))\n    slope_deg = np.degrees(slope_rad)\n\n    # Calculate aspect (direction of the steepest descent)\n    aspect_rad = np.arctan2(slope_y, -slope_x)\n    aspect_deg = (np.degrees(aspect_rad) + 360) % 360\n\n    return slope_deg, aspect_deg\n\ndef save_as_geotiff(data, output_file, src_file):\n    \"\"\"\n    Save data as a GeoTIFF file with metadata from the source file.\n\n    Args:\n        data (array): Data to be saved.\n        output_file (str): Path to the output GeoTIFF file.\n        src_file (str): Path to the source GeoTIFF file to inherit metadata from.\n    \"\"\"\n    with rasterio.open(src_file) as src_dataset:\n        profile = src_dataset.profile\n        transform = src_dataset.transform\n\n        # Update the data type, count, and set the transform for the new dataset\n        profile.update(dtype=rasterio.float32, count=1, transform=transform)\n\n        # Create the new GeoTIFF file\n        with rasterio.open(output_file, 'w', **profile) as dst_dataset:\n            # Write the data to the new GeoTIFF\n            dst_dataset.write(data, 1)\n\ndef print_statistics(data):\n    \"\"\"\n    Print basic statistics of a data array.\n\n    Args:\n        data (array): Data array to calculate statistics for.\n    \"\"\"\n    # Calculate multiple statistics in one line\n    data = data[~np.isnan(data)]\n    mean, median, min_val, max_val, sum_val, std_dev, variance = [np.mean(data), np.median(data), np.min(data), np.max(data), np.sum(data), np.std(data), np.var(data)]\n\n    # Print the calculated statistics\n    print(\"Mean:\", mean)\n    print(\"Median:\", median)\n    print(\"Minimum:\", min_val)\n    print(\"Maximum:\", max_val)\n    print(\"Sum:\", sum_val)\n    print(\"Standard Deviation:\", std_dev)\n    print(\"Variance:\", variance)\n\ndef calculate_slope(dem_file):\n    from osgeo import gdal\n    import numpy as np\n    import rasterio\n    gdal.DEMProcessing(f'{dem_file}_slope.tif', dem_file, 'slope')\n    with rasterio.open(f'{dem_file}_slope.tif') as dataset:\n        slope=dataset.read(1)\n    return slope\n  \ndef calculate_aspect(dem_file):\n    from osgeo import gdal\n    import numpy as np\n    import rasterio\n    gdal.DEMProcessing(f'{dem_file}_aspect.tif', dem_file, 'aspect')\n    with rasterio.open(f'{dem_file}_aspect.tif') as dataset:\n        aspect=dataset.read(1)\n    return aspect\n    \n    \ndef calculate_slope_aspect(dem_file):\n    \"\"\"\n    Calculate slope and aspect from a DEM (Digital Elevation Model) file.\n\n    Args:\n        dem_file (str): Path to the DEM GeoTIFF file.\n\n    Returns:\n        tuple: Slope array (in degrees), aspect array (in degrees).\n    \"\"\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n        # Calculate the slope and aspect using numpy\n        dx, dy = np.gradient(dem_data)\n#         print(\"dx : \", dx)\n#         print(\"dy : \", dy)\n        slope = np.arctan(np.sqrt(dx**2 + dy**2))\n        slope = 90 - np.degrees(slope)\n        aspect = np.degrees(np.arctan2(-dy, dx))\n\n        # Adjust aspect values to range from 0 to 360 degrees\n        aspect[aspect < 0] += 360\n\n    return slope, aspect\n\ndef calculate_curvature(elevation_data, pixel_size_x, pixel_size_y):\n    \"\"\"\n    Calculate curvature from elevation data using the Laplacian operator.\n\n    Args:\n        elevation_data (array): Elevation data.\n        pixel_size_x (float): Pixel size in the x-direction.\n        pixel_size_y (float): Pixel size in the y-direction.\n\n    Returns:\n        array: Curvature data.\n    \"\"\"\n    # Calculate curvature using the Laplacian operator\n    curvature_x = np.gradient(np.gradient(elevation_data, pixel_size_x, axis=1), pixel_size_x, axis=1)\n    curvature_y = np.gradient(np.gradient(elevation_data, pixel_size_y, axis=0), pixel_size_y, axis=0)\n    curvature = curvature_x + curvature_y\n\n    return curvature\n  \ndef calculate_curvature(dem_file, sigma=1):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n        \n        # the dem is in meter unit\n        dem_data = dem_data\n\n        # Calculate the gradient using the Sobel filter\n        dx = sobel(dem_data, axis=1, mode='constant')\n        dy = sobel(dem_data, axis=0, mode='constant')\n\n        # Calculate the second derivatives using the Sobel filter\n        dxx = sobel(dx, axis=1, mode='constant')\n        dyy = sobel(dy, axis=0, mode='constant')\n\n        # Calculate the curvature using the second derivatives\n        curvature = dxx + dyy\n\n        # Smooth the curvature using Gaussian filtering (optional)\n        curvature = gaussian_filter(curvature, sigma)\n\n    return curvature\n\ndef calculate_gradients(dem_file):\n    \"\"\"\n    Calculate Northness and Eastness gradients from a DEM (Digital Elevation Model) file.\n\n    Args:\n        dem_file (str): Path to the DEM GeoTIFF file.\n\n    Returns:\n        tuple: Northness array, Eastness array (both in radians).\n    \"\"\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradients along the North and East directions\n        dy, dx = np.gradient(dem_data, dataset.res[0], dataset.res[1])\n\n        # Calculate the Northness and Eastness\n        northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n        eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\n\n    return northness, eastness\n\ndef geotiff_to_csv(geotiff_file, csv_file, column_name):\n    \"\"\"\n    Convert a GeoTIFF file to a CSV file containing latitude, longitude, and image values.\n\n    Args:\n        geotiff_file (str): Path to the input GeoTIFF file.\n        csv_file (str): Path to the output CSV file.\n        column_name (str): Name for the image value column.\n    \"\"\"\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_file) as dataset:\n        # Get the pixel values as a 2D array\n        data = dataset.read(1)\n\n        if column_name == \"Elevation\":\n            # default unit is meter\n            data = data\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Get the width and height of the GeoTIFF\n        height, width = data.shape\n\n        # Open the CSV file for writing\n        with open(csv_file, 'w', newline='') as csvfile:\n            csvwriter = csv.writer(csvfile)\n\n            # Write the CSV header\n            csvwriter.writerow(['Latitude', 'Longitude', 'x', 'y', column_name])\n\n            # Loop through each pixel and extract latitude, longitude, and image value\n            for y in range(height):\n                for x in range(width):\n                    # Get the pixel value\n                    image_value = data[y, x]\n\n                    # Convert pixel coordinates to geographic coordinates\n                    lon, lat = transform * (x, y)\n\n                    # Write the data to the CSV file\n                    csvwriter.writerow([lat, lon, x, y, image_value])\n\ndef read_elevation_data(file_path, result_dem_csv_path, result_dem_feature_csv_path):\n    \"\"\"\n    Read and process elevation data from a CSV file and save it to another CSV file with additional features.\n\n    Args:\n        file_path (str): Path to the input CSV file containing elevation data.\n        result_dem_csv_path (str): Path to the output CSV file for elevation data.\n        result_dem_feature_csv_path (str): Path to the output CSV file for elevation data with additional features.\n\n    Returns:\n        DataFrame: Merged dataframe with elevation and additional features.\n    \"\"\"\n    neighborhood_size = 4\n    df = pd.read_csv(file_path)\n    \n    dataset = rasterio.open(geotiff_file)\n    data = dataset.read(1)\n\n    # Get the width and height of the GeoTIFF\n    height, width = data.shape\n    \n    # Create an empty DataFrame with column names\n    columns = ['lat', 'lon', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    all_df = pd.DataFrame(columns=columns)\n    \n    all_df.to_csv(result_dem_feature_csv_path)\n    print(f\"DEM and other columns are saved to file {result_dem_feature_csv_path}\")\n    return all_df\n\n\nif __name__ == \"__main__\":\n    # Usage example:\n    result_dem_csv_path = f\"{homedir}/srtm/dem_template.csv\"\n    result_dem_feature_csv_path = f\"{homedir}/srtm/dem_all.csv\"\n\n    dem_file = f\"{homedir}/srtm/dem_file.tif\"\n    slope_file = f\"{homedir}/srtm/dem_file.tif_slope.tif\"\n    aspect_file = f\"{homedir}/srtm/dem_file.tif_aspect.tif\"\n    curvature_file = f\"{homedir}/srtm/curvature_file.tif\"\n    northness_file = f\"{homedir}/srtm/northness_file.tif\"\n    eastness_file = f\"{homedir}/srtm/eastness_file.tif\"\n\n    slope, aspect = calculate_slope_aspect(dem_file)\n    # slope = calculate_slope(dem_file)\n    # aspect = calculate_aspect(dem_file)\n    curvature = calculate_curvature(dem_file)\n    northness, eastness = calculate_gradients(dem_file)\n\n    # Save the slope and aspect as new GeoTIFF files\n    save_as_geotiff(slope, slope_file, dem_file)\n    save_as_geotiff(aspect, aspect_file, dem_file)\n    save_as_geotiff(curvature, curvature_file, dem_file)\n    save_as_geotiff(northness, northness_file, dem_file)\n    save_as_geotiff(eastness, eastness_file, dem_file)\n\n    geotiff_to_csv(dem_file, dem_file+\".csv\", \"Elevation\")\n    geotiff_to_csv(slope_file, slope_file+\".csv\", \"Slope\")\n    geotiff_to_csv(aspect_file, aspect_file+\".csv\", \"Aspect\")\n    geotiff_to_csv(curvature_file, curvature_file+\".csv\", \"Curvature\")\n    geotiff_to_csv(northness_file, northness_file+\".csv\", \"Northness\")\n    geotiff_to_csv(eastness_file, eastness_file+\".csv\", \"Eastness\")\n\n    # List of file paths for the CSV files\n    csv_files = [dem_file+\".csv\", slope_file+\".csv\", aspect_file+\".csv\", \n                 curvature_file+\".csv\", northness_file+\".csv\", eastness_file+\".csv\"]\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8')\n        dfs.append(df)\n\n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude', 'x', 'y'])\n\n    # check the statistics of the columns\n    for column in merged_df.columns:\n        merged_df[column] = pd.to_numeric(merged_df[column], errors='coerce')\n        print(merged_df[column].describe())\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(result_dem_feature_csv_path, index=False)\n    print(f\"New dem features are updated in {result_dem_feature_csv_path}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "fa7e4u",
  "name" : "download_srtm_1arcsec (caution!)",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Directory to save the downloaded files\nDOWNLOAD_DIR=\"/media/volume1/swe/data/srtm/\"\n\necho 'export PROJ_LIB=/home/geo2021/anaconda3/share/proj' >> ~/.bashrc\nsource ~/.bashrc\n\nprojinfo EPSG:4326\n\n# Ensure the download directory exists\nmkdir -p \"$DOWNLOAD_DIR\"\n\ncd $DOWNLOAD_DIR\n\nfetch_urls() {\n  while read -r line; do\n      # Extract the file name from the URL\n      filename=$(basename \"$line\")\n      filepath=\"$DOWNLOAD_DIR/$filename\"\n\n      if [[ -f \"$filepath\" ]]; then\n        #   echo \"File already exists: $filepath. Skipping download.\"\n        continue\n      else\n        echo \"Downloading $line\"\n        curl -b ~/.urs_cookies -c ~/.urs_cookies -L -n -f -Og \"$line\" -o \"$filepath\" \\\n            && echo \"Downloaded $filename\" \\\n            || exit_with_error \"Command failed with error. Please retrieve the data manually.\"\n      fi\n  done\n}\nfetch_urls <<'EDSCEOF'\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N33E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S27E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17W026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N33E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N41E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N35E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N39W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N41W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N36E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N36E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N37W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E050.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N37W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N38W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N37W025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N37W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N36E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N38W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E057.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S33E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S27E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S21E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S10E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S28E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E059.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E071.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S24E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S35E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S29E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N33E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S19E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32W018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S14E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S29E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S27E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E071.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S21E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S14E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S08E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E042.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S31E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S28E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N39W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S10E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S21E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N40W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S24E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S31E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N04E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S24E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N33E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S27E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S19E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S28E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N04E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N35W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S17E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E063.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S15E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N36E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N10E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S14E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S32E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N41W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N35E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14W024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N38W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S16E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N04E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N36E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S29E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N03E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S14E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N13E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N41W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N30E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N36W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N27W019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S05E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N07E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S23E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S31E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S22E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N23E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N31E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S03E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S08E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S28E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N32W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S28E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S13E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S08E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S31E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N22W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N21E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S31E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N38W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N19W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N01E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S10E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S07E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S02E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S21E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S27E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S10E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S18E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N12E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N29E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N28E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N26E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N24E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N39W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S06E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S12E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N14E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N40W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N41E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S01E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S11E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N06E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N08E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S29E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N18E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N17E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N02E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S26E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S27E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N11E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S04E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N09E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N40E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S25E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S09E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N16W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N00E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N20W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N38W029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N05W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/S20E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N15E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N34W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N25E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov/MEASURES/SRTMGL1.003/2000.02.11/N39E015.SRTMGL1.hgt.zip\nEDSCEOF\n\n\necho \"Start to unzipping..\"\n# for file in *.zip; do\n#     # Extract the base name of the zip file (without extension)\n#     base_name=\"${file%.zip}\"\n#     echo \"$base_name\"\n    \n#     # Check if the corresponding folder or file already exists\n#     if [ -d \"$base_name\" ] || [ -f \"$base_name\" ]; then\n#         echo \"Skipping $file, already unzipped.\"\n#     else\n#         echo \"Unzipping $file...\"\n#         unzip \"$file\" || echo \"Failed to unzip $file\"\n#     fi\n# done\n\ngdalinfo --version\n\necho \">> Check if DEM is there\"\n\nls merged_dem.tif -lhtra\n\ngdalinfo merged_dem.tif\n\nls *.tif -lhtra\n\necho \">> Check subwindows\"\n\ngdal_translate -projwin -113 38 -111 36 -of GTiff \\\n  /media/volume1/swe/data/srtm/merged_dem.tif \\\n  /tmp/srtm_subwindow.tif\n\ngdalinfo -stats /tmp/srtm_subwindow.tif\n\necho \">> Remove merged_dem.tif and regenerate: \"\n\necho \">> Start to merging files into one GEOTIFF\"\ngdal_merge.py -o merged_dem_new.tif *.hgt\n\necho \">> Check new DEM.\"\ngdalinfo merged_dem_new.tif\n\necho \">> Check windows of new merged DEM\"\ngdal_translate -projwin -113 38 -111 36 -of GTiff \\\n  /media/volume1/swe/data/srtm/merged_dem.tif \\\n  /tmp/srtm_subwindow_new.tif\n\n# I have used the old DEM tif and cut a new 4km WGS84 version here\n# /media/volume1/swe/data/srtm/srtm_wgs84_4km.tif\n# the command I used: gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -r bilinear -of GTiff -overwrite   -dstnodata -999   srtm_globe.tif /media/volume1/swe/data/srtm/srtm_wgs84_4km.tif\n\ngdalinfo -stats /tmp/srtm_subwindow_new.tif\n\necho \"Done\"\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "drwmbo",
  "name" : "gridmet_testing",
  "description" : null,
  "code" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, test_end_date, work_dir, homedir, data_dir, cumulative_mode, process_dates_in_range\nimport matplotlib.pyplot as plt\n\n# Define the folder to store downloaded files\ngridmet_folder_name = f'{work_dir}/gridmet_climatology'\n\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\n# Define your value ranges for color mapping\n#value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n#value_ranges = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.5, 3]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n  if value_ranges == None:\n    max_value = df_col.max()\n    min_value = df_col.min()\n    if min_value < 0:\n      min_value = 0\n    step_size = (max_value - min_value) / 12\n\n    # Create 10 periods\n    new_value_ranges = [min_value + i * step_size for i in range(12)]\n  # Define your custom function to map data values to colors\n  def map_value_to_color(value):\n    # Iterate through the value ranges to find the appropriate color index\n    for i, range_max in enumerate(new_value_ranges):\n      if value <= range_max:\n        return colors[i]\n\n      # If the value is greater than the largest range, return the last color\n      return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n  color_mapping = [map_value_to_color(value) for value in df_col.values]\n  return color_mapping, new_value_ranges\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path, current_year):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list, input_date):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset\n    if the file's last modified date is older than the input date.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    gridmet_folder_name = f\"{work_dir}/gridmet_climatology/\"\n    os.makedirs(gridmet_folder_name, exist_ok=True)\n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + f\"{var}_{y}.nc\"\n            target_file_path = os.path.join(gridmet_folder_name, f\"{var}_{y}.nc\")\n            if os.path.exists(target_file_path):\n                # Check file modification date\n                file_mod_time = datetime.fromtimestamp(os.path.getmtime(target_file_path))\n                # print(\"file_mod_time = \", file_mod_time, input_date)\n                cutoff_date = file_mod_time - timedelta(days=3)\n                if cutoff_date < input_date:\n                    print(f\"File {target_file_path} is outdated. Re-downloading...\")\n                    download_file(download_link, target_file_path, var)\n                # else:\n                #     print(f\"File {target_file_path} is up-to-date.\")\n            else:\n                print(f\"Downloading to {target_file_path}\")\n                download_file(download_link, target_file_path, var)\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef create_gridmet_to_dem_mapper(nc_file):\n    western_us_dem_df = pd.read_csv(western_us_coords)\n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n    if os.path.exists(target_csv_path):\n        # print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    print(\"creating gridmet to dem mapper\")\n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n    \n      # Use the apply function to apply the custom function to each row\n      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n      \n    # print(western_us_dem_df.head())\n    \n    # Save the new converted AMSR to CSV file\n    western_us_dem_df.to_csv(target_csv_path, index=False)\n    \n    return western_us_dem_df\n  \n  \ndef get_nc_csv_by_coords_and_variable(nc_file,\n                                      var_name,\n                                      target_date=test_start_date):\n    # print(\"get_nc_csv_by_coords_and_variable\")\n    create_gridmet_to_dem_mapper(nc_file)\n  \t\n    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      var_col = nc_file.variables[long_var_name][:]\n      # print(\"val_col.shape: \", var_col.shape)\n      \n      # Calculate the day of the year\n      day_of_year = selected_date.timetuple().tm_yday\n      day_index = day_of_year - 1\n      # print('day_index:', day_index)\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        lat_index = int(row[\"gridmet_lat_idx\"])\n        lon_index = int(row[\"gridmet_lon_idx\"])\n        var_value = var_col[day_index, lat_index, lon_index]\n        \n        return var_value\n    \n      # Use the apply function to apply the custom function to each row\n      # print(mapper_df.columns)\n      # print(mapper_df.head())\n      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n      \n      # print(\"mapper_df[var_name]: \", mapper_df[var_name].describe())\n      \n      # drop useless columns\n      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n                               \"dem_lon\": \"Longitude\"}, inplace=True)\n\n      \n    # print(mapper_df.head())\n    return mapper_df\n\n\ndef turn_gridmet_nc_to_csv(target_date=test_start_date):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    generated_csvs = []\n    for root, dirs, files in os.walk(gridmet_folder_name):\n        for file_name in files:\n            \n            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n                print(f\"Checking file: {file_name}\")\n                var_name = get_var_from_file_name(file_name)\n                # print(\"Variable name:\", var_name)\n                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n\n                if os.path.exists(res_csv):\n                    #os.remove(res_csv)\n                    # print(f\"{res_csv} already exists. Skipping..\")\n                    generated_csvs.append(res_csv)\n                    continue\n\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n                                                       var_name, target_date)\n                df.replace('--', pd.NA, inplace=True)\n                df.to_csv(res_csv, index=False)\n                print(\"gridmet var saved: \", res_csv)\n                generated_csvs.append(res_csv)\n    return generated_csvs   \n\ndef plot_gridmet(target_date=test_start_date):\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  var_name = \"pr\"\n  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n  gridmet_var_df = pd.read_csv(test_csv)\n  gridmet_var_df.replace('--', pd.NA, inplace=True)\n  gridmet_var_df.dropna(inplace=True)\n  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n  #print(gridmet_var_df.head())\n  #print(gridmet_var_df[\"Latitude\"].describe())\n  #print(gridmet_var_df[\"Longitude\"].describe())\n  #print(gridmet_var_df[\"pr\"].describe())\n  \n  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n  \n  # Create a scatter plot\n  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n              gridmet_var_df[\"Latitude\"].values, \n              label='Pressure', \n              color=colormaplist, \n              marker='o')\n\n  # Add labels and a legend\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Scatter Plot Example')\n  plt.legend()\n  \n  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n  plt.savefig(res_png_path)\n  print(f\"test image is saved at {res_png_path}\")\n                \ndef prepare_folder_and_get_year_list(target_date=test_start_date):\n  # Check if the folder exists, if not, create it\n  if not os.path.exists(gridmet_folder_name):\n      os.makedirs(gridmet_folder_name)\n\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n  year_list = [selected_date.year, past_october_1.year]\n\n  # Remove any existing files in the folder\n  if selected_date.year == datetime.now().year:\n    # check if the current year's netcdf contains the selected date\n    # get etr netcdf and read\n    nc_file = f\"{gridmet_folder_name}/tmmx_{selected_date.year}.nc\"\n    ifremove = False\n    if os.path.exists(nc_file):\n      with nc.Dataset(nc_file) as ncd:\n        day = ncd.variables['day'][:]\n        # Calculate the day of the year\n        day_of_year = selected_date.timetuple().tm_yday\n        day_index = day_of_year - 1\n        if len(day) <= day_index:\n          ifremove = True\n    \n    if ifremove:\n      print(\"The current year netcdf has new data. Redownloading..\")\n      remove_files_in_folder(gridmet_folder_name, selected_date.year)  # only redownload when the year is the current year\n    else:\n      print(\"The existing netcdf already covers the selected date. Avoid downloading..\")\n  return year_list\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].sum()\n  return df\n    \n\ndef prepare_cumulative_history_csvs(target_date=test_start_date, force=False):\n  \"\"\"\n    Prepare cumulative history CSVs for a specified target date.\n\n    Parameters:\n    - target_date (str, optional): The target date in the format 'YYYY-MM-DD'. Default is 'test_start_date'.\n    - force (bool, optional): If True, forcefully regenerate cumulative CSVs even if they already exist. Default is False.\n\n    Returns:\n    None\n\n    This function generates cumulative history CSVs for a specified target date. It traverses the date range from the past\n    October 1 to the target date, downloads gridmet data, converts it to CSV, and merges it into a big DataFrame.\n    The cumulative values are calculated and saved in new CSV files.\n\n    Example:\n    ```python\n    prepare_cumulative_history_csvs(target_date='2023-01-01', force=True)\n    ```\n\n    Note: This function assumes the existence of the following helper functions:\n    - download_gridmet_of_specific_variables\n    - prepare_folder_and_get_year_list\n    - turn_gridmet_nc_to_csv\n    - add_cumulative_column\n    - process_group_value_filling\n    ```\n\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n        past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n        past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Rest of the function logic...\n\n    filled_data = filled_data.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n    print(\"new_df final shape: \", filled_data.head())\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    print(filled_data.describe())\n    ```\nNote: This docstring includes placeholders such as \"download_gridmet_of_specific_variables\" and \"prepare_folder_and_get_year_list\" for the assumed existence of related helper functions. You should replace these placeholders with actual documentation for those functions.\n  \"\"\"\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  \n  date_keyed_objects = {}\n  \n  download_gridmet_of_specific_variables(\n    prepare_folder_and_get_year_list(target_date=target_date), selected_date\n  )\n\n  print(\"Downloading gridmet completed. Start to generate csv..\")\n  \n  while current_date <= selected_date:\n    if not cumulative_mode and current_date != selected_date:\n      current_date += timedelta(days=1)\n      continue;\n    print(current_date.strftime('%Y-%m-%d'))\n    current_date_str = current_date.strftime('%Y-%m-%d')\n    generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n    \n    # read the csv into dataframe and merge to the big dataframe\n    date_keyed_objects[current_date_str] = generated_csvs\n    current_date += timedelta(days=1)\n    \n    \n  # print(\"date_keyed_objects: \", date_keyed_objects)\n  target_generated_csvs = date_keyed_objects[target_date]\n  for index, single_csv in enumerate(target_generated_csvs):\n    # traverse the variables of gridmet here\n    # each variable is a loop\n    # print(f\"creating cumulative for {single_csv}\")\n    \n    cumulative_target_path = f\"{single_csv}_cumulative.csv\"\n    # print(\"cumulative_target_path = \", cumulative_target_path)\n    \n    if os.path.exists(cumulative_target_path) and not force:\n      # print(f\"{cumulative_target_path} already exists, skipping..\")\n      continue\n    \n    # Extract the file name without extension\n    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n    gap_filled_csv = f\"{cumulative_target_path}_gap_filled.csv\"\n\n\t# Split the file name using underscores\n    var_name = file_name.split('_')[1]\n    # print(f\"Found variable name {var_name}\")\n    current_date = past_october_1\n    new_df = pd.read_csv(single_csv)\n    # print(new_df.head())\n    \n    all_df = pd.read_csv(f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\")\n    all_df[\"date\"] = target_date\n    all_df[var_name] = pd.to_numeric(all_df[var_name], errors='coerce')\n    filled_data = all_df\n    filled_data = filled_data[(filled_data['date'] == target_date)]\n    \n    filled_data.fillna(0, inplace=True)\n    \n    # print(\"Finished correctly \", filled_data.head())\n    #filled_data.to_csv(gap_filled_csv, index=False)\n    #print(f\"New filled values csv is saved to {gap_filled_csv}_gap_filled.csv\")\n    \n    filled_data = filled_data[['Latitude', 'Longitude', \n                               var_name, \n#                                f'cumulative_{var_name}'\n                              ]]\n    # print(filled_data.shape)\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    # print(filled_data.describe())\n\ndef gridmet_callback(current_date, force=False):\n  # Prepare the cumulative history CSVs for the current date\n  print(\"Getting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n  prepare_cumulative_history_csvs(target_date=current_date.strftime(\"%Y-%m-%d\"), force=force)\n\nif __name__ == \"__main__\":\n  # Run the download function\n#   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n#   turn_gridmet_nc_to_csv()\n#   plot_gridmet()\n\n  # prepare testing data with cumulative variables\n  # prepare_cumulative_history_csvs(force=True)\n\n  process_dates_in_range(\n    start_date=test_start_date,\n    end_date=test_end_date,\n    days_look_back=7,\n    callback=gridmet_callback,\n    force=False\n  )\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2n7b06",
  "name" : "create_output_tif_template",
  "description" : null,
  "code" : "import os\nimport rasterio\nfrom rasterio.transform import from_origin\nimport numpy as np\nfrom snowcast_utils import homedir\n\ndef create_western_us_geotiff():\n    \"\"\"\n    Create a GeoTIFF template for the western U.S. region with specified spatial extent and resolution.\n    The resulting GeoTIFF file will contain an empty 2D array with a single band.\n    \"\"\"\n    # Define the spatial extent of the western U.S. (minx, miny, maxx, maxy)\n    minx, miny, maxx, maxy = -125, 25, -100, 49\n\n    # Define the resolution in degrees (4km is approximately 0.036 degrees)\n    resolution = 0.036\n\n    # Calculate the image size (width and height in pixels) based on the spatial extent and resolution\n    width = int((maxx - minx) / resolution)\n    height = int((maxy - miny) / resolution)\n\n    # Create an empty 2D NumPy array with a single band to store the image data\n    data = np.zeros((height, width), dtype=np.float32)\n    \n    # Read the user's home directory\n    # homedir = os.path.expanduser('~')\n    print(homedir)\n\n    # Define the output filename\n    output_filename = f\"{homedir}/western_us_geotiff_template.tif\"\n\n    # Create the GeoTIFF file and specify the metadata\n    with rasterio.open(\n        output_filename,\n        'w',\n        driver='GTiff',\n        height=height,\n        width=width,\n        count=1,  # Single band\n        dtype=np.float32,\n        crs='EPSG:4326',  # WGS84\n        transform=from_origin(minx, maxy, resolution, resolution),\n    ) as dst:\n        # Write the data to the raster\n        dst.write(data, 1)\n\nif __name__ == \"__main__\":\n    create_western_us_geotiff()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "bwdy3s",
  "name" : "resample_dem",
  "description" : null,
  "code" : "#!/bin/bash\n# this script will reproject and resample the western US dem, clip it, to match the exact spatial extent and resolution as the template tif\nsource ~/.bashrc\n\nexport PROJ_LIB=\"/home/geo2021/anaconda3/share/proj/\"\n\ngdalinfo --version\n\n# Change directory to the working directory\ncd /media/volume1/swe/gridmet_test_run\necho \">> Changed directory to /media/volume1/swe/data/gridmet_test_run\"\n\n# Create a directory for the template shapefile\nmkdir -p /media/volume1/swe/data/template_shp/\necho \">> Created directory for template shapefile at /media/volume1/swe/data/template_shp/\"\n\n# Copy the template GeoTIFF to the template shapefile directory\ncp /media/volume1/swe/data/western_us_geotiff_template.tif /media/volume1/swe/data/template_shp/\necho \">> Copied template GeoTIFF to template_shp directory\"\n\n# Generate the template shapefile from the copied GeoTIFF\n# Remove the existing shapefile if it exists\nif [ -f /media/volume1/swe/data/template.shp ]; then\n    echo \">> Existing shapefile found. Removing it...\"\n    rm /media/volume1/swe/data/template.*\nfi\n\n# Generate the shapefile with the desired projection\necho \">> Generating new shapefile with WGS84 projection...\"\ngdaltindex -t_srs EPSG:4326 /media/volume1/swe/data/template.shp /media/volume1/swe/data/template_shp/*.tif\n\necho \">> Shapefile generation complete.\"\necho \">> Generated template shapefile using gdaltindex\"\n\necho \">> Check on the cutline shapefile:\"\nogrinfo -al -so /media/volume1/swe/data/template.shp\n\necho \">> Check on the merged DEM\"\nls /media/volume1/swe/data/srtm/merged_dem.tif -lhtra\ngdalinfo /media/volume1/swe/data/srtm/merged_dem.tif\n\necho \">> Check on new translated DEM\"\ngdalinfo /media/volume1/swe/data/srtm/srtm_wgs84_4km.tif\n\necho \">> Check test window on the DEM\"\ngdal_translate -projwin -113 38 -111 36 -of GTiff \\\n  /media/volume1/swe/data/srtm/srtm_wgs84_4km.tif \\\n  /tmp/srtm_subwindow_2.tif\ngdalinfo -stats /tmp/srtm_subwindow_2.tif\n\n# Reproject and resample the DEM, clipping it to match the template shapefile\necho \">> Cut the DEM with the shapefile\"\ngdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /media/volume1/swe/data/template.shp -crop_to_cutline -overwrite /media/volume1/swe/data/srtm/srtm_wgs84_4km.tif /media/volume1/swe/data/srtm/output_dem_4km_clipped.tif\necho \">> Reprojected and resampled the DEM to match template shapefile, output saved to output_dem_4km_clipped.tif\"\n\n\necho \">> Display information about the clipped output DEM\"\ngdalinfo -stats /media/volume1/swe/data/srtm/output_dem_4km_clipped.tif\necho \">> Displayed information about the clipped output DEM\"\n\ncp /media/volume1/swe/data/srtm/output_dem_4km_clipped.tif /media/volume1/swe/data/srtm/dem_file.tif\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2wkl6e",
  "name" : "convert_results_to_images",
  "description" : null,
  "code" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport uuid\nimport matplotlib.colors as mcolors\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.transform import from_origin\nfrom rasterio.enums import Resampling\nfrom rasterio import warp\nfrom shapely.geometry import Point\nfrom rasterio.crs import CRS\nimport rasterio.features\nfrom rasterio.features import rasterize\nimport os\nimport math\nfrom datetime import datetime, timedelta\n\nfrom scipy.interpolate import griddata\n\n# Import utility functions and variables from 'snowcast_utils'\nfrom snowcast_utils import data_dir, work_dir, plot_dir, output_dir, test_start_date, test_end_date,process_dates_in_range\n\n# Define a custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\ncmap_name = 'custom_snow_colormap'\ncustom_cmap = mcolors.ListedColormap(colors)\n\nlon_min, lon_max = -125, -100\nlat_min, lat_max = 25, 49.5\n\n# Define value ranges for color mapping\nfixed_value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n\nimport os\nimport requests\nimport zipfile\n\ndef download_and_unzip_shapefile(url, output_dir):\n    \"\"\"\n    Download and unzip a shapefile from the given URL.\n\n    Args:\n        url (str): URL of the zip file containing the shapefile.\n        output_dir (str): Directory to save the downloaded file and its contents.\n\n    Returns:\n        str: Path to the extracted shapefile directory.\n    \"\"\"\n    # Ensure the output directory exists\n    os.makedirs(output_dir, exist_ok=True)\n\n    # Define the file paths\n    zip_file_path = os.path.join(output_dir, \"tl_2023_us_state.zip\")\n\n    print(f\"Step 1: Downloading shapefile from {url}...\")\n    # Download the zip file\n    response = requests.get(url, stream=True)\n    response.raise_for_status()  # Raise an error if the download fails\n    with open(zip_file_path, \"wb\") as file:\n        for chunk in response.iter_content(chunk_size=8192):\n            file.write(chunk)\n\n    print(f\"Step 2: Download complete. Saved to {zip_file_path}\")\n\n    # Extract the zip file\n    print(f\"Step 3: Extracting files to {output_dir}...\")\n    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n        zip_ref.extractall(output_dir)\n\n    print(\"Step 4: Extraction complete.\")\n    return output_dir\n\ndef retrieve_state_boundary():\n    \"\"\"\n    Retrieve the state boundary shapefile.\n    Downloads and extracts the file only if it doesn't already exist.\n\n    Returns:\n        str: Absolute path to the main shapefile.\n    \"\"\"\n    # URL of the shapefile zip\n    shapefile_url = \"https://www2.census.gov/geo/tiger/TIGER2023/STATE/tl_2023_us_state.zip\"\n\n    # Output directory for the downloaded and extracted files\n    output_directory = os.path.join(data_dir, \"shapefiles\", \"tl_2023_us_state\")\n    os.makedirs(output_directory, exist_ok=True)\n\n    # Path to the main shapefile\n    shapefile_path = os.path.join(output_directory, \"tl_2023_us_state.shp\")\n\n    # Check if the shapefile already exists\n    if os.path.exists(shapefile_path):\n        print(f\"Shapefile already exists at: {shapefile_path}. Skipping download.\")\n    else:\n        print(\"Shapefile not found. Downloading...\")\n        # Download and unzip the shapefile\n        extracted_dir = download_and_unzip_shapefile(shapefile_url, output_directory)\n        print(f\"Shapefile extracted to: {extracted_dir}\")\n    \n    return os.path.abspath(shapefile_path)\n\n# Define the lat_lon_to_map_coordinates function\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    \"\"\"\n    Convert latitude and longitude coordinates to map coordinates.\n\n    Args:\n        lon (float or array-like): Longitude coordinate(s).\n        lat (float or array-like): Latitude coordinate(s).\n        m (Basemap): Basemap object representing the map projection.\n\n    Returns:\n        tuple: Tuple containing the converted map coordinates (x, y).\n    \"\"\"\n    x, y = m(lon, lat)\n    return x, y\n\n\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n    \"\"\"\n    Create a colormap for value ranges and map data values to colors.\n\n    Args:\n        df_col (pd.Series): A Pandas Series containing data values.\n        value_ranges (list, optional): A list of value ranges for color mapping.\n            If not provided, the ranges will be determined automatically.\n\n    Returns:\n        tuple: Tuple containing the color mapping and the updated value ranges.\n    \"\"\"\n    new_value_ranges = value_ranges\n    if value_ranges is None:\n        max_value = df_col.max()\n        min_value = df_col.min()\n        if min_value < 0:\n            min_value = 0\n        step_size = (max_value - min_value) / 12\n\n        # Create 10 periods\n        new_value_ranges = [min_value + i * step_size for i in range(12)]\n    \n    #print(\"new_value_ranges: \", new_value_ranges)\n  \n    # Define a custom function to map data values to colors\n    def map_value_to_color(value):\n        # Iterate through the value ranges to find the appropriate color index\n        for i, range_max in enumerate(new_value_ranges):\n            if value <= range_max:\n                return colors[i]\n\n        # If the value is greater than the largest range, return the last color\n        return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n    color_mapping = [map_value_to_color(value) for value in df_col.values]\n    return color_mapping, new_value_ranges\n\ndef convert_csvs_to_images(input_csv: str = None, new_plot_path: str = None):\n    \"\"\"\n    Convert CSV data to images with color-coded SWE predictions.\n\n    Returns:\n        None\n    \"\"\"\n    global fixed_value_ranges\n    if input_csv is None:\n        input_csv = f\"{work_dir}/test_data_predicted_n97KJ.csv\"\n    \n    data = pd.read_csv(input_csv)\n    print(\"statistic of predicted_swe: \", data['predicted_swe'].describe())\n    data['predicted_swe'].fillna(0, inplace=True)\n    \n    for column in data.columns:\n        column_data = data[column]\n        print(column_data.describe())\n    \n    # Create a figure with a white background\n    fig = plt.figure(facecolor='white')\n\n    \n\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    if \"Latitude\" in data.columns and \"Longitude\" in data.columns:\n        data.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"}, inplace=True)\n\n    x, y = m(data['lon'].values, data['lat'].values)\n    print(data.columns)\n\n    color_mapping, value_ranges = create_color_maps_with_value_range(data[\"predicted_swe\"], fixed_value_ranges)\n    \n    # Plot the data using the custom colormap\n    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n    \n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_date = datetime(1900, 1, 1)\n    day_value = day_index\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n    \n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}??W\" if lon < 0 else f\"{lon:.1f}??E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}??N\" if lat >= 0 else f\"{abs(lat):.1f}??S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}??N\" if lat >= 0 else f\"{abs(lat):.1f}??S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    # Create custom legend elements using the same colormap\n    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n\n    # Create the legend outside the map\n    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n    legend.set_bbox_to_anchor((1.01, 1)) \n\n    # Remove the color bar\n    #plt.colorbar().remove()\n\n    plt.text(0.98, 0.02, 'Copyright ?? SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Set the aspect ratio to 'equal' to keep the plot at the center\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n    # Show the plot or save it to a file\n    if new_plot_path is None:\n        new_plot_path = f'{work_dir}/predicted_swe-{test_start_date}.png'\n    \n    print(f\"The new plot is saved to {new_plot_path}\")\n    plt.savefig(new_plot_path)\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\ndef plot_all_variables_in_one_csv(csv_path, res_png_path, target_date = test_start_date):\n    result_var_df = pd.read_csv(csv_path)\n    # Convert the 'date' column to datetime\n    result_var_df['date'] = pd.to_datetime(result_var_df['date'])\n    result_var_df.rename(\n      columns={\n        'Latitude': 'lat', \n        'Longitude': 'lon',\n        'gridmet_lat': 'lat',\n        'gridmet_lon': 'lon',\n      }, \n      inplace=True)\n    \n  \t# Create subplots with a number of rows based on the number of columns in the DataFrame\n    \n    us_boundary = gpd.read_file(retrieve_state_boundary())\n    us_boundary_clipped = us_boundary.cx[lon_min:lon_max, lat_min:lat_max]\n\t\n    lat_col = result_var_df[[\"lat\"]]\n    lon_col = result_var_df[[\"lon\"]]\n    print(\"lat_col.values = \", lat_col[\"lat\"].values)\n#     if \"lat\" == column_name or \"lon\" == column_name or \"date\" == column_name:\n    columns_to_remove = [ \"date\", \"Latitude\", \"Longitude\", \"gridmet_lat\", \"gridmet_lon\", \"lat\", \"lon\"]\n\n    # Check if each column exists before removing it\n    for col in columns_to_remove:\n        if col in result_var_df.columns:\n            result_var_df = result_var_df.drop(columns=col)\n        else:\n            print(f\"Column '{col}' not found in DataFrame.\")\n    \n    print(\"result_var_df.shape: \", result_var_df.shape)\n    print(\"result_var_df.head: \", result_var_df.head())\n    \n    \n    num_columns = len(result_var_df.columns)  # don't plot lat and lon\n    fig_width = 7 * num_columns  # You can adjust this multiplier based on your preference\n    num_variables = len(result_var_df.columns)\n    num_cols = int(math.sqrt(num_variables))\n    num_rows = math.ceil(num_variables / num_cols)\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols*7, num_rows*6))\n    \n    \n    # Flatten the axes array to simplify indexing\n    axes = axes.flatten()\n    \n  \t# Plot each variable in a separate subplot\n    for i, column_name in enumerate(result_var_df.columns):\n  \t    print(f\"Plot {column_name}\")\n  \t    if column_name in [\"lat\", \"lon\"]:\n  \t        continue\n        \n        # Filter the DataFrame based on the target date\n  \t    result_var_df[column_name] = pd.to_numeric(result_var_df[column_name], errors='coerce')\n  \t    \n  \t    colormaplist, value_ranges = create_color_maps_with_value_range(result_var_df[column_name], fixed_value_ranges)\n  \t    scatter_plot = axes[i].scatter(\n            lon_col[\"lon\"].values, \n  \t        lat_col[\"lat\"].values, \n            label=column_name, \n            c=result_var_df[column_name], \n            cmap='viridis', \n              #s=200, \n            s=10, \n            marker='s',\n            edgecolor='none',\n        )\n        \n        # Add a colorbar\n  \t    cbar = plt.colorbar(scatter_plot, ax=axes[i])\n  \t    cbar.set_label(column_name)  # Label for the colorbar\n        \n        # Add boundary over the figure\n  \t    us_boundary_clipped.plot(ax=axes[i], color='none', edgecolor='black', linewidth=1)\n\n        # Add labels and a legend\n  \t    axes[i].set_xlabel('Longitude')\n  \t    axes[i].set_ylabel('Latitude')\n  \t    axes[i].set_title(column_name+\" - \"+target_date)  # You can include target_date if needed\n  \t    axes[i].legend(loc='lower left')\n    \n    # Remove any empty subplots\n    for i in range(num_variables, len(axes)):\n        fig.delaxes(axes[i])\n    \n    plt.tight_layout()\n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n    \n    \ndef plot_all_variables_in_one_figure_for_date(target_date=test_start_date):\n  \tselected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  \ttest_csv = f\"{output_dir}/test_data_predicted_latest.csv\"\n  \tres_png_path = f\"{plot_dir}/{str(selected_date.year)}_all_variables_{target_date}.png\"\n  \tplot_all_variables_in_one_csv(test_csv, res_png_path, target_date)\n    \ndef convert_csvs_to_images_simple(\n    target_date=test_start_date, \n    column_name = \"predicted_swe\", \n    test_csv: str = None,\n    res_png_path: str = None,\n):\n    \"\"\"\n    Convert CSV data to simple scatter plot images for predicted SWE.\n\n    Returns:\n        None\n    \"\"\"\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    var_name = column_name\n    if test_csv is None:\n        test_csv = f\"{output_dir}/test_data_predicted_latest_{target_date}.csv_snodas_mask.csv\"\n\n    # Extract the directory from the target path\n    target_plot_dir = os.path.dirname(test_csv)\n\n    # Create all layers of directories if they don't exist\n    os.makedirs(target_plot_dir, exist_ok=True)\n\n    if res_png_path is None:\n        res_png_path = f\"{plot_dir}/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n    \n    result_var_df = pd.read_csv(test_csv)\n    # Convert the 'date' column to datetime\n    if 'date_x' in result_var_df.columns and 'date_y' in result_var_df.columns:\n        # Drop one of the date columns (let's drop 'date_y')\n        result_var_df.drop(columns=['date_y'], inplace=True)\n        \n        # Rename 'date_x' to 'date'\n        result_var_df.rename(columns={'date_x': 'date'}, inplace=True)\n    \n    if 'date' in result_var_df.columns:\n        result_var_df['date'] = pd.to_datetime(result_var_df['date'])\n\n    # Filter the DataFrame based on the target date\n    result_var_df[var_name] = pd.to_numeric(result_var_df[var_name], errors='coerce')\n    \n    colormaplist, value_ranges = create_color_maps_with_value_range(result_var_df[var_name], fixed_value_ranges)\n\n    if \"Latitude\" in result_var_df.columns and \"Longitude\" in result_var_df.columns:\n        result_var_df.rename(columns={\"Latitude\": \"lat\", \"Longitude\": \"lon\"}, inplace=True)\n\n    # Create a scatter plot\n    plt.scatter(result_var_df[\"lon\"].values, \n                result_var_df[\"lat\"].values, \n                label=column_name, \n                c=result_var_df[column_name], \n                cmap='viridis', \n                #s=200, \n                s=10, \n                marker='s',\n                edgecolor='none',\n               )\n\n    # Add a colorbar\n    cbar = plt.colorbar()\n    cbar.set_label(column_name)  # Label for the colorbar\n    \n    # Add labels and a legend\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title(f'{column_name} - {target_date}')\n    plt.legend(loc='lower left')\n    \n    us_boundary = gpd.read_file(retrieve_state_boundary())\n    us_boundary_clipped = us_boundary.cx[lon_min:lon_max, lat_min:lat_max]\n\n    us_boundary_clipped.plot(ax=plt.gca(), color='none', edgecolor='black', linewidth=1)\n\n    \n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n\ndef convert_csv_to_geotiff(\n    target_date,\n    test_csv: str = None,\n    target_geotiff_file: str = None,\n    output_dir: str = \".\",\n    resolution: float = 0.01,  # Define the grid resolution\n):\n    # Load your CSV file\n    if test_csv is None:\n        test_csv = f\"{output_dir}/test_data_predicted_latest_{target_date}.csv\"\n    \n    result_var_df = pd.read_csv(test_csv)\n    result_var_df.rename(\n        columns={\n            'Latitude': 'lat',\n            'Longitude': 'lon',\n            'gridmet_lat': 'lat',\n            'gridmet_lon': 'lon',\n        },\n        inplace=True\n    )\n\n    # Specify the output GeoTIFF file\n    if target_geotiff_file is None:\n        target_geotiff_file = f\"{output_dir}/swe_predicted_{target_date}.tif\"\n\n    target_plot_dir = os.path.dirname(target_geotiff_file)\n    os.makedirs(target_plot_dir, exist_ok=True)\n\n    # Extract latitude, longitude, and variable of interest\n    df = result_var_df[[\"lat\", \"lon\", \"predicted_swe\"]]\n    latitude = df['lat'].values\n    longitude = df['lon'].values\n    swe = df['predicted_swe'].values\n\n    # Define raster grid bounds and resolution\n    lat_min, lat_max = latitude.min(), latitude.max()\n    lon_min, lon_max = longitude.min(), longitude.max()\n\n    # Create the raster grid\n    lon_grid, lat_grid = np.meshgrid(\n        np.arange(lon_min, lon_max, resolution),\n        np.arange(lat_min, lat_max, resolution)\n    )\n\n    # Interpolate data to the grid\n    grid_swe = griddata(\n        (longitude, latitude),  # Input coordinates\n        swe,                   # Input values\n        (lon_grid, lat_grid),  # Grid coordinates\n        method='linear'        # Interpolation method: 'linear', 'nearest', 'cubic'\n    )\n\n    # Flip the grid vertically to align with raster conventions\n    grid_swe = np.flipud(grid_swe)\n\n    # Define transform for the raster\n    transform = from_origin(\n        lon_min, lat_max,  # Upper-left corner\n        resolution, resolution  # Pixel size\n    )\n\n    # Save the GeoTIFF\n    with rasterio.open(\n        target_geotiff_file,\n        'w',\n        driver='GTiff',\n        height=grid_swe.shape[0],\n        width=grid_swe.shape[1],\n        count=1,  # Number of bands\n        dtype=grid_swe.dtype,\n        crs=\"EPSG:4326\",  # WGS84 Latitude/Longitude\n        transform=transform\n    ) as dst:\n        dst.write(grid_swe, 1)  # Write the raster data to the first band\n\n    print(f\"GeoTIFF saved to {target_geotiff_file}\")\n\ndef process_swe_prediction(current_date):\n    \"\"\"\n    Example callback function to process SWE prediction for a specific date.\n\n    Args:\n        current_date (datetime): The date to process.\n        output_dir (str): Directory for output files.\n        plot_dir (str): Directory for plot files.\n    \"\"\"\n    current_date_str = current_date.strftime(\"%Y-%m-%d\")\n    test_csv = f\"{output_dir}/test_data_predicted_latest_{current_date_str}.csv_snodas_mask.csv\"\n\n    if not os.path.exists(test_csv):\n        print(f\"Warning: {test_csv} is missing. Skipping this day.\")\n        return\n\n    # Example processing steps\n    convert_csvs_to_images_simple(current_date_str, test_csv=test_csv)\n    convert_csv_to_geotiff(\n        current_date_str,\n        test_csv=test_csv,\n        target_geotiff_file=f\"{output_dir}/swe_predicted_{current_date_str}.tif\",\n    )\n\n\nif __name__ == \"__main__\":\n    # Uncomment the function call you want to use:\n    #convert_csvs_to_images()\n    \n    #test_start_date = \"2022-10-09\"\n    process_dates_in_range(\n        # start_date=\"2025-01-14\",\n        # end_date=\"2025-01-14\",\n        start_date=test_start_date,\n        end_date=test_end_date,\n        days_look_back=0,\n        callback=process_swe_prediction,\n    )\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "i2fynz",
  "name" : "deploy_images_to_website",
  "description" : null,
  "code" : "import distutils.dir_util\nfrom snowcast_utils import work_dir, output_dir, plot_dir\nimport os\nimport shutil\nimport re\nimport pandas as pd\nfrom datetime import datetime\nimport time\n\n\nprint(\"move the plots and the results into the http folder\")\n\ndef copy_if_modified(source_file, destination_file):\n    if os.path.exists(destination_file):\n        source_modified_time = os.path.getmtime(source_file)\n        dest_modified_time = os.path.getmtime(destination_file)\n        \n        # If the source file is modified after the destination file\n        if source_modified_time > dest_modified_time:\n            shutil.copy(source_file, destination_file)\n            print(f'Copied: {source_file}')\n    else:\n        shutil.copy(source_file, destination_file)\n        print(f'Copied: {source_file}')\n\ndef create_mapserver_map_config(target_geotiff_file_path, force=False):\n  geotiff_file_name = os.path.basename(target_geotiff_file_path)\n  geotiff_mapserver_file_path = f\"/var/www/html/swe_forecasting/map/{geotiff_file_name}.map\"\n  \n  if os.path.exists(geotiff_mapserver_file_path) and not force:\n    print(f\"{geotiff_mapserver_file_path} already exists\")\n    return geotiff_mapserver_file_path\n  \n  # Define a regular expression pattern to match the date in the filename\n  pattern = r\"\\d{4}-\\d{2}-\\d{2}\"\n\n  # Use re.search to find the match\n  match = re.search(pattern, geotiff_file_name)\n\n  # Check if a match is found\n  if match:\n      date_string = match.group()\n      print(\"Date:\", date_string)\n  else:\n      print(\"No date found in the filename.\")\n      return f\"The file's name {target_geotiff_file} is wrong\"\n  \n  mapserver_config_content = f\"\"\"\nMAP\n  NAME \"swemap\"\n  STATUS ON\n  EXTENT -125 25 -100 49\n  SIZE 800 400\n  UNITS DD\n  SHAPEPATH \"/var/www/html/swe_forecasting/output/\"\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    IMAGEPATH \"/temp/\"\n    IMAGEURL \"/temp/\"\n    METADATA\n      \"wms_title\" \"SWE MapServer WMS\"\n      \"wms_onlineresource\" \"http://geobrain.csiss.gmu.edu/cgi-bin/mapserv?map=/var/www/html/swe_forecasting/output/swe.map&\"\n      WMS_ENABLE_REQUEST      \"*\"\n      WCS_ENABLE_REQUEST      \"*\"\n      \"wms_srs\" \"epsg:5070 epsg:4326 epsg:3857\"\n    END\n  END\n\n\n  LAYER\n    NAME \"predicted_swe_{date_string}\"\n    TYPE RASTER\n    STATUS DEFAULT\n    DATA \"{target_geotiff_file_path}\"\n\n    PROJECTION\n      \"init=epsg:4326\"\n    END\n\n    METADATA\n      \"wms_include_items\" \"all\"\n    END\n    PROCESSING \"NODATA=0\"\n    STATUS ON\n    DUMP TRUE\n    TYPE RASTER\n    OFFSITE 0 0 0\n    CLASSITEM \"[pixel]\"\n    TEMPLATE \"template.html\"\n    INCLUDE \"legend_swe.map\"\n  END\nEND\n\"\"\"\n  \n  with open(geotiff_mapserver_file_path, \"w\") as file:\n    file.write(mapserver_config_content)\n    \n  print(f\"Mapserver config is created at {geotiff_mapserver_file_path}\")\n  return geotiff_mapserver_file_path\n\ndef refresh_available_date_list():\n  \n  # Define columns for the DataFrame\n  columns = [\"date\", \"predicted_swe_url_prefix\"]\n\n  # Create an empty DataFrame with columns\n  df = pd.DataFrame(columns=columns)\n  \n  for filename in os.listdir(geotiff_destination_folder):\n    target_geotiff_file = os.path.join(geotiff_destination_folder, filename)\n    \n    date_str = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", filename).group()\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # # Append a new row to the DataFrame\n    # df = df.append({\n    #   \"date\": date, \n    #   \"predicted_swe_url_prefix\": f\"../swe_forecasting/output/{filename}\"\n    # }, ignore_index=True)\n    # Create a new DataFrame with the new row data\n    new_row = pd.DataFrame([{\n        \"date\": date, \n        \"predicted_swe_url_prefix\": f\"../swe_forecasting/output/{filename}\"\n    }])\n\n    # Use pd.concat to append the new row\n    df = pd.concat([df, new_row], ignore_index=True)\n  \n  # Save DataFrame to a CSV file\n  df.to_csv(\"/var/www/html/swe_forecasting/date_list.csv\", index=False)\n  print(\"directly write into the server file which might be used at the time might not be a good idea. \")\n\n  # Display the final DataFrame\n  print(df)\n  \n\n               \ndef copy_files_to_right_folder():\n  \n  # copy the variable comparison folder\n  source_folder = f\"{work_dir}/var_comparison/\"\n  figure_destination_folder = f\"/var/www/html/swe_forecasting/plots/\"\n  geotiff_destination_folder = f\"/var/www/html/swe_forecasting/output/\"\n\n  # Copy the folder with overwriting existing files/folders\n  distutils.dir_util.copy_tree(source_folder, figure_destination_folder, update=1)\n\n  print(f\"Folder '{source_folder}' copied to '{figure_destination_folder}' with overwriting.\")\n\n\n  # copy the png from testing_output to plots\n  source_folder = f\"{output_dir}/\"\n\n  # Ensure the destination folder exists, create it if necessary\n  if not os.path.exists(figure_destination_folder):\n    os.makedirs(figure_destination_folder)\n    \n  if not os.path.exists(geotiff_destination_folder):\n    os.makedirs(geotiff_destination_folder)\n\n  # Loop through the files in the source folder\n  for filename in os.listdir(source_folder):\n    # Check if the file is a PNG file\n    if filename.endswith('.png') or filename.endswith('.tif'):\n      # Build the source and destination file paths\n      source_file = os.path.join(source_folder, filename)\n      destination_file = os.path.join(figure_destination_folder, filename)\n\n      # Copy the file from the source to the destination\n      copy_if_modified(source_file, destination_file)\n      \n      # Copy the file to the output folder if it is geotif\n      if filename.endswith('.tif'):\n        output_dest_file = os.path.join(geotiff_destination_folder, filename)\n        copy_if_modified(source_file, output_dest_file)\n\n\nif __name__ == \"__main__\":\n  \n  geotiff_destination_folder = f\"/var/www/html/swe_forecasting/output/\"\n  copy_files_to_right_folder()\n  \n  # create mapserver config for all geotiff files in output folder\n  for filename in os.listdir(geotiff_destination_folder):\n    destination_file = os.path.join(geotiff_destination_folder, filename)\n    create_mapserver_map_config(destination_file, force=True)\n  print(\"Finished creation of all mapserver files.\")\n    \n  # refresh the output file list for the website to refresh its calendar\n  refresh_available_date_list()\n  print(\"All done\")\n  time.sleep(10)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2o6cp8",
  "name" : "training_feature_selection",
  "description" : null,
  "code" : "import dask.dataframe as dd\n\n# Replace 'data.csv' with the path to your 50GB CSV file\ninput_csv = '/home/chetana/gridmet_test_run/model_training_data.csv'\n\n# List of columns you want to extract\nselected_columns = ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n                    'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n                    'elevation',\n                    'slope', 'curvature', 'aspect', 'eastness',\n                    'northness', 'Snow Water Equivalent (in) Start of Day Values']\n\n# Read the CSV file into a Dask DataFrame\ndf = dd.read_csv(input_csv, usecols=selected_columns)\n\n# Rename the column as you intended\ndf = df.rename(columns={\"Snow Water Equivalent (in) Start of Day Values\": \"swe_value\"})\n\n# Replace 'output.csv' with the desired output file name\noutput_csv = '/home/chetana/gridmet_test_run/model_training_cleaned.csv'\n\n# Write the selected columns to a new CSV file\ndf.to_csv(output_csv, index=False, single_file=True)  # single_file=True \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "0n26v2",
  "name" : "amsr_testing_realtime",
  "description" : null,
  "code" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, data_dir, test_start_date, test_end_date, cumulative_mode, process_dates_in_range\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\nimport sys\nfrom convert_results_to_images import plot_all_variables_in_one_csv\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef is_binary(file_path):\n    try:\n        with open(file_path, 'rb') as file:\n            # Read a chunk of bytes from the file\n            chunk = file.read(1024)\n\n            # Check for null bytes, a common indicator of binary data\n            if b'\\x00' in chunk:\n                return True\n\n            # Check for a high percentage of non-printable ASCII characters\n            text_characters = \"\".join(chr(byte) for byte in chunk if 32 <= byte <= 126)\n            if not text_characters:\n                return True\n\n            # If none of the binary indicators are found, assume it's a text file\n            return False\n\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found.\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n  \ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    print(\"prepare_amsr_grid_mapper\")\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{data_dir}/amsr_testing/amsr_to_gridmet_mapper.csv'\n    # Extract the directory from the target path\n    target_dir = os.path.dirname(target_csv_path)\n\n    # Create all layers of directories if they don't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Now you can safely use the target_csv_path\n    print(f\"Target directory ensured: {target_dir}\")\n    print(f\"Target file path: {target_csv_path}\")\n    \n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{data_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    parent_directory = os.path.dirname(target_amsr_hdf_path)\n    if not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n        print(f\"Parent directory '{parent_directory}' created successfully.\")\n    \n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    print(\"Start to create the grid mapper csv..\")\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    print(\"download_amsr_and_convert_grid\")\n    # the mapper\n    target_mapper_csv_path = f'{data_dir}/amsr_testing/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{data_dir}/amsr_testing/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{data_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n        # Check the exit code\n        if result.returncode != 0:\n            print(f\"Command failed with exit code {result.returncode}.\")\n            if os.path.exists(target_amsr_hdf_path):\n              os.remove(target_amsr_hdf_path)\n              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n    \n    # Read the HDF\n    print(f\"Reading {target_amsr_hdf_path}\")\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    # print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\ndef add_cumulative_column(df, column_name):\n    df[f'cumulative_{column_name}'] = df[column_name].sum()\n    return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  # Extract X series (column names)\n  x_all_key = row.index\n  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n  if are_all_values_between_0_and_240:\n    print(\"row[x_subset_key] = \", row[x_subset_key])\n    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n  return row\n    \n    \ndef get_cumulative_amsr_data(target_date = test_start_date, force=False):\n    \n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  target_csv_path = f'{data_dir}/amsr_testing/testing_ready_amsr_{target_date}_cumulative.csv'\n\n  columns_to_be_cumulated = [\"AMSR_SWE\"]\n  \n  gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    # print(df[\"AMSR_SWE\"].describe())\n  else:\n    date_keyed_objects = {}\n    data_dict = {}\n    new_df = None\n    print(\"cumulative_mode = \", cumulative_mode)\n    while current_date <= selected_date:\n      if not cumulative_mode and current_date != selected_date:\n        current_date += timedelta(days=1)\n        continue;\n      print(current_date.strftime('%Y-%m-%d'))\n      current_date_str = current_date.strftime('%Y-%m-%d')\n\n      data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n      current_df = pd.read_csv(data_dict[current_date_str])\n      current_df.drop(columns=[\"date\"], inplace=True)\n\n      if current_date != selected_date:\n        current_df.rename(columns={\n          \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n          \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n        }, inplace=True)\n      #print(current_df.head())\n\n      if new_df is None:\n        new_df = current_df\n      else:\n        new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n        #new_df = new_df.append(current_df, ignore_index=True)\n\n      current_date += timedelta(days=1)\n\n    print(\"new_df.columns = \", new_df.columns)\n    # print(\"new_df.head = \", new_df.head())\n    df = new_df\n\n    #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n    # print(\"All current head: \", df.head())\n    print(\"the new_df.shape: \", df.shape)\n\n    if cumulative_mode:\n      print(\"Start to fill in the missing values\")\n      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n      filled_data = pd.DataFrame()\n\n      # Apply the function to each group\n      for column_name in columns_to_be_cumulated:\n        start_time = time.time()\n        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n        #alike_columns = filled_data.filter(like=column_name)\n        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n        print(\"filled_data.columns = \", filled_data.columns)\n        filtered_columns = df.filter(like=column_name)\n        print(filtered_columns.columns)\n        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n        filtered_columns.fillna(0, inplace=True)\n        \n        sum_column = filtered_columns.sum(axis=1)\n        # Define a specific name for the new column\n        df[f'cumulative_{column_name}'] = sum_column\n        df[filtered_columns.columns] = filtered_columns\n        \n        if filtered_columns.isnull().any().any():\n          print(\"filtered_columns :\", filtered_columns)\n          raise ValueError(\"Single group: shouldn't have null values here\")\n      \n        # Concatenate the original DataFrame with the Series containing the sum\n        #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n#         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n#         filled_data[f'cumulative_{column_name}'] = cumulative_column\n        #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n        print(\"filled_data.columns: \", filled_data.columns)\n        end_time = time.time()\n        # Calculate the elapsed time\n        elapsed_time = end_time - start_time\n        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n\n#       if any(filled_data['AMSR_SWE'] > 240):\n#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n    filled_data = df\n    filled_data[\"date\"] = target_date\n    # print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {gap_filled_csv}\")\n    df = filled_data\n  \n  result = df\n  # print(\"result.head = \", result.head())\n  # fill in the rest NA as 0\n  if result.isnull().any().any():\n    print(\"result :\", result)\n    raise ValueError(\"Single group: shouldn't have null values here\")\n  \n  # only retain the rows of the target date\n  print(result['date'].unique())\n  print(result.shape)\n  # print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n  result.to_csv(target_csv_path, index=False)\n  print(f\"New data is saved to {target_csv_path}\")\n\ndef amsr_callback(current_date, force=False):\n  # Prepare the cumulative history CSVs for the current date\n  print(\"Getting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n  get_cumulative_amsr_data(target_date=current_date.strftime(\"%Y-%m-%d\"), force=force)\n\nif __name__ == \"__main__\":\n  # Run the download and conversion function\n  #prepare_amsr_grid_mapper()\n  prepare_amsr_grid_mapper()\n  process_dates_in_range(\n    start_date=test_start_date,\n    end_date=test_end_date,\n    days_look_back=7,\n    callback=amsr_callback,\n    force=True\n  )\n#     download_amsr_and_convert_grid()\n    \n  # get_cumulative_amsr_data(force=False)\n  input_time_series_file = f'{data_dir}/amsr_testing/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n\n  #plot_all_variables_in_one_csv(input_time_series_file, f\"{input_time_series_file}.png\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rvqv35",
  "name" : "perform_download.sh",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Specify the file containing the download links\ninput_file=\"/home/chetana/gridmet_test_run/amsr/download_links.txt\"\n\n# Specify the base wget command with common options\nbase_wget_command=\"wget --http-user=<your_username> --http-password=<your_password> --load-cookies /home/chetana/gridmet_test_run/amsr/mycookies.txt --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate -$\n\n# Specify the output directory for downloaded files\noutput_directory=\"/home/chetana/gridmet_test_run/amsr\"\n\n# Ensure the output directory exists\nmkdir -p \"$output_directory\"\n\n# Loop through each line (URL) in the input file and download it using wget\nwhile IFS= read -r url; do\n    echo \"Downloading: $url\"\n    $base_wget_command -P \"$output_directory\" \"$url\"\ndone < \"$input_file\"",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "vo8bc9",
  "name" : "merge_custom_traning_range",
  "description" : null,
  "code" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\nimport time\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'lat',\n                                       'LONGITUDE': 'lon',\n                                       'SNWD': 'snow_depth',})\n    df_combined = pd.concat([snotel_df, ghcnd_df], axis=0, ignore_index=True)\n    df_combined.to_csv(all_station_obs_file, index=False)\n    print(f\"All snotel ang ghcnd are saved to {all_station_obs_file}\")\n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    ground_truth = dd.read_csv(all_station_obs_file, blocksize=chunk_size)\n    print(\"ground_truth.columns = \", ground_truth.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    ground_truth = ground_truth.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and ground_truth\")\n    merged_df = dd.merge(amsr, ground_truth, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_ground_truth.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n#     merge_snotel_ghcnd_together()\n  \n#     merge_all_data_together()\n#     cleanup_dataframe()\n#     final_final_output_file = f'{work_dir}/{final_output_name}'\n#     sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    \n    start_time = time.time()\n    \n    merge_all_data_together()\n    print(f\"Time taken for merge_all_data_together: {time.time() - start_time} seconds\")\n    \n    start_time = time.time()\n    cleanup_dataframe()\n    print(f\"Time taken for cleanup_dataframe: {time.time() - start_time} seconds\")\n    \n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sorted_output_file = f'{work_dir}/{final_output_name}_sorted.csv'\n    \n    start_time = time.time()\n    sort_training_data(final_final_output_file, sorted_output_file)\n    print(f\"Time taken for sort_training_data: {time.time() - start_time} seconds\")\n    ",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6evkh4",
  "name" : "training_data_range",
  "description" : null,
  "code" : "\"\"\"\nThis script loads and processes several CSV files into Dask DataFrames, applies filters, renames columns,\nand saves the resulting DataFrames to new CSV files based on a specified time range.\n\nAttributes:\n    gridmet_20_years_file (str): File path of the GridMET climatology data CSV file.\n    snotel_20_years_file (str): File path of the SNOTEL data CSV file.\n    terrain_file (str): File path of the terrain data CSV file.\n    amsr_3_years_file (str): File path of the AMSR data CSV file.\n    output_file (str): File path where the merged and processed data will be saved.\n\nFunctions:\n    clip_csv_using_time_range: Main function that loads, processes, and saves CSV data based on a specified time range.\n\"\"\"\n\nfrom snowcast_utils import work_dir\nimport dask.dataframe as dd\n\ndef clip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file):\n    \"\"\"\n    Loads, processes, and saves CSV data into Dask DataFrames, applies filters, renames columns, and saves the resulting\n    DataFrames to new CSV files based on a specified time range.\n\n    Args:\n        gridmet_20_years_file (str): File path of the GridMET climatology data CSV file.\n        snotel_20_years_file (str): File path of the SNOTEL data CSV file.\n        terrain_file (str): File path of the terrain data CSV file.\n        amsr_3_years_file (str): File path of the AMSR data CSV file.\n        output_file (str): File path where the merged and processed data will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Load CSV files into Dask DataFrames\n    gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n    snotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\n    terrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\n    amsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n    # Filter and rename columns for each DataFrame in a single step\n    # (Code to filter and rename columns...)\n\n    # Save the processed Dask DataFrames to new CSV files\n    gridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\n    amsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n    snotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n\n# Define file paths and execute the function\ngridmet_20_years_file = f\"{work_dir}/gridmet_climatology/testing_ready_gridmet.csv\"\nsnotel_20_years_file = f\"{work_dir}/training_ready_snotel_data.csv\"\nterrain_file = f'{work_dir}/training_ready_terrain.csv'\namsr_3_years_file = f\"{work_dir}/training_amsr_data.csv\"\noutput_file = f\"{work_dir}/training_ready_merged_data_dd.csv\"\n\nclip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "76ewp5",
  "name" : "amsr_features",
  "description" : null,
  "code" : "import os\nimport csv\nimport h5py\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport dask\nimport dask.dataframe as dd\nimport dask.delayed as delayed\nimport dask.bag as db\nimport xarray as xr\nfrom snowcast_utils import work_dir, train_start_date, train_end_date\nfrom snowcast_utils import homedir, snowcast_github_dir, \\\nsupplement_point_for_correction_file, data_dir, gridcells_file, \\\nstations_file, all_training_points_with_station_and_non_station_file, \\\nall_training_points_with_snotel_ghcnd_file, gridcells_outfile, stations_outfile\nimport warnings\n\n# Suppress specific warning\nwarnings.filterwarnings(\"ignore\", message=\"overflow encountered in add\")\n\n\ndef copy_he5_files(source_dir, destination_dir):\n    '''\n    Copy .he5 files from the source directory to the destination directory.\n\n    Args:\n        source_dir (str): The source directory containing .he5 files to copy.\n        destination_dir (str): The destination directory where .he5 files will be copied.\n\n    Returns:\n        None\n    '''\n    # Get a list of all subdirectories and files in the source directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.endswith('.he5'):\n                # Get the absolute path of the source file\n                source_file_path = os.path.join(root, file)\n                # Copy the file to the destination directory\n                shutil.copy(source_file_path, destination_dir)\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    '''\n    Find the index of the grid cell with the closest coordinates to the target latitude and longitude.\n\n    Args:\n        target_latitude (float): The target latitude.\n        target_longitude (float): The target longitude.\n        lat_grid (numpy.ndarray): An array of latitude values.\n        lon_grid (numpy.ndarray): An array of longitude values.\n\n    Returns:\n        Tuple[int, int, float, float]: A tuple containing the row index, column index, closest latitude, and closest longitude.\n    '''\n    # Compute the absolute differences between target and grid coordinates\n    lat_diff = np.abs(lat_grid - target_latitude)\n    lon_diff = np.abs(lon_grid - target_longitude)\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n\ndef create_snotel_ghcnd_station_to_amsr_mapper(\n  new_base_station_list_file, \n  target_csv_path\n):\n    station_data = pd.read_csv(new_base_station_list_file)\n    \n    \n    date = \"2022-10-01\"\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    \n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        df = pd.read_csv(target_csv_path)\n        return df\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'station_lat', 'station_lon'])\n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    for idx, row in station_data.iterrows():\n        target_lat = row['latitude']\n        target_lon = row['longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat,\n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n    return df\n  \n  \ndef extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, new_base_station_list_file, start_date, end_date):\n    if os.path.exists(output_csv_file):\n        os.remove(output_csv_file)\n    \n    target_csv_path = f'{new_base_station_list_file}_amsr_grid_mapper.csv'\n    mapper_df = create_snotel_ghcnd_station_to_amsr_mapper(new_base_station_list_file, \n                                         target_csv_path)\n        \n    # station_data = pd.read_csv(new_base_station_list_file)\n\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n    # Create a Dask DataFrame\n    dask_station_data = dd.from_pandas(mapper_df, npartitions=1)\n\n    # Function to process each file\n    def process_file(filename):\n        file_path = os.path.join(amsr_data_dir, filename)\n        print(file_path)\n        \n        file = h5py.File(file_path, 'r')\n        hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n\n        date_str = filename.split('_')[-1].split('.')[0]\n        date = datetime.strptime(date_str, '%Y%m%d')\n\n        if not (start_date <= date <= end_date):\n            print(f\"{date} is not in the training period, skipping..\")\n            return None\n\n        new_date_str = date.strftime(\"%Y-%m-%d\")\n        swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n        flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n        # Create an empty Pandas DataFrame with the desired columns\n        result_df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE'])\n\n        # Sample loop to add rows to the Pandas DataFrame using dask.delayed\n        @delayed\n        def process_row(row, swe, new_date_str):\n          closest_lat_idx = int(row['amsr_lat_idx'])\n          closest_lon_idx = int(row['amsr_lon_idx'])\n          closest_swe = swe[closest_lat_idx, closest_lon_idx]\n          \n          return pd.DataFrame([[\n            new_date_str, \n            row['station_lat'],\n            row['station_lon'],\n            closest_swe]], \n            columns=result_df.columns\n          )\n\n\n        # List of delayed computations\n        delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]\n\n        # Compute the delayed results and concatenate them into a Pandas DataFrame\n        result_df = dask.compute(*delayed_results)\n        result_df = pd.concat(result_df, ignore_index=True)\n\n        # Print the final Pandas DataFrame\n        #print(result_df)\n          \n        return result_df\n\n    # Get the list of files\n    files = [f for f in os.listdir(amsr_data_dir) if f.endswith('.he5')]\n\n    # Create a Dask Bag from the files\n    dask_bag = db.from_sequence(files, npartitions=2)\n\n    # Process files in parallel\n    processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()\n\n    # Concatenate the processed data\n    combined_df = pd.concat(processed_data, ignore_index=True)\n\n    # Save the combined DataFrame to a CSV file\n    combined_df.to_csv(output_csv_file, index=False)\n\n    print(f\"Merged data saved to {output_csv_file}\")\n\n                    \nif __name__ == \"__main__\":\n    amsr_data_dir = '/home/chetana/gridmet_test_run/amsr'\n    # new_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n    # all_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\n    # new_base_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n    # print(new_base_df.head())\n    # output_csv_file = f\"{all_training_points_with_snotel_ghcnd_file}_amsr_dask_all_training_ponits_with_ghcnd.csv\"\n\n    start_date = train_start_date\n    end_date = train_end_date\n\n    # extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, all_training_points_with_snotel_ghcnd_file, start_date, end_date)\n\n\n    # to get AMSR for the new salt-pepper training points\n    extract_amsr_values_save_to_csv(amsr_data_dir, f\"{supplement_point_for_correction_file}_amsr_training.csv\", supplement_point_for_correction_file, start_date, end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "5wzgx5",
  "name" : "amsr_swe_data_download",
  "description" : null,
  "code" : "from datetime import datetime, timedelta\nimport os\nimport subprocess\n\ndef generate_links(start_year, end_year):\n    '''\n    Generate a list of download links for AMSR daily snow data files.\n\n    Args:\n        start_year (int): The starting year.\n        end_year (int): The ending year (inclusive).\n\n    Returns:\n        list: A list of download links for AMSR daily snow data files.\n    '''\n    base_url = \"https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/\"\n    date_format = \"%Y.%m.%d\"\n    delta = timedelta(days=1)\n\n    start_date = datetime(start_year, 1, 1)\n    end_date = datetime(end_year + 1, 1, 1)\n\n    links = []\n    current_date = start_date\n\n    while current_date < end_date:\n        date_str = current_date.strftime(date_format)\n        link = base_url + date_str + \"/AMSR_U2_L3_DailySnow_B02_\" + date_str + \".he5\"\n        links.append(link)\n        current_date += delta\n\n    return links\n\nif __name__ == \"__main__\":\n    start_year = 2019\n    end_year = 2022\n\n    links = generate_links(start_year, end_year)\n    save_location = \"/home/chetana/gridmet_test_run/amsr\"\n    with open(\"/home/chetana/gridmet_test_run/amsr/download_links.txt\", \"w\") as txt_file:\n      for l in links:\n        txt_file.write(\" \".join(l) + \"\\n\")\n\n    #if not os.path.exists(save_location):\n    #    os.makedirs(save_location)\n\n    #for link in links:\n    #    filename = link.split(\"/\")[-1]\n    #    save_path = os.path.join(save_location, filename)\n    #    curl_cmd = f\"curl -b ~/.urs_cookies -c ~/.urs_cookies -L -n -o {save_path} {link}\"\n    #    subprocess.run(curl_cmd, shell=True, check=True)\n        # print(f\"Downloaded: {filename}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "d4zcq6",
  "name" : "install_dependencies",
  "description" : null,
  "code" : "#!/bin/bash\n\nsource /home/chetana/miniconda/bin/activate\npip install geojson seaborn xarray dask pandas numpy geopandas pyarrow netCDF4 h5netcdf torch scikit-learn psutil\n\n# mkdir -p /home/chetana/data/ucla/raw/\n\n# cd /home/chetana/data/ucla/raw/\n\n# wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies --no-check-certificate --auth-no-challenge=on -r --reject \"index.html*\" -np -e robots=off https://n5eil01u.ecs.nsidc.org/DP6/SNOWEX/WUS_UCLA_SR.001/1984.10.01/WUS_UCLA_SR_v01_N31_0W104_0_agg_16_WY1984_85_SWE_SCA_POST.nc\n\n# wget https://n5eil01u.ecs.nsidc.org/DP6/SNOWEX/WUS_UCLA_SR.001/1984.10.01/WUS_UCLA_SR_v01_N31_0W104_0_agg_16_WY1984_85_SWE_SCA_POST.nc\n\nexit 0\n\n# change this line if you are creating a new env\n# source /home/ubuntu/anaconda3/bin/activate\n# conda activate new_env\n# which python\n# python --version\n\n# install GDAL\n# sudo apt update\n# sudo apt install gdal-bin libgdal-dev -y\n\n# Use cat to create the requirements.txt file\ncat <<EOL > requirements.txt\nabsl-py==1.4.0\naffine==2.4.0\naiobotocore==2.7.0\naioitertools==0.11.0\nansi2html==1.8.0\nanyio==3.6.2\nargon2-cffi==21.3.0\nargon2-cffi-bindings==21.2.0\nasciitree==0.3.3\nastunparse==1.6.3\nasync-generator==1.10\nattrs==22.2.0\nBabel==2.11.0\nbackcall==0.2.0\nbackports-datetime-fromisoformat==2.0.0\nbackports.weakref==1.0.post1\nbackports.zoneinfo==0.2.1\nbasemap-data==1.3.2\nbasemap-data-hires==1.3.2\nbleach==4.1.0\nbotocore==1.31.64\nbounded-pool-executor==0.0.3\nbrotlipy==0.7.0\n# catboost==1.2\ncategory-encoders==2.6.1\ncertifi==2023.5.7\ncffi==1.15.1\ncftime==1.6.2\ncharset-normalizer==2.0.12\nclick==8.0.4\nclick-plugins==1.1.1\ncligj==0.7.2\ncloudpickle==2.2.1\n# clyent==1.2.2\n# colorcet==3.0.1\nconda-repo-cli==1.0.75\nconda-verify==3.4.2\nConfigSpace==0.7.1\ncontextvars==2.4\ncontourpy==1.1.1\ncycler==0.11.0\ndash==2.11.1\ndash-core-components==2.0.0\ndash-html-components==2.0.0\ndash-table==5.0.0\ndask==2023.9.2\ndataclasses==0.8\ndatashader==0.15.2\ndatashape==0.5.2\ndecorator==5.1.1\ndefusedxml==0.7.1\nDeprecated==1.2.14\ndeprecation==2.1.0\ndistributed==2023.9.2\nearthaccess==0.7.1\nemcee==3.1.4\nentrypoints==0.4\nfasteners==0.18\nfilelock==3.4.1\nFiona==1.9.4.post1\nflaky==3.7.0\nFlask==2.2.5\nflatbuffers==23.5.26\nfonttools==4.42.1\nfsspec==2023.10.0\ngast==0.4.0\nGDAL\ngeojson==3.0.1\ngeopandas==0.14.0\ngoogle-auth-oauthlib==1.0.0\ngoogle-pasta==0.2.0\ngraphviz==0.20.1\ngstatsim==1.0.1\nh5py==3.9.0\nhtmldate==1.4.2\nhuggingface-hub==0.4.0\nidna==3.4\nimageio==2.31.3\nimbalanced-learn==0.11.0\nimgaug==0.4.0\nimmutables==0.19\nimportlib-resources==5.4.0\nipykernel==5.5.6\nipython==7.16.3\nipython-genutils==0.2.0\nisodate==0.6.1\nitsdangerous==2.1.2\njedi==0.17.2\nJinja2==3.0.3\njmespath==1.0.1\njoblib==1.3.2\njson5==0.9.11\njsonpointer==2.1\njsonschema==3.2.0\njupyter-client==7.1.2\njupyter-core==4.9.2\njupyter-dash==0.4.2\njupyter-server==1.13.1\njupyterlab==3.2.9\njupyterlab-iframe==0.4.0\njupyterlab-pygments==0.1.2\njupyterlab-server==2.10.3\nkaleido==0.2.1\nkeras==2.13.1\nkeras-core==0.1.0\nkeras-nlp==0.6.0\nkeras-tuner==1.3.5\nkiwisolver==1.4.5\nkt-legacy==1.0.5\nlazy_loader==0.1\nlibclang==16.0.0\nlightgbm==3.3.5\nllvmlite==0.40.1\nlocket==1.0.0\nlockfile==0.12.2\nMarkdown==3.4.4\nmarkdown-it-py==3.0.0\nmatplotlib==3.8.0\nmdurl==0.1.2\nmistune==0.8.4\nmkl-fft==1.3.1\nmkl-service==2.4.0\nmsgpack==1.0.5\nmultimethod==1.10\nmultipledispatch==1.0.0\nnamex==0.0.7\nnavigator-updater==0.3.0\nnbclassic==0.3.5\nnbclient==0.5.9\nnbconvert==6.0.7\nnbformat==5.1.3\nnest-asyncio==1.5.6\nnetCDF4==1.5.8\nnotebook==6.4.10\nnumba==0.57.1\nnumcodecs==0.11.0\nnvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-nvrtc-cu11==11.7.99\nnvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\noauthlib==3.2.2\nopencv-python==4.7.0.68\nopt-einsum==3.3.0\norjson==3.9.2\npackage-skimage==0.0.1\npackaging==21.3\npandas==1.5.3\npandocfilters==1.5.0\nparam==1.13.0\nparso==0.7.1\npartd==1.4.0\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.4.0\nPint==0.18\nplanetary-computer==0.4.9\nplotly==5.17.0\nplotly-resampler==0.8.3.2\nply==3.11\npmdarima==2.0.3\nPolygon3==3.0.9.1\npqdm==0.2.0\nprometheus-client==0.16.0\nprompt-toolkit==3.0.36\nprotobuf==3.20.3\nptyprocess==0.7.0\npyasn1-modules==0.2.8\npycparser==2.21\npyct==0.5.0\npydantic==1.10.5\npyEddyTracker==3.6.1\npygeoweaver==0.9.14\nPygments==2.14.0\npynisher==0.6.4\npyod==1.1.0\npyparsing==3.0.9\npyproj==3.6.1\nPyQt5-sip==12.11.0\npyrfr==0.8.3\npyrsistent==0.18.0\npyshp==2.3.1\npystac==1.6.1\npystac-client==0.6.0\npython-cmr==0.9.0\npython-dateutil==2.8.2\npython-dotenv==1.0.0\npytz==2022.7.1\n# PyYAML==6.0\npyzmq==25.0.2\nrasterio==1.3.9\nregex==2023.6.3\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrequests-toolbelt==0.10.1\nretrying==1.3.4\nrich==13.4.2\nrioxarray==0.13.4\ns3fs==2023.10.0\nsacremoses==0.0.53\nschemdraw==0.15\nscikit-base==0.5.0\nscikit-image==0.20.0\nscikit-learn==1.3.0\nscikit-plot==0.3.7\nscipy==1.11.2\nseaborn==0.12.2\nSend2Trash==1.8.0\nshap==0.42.1\nshapely==2.0.1\nsix==1.16.0\nsktime==0.20.0\nslicer==0.0.7\nsmac==1.4.0\nsniffio==1.2.0\nsnuggs==1.4.7\nsortedcontainers==2.4.0\ntbats==1.1.3\ntblib==2.0.0\ntenacity==8.2.3\ntensorboard==2.13.0\ntensorboard-data-server==0.7.0\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.13.0\ntensorflow-estimator==2.13.0\ntensorflow-hub==0.13.0\ntensorflow-io-gcs-filesystem==0.32.0\ntensorflow-text==2.13.0\ntermcolor==2.3.0\nterminado==0.12.1\ntestpath==0.6.0\nthreadpoolctl==3.2.0\ntinynetrc==1.3.1\ntokenizers==0.12.1\ntorch==1.13.1\ntorchaudio==0.13.1\ntorchmetrics==0.11.1\ntorchvision==0.14.1\ntornado==6.1\ntornado-proxy-handlers==0.0.5\ntqdm==4.64.1\ntrace-updater==0.0.9.1\ntraitlets==4.3.3\ntransformers==4.18.0\ntyping_extensions==4.5.0\nuritemplate==3.0.1\nurllib3==1.26.15\nwcwidth==0.2.6\nwebencodings==0.5.1\nwebsocket-client==1.3.1\nWerkzeug==2.2.3\nwrapt==1.15.0\nxarray==2023.8.0\nxarray-spatial==0.3.5\nxxhash==3.2.0\nyellowbrick==1.5\nzarr==2.14.1\nzeep==4.2.1\nzict==3.0.0\nzipp==3.6.0\nEOL\n\npython -m pip install -r requirements.txt\n# clean up\nrm requirements.txt\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6x6myw",
  "name" : "model_evaluation",
  "description" : null,
  "code" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit(0)  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "r4knm9",
  "name" : "interpret_model_results",
  "description" : null,
  "code" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season, output_dir, plot_dir, model_dir\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\nfrom model_creation_et import selected_columns\nimport traceback\n\nimport shap\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    return pd.read_csv(file_path)\n\ndef explain_predictions(\n    model, input_data, feature_names, output_csv, name, plot_path\n):\n    \"\"\"\n    Explains predictions using SHAP, saves the explanations into a CSV file,\n    and generates SHAP plots for each row.\n\n    Parameters:\n    - model: Trained tree-based model (e.g., RandomForest, LightGBM, XGBoost)\n    - input_data: Input data as a numpy array or pandas DataFrame\n    - feature_names: List of feature names\n    - output_csv: Path to save the explanation report as a CSV\n    - plot_path: Directory to save the SHAP plots\n    \"\"\"\n    print(\"Starting the explanation process...\")\n\n    # Ensure input_data is a DataFrame\n    if not isinstance(input_data, pd.DataFrame):\n        print(\"Converting input data to a DataFrame...\")\n        input_data = pd.DataFrame(input_data, columns=feature_names)\n\n    input_data = input_data.apply(pd.to_numeric, errors='coerce')\n    # Identify non-numeric columns\n    print(\"input_data = \", input_data)\n    non_numeric_columns = [\n        col for col in input_data.columns if not pd.api.types.is_numeric_dtype(input_data[col])\n    ]\n\n    if non_numeric_columns:\n        print(\"Non-numeric data found in the following columns:\")\n        for col in non_numeric_columns:\n            print(f\"  - {col}\")\n\n        # Attempt to convert non-numeric columns to numeric\n        for col in non_numeric_columns:\n            try:\n                print(f\"Converting column '{col}' to numeric...\")\n                input_data[col] = pd.to_numeric(input_data[col], errors='coerce')\n                if input_data[col].isna().any():\n                    print(f\"Replacing NaN values in column '{col}' with 0.\")\n                    input_data[col].fillna(0, inplace=True)\n            except Exception as e:\n                raise ValueError(f\"Failed to convert column '{col}' to numeric: {e}\")\n\n    print(\"Double check .. \")\n    non_numeric_columns = [\n        col for col in input_data.columns if not pd.api.types.is_numeric_dtype(input_data[col])\n    ]\n    if non_numeric_columns:\n        raise ValueError(\"WTF\")\n\n    print(\"Initializing SHAP explainer...\")\n    explainer = shap.TreeExplainer(model)\n\n    # Calculate SHAP values for the data\n    print(f\"Calculating SHAP values for {len(input_data)} samples...\")\n    shap_values = explainer.shap_values(input_data)\n\n    # Handle single-output vs multi-output models\n    if isinstance(shap_values, list):\n        print(\"Multi-output model detected. Using the first output for SHAP values.\")\n        shap_values = shap_values[0]\n\n    # Store explanations\n    explanations = []\n\n    print(f\"Processing {len(input_data)} rows...\")\n    for i in range(len(input_data)):\n        print(f\"Explaining row {i + 1}/{len(input_data)}...\")\n\n        try:\n            # Get SHAP values for the current row\n            shap_row = shap_values[i]\n            prediction = model.predict(input_data.iloc[[i]])[0]\n\n            # Collect all feature contributions for the row\n            feature_contributions = {\n                feature_names[j]: shap_row[j] for j in range(len(feature_names))\n            }\n\n            # Append explanation to the list\n            explanations.append({\n                \"Row\": i,\n                \"Prediction\": prediction,\n                **feature_contributions\n            })\n\n            # Generate and save SHAP waterfall plot\n            print(\"Checking for non-numeric columns in input_data...\")\n\n            # Generate and save SHAP waterfall plot\n            data_row = input_data.iloc[i].values.astype(float)\n\n            # Ensure shap_row is 1D\n            shap_row = np.ravel(shap_row)  # Flatten it if necessary\n\n            # Ensure data_row is also 1D\n            data_row = np.ravel(data_row)\n\n            # Ensure base_values is a scalar (for single-output models)\n            base_value = explainer.expected_value\n\n            print(f\"Shape of shap_row: {shap_row.shape}\")  # Should be (n_features,)\n            print(f\"Shape of data_row: {data_row.shape}\")  # Should be (n_features,)\n            print(f\"Number of features: {len(feature_names)}\")  # Should match the previous two\n            # Check if base_value is a scalar (single-output model) or array (multi-output model)\n            if isinstance(base_value, np.ndarray):\n                # For multi-output models, select the correct base value (e.g., base_value[0] for the first output)\n                base_value = base_value[0]  # Adjust index if necessary for your model\n                print(f\"Using base_value from multi-output model: {base_value}\")\n            else:\n                print(f\"Using base_value for single-output model: {base_value}\")\n\n            shap_results = shap.Explanation(\n                values=shap_row,\n                base_values=base_value,\n                data=data_row,\n                feature_names=feature_names\n            )\n\n            print(\"shap_results = \", shap_results)\n            print(\"SHAP Explanation object created successfully.\")\n            plot_file = f\"{plot_path}_row_{i + 1}.png\"\n            shap.waterfall_plot(shap_results, max_display=len(feature_names))\n            plt.title(\n                f\"SHAP Waterfall Plot {name}\", \n                fontsize=14\n            )\n            plt.savefig(plot_file, bbox_inches=\"tight\")\n            plt.close()\n            print(f\"Plot saved: {plot_file}\")\n\n        except Exception as e:\n            print(\"Detailed traceback:\")\n            traceback.print_exc()\n            print(f\"Failed to generate plot for row {i + 1}: {e}\")\n            continue\n\n    # Save explanations to a CSV file\n    explanations_df = pd.DataFrame(explanations)\n    try:\n        print(f\"Saving explanations to {output_csv}...\")\n        explanations_df.to_csv(output_csv, index=False)\n        print(f\"Explanations successfully saved to {output_csv}\")\n    except Exception as e:\n        print(f\"Failed to save explanations CSV: {e}\")\n\n    print(f\"Process completed. Explanations saved to {output_csv}. Plots saved in {plot_path}.\")\n\ndef explain_predictions_for_latlon(\n    lat, lon, target_date,\n    model_path: str = f'{model_dir}/wormhole_ETHole_latest.joblib'\n):\n    \"\"\"\n    Explains predictions for a specific location using SHAP, saves the explanation report,\n    and generates a SHAP plot.\n\n    Parameters:\n    - lat (float): Latitude of the target location.\n    - lon (float): Longitude of the target location.\n    - target_date (str): Target date in the format 'YYYY-MM-DD'.\n    - predicted_csv (str): Path to the CSV file with predictions.\n    - model: Trained tree-based model (e.g., RandomForest, LightGBM, XGBoost).\n    - feature_names: List of feature names.\n    - output_csv (str): Path to save the explanation report as a CSV.\n    - output_plots_dir (str): Directory to save the SHAP plot.\n    \"\"\"\n    print(f\"Starting explanation for location (lat: {lat}, lon: {lon}) on {target_date}...\")\n\n    #  predicted_csv, model, feature_names, output_csv,\n    \n    # Load the predictions CSV\n    predicted_csv = f\"{output_dir}/test_data_predicted_latest_{target_date}.csv_snodas_mask.csv\"\n    try:\n        print(f\"Loading predictions from {predicted_csv}...\")\n        predictions_df = pd.read_csv(predicted_csv)\n    except FileNotFoundError:\n        print(f\"Error: Predicted CSV file not found at {predicted_csv}.\")\n        return\n    \n    # Find the closest row to the specified latitude and longitude\n    print(f\"Finding closest row to (lat: {lat}, lon: {lon})...\")\n\n    # Calculate the squared distance\n    predictions_df['distance'] = ((predictions_df['lat'] - lat) ** 2 + (predictions_df['lon'] - lon) ** 2)\n\n    # Find the row with the minimum distance\n    closest_row_index = predictions_df['distance'].idxmin()\n    closest_row = predictions_df.loc[closest_row_index]\n\n    # Print the index and the row details\n    print(f\"Closest row index: {closest_row_index}\")\n    print(f\"Closest row details: {closest_row.to_dict()}\")\n\n    # Get the right feature names, minus the target column\n    feature_names = [col for col in selected_columns if col != \"swe_value\"]\n\n    # Extract the input features for SHAP\n    input_data = closest_row[feature_names].to_frame().T\n    # print(\"input data columns: \", input_data.columns)\n    # print(\"feature names: \", feature_names)\n\n    # Load model\n    model = load_model(model_path)\n\n    output_csv = f\"{output_dir}/eai_et_model_{lat}_{lon}_{target_date}.csv_snodas_mask.csv\"\n\n    # Explain the prediction for this specific row\n    explain_predictions(\n        model=model,\n        input_data=input_data,\n        feature_names=feature_names,\n        output_csv=output_csv,\n        name = f\"{lat}_{lon}_{target_date}\",\n        plot_path=f\"{plot_dir}/eai_plot_{lat}_{lon}_{target_date}.png\",\n    )\n\n    print(f\"Explanation completed for location (lat: {lat}, lon: {lon}) on {target_date}.\")\n\n\nif __name__ == \"__main__\":\n    #plot_feature_importance()  # no need, this step is already done in the model post processing step. \n    # interpret_prediction()\n    # plot_model()\n\n    explain_predictions_for_latlon(\n        41.742627, -102.255249,\n        target_date = \"2025-01-15\",\n    )\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "9c573m",
  "name" : "train_test_pattern_compare",
  "description" : null,
  "code" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom model_creation_et import selected_columns\n\n\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    # data['date'] = pd.to_datetime(data['date'])\n    # reference_date = pd.to_datetime('1900-01-01')\n    # data['date'] = (data['date'] - reference_date).dt.days\n    # data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    print(data.describe())\n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"Slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    # data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\nimport traceback\n\ndef compare(\n    target_date: str = \"test_date\", \n    train_csv_path: str = None, \n    test_csv_path: str = None, \n    plot_path: str = None\n):\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    print(\"Starting the comparison process...\")\n    try:\n        # Set default paths if not provided\n        if test_csv_path is None:\n            test_csv_path = f'{work_dir}/testing_all_ready_for_check.csv'\n        if train_csv_path is None:\n            train_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        if plot_path is None:\n            plot_path = f'{work_dir}/var_comparison/{target_date}_final_comparison.png'\n\n        print(f\"Training CSV Path: {train_csv_path}\")\n        print(f\"Testing CSV Path: {test_csv_path}\")\n        print(f\"Plot Output Path: {plot_path}\")\n\n        # Read and clean training data\n        print(\"Reading and cleaning training data...\")\n        tr_df = pd.read_csv(train_csv_path)\n        tr_df = clean_train_df(tr_df)\n        tr_df = tr_df[selected_columns+[\"lat\", \"lon\"]]\n        print(f\"Training DataFrame loaded with shape: {tr_df.shape}\")\n\n        # Read testing data\n        print(\"Reading testing data...\")\n        te_df = pd.read_csv(test_csv_path)\n        selected_columns.remove(\"swe_value\")\n        test_features = selected_columns+[\"lat\", \"lon\"]\n        te_df = te_df[test_features]\n        print(f\"Testing DataFrame loaded with shape: {te_df.shape}\")\n\n        # Convert testing data to numeric\n        print(\"Converting testing data to numeric...\")\n        te_df = te_df.apply(pd.to_numeric, errors='coerce')\n        print(f\"Testing DataFrame statistics:\\n{te_df.describe()}\")\n\n        print(f\"Training columns: {list(tr_df.columns)}\")\n        print(f\"Testing columns: {list(te_df.columns)}\")\n\n        # Ensure output directory exists\n        print(\"Ensuring the output directory exists...\")\n        var_comparison_plot_path = os.path.dirname(plot_path)\n        if not os.path.exists(var_comparison_plot_path):\n            os.makedirs(var_comparison_plot_path)\n            print(f\"Created directory: {var_comparison_plot_path}\")\n\n        # Prepare subplot dimensions\n        print(\"Preparing subplot dimensions...\")\n        num_cols = len(tr_df.columns)\n        new_num_cols = int(num_cols**0.5)  # Square grid\n        new_num_rows = int(num_cols / new_num_cols) + 1\n        print(f\"Subplot grid: {new_num_rows} rows x {new_num_cols} columns\")\n\n        # Create a figure with multiple subplots\n        print(\"Creating subplots...\")\n        fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n        axs = axs.flatten()\n\n        # Iterate over columns and create plots\n        print(\"Generating plots for each column...\")\n        for i, col in enumerate(tr_df.columns):\n            print(f\"Processing column {i + 1}/{num_cols}: {col}\")\n            try:\n                # Filter out 0 and -999 from training and testing data\n                train_filtered = tr_df[col][~tr_df[col].isin([0, -999])]\n                test_filtered = te_df[col][~te_df[col].isin([0, -999])] if col in te_df.columns else None\n\n                # Plot the filtered data\n                axs[i].hist(train_filtered, bins=100, alpha=0.5, color='blue', label='Train')\n                if test_filtered is not None:\n                    axs[i].hist(test_filtered, bins=100, alpha=0.5, color='red', label='Test')\n                else:\n                    print(f\"Warning: Column '{col}' not found in testing data.\")\n\n                axs[i].set_title(f'{col}')\n                axs[i].legend()\n            except Exception as e:\n                print(f\"Error processing column '{col}': {e}\")\n                traceback.print_exc()\n\n        # Adjust layout and save the plot\n        print(\"Adjusting layout and saving the plot...\")\n        plt.tight_layout()\n        plt.savefig(plot_path)\n        plt.close()\n        print(f\"Comparison plot saved at: {plot_path}\")\n\n    except Exception as e:\n        print(f\"Error during comparison process: {e}\")\n        traceback.print_exc()\n\n    print(\"Comparison process completed.\")\n\n\ndef calculate_feature_colleration_in_training():\n#   training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    \n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n  \n\nif __name__ == \"__main__\":\n    training_data_path = f\"{homedir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n    target_date = \"2025-01-01\"\n    test_csv_path = f'{work_dir}/test_data_predicted_latest_{target_date}.csv'\n    plot_path = f\"{homedir}/../plots/all_var_comparison_{target_date}.png\"\n    compare(\n        target_date=target_date, \n        train_csv_path=training_data_path, \n        test_csv_path=test_csv_path, \n        plot_path=plot_path\n    )\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ee5ur4",
  "name" : "correct_slope",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom snowcast_utils import work_dir\nfrom scipy.spatial import KDTree\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\n\n\n\ndef replace_slope(row, tree, dem_df):\n    '''\n    Replace the 'slope' column in the input DataFrame row with the closest slope value from the DEM data.\n\n    Args:\n        row (pandas.Series): A row of data containing 'lat' and 'lon' columns.\n        tree (scipy.spatial.KDTree): KDTree built from DEM data.\n        dem_df (pandas.DataFrame): DataFrame containing DEM data.\n\n    Returns:\n        float: The closest slope value from the DEM data for the given latitude and longitude.\n    '''\n    # print(\"row = \", row)\n    target_lat = row[\"lat\"]\n    target_lon = row[\"lon\"]\n    _, idx = tree.query([(target_lat, target_lon)])\n    closest_row = dem_df.iloc[idx[0]]\n    return closest_row[\"Slope\"]\n\ndef parallelize_slope_correction(ready_csv_path, dem_slope_csv_path, new_result_csv_path):\n    train_ready_df = pd.read_csv(ready_csv_path)\n    dem_slope_df = pd.read_csv(dem_slope_csv_path)\n\n    print(train_ready_df.head())\n    print(dem_slope_df.head())\n\n    print(\"all train.csv columns: \", train_ready_df.columns)\n    print(\"all dem slope columns: \", dem_slope_df.columns)\n    \n    # Build KDTree for DEM data\n    tree = KDTree(dem_slope_df[['Latitude', 'Longitude']].values)\n    \n    print(\"deduplicate the training point lat/lon\")\n    print(\"train_ready_df.shape = \", train_ready_df.shape)\n    lat_lon_df = train_ready_df[['lat', 'lon']].drop_duplicates()\n\n    print(\"lat_lon_df.shape\", lat_lon_df.shape)\n    # Apply the 'replace_slope' function to calculate and replace slope values in the DataFrame\n    print(\"start to correct slope\")\n    #train_ready_df['corrected_slope'] = train_ready_df.apply(replace_slope, args=(tree, dem_slope_df), axis=1)\n    \n    lat_lon_df['corrected_slope'] = lat_lon_df.apply(replace_slope, args=(tree, dem_slope_df), axis=1)\n  \n    train_ready_df = train_ready_df.merge(lat_lon_df, on=['lat', 'lon'], how='left')\n    \n    print(\"The new train_ready_df.shape with corrected slope is \", train_ready_df.shape)\n\n    print(\"finished correcting slope\")\n    print(train_ready_df.head())\n    print(train_ready_df.columns)\n    print(f\"saving the correct data into {new_result_csv_path}\")\n    # Save the modified DataFrame to a new CSV file\n    train_ready_df.to_csv(new_result_csv_path, index=False)\n    print(f\"The new slope corrected dataframe is saved to {new_result_csv_path}\")\n    \n  \n  \nif __name__ == \"__main__\":\n    # ready_csv_path = f'{work_dir}/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_sorted.csv'\n    # dem_slope_csv_path = f\"{work_dir}/slope_file.tif.csv\"\n    # print(f\"ready_csv_path = {ready_csv_path}\")\n    # new_result_csv_path = f'{work_dir}/final_merged_data_4yrs_snotel_ghcnd.csv_sorted_slope_corrected.csv'\n    # parallelize_slope_correction(ready_csv_path, dem_slope_csv_path, new_result_csv_path)\n\n    ready_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted.csv'\n    dem_slope_csv_path = f\"{work_dir}/slope_file.tif.csv\"\n    print(f\"ready_csv_path = {ready_csv_path}\")\n    new_result_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv'\n    parallelize_slope_correction(ready_csv_path, dem_slope_csv_path, new_result_csv_path)\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "f03i7p",
  "name" : "convert_to_time_series",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom snowcast_utils import work_dir\nimport shutil\nimport numpy as np\nimport dask.dataframe as dd\n\n# Set Pandas options to display all columns\npd.set_option('display.max_columns', None)\n\n\ndef array_describe(arr):\n    \"\"\"\n    Calculate descriptive statistics for a given NumPy array.\n\n    Args:\n        arr (numpy.ndarray): The input array for which statistics are calculated.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics such as Mean, Median, Standard Deviation, Variance, Minimum, Maximum, and Sum.\n    \"\"\"\n    stats = {\n        'Mean': np.mean(arr),\n        'Median': np.median(arr),\n        'Standard Deviation': np.std(arr),\n        'Variance': np.var(arr),\n        'Minimum': np.min(arr),\n        'Maximum': np.max(arr),\n        'Sum': np.sum(arr),\n    }\n    return stats\n\ndef interpolate_missing_inplace(df, column_name, degree=3):\n    x = df.index\n    y = df[column_name]\n\n    # Create a mask for missing values\n    if column_name == \"SWE\":\n      mask = (y > 240) | y.isnull()\n    elif column_name == \"fsca\":\n      y = y.replace([225, 237, 239], 0)\n      y[y < 0] = 0\n      mask = (y > 100) | y.isnull()\n    else:\n      mask = y.isnull()\n\n    # Check if all elements in the mask array are True\n    all_true = np.all(mask)\n\n    if all_true:\n      df[column_name] = 0\n    else:\n      # Perform interpolation\n      new_y = np.interp(x, x[~mask], y[~mask])\n      # Replace missing values with interpolated values\n      df[column_name] = new_y\n\n    if np.any(df[column_name].isnull()):\n      raise ValueError(\"Single group: shouldn't have null values here\")\n        \n    return df\n\ndef convert_to_time_series(input_csv, output_csv, force=False):\n    \"\"\"\n    Convert the data from the ready CSV file into a time series format.\n\n    This function reads the cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation, and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n    \"\"\"\n    \n    columns_to_be_time_series = [\"SWE\", \n                                 'air_temperature_tmmn',\n                                 'potential_evapotranspiration', \n                                 'mean_vapor_pressure_deficit',\n                                 'relative_humidity_rmax', \n                                 'relative_humidity_rmin',\n                                 'precipitation_amount', \n                                 'air_temperature_tmmx', \n                                 'wind_speed',\n                                 'fsca']\n\n    # Read the cleaned ready CSV\n    df = pd.read_csv(input_csv, dtype={'station_name': 'object'})\n    df.sort_values(by=['lat', 'lon', 'date'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    # rename all columns to unified names\n    #     ['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n# 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n# 'fsca']\n    df.rename(columns={'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                        }, inplace=True)\n    \n    filled_csv = f\"{output_csv}_gap_filled.csv\"\n    if os.path.exists(filled_csv) and not force:\n        print(f\"{filled_csv} already exists, skipping\")\n        filled_data = pd.read_csv(filled_csv)\n    else:\n        # Function to perform polynomial interpolation and fill in missing values\n        def process_group_filling_value(group):\n          # Sort the group by 'date'\n          group = group.sort_values(by='date')\n      \n          for column_name in columns_to_be_time_series:\n            group = interpolate_missing_inplace(group, column_name)\n          # Return the processed group\n          return group\n        # Group the data by 'lat' and 'lon' and apply interpolation for each column\n        print(\"Start to fill in the missing values\")\n        grouped = df.groupby(['lat', 'lon'])\n        filled_data = grouped.apply(process_group_filling_value).reset_index(drop=True)\n    \n\n        if any(filled_data['SWE'] > 240):\n          raise ValueError(\"Error: shouldn't have SWE>240 at this point\")\n\n        filled_data.to_csv(filled_csv, index=False)\n        \n        print(f\"New filled values csv is saved to {filled_csv}\")\n    \n    if os.path.exists(output_csv) and not force:\n        print(f\"{output_csv} already exists, skipping\")\n    else:\n        df = filled_data\n        # Create a new DataFrame to store the time series data for each location\n        print(\"Start to create the training csv with previous 7 days columns\")\n        result = pd.DataFrame()\n\n        # Define the number of days to consider (7 days in this case)\n        num_days = 7\n\n        grouped = df.groupby(['lat', 'lon'])\n        \n        def process_group_time_series(group, num_days):\n          group = group.sort_values(by='date')\n          for day in range(1, num_days + 1):\n            for target_col in columns_to_be_time_series:\n              new_column_name = f'{target_col}_{day}'\n              group[new_column_name] = group[target_col].shift(day)\n              \n          return group\n        \n        result = grouped.apply(lambda group: process_group_time_series(group, num_days)).reset_index(drop=True)\n        result.fillna(0, inplace=True)\n        \n        result.to_csv(output_csv, index=False)\n        print(f\"New data is saved to {output_csv}\")\n        shutil.copy(output_csv, backup_time_series_csv_path)\n        print(f\"File is backed up to {backup_time_series_csv_path}\")\n\ndef add_cumulative_columns(input_csv, output_csv, force=False):\n    \"\"\"\n    Add cumulative columns to the time series dataset.\n\n    This function reads the time series CSV file created by `convert_to_time_series`, calculates cumulative values for specific columns, and saves the data to a new CSV file.\n    \"\"\"\n    \n    columns_to_be_cumulated = [\n      \"SWE\",\n      'air_temperature_tmmn',\n      'potential_evapotranspiration', \n      'mean_vapor_pressure_deficit',\n      'relative_humidity_rmax', \n      'relative_humidity_rmin',\n      'precipitation_amount', \n      'air_temperature_tmmx', \n      'wind_speed',\n      'fsca'\n    ]\n\n    # Read the time series CSV (ensure it was created using `convert_to_time_series` function)\n    # directly read from original file\n    df = pd.read_csv(input_csv, dtype={'station_name': 'object'})\n    print(\"the column statistics from time series before cumulative: \", df.describe())\n    \n    df['date'] = pd.to_datetime(df['date'])\n    \n    unique_years = df['date'].dt.year.unique()\n    print(\"This is our unique years\", unique_years)\n    #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n    \n    # only start from the water year 10-01\n    # Filter rows based on the date range (2019 to 2022)\n    start_date = pd.to_datetime('2018-10-01')\n    end_date = pd.to_datetime('2021-09-30')\n    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n    print(\"how many rows are left in the three water years?\", df.describe())\n    df.to_csv(f\"{current_ready_csv_path}.test_check.csv\")\n\n    # Define a function to calculate the water year\n    def calculate_water_year(date):\n        year = date.year\n        if date.month >= 10:  # Water year starts in October\n            return year + 1\n        else:\n            return year\n    \n    # every water year starts at Oct 1, and ends at Sep 30. \n    df['water_year'] = df['date'].apply(calculate_water_year)\n    \n    # Group the DataFrame by 'lat' and 'lon'\n    grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n    print(\"how many groups? \", grouped)\n    \n    grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n    for column in columns_to_be_cumulated:\n        df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n    print(\"This is the dataframe after cumulative columns are added\")\n    print(df[\"cumulative_fsca\"].describe())\n    \n    df.to_csv(output_csv, index=False)\n    \n    print(f\"All the cumulative variables are added successfully! {target_time_series_cumulative_csv_path}\")\n    print(\"double check the swe_value statistics:\", df[\"swe_value\"].describe())\n\n    \ndef assign_zero_swe_value_to_all_fsca_zero_rows(na_filled_csv, non_station_zero_csv, force=False):\n    \n    # Define the conditions\n    condition_column = 'fsca'\n    target_column = 'swe_value'\n    values_to_check = [0, 225, 237, 239]\n    \n    \n    df = pd.read_csv(na_filled_csv, dtype={'station_name': 'object'})\n    if target_column not in df.columns:\n      df[target_column] = 0\n    empty_count = df[target_column].isnull().values.ravel().sum()\n    \n    print(f\"The empty number of rows are {empty_count} before filling in\")\n    print(\"double check the swe_value statistics before filling in:\", df[\"swe_value\"].describe())\n    \n    rows_less_than_zero = (df[target_column] < 0).sum()\n    print(\"Number of rows where '{}' is less than 0: {}\".format(target_column, rows_less_than_zero))\n    \n\n    # Mask the target column where the condition is met\n    df[target_column] = df[target_column].mask(\n        (df[target_column].isna()) & df[condition_column].isin(values_to_check),\n        0\n    )\n    \n    empty_count = df[target_column].isnull().values.ravel().sum()\n    \n    print(f\"The empty number of rows are {empty_count} after filling in\")\n    \n    print(\"total dataframe row number : \", len(df))\n    \n    df.to_csv(non_station_zero_csv, index=False)\n    \n    print(f\"The rows without snotel but fsca is zero or land or water or ocean are set to 0! {non_station_zero_csv}\")\n    print(\"double check the swe_value statistics after filling in:\", df[\"swe_value\"].describe())\n    \n    \ndef clean_non_swe_rows(current_ready_csv_path, cleaned_csv_path, force=False):\n    # Read Dask DataFrame from CSV\n    dask_df = dd.read_csv(current_ready_csv_path, dtype={'station_name': 'object'})\n\n    # Remove rows where 'swe_value' is empty\n    dask_df_filtered = dask_df.dropna(subset=['swe_value'])\n\n    # Save the result to a new CSV file\n    dask_df_filtered.to_csv(cleaned_csv_path, index=False, single_file=True)\n    print(\"dask_df_filtered.shape = \", dask_df_filtered.shape)\n    print(f\"The filtered csv with no swe values is saved to {cleaned_csv_path}\")\n\ndef rename_corrected_slope(corrected_slope_path, renamed_slope_path, force=False):\n    df = pd.read_csv(corrected_slope_path, dtype={'station_name': 'object'})\n    df.drop(columns=['Slope'], inplace=True)\n\t# Rename 'column_to_rename' to 'old_column'\n    df.rename(columns={'corrected_slope': 'Slope'}, inplace=True)\n    df.to_csv(renamed_slope_path, index=False)\n    print(\"dask_df.shape = \", df.shape)\n    print(f\"The log10 file is saved to {renamed_slope_path}\")\n    \ndef log10_all_fields(cleaned_csv_path, logged_csv_path, force=False):\n    print(\"convert all cumulative columns into log10\")\n    # Read Dask DataFrame from CSV\n    df = pd.read_csv(cleaned_csv_path, dtype={'station_name': 'object'})\n    \n    # Get columns with \"cumulative\" in their names\n    for col in df.columns:\n        print(\"Checking \", col)\n        if \"cumulative\" in col:\n\t        # Apply log10 transformation to selected columns\n            df[col] = np.log10(df[col] + 0.1)  # Adding 1 to avoid log(0)\n            print(f\"converted {col} to log10\")\n\n    df.to_csv(logged_csv_path, index=False)\n    print(\"dask_df.shape = \", df.shape)\n    print(f\"The log10 file is saved to {logged_csv_path}\")\n\n\ndef merge_all_training_csvs(large_training_csv, small_training_csv, final_all_point_training_csv):\n  # Load the first CSV file\n  df1 = pd.read_csv(large_training_csv)\n\n  # Load the second CSV file\n  df2 = pd.read_csv(small_training_csv)\n\n  # Identify columns missing in the second CSV and add them with zeros\n  missing_columns = set(df1.columns) - set(df2.columns)\n  print(\"missing_columns = \", missing_columns)\n  for col in missing_columns:\n      df2[col] = 0\n\n  # Reorder columns in the second CSV to match the first CSV\n  df2 = df2[df1.columns]\n\n  # Merge the two dataframes by appending rows of df2 to df1\n  df_merged = pd.concat([df1, df2], ignore_index=True)\n\n  # Remove rows that contain at least one zero in any column\n  # df_filtered = df_merged[df_merged['Elevation'] != 0] # should we remove ocean?\n\n  # Save the merged dataframe to a new CSV file\n  df_merged.to_csv(final_all_point_training_csv, index=False)\n  print(f\"merged training samples are saved to {final_all_point_training_csv}\")\n\n\nif __name__ == \"__main__\":\n\n  # Define file paths for various CSV files\n  # current_ready_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  current_ready_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv'\n  non_station_counted_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_fill_empty_snotel.csv'\n  cleaned_csv_path = f\"{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_cleaned.csv\"\n  target_time_series_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_time_series.csv'\n  backup_time_series_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_time_series_backup.csv'\n  # target_time_series_cumulative_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  target_time_series_cumulative_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_cumulative.csv'\n  slope_renamed_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_slope_renamed.csv'\n  logged_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_4yrs_all_cols_log10.csv'\n  deduplicated_csv_path = f'{work_dir}/salt_pepper_points_for_training.csv_all_training.csv_sorted_slope_corrected.csv_deduplicated_training_points_final.csv'\n\n  merged_all_training_csv_path = f\"{work_dir}/all_points_final_merged_training.csv\"\n    \n  # filling the non station rows with fsca indicating no snow\n  assign_zero_swe_value_to_all_fsca_zero_rows(current_ready_csv_path, non_station_counted_csv_path, force=True)\n\n  # remove the empty swe_value rows first\n  clean_non_swe_rows(non_station_counted_csv_path, cleaned_csv_path, force=True)\n\n  # Uncomment this line to execute the 'convert_to_time_series' function\n  convert_to_time_series(cleaned_csv_path, target_time_series_csv_path, force=True)\n\n  # Uncomment this line to execute the 'add_cumulative_columns' function\n  add_cumulative_columns(target_time_series_csv_path, target_time_series_cumulative_csv_path, force=True)\n\n  # Rename the corrected slope to slope\n  rename_corrected_slope(target_time_series_cumulative_csv_path, slope_renamed_path, force=True)\n\n  # convert all cumulative columns to log10\n  log10_all_fields(slope_renamed_path, logged_csv_path, force=True)\n\n  df = pd.read_csv(logged_csv_path, dtype={'station_name': 'object'})\n  print(\"the number of the total rows: \", len(df))\n\n  deduplicated_df = df.drop_duplicates(subset=['lat', 'lon', 'date'])\n  # Export the deduplicated DataFrame to a CSV file\n  deduplicated_df.to_csv(deduplicated_csv_path, index=False)\n  print(f\"deduplicated_df.to_csv('{deduplicated_csv_path}', index=False)\")\n  merge_all_training_csvs(f\"{work_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\", deduplicated_csv_path, merged_all_training_csv_path)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "83d2yv",
  "name" : "data_merge_hackweek",
  "description" : null,
  "code" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \n\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  print(f\"the snotel covered area: {snotel_covered_len}\")\n  old_train_df.drop([\"pmv\", \"SnowClass\", \"cumulative_fSCA\"], axis=1, inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  print(f\"len of no snow dataframe: {len(chosen_no_snow_row_df)}\")\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\nadd_no_snow_data_points()\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "j8swco",
  "name" : "data_gee_smap_station_only",
  "description" : null,
  "code" : "# for Bonan to work on pulling the SMAP data for training and testing points\n\n\n\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import work_dir\n\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{work_dir}/testing_points.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_names = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'lat', 'lon'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      #current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      #print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{work_dir}/{org_name}_{column_name}_{ind}.csv\"\n\n#       if os.path.exists(single_csv_file):\n#           print(\"exists skipping..\")\n#           continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      #poi = ee.Geometry.Point(longitude, latitude)\n      viirs = ee.ImageCollection(product_name).filterDate('2017-10-01','2018-07-01').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      #df['cell_id'] = current_cell_id\n      df['lat'] = latitude\n      df['lon'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nprint(all_cell_df.head())\nprint(all_cell_df[\"s1_grd_vv\"].describe())\nall_cell_df.to_csv(f\"{work_dir}/Sentinel1_Testing.csv\")\nprint(\"The Sentinel 1 is downloaded successfully. \")\n\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pnr64x",
  "name" : "data_merge_hackweek_testing",
  "description" : null,
  "code" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n\ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "qg80lj",
  "name" : "training_sanity_check",
  "description" : null,
  "code" : "# Do sanity checks on the training.csv to make sure all the columns' vales are extracted correctly\nfrom snowcast_utils import work_dir\nimport pandas as pd\nfrom gridmet_testing import download_gridmet_of_specific_variables\nfrom datetime import datetime\nimport xarray as xr\nfrom datetime import date\nimport numpy as np\n\n\n# pick the final training csv\ncurrent_training_csv_path = f'{work_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_time_series_cumulative_v1.csv'\ndf = pd.read_csv(current_training_csv_path)\n\nprint(\"all the current columns: \", df.columns)\n\n# choose several random sample rows for sanity checks\nsample_size = 10\nrandom_sample = df.sample(n=sample_size)\n\nprint(random_sample)\n\n# all the current columns: Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n# 'Unnamed: 0', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n# 'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n# 'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n# 'relative_humidity_rmin_1', 'precipitation_amount_1',\n# 'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n# 'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n# 'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n# 'relative_humidity_rmin_2', 'precipitation_amount_2',\n# 'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n# 'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n# 'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n# 'relative_humidity_rmin_3', 'precipitation_amount_3',\n# 'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n# 'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n# 'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n# 'relative_humidity_rmin_4', 'precipitation_amount_4',\n# 'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n# 'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n# 'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n# 'relative_humidity_rmin_5', 'precipitation_amount_5',\n# 'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n# 'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n# 'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n# 'relative_humidity_rmin_6', 'precipitation_amount_6',\n# 'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n# 'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n# 'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n# 'relative_humidity_rmin_7', 'precipitation_amount_7',\n# 'air_temperature_tmmx_7', 'wind_speed_7'],\n\n\ndef check_gridmet(row):\n  # check air_temperature_tmmn, precipitation_amount\n  date_value = row[\"date\"]\n  lat = row[\"lat\"]\n  lon = row[\"lon\"]\n  # Specify the format of the date string\n  date_format = '%Y-%m-%d'\n\n  # Convert the date string to a date object\n  date_object = datetime.strptime(date_value, date_format).date()\n  yearlist = [date_object.year]\n  download_gridmet_of_specific_variables(yearlist)\n  \n  nc_file = f\"{work_dir}/gridmet_climatology/tmmn_{date_object.year}.nc\"\n  \n  dataset = xr.open_dataset(nc_file)\n  reference_date = date(1900, 1, 1)\n\n  # Calculate the difference in days\n  days_difference = (date_object - reference_date).days\n  \n  # Calculate the Euclidean distance between the target coordinate and all grid points\n  lat_diff = dataset['lat'].values - lat\n  lon_diff = dataset['lon'].values - lon\n  #distance = np.sqrt(lat_diff**2 + lon_diff**2)\n\n  # Find the indices (i, j) of the grid cell with the minimum distance\n  i = np.argmin(np.abs(lat_diff))\n  j = np.argmin(np.abs(lon_diff))\n  nearest_gridmet_lat = dataset['lat'][i]\n  nearest_gridmet_lon = dataset['lon'][j]\n  \n  selected_data = dataset.sel(day=date_value, lat=nearest_gridmet_lat, lon=nearest_gridmet_lon)\n  \n  tmmn_check_values = selected_data.air_temperature.values\n  \n  if str(tmmn_check_values) != str(row[\"air_temperature_tmmn\"]):\n    print(\"Failed sanity check. Gridmet doesn't match\")\n    exit(1)\n\ndef check_elevation(row):\n  lat = row[\"lat\"]\n  lon = row[\"lon\"]\n  \n  pass\n\ndef check_amsr(row):\n  \n  pass\n\ndef check_snow_cover_area(row):\n  pass\n\ndef check_passive_microwave(row):\n  pass\n\ndef check_snotel_cdec(row):\n  \n  pass\n\n\ndef check_observed_columns():\n  # Assuming 'work_dir' is the path to your working directory\n  dask_df = dd.read_csv(f\"{work_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv\")\n\n  # Print the shape and head of the Dask DataFrame\n  print(dask_df.shape[0].compute(), 'rows and', dask_df.shape[1].compute(), 'columns')\n  print(dask_df.head().compute())\n\n  # Count the number of empty rows in 'swe_value'\n  empty_rows_count = dask_df['swe_value'].isnull().sum().compute()\n  print(f\"Number of empty rows in 'swe_value': {empty_rows_count}\")\n\nif __name__ == \"__main__\":\n  # random_sample.apply(check_gridmet, axis=1)\n  # random_sample.apply(check_elevation, axis=1)\n  # random_sample.apply(check_amsr, axis=1)\n  # random_sample.apply(check_snow_cover_area, axis=1)\n  # random_sample.apply(check_passive_microwave, axis=1)\n#   random_sample.apply(check_snotel_cdec, axis=1)\n  check_observed_columns()\n\n  print(\"If it reaches here, everything is good. The training.csv passed all our sanity cheks!\")\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ggy7gf",
  "name" : "fSCA_training",
  "description" : null,
  "code" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nimport time\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import date_to_julian, work_dir, data_dir, train_start_date, train_end_date\n\n# change directory before running the code\nos.chdir(f\"{data_dir}/fsca/\")\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \"h12v04\", \"h12v05\",\n             \"h13v04\", \"h13v05\", \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nconda_bin = \"/home/chetana/miniconda/bin/\"\n\n\ndef get_env_var_for_gdalwarp():\n    # if \"PROJ_LIB\" in os.environ:\n    #     os.environ.pop(\"PROJ_LIB\")\n    #     print(f\"Environment variable PROJ_LIB removed.\")\n    os.environ[\"PROJ_LIB\"] = \"/home/chetana/miniconda/share/proj/\"\n\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n  hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n  # Specific subdataset name you're interested in\n  target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n  # Create a name for the output file based on the HDF file name and subdataset\n  output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n  output_path = os.path.join(output_folder, output_file_name)\n\n  # student 1 changed something here\n\n  if os.path.exists(output_path):\n    pass\n    #print(f\"The file {output_path} exists. skip.\")\n  else:\n    for subdataset in hdf_ds.GetSubDatasets():\n      # Check if the subdataset is the one we want to convert\n      if target_subdataset_name in subdataset[0]:\n        ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n        # Convert to GeoTIFF\n        gdal.Translate(output_path, ds)\n        ds = None\n        break  # Exit the loop after converting the target subdataset\n\n  hdf_ds = None\n\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n  file_lst = list()\n  for file in os.listdir(folder_path):\n    file_lst.append(file)\n    if file.lower().endswith(\".hdf\"):\n      hdf_file = os.path.join(folder_path, file)\n      convert_hdf_to_geotiff(hdf_file, output_folder)\n      #print(f\"Converted {file} to GeoTIFF\")\n  return file_lst\n\n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = [f'{conda_bin}gdal_translate', '-b', '1', '-outsize', '100%', '100%', '-scale', '0', '255', '200', '200', f\"{modis_day_wise}/fsca_template.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    gdal_command = [f'{conda_bin}gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = [f'{conda_bin}gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [f\"{conda_bin}gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        try:\n          earthaccess.login(strategy=\"netrc\")\n          results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                            cloud_hosted=True, \n                                            bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                            temporal=(current_date, current_date))\n          earthaccess.download(results, input_folder)\n          print(\"done with downloading, start to convert HDF to geotiff..\")\n        except Exception as e:\n          print(f\"Failed to download {granule['title']}: {e}\")\n          traceback.print_exc()\n          print(f\"Unauthorized error for {granule['title']}. Skipping and waiting 5 seconds...\")\n          time.sleep(5)\n\n\n  convert_all_hdf_in_folder(input_folder, output_folder)\n  print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n  print(\"Merging tifs of one date into one file..\")\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\n    \nif __name__ == \"__main__\":\n  download_tiles_and_merge(train_start_date, train_end_date)\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c2qa9u",
  "name" : "fsCA_testing",
  "description" : null,
  "code" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, data_dir, test_start_date, test_end_date, date_to_julian, cumulative_mode, process_dates_in_range\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\nimport os\nos.environ[\"OPENBLAS_NUM_THREADS\"] = \"4\"\nfrom osgeo import gdal\ngdal.UseExceptions()  # Enable exceptions\n\n# change directory before running the code\nos.chdir(f\"{data_dir}/fsca/\")\n\n# Set the PROJ_LIB environment variable\nenv = os.environ.copy()  # Get the current environment variables\nenv['PROJ_LIB'] = '/home/geo2021/anaconda3/share/proj'  # Set the path to PROJ_LIB\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n# @dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder, datestr):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\") and datestr in file:\n              print(\"Converting \", file)\n              hdf_file = os.path.join(folder_path, file)\n              convert_hdf_to_geotiff(hdf_file, output_folder)\n                # delayed_tasks.append(task)\n\n\n    # results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list\n\ndef get_env_var_for_gdalwarp():\n    # if \"PROJ_LIB\" in os.environ:\n    #     os.environ.pop(\"PROJ_LIB\")\n    #     print(f\"Environment variable PROJ_LIB removed.\")\n    os.environ[\"PROJ_LIB\"] = \"/home/geo2021/anaconda3/share/proj/\"\n\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  count = 0\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    print(\"Processing \", current_date)\n    \n    if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                        cloud_hosted=False, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(current_date, current_date))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder, i.strftime(\"%Y%j\"))\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(\n        folder_path=output_folder, \n        target_date = current_date, \n        output_file=target_output_tif\n      )\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing(target_date = test_start_date):\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get target_date = \", target_date)\n  end_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  # only deal with 1 day\n  start_date = end_date\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  count = 0\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  if cumulative_mode:\n    add_time_series_columns(start_date, end_date, force=True)\n  else:\n    cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n    shutil.copy(outfile, cumulative_file_path)\n    print(f\"File is backed up to {cumulative_file_path}\")\n\ndef fsca_callback(current_date):\n    # Prepare the cumulative history CSVs for the current date\n    print(\"Getting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n    extract_data_for_testing(target_date=current_date.strftime(\"%Y-%m-%d\"))\n  \n\nif __name__ == \"__main__\":\n  # SnowCover is missing from 10-12 to 10-23\n  # download_tiles_and_merge(datetime.strptime(\"2022-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-01\", \"%Y-%m-%d\"))\n  \n  process_dates_in_range(\n    start_date=test_start_date,\n    end_date=test_end_date,\n    days_look_back=7,\n    callback=fsca_callback,\n  )\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  \n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "lnrsop",
  "name" : "fsca_py",
  "description" : null,
  "code" : "import requests\nfrom bs4 import BeautifulSoup\nimport os\nimport subprocess\nimport csv\nfrom datetime import datetime, timedelta\nfrom snowcast_utils import homedir\n\nmod_folder = f\"{homedir}/../fsca/\"\noutput_csv_file = f\"{homedir}/../fsca/mod10a1_snow_cover.csv\"\n\n# Function to extract snow cover value at a given lat lon\ndef extract_snow_cover_value(geotiff_path, lon, lat):\n    gdallocationinfo_cmd = [\n        \"gdallocationinfo\",\n        \"-valonly\",\n        geotiff_path,\n        str(lon),\n        str(lat)\n    ]\n    result = subprocess.run(gdallocationinfo_cmd, stdout=subprocess.PIPE, text=True)\n    return result.stdout.strip()\n\n# Define the date range for data extraction\nstart_date = datetime(2018,1, 1)\nend_date = datetime(2018, 1, 3)\n\n# Load the CSV file with latitude and longitude coordinates\ncsv_file_path = f\"{homedir}/../code/SnowCast/data/ready_for_training/station_cell_mapping.csv\"\n\n# CSV file header\ncsv_header = [\"Date\", \"Latitude\", \"Longitude\", \"Snow Cover Value\"]\n\n# Set the PROJ_LIB environment variable\nenv = os.environ.copy()  # Get the current environment variables\nenv['PROJ_LIB'] = '/home/geo2021/anaconda3/share/proj'  # Set the path to PROJ_LIB\n\n# Loop through the date range\ncurrent_date = start_date\nwhile current_date <= end_date:\n    extracted_data = list()\n    # URL and reference link with dynamic date\n    date_str = current_date.strftime(\"%Y.%m.%d\")\n    url = f\"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A1.061/{date_str}/\"\n    reference_link = f\"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A1.061/{date_str}/\"\n    \n    # Send an HTTP GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful (HTTP status code 200)\n    if response.status_code == 200:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        \n        # Find all <a> tags (links) on the page\n        links = soup.find_all(\"a\")\n        \n        # Filter the links to keep only those with the .hdf extension\n        all_hdf_files = [link.get(\"href\") for link in links if link.get(\"href\").endswith(\".hdf\")]\n        \n        # Define the sinusoidal tiles of interest\n        sinusoidal_tiles = [\"h08v04\", \"h08v05\", \n                            \"h09v04\", \"h09v05\", \n                            \"h10v04\", \"h10v05\", \n                            \"h11v04\", \"h11v05\", \n                            \"h12v04\", \"h12v05\", \n                            \"h13v04\", \"h13v05\", \n                            \"h15v04\", \"h16v03\", \n                            \"h16v04\"]\n        \n        # Filter the HDF files based on sinusoidal tiles\n        filtered_hdf_files = [hdf_file for hdf_file in all_hdf_files if any(tile in hdf_file for tile in sinusoidal_tiles)]\n        \n        # List to store the paths of converted GeoTIFF files\n        geotiff_files = []\n        \n        # Loop through the filtered HDF files and download/convert/delete them\n        for hdf_file in filtered_hdf_files:\n            # Construct the complete URL for the HDF file\n            hdf_url = reference_link + hdf_file\n            \n            # Define the local filename to save the HDF file\n            local_hdf_filename = os.path.join(mod_folder, hdf_file)  # Specify the directory where the file should be saved\n\n            # Check if the file already exists\n            if os.path.exists(local_hdf_filename):\n                print(f\"File {hdf_file} already exists, skipping download.\")\n            else:\n                # Send an HTTP GET request to download the HDF file\n                hdf_response = requests.get(hdf_url)\n                \n                if hdf_response.status_code == 200:\n                    # Save the content to the local file\n                    with open(local_hdf_filename, 'wb') as f:\n                        f.write(hdf_response.content)\n                    print(f\"Downloaded and saved {hdf_file}.\")\n                    \n                else:\n                    print(f\"Failed to download {hdf_file}, HTTP status code {hdf_response.status_code}.\")\n                    continue\n            \n\n            # Construct the output GeoTIFF file path and name in the same directory\n            local_geotiff_filename = os.path.splitext(local_hdf_filename)[0] + \".tif\"\n            \n            # Run the gdal_translate command to convert HDF to GeoTIFF\n            gdal_translate_cmd = [\n                \"gdal_translate\",\n                \"-of\", \"GTiff\",\n                f\"HDF4_EOS:EOS_GRID:{local_hdf_filename}:MOD_Grid_Snow_500m:NDSI_Snow_Cover\",\n                local_geotiff_filename\n            ]\n\n            # Execute the gdal_translate command\n            subprocess.run(gdal_translate_cmd, env=env, check=True)\n            \n            # Append the path of the converted GeoTIFF to the list\n            geotiff_files.append(local_geotiff_filename)\n            \n        \n        # Merge all the GeoTIFF files into a single GeoTIFF\n        merged_geotiff = \"merged_geotiff.tif\"\n        gdal_merge_cmd = [\n            \"gdal_merge.py\",\n            \"-o\", merged_geotiff,\n            \"-of\", \"GTiff\"\n        ] + geotiff_files\n        \n        subprocess.run(gdal_merge_cmd, env=env, check=True)\n        \n        print(f\"Merged all GeoTIFF files into {merged_geotiff}\")\n        \n        # Loop through the CSV file with latitude and longitude coordinates\n        with open(csv_file_path, \"r\") as csv_file:\n            csv_reader = csv.reader(csv_file)\n            next(csv_reader)  # Skip the header row\n            \n            for row in csv_reader:\n                lat = float(row[3])  # Assuming latitude is in the first column\n                lon = float(row[4])  # Assuming longitude is in the second column\n                \n                # Extract snow cover value\n                snow_cover_value = extract_snow_cover_value(merged_geotiff, lon, lat)\n                \n                # Append the extracted data to the list\n                extracted_data.append([date_str, lat, lon, snow_cover_value])\n        \n        # Delete the merged GeoTIFF file\n#         os.remove(merged_geotiff)\n        print(f\"Deleted {merged_geotiff}\")\n        \n        # Delete individual GeoTIFF files after a successful merge\n        #for geotiff_file in geotiff_files:\n        #    try:\n        #        os.remove(geotiff_file)\n        #        print(f\"Deleted {geotiff_file}\")\n        #    except Exception as e:\n        #        pass\n        \n        # Append the extracted data to the CSV file after each date\n        with open(output_csv_file, \"a\", newline=\"\") as csv_file:\n            csv_writer = csv.writer(csv_file)\n            csv_writer.writerows(extracted_data)\n        \n        print(f\"Extracted data appended to {output_csv_file}\")\n    \n    else:\n        print(\"Failed to fetch the HTML content.\")\n    \n    # Move to the next date\n    current_date += timedelta(days=1)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c8isgf",
  "name" : "fSCA_training_extract_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nimport rasterio\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport concurrent.futures\nfrom snowcast_utils import homedir, work_dir, train_start_date, train_end_date\nfrom datetime import datetime, timedelta\nimport dask.dataframe as dd\nimport numpy as np\nimport re\n\nfsca_working_dir = f\"{homedir}/fsca\"\nfolder_path = f\"{fsca_working_dir}/final_output/\"\nnew_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\ncell_to_modis_mapping = f\"{fsca_working_dir}/training_cell_to_modis_mapper_original_snotel_stations.csv\"\nnon_station_random_points_file = f\"{work_dir}/non_station_random_points_in_westus.csv\"\nonly_active_ghcd_station_in_west_conus_file = f\"{fsca_working_dir}/active_ghcnd_station_only_list.csv\"\nsalt_pepper_training_point_file = f\"{work_dir}/salt_pepper_points_for_training.csv\"\nghcd_station_to_modis_mapper_file = f\"{fsca_working_dir}/active_ghcnd_mapper_modis.csv\"\nall_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\nmodis_day_wise = f\"{fsca_working_dir}/final_output/\"\nos.makedirs(modis_day_wise, exist_ok=True)\n\n\ndef map_modis_to_station(row, src):\n  drow, dcol = src.index(row[\"lon\"], row[\"lat\"])\n  return drow, dcol\n\n\ndef generate_random_non_station_points():\n  # Load the GeoTIFF file\n  sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n  print(f\"loading geotiff {sample_modis_tif}\")\n  with rasterio.open(sample_modis_tif) as src:\n    # Get the raster metadata\n    bounds = src.bounds\n    transform = src.transform\n    width = src.width\n    height = src.height\n\n    # Read the raster values as a numpy array\n    raster_array = src.read(1)  # Assuming it's a single-band raster\n\n    # Generate random points\n    random_points = []\n    while len(random_points) < 4000:\n      # Generate random coordinates within the bounds of the raster\n      random_x = np.random.uniform(bounds.left, bounds.right)\n      random_y = np.random.uniform(bounds.bottom, bounds.top)\n\n      # Convert random coordinates to pixel coordinates\n      col, row = ~transform * (random_x, random_y)\n\n      # Ensure the generated pixel coordinates are within the raster bounds\n      if 0 <= row < height and 0 <= col < width:\n        # Get the value at the generated pixel coordinates\n        value = raster_array[int(row), int(col)]\n\n        # Check if the value is not 239\n        if value != 239 and value != 255:\n          # Append the coordinates to the list\n          random_points.append((random_x, random_y, col, row))\n\n    # Assuming random_points is a list of tuples where each tuple contains latitude and longitude\n    random_points = [(lat, lon, col, row) for lon, lat, col, row in random_points]  # Swap the order to (latitude, longitude)\n\n    # Create a DataFrame from the random_points list\n    random_points_df = pd.DataFrame(random_points, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n\n    # Save the DataFrame to a CSV file\n    random_points_df.to_csv(non_station_random_points_file, index=False)\n    print(f\"random points are saved to {non_station_random_points_file}\")\n\n    \n\n\ndef prepare_modis_grid_mapper_training():\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(cell_to_modis_mapping):\n    print(f\"The file {cell_to_modis_mapping} exists. skip.\")\n  else:\n    print(f\"start to generate {cell_to_modis_mapping}\")\n    station_df = pd.read_csv(new_base_station_list_file)\n    print(\"original station_df describe() = \", station_df.describe())\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"], \n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(cell_to_modis_mapping, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      \n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n\ndef merge_station_and_non_station_to_one_csv():\n  print(f\"new_base_station_list_file = {new_base_station_list_file}\")\n  print(f\"cell_to_modis_mapping = {cell_to_modis_mapping}\")\n  print(f\"non_station_random_points_file = {non_station_random_points_file}\")\n  df1 = pd.read_csv(cell_to_modis_mapping)\n  df2 = pd.read_csv(non_station_random_points_file)\n  combined_df = pd.concat([df1, df2], ignore_index=True)\n  combined_df.to_csv(all_training_points_with_station_and_non_station_file, index=False)\n\n  print(f\"Combined CSV saved to {all_training_points_with_station_and_non_station_file}\")\n\ndef merge_snotel_ghcnd_station_to_one_csv():\n  print(f\"new_base_station_list_file = {new_base_station_list_file}\")\n  print(f\"cell_to_modis_mapping = {cell_to_modis_mapping}\")\n  print(f\"ghcnd_to_modis_mapping = {ghcd_station_to_modis_mapper_file}\")\n  df1 = pd.read_csv(cell_to_modis_mapping)\n  df2 = pd.read_csv(ghcd_station_to_modis_mapper_file)\n  combined_df = pd.concat([df1, df2], ignore_index=True)\n  combined_df.to_csv(all_training_points_with_snotel_ghcnd_file, index=False)\n\n  print(f\"Combined CSV saved to {all_training_points_with_snotel_ghcnd_file}\")\n\n\ndef prepare_training_points_to_modis_grid_mapper(training_points_csv):\n  new_mapper_file = f\"{training_points_csv}_modis_mapper.csv\"\n  if os.path.exists(new_mapper_file):\n    print(f\"The file {new_mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {new_mapper_file}\")\n    station_df = pd.read_csv(training_points_csv)\n\n    if 'Latitude' in station_df.columns:\n      station_df = station_df.rename(columns={\n        'Latitude': 'latitude',\n        'Longitude': 'longitude'\n      })\n    \n    print(\"original station_df describe() = \", station_df.describe())\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"],\n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(new_mapper_file, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      print(f\"the new mapper to the ghcnd is saved to {new_mapper_file}\")\n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n  return new_mapper_file\n\n\ndef prepare_ghcnd_station_mapping_training():\n  if os.path.exists(ghcd_station_to_modis_mapper_file):\n    print(f\"The file {ghcd_station_to_modis_mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {ghcd_station_to_modis_mapper_file}\")\n    station_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n    station_df = station_df.rename(columns={'Latitude': 'latitude', \n                                            'Longitude': 'longitude'})\n    print(\"original station_df describe() = \", station_df.describe())\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"],\n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(ghcd_station_to_modis_mapper_file, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      print(f\"the new mapper to the ghcnd is saved to {ghcd_station_to_modis_mapper_file}\")\n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n  \ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n  if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n    return None\n  row, col = src.index(lat, lon)\n  if (0 <= row < src.height) and (0 <= col < src.width):\n    return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n  else:\n    return None\n\ndef get_band_value(row, src):\n  if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n    # print(\"src.height = \", src.height, \" - \", row[\"modis_y\"])\n    # print(\"src.width = \", src.width, \" - \", row[\"modis_x\"])\n    # print(row)\n    valid_value =  src.read(1, \n                            window=((int(row[\"modis_y\"]),\n                                     int(row[\"modis_y\"])+1), \n                                    (int(row[\"modis_x\"]),\n                                     int(row[\"modis_x\"])+1)))\n    # print(\"valid_value[0,0] = \", valid_value[0,0])\n    return valid_value[0,0]\n  else:\n    return None\n          \ndef process_file(fsca_tif_file_path, new_point_to_modis_mapper_file, current_date_str, outfile):\n  print(f\"processing {fsca_tif_file_path}\")\n  station_df = pd.read_csv(new_point_to_modis_mapper_file)\n  # print(\"station_df.head() = \", station_df.head())\n\n  # Apply get_band_value for each row in the DataFrame\n  with rasterio.open(fsca_tif_file_path) as src:\n    # Apply get_band_value for each row in the DataFrame\n    # Get the affine transformation matrix\n    transform = src.transform\n\n    # Extract the spatial extent using the affine transformation\n    left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n    # Print the spatial extent\n    # print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n\n    station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    \n  # Prepare final data\n  station_df['date'] = current_date_str\n  station_df.to_csv(outfile, index=False, \n                    columns=['date', 'latitude', 'longitude', 'fsca'])\n  print(f\"Saved to csv: {outfile}\")\n\ndef get_date_str_from_path(file_path):\n  # Define the pattern to match the date (in the format YYYY-MM-DD)\n  pattern = r'_(\\d{4}-\\d{2}-\\d{2})_'\n\n  # Use re.search to find the date string in the file path\n  match = re.search(pattern, file_path)\n\n  # Extract the date string if the pattern is found\n  if match:\n      date_string = match.group(1)\n      print(\"Date string found:\", date_string)\n      return date_string\n  else:\n      print(\"No date string found in the path.\")\n      return None\n\n\ndef merge_csv(start_date, end_date, training_point_file):\n  import glob\n  # Find CSV files within the specified date range\n  training_point_filename = os.path.basename(training_point_file)\n  csv_files = glob.glob(folder_path + f'{training_point_filename}*_fsca.csv')\n  relevant_csv_files = []\n\n  for c in csv_files:\n    # Extract the date from the file name\n    # print(\"c = \", c)\n    file_name = os.path.basename(c)\n    date_str = get_date_str_from_path(file_name)  # Assuming the date is part of the file name\n    # print(\"date_str = \", date_str)\n    file_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n\n    # Check if the file date is within the specified range\n    if start_date <= file_date <= end_date:\n      relevant_csv_files.append(c)\n\n  # Initialize a Dask DataFrame\n  print(\"start to use dask to read all csv files\")\n  dask_df = dd.read_csv(relevant_csv_files)\n\n  # Save the merged DataFrame to a CSV file\n  output_file = f'{fsca_working_dir}/{training_point_filename}_fsca_training.csv'\n  # Write the Dask DataFrame to a single CSV file\n  print(f\"saving all csvs into one file: {output_file}\")\n  dask_df.to_csv(output_file, index=False, single_file=True)\n  #combined_df.to_csv(output_file, index=False)\n\n  #print(combined_df.describe())\n  print(f\"Merged data saved to {output_file}\")\n  \ndef main():\n  \n  start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n  \n  end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n  \n  # prepare_modis_grid_mapper_training()\n  # prepare_ghcnd_station_mapping_training()\n  new_mapper_file = prepare_training_points_to_modis_grid_mapper(training_points_csv = salt_pepper_training_point_file)\n\n  # running this function will generate a new set of random points\n  # generate_random_non_station_points()\n  #merge_station_and_non_station_to_one_csv()\n  #merge_snotel_ghcnd_station_to_one_csv()\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  training_points_filename = os.path.basename(salt_pepper_training_point_file)\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    #print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{training_points_filename}_{current_date}_fsca.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n      pass\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', new_mapper_file, current_date, outfile)\n  \n  merge_csv(start_date, end_date, salt_pepper_training_point_file)\n\nif __name__ == \"__main__\":\n  main()\n  print(\"fsca Data extraction complete.\")\n  \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "16qpco",
  "name" : "data_ghcnd_station_only",
  "description" : null,
  "code" : "\"\"\"\nThis process need to retrieve the snow depth ground station data from GHCNd.\n\n\"\"\"\n\nimport pandas as pd\nimport requests\nimport re\nfrom io import StringIO\nfrom snowcast_utils import work_dir, southwest_lat, southwest_lon, northeast_lat, northeast_lon, train_start_date, train_end_date\nimport dask\nimport dask.dataframe as dd\n\nworking_dir = work_dir\nall_ghcd_station_file = f\"{working_dir}/all_ghcn_station_list.csv\"\nonly_active_ghcd_station_in_west_conus_file = f\"{working_dir}/active_station_only_list.csv\"\nsnowdepth_csv_file = f'{only_active_ghcd_station_in_west_conus_file}_all_vars.csv'\nmask_non_snow_days_ghcd_csv_file =  f'{only_active_ghcd_station_in_west_conus_file}_all_vars_masked_non_snow.csv'\n\ndef download_convert_and_read():\n  \n    url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n    # Download the text file from the URL\n    response = requests.get(url)\n    if response.status_code != 200:\n        print(\"Error: Failed to download the file.\")\n        return None\n    \n    # Parse the text content into columns using regex\n    pattern = r\"(\\S+)\\s+\"\n    parsed_data = re.findall(pattern, response.text)\n    print(\"len(parsed_data) = \", len(parsed_data))\n    \n    # Create a new list to store the rows\n    rows = []\n    for i in range(0, len(parsed_data), 6):\n        rows.append(parsed_data[i:i+6])\n    \n    print(\"rows[0:20] = \", rows[0:20])\n    # Convert the rows into a CSV-like format\n    csv_data = \"\\n\".join([\",\".join(row) for row in rows])\n    \n    # Save the CSV-like string to a file\n    with open(all_ghcd_station_file, \"w\") as file:\n        file.write(csv_data)\n    \n    # Read the CSV-like data into a pandas DataFrame\n    column_names = ['Station', 'Latitude', 'Longitude', 'Variable', 'Year_Start', 'Year_End']\n    df = pd.read_csv(all_ghcd_station_file, header=None, names=column_names)\n    print(df.head())\n    \n    # Remove rows where the last column is not equal to \"2024\"\n    df = df[(df['Year_End'] == 2024) & (df['Variable']=='SNWD')]\n    print(\"Removed non-active stations: \", df.head())\n    \n    # Filter rows within the latitude and longitude ranges\n    df = df[\n      (df['Latitude'] >= southwest_lat) & (df['Latitude'] <= northeast_lat) &\n      (df['Longitude'] >= southwest_lon) & (df['Longitude'] <= northeast_lon)\n    ]\n    \n    df.to_csv(only_active_ghcd_station_in_west_conus_file, index=False)\n    print(f\"saved to {only_active_ghcd_station_in_west_conus_file}\")\n    \n    \n    return df\n\n  \ndef get_snow_depth_observations_from_ghcn():\n    \n    new_base_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n    print(new_base_df.shape)\n    \n    start_date = train_start_date\n    end_date = train_end_date\n\t\n    # Create an empty Pandas DataFrame with the desired columns\n    result_df = pd.DataFrame(columns=[\n      'station_name', \n      'date', \n      'lat', \n      'lon', \n      'snow_depth',\n    ])\n    \n    train_start_date_obj = pd.to_datetime(train_start_date)\n    train_end_date_obj = pd.to_datetime(train_end_date)\n\n    # Function to process each station\n    @dask.delayed\n    def process_station(station):\n        station_name = station['Station']\n        print(f\"retrieving for {station_name}\")\n        station_lat = station['Latitude']\n        station_long = station['Longitude']\n        try:\n          url = f\"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/{station_name}.csv\"\n          response = requests.get(url)\n          df = pd.read_csv(StringIO(response.text))\n          #\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNOW\",\"SNOW_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"PGTM\",\"PGTM_ATTRIBUTES\",\"WDFG\",\"WDFG_ATTRIBUTES\",\"WSFG\",\"WSFG_ATTRIBUTES\",\"WT03\",\"WT03_ATTRIBUTES\",\"WT08\",\"WT08_ATTRIBUTES\",\"WT16\",\"WT16_ATTRIBUTES\"\n          columns_to_keep = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD']\n          df = df[columns_to_keep]\n          # Convert the date column to datetime objects\n          df['DATE'] = pd.to_datetime(df['DATE'])\n          # Filter rows based on the training period\n          df = df[(df['DATE'] >= train_start_date_obj) & (df['DATE'] <= train_end_date_obj)]\n          # print(df.head())\n          return df\n        except Exception as e:\n          print(\"An error occurred:\", e)\n\n    # List of delayed computations for each station\n    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n\n    # Compute the delayed results\n    result_lists = dask.compute(*delayed_results)\n\n    # Concatenate the lists into a Pandas DataFrame\n    result_df = pd.concat(result_lists, ignore_index=True)\n\n    # Print the final Pandas DataFrame\n    print(result_df.head())\n\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(snowdepth_csv_file, index=False)\n    print(f\"All the data are saved to {snowdepth_csv_file}\")\n#     result_df.to_csv(csv_file, index=False)\n\ndef mask_out_all_non_zero_snowdepth_days():\n    print(f\"reading {snowdepth_csv_file}\")\n    df = pd.read_csv(snowdepth_csv_file)\n    # Create the new column 'swe_value' and assign values based on conditions\n    df['swe_value'] = 0  # Initialize all values to 0\n\n    # Assign NaN to 'swe_value' where 'snow_depth' is non-zero\n    df.loc[df['SNWD'] != 0, 'swe_value'] = -999\n\n    # Display the first few rows of the DataFrame\n    print(df.head())\n    df.to_csv(mask_non_snow_days_ghcd_csv_file, index=False)\n    print(f\"The masked non snow var file is saved to {mask_non_snow_days_ghcd_csv_file}\")\n\nif __name__ == \"__main__\":\n    #download_convert_and_read()\n    #get_snow_depth_observations_from_ghcn()\n    mask_out_all_non_zero_snowdepth_days()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "1xdwd6",
  "name" : "mod_water_mask",
  "description" : null,
  "code" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, data_dir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nwater_mask_dir = f\"{data_dir}/water_mask/\"\nos.makedirs(water_mask_dir, exist_ok=True)\nos.chdir(water_mask_dir)\n\nenv = os.environ.copy()  # Get the current environment variables\nenv['PROJ_LIB'] = '/home/geo2021/anaconda3/share/proj'  # Set the path to PROJ_LIB\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\n\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\ntarget_dir = os.path.dirname(mapper_file)\nos.makedirs(target_dir, exist_ok=True)\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    # if \"PROJ_LIB\" in os.environ:\n    #     os.environ.pop(\"PROJ_LIB\")\n    #     print(f\"Environment variable PROJ_LIB removed.\")\n    os.environ[\"PROJ_LIB\"] = \"\"\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=env)\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=env)\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=env)\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2024__water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "uw1w1u",
  "name" : "prepare_water_mask_template",
  "description" : null,
  "code" : "# prepare the template tifs for the water mask from MODIS\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\nimport subprocess\nimport csv\nfrom datetime import datetime, timedelta\n\noutput_csv_file = \"/home/chetana/gridmet_test_run/mod44w_water_mask.csv\"\n\n# Function to extract snow cover value at a given lat lon\ndef extract_snow_cover_value(geotiff_path, lon, lat):\n    gdallocationinfo_cmd = [\n        \"gdallocationinfo\",\n        \"-valonly\",\n        geotiff_path,\n        str(lon),\n        str(lat)\n    ]\n    result = subprocess.run(gdallocationinfo_cmd, stdout=subprocess.PIPE, text=True)\n    return result.stdout.strip()\n\ndef generate_template():\n    # Define the date range for data extraction\n    start_date = datetime(2018,1, 1)\n    end_date = datetime(2018, 1, 3)\n\n    # Load the CSV file with latitude and longitude coordinates\n    csv_file_path = \"/home/chetana/gridmet_test_run/station_cell_mapping.csv\"\n\n    # CSV file header\n    csv_header = [\"Date\", \"Latitude\", \"Longitude\", \"Snow Cover Value\"]\n\n    # Loop through the date range\n    current_date = start_date\n    while current_date <= end_date:\n        extracted_data = list()\n        # URL and reference link with dynamic date\n        date_str = current_date.strftime(\"%Y.%m.%d\")\n        url = f\"https://e4ftl01.cr.usgs.gov/MOLT/MOD44W.061/{date_str}/\"\n        reference_link = f\"https://e4ftl01.cr.usgs.gov/MOLT/MOD10A1.061/{date_str}/\"\n        \n        print(\"url = \", url)\n        # Send an HTTP GET request to the URL\n        response = requests.get(url)\n        \n        # Check if the request was successful (HTTP status code 200)\n        if response.status_code == 200:\n            # Parse the HTML content using BeautifulSoup\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            \n            # Find all <a> tags (links) on the page\n            links = soup.find_all(\"a\")\n            \n            # Filter the links to keep only those with the .hdf extension\n            all_hdf_files = [link.get(\"href\") for link in links if link.get(\"href\").endswith(\".hdf\")]\n            \n            # Define the sinusoidal tiles of interest\n            sinusoidal_tiles = [\"h08v04\", \"h08v05\", \n                                \"h09v04\", \"h09v05\", \n                                \"h10v04\", \"h10v05\", \n                                \"h11v04\", \"h11v05\", \n                                \"h12v04\", \"h12v05\", \n                                \"h13v04\", \"h13v05\", \n                                \"h15v04\", \"h16v03\", \n                                \"h16v04\"]\n            \n            # Filter the HDF files based on sinusoidal tiles\n            filtered_hdf_files = [hdf_file for hdf_file in all_hdf_files if any(tile in hdf_file for tile in sinusoidal_tiles)]\n            \n            # List to store the paths of converted GeoTIFF files\n            geotiff_files = []\n            \n            # Loop through the filtered HDF files and download/convert/delete them\n            for hdf_file in filtered_hdf_files:\n                # Construct the complete URL for the HDF file\n                hdf_url = reference_link + hdf_file\n                \n                # Define the local filename to save the HDF file\n                local_hdf_filename = hdf_file\n                \n                # Send an HTTP GET request to download the HDF file\n                print(\"hdf_url = \", hdf_url)\n                hdf_response = requests.get(hdf_url)\n                \n                # Check if the download was successful (HTTP status code 200)\n                if hdf_response.status_code == 200:\n                    with open(local_hdf_filename, \"wb\") as f:\n                        f.write(hdf_response.content)\n                    print(f\"Downloaded {local_hdf_filename}\")\n                    \n                    # Construct the output GeoTIFF file path and name in the same directory\n                    local_geotiff_filename = os.path.splitext(local_hdf_filename)[0] + \".tif\"\n                    \n                    # Run the gdal_translate command to convert HDF to GeoTIFF\n                    gdal_translate_cmd = [\n                        \"gdal_translate\",\n                        \"-of\", \"GTiff\",\n                        f\"HDF4_EOS:EOS_GRID:{local_hdf_filename}:MOD_Grid_Snow_500m:NDSI_Snow_Cover\",\n                        local_geotiff_filename\n                    ]\n                    \n                    # Execute the gdal_translate command\n                    subprocess.run(gdal_translate_cmd)\n                    \n                    # Append the path of the converted GeoTIFF to the list\n                    geotiff_files.append(local_geotiff_filename)\n                    \n                    # Delete the original HDF file\n    #                 os.remove(local_hdf_filename)\n                    \n                    #print(f\"Converted and deleted: {local_hdf_filename}\")\n                else:\n                    pass\n                    #print(f\"Failed to download {local_hdf_filename}\")\n            \n            # Merge all the GeoTIFF files into a single GeoTIFF\n            merged_geotiff = \"merged_geotiff.tif\"\n            gdal_merge_cmd = [\n                \"gdal_merge.py\",\n                \"-o\", merged_geotiff,\n                \"-of\", \"GTiff\"\n            ] + geotiff_files\n            \n            subprocess.run(gdal_merge_cmd)\n            \n            print(f\"Merged all GeoTIFF files into {merged_geotiff}\")\n            \n            # Loop through the CSV file with latitude and longitude coordinates\n            with open(csv_file_path, \"r\") as csv_file:\n                csv_reader = csv.reader(csv_file)\n                next(csv_reader)  # Skip the header row\n                \n                for row in csv_reader:\n                    lat = float(row[3])  # Assuming latitude is in the first column\n                    lon = float(row[4])  # Assuming longitude is in the second column\n                    \n                    # Extract snow cover value\n                    snow_cover_value = extract_snow_cover_value(merged_geotiff, lon, lat)\n                    \n                    # Append the extracted data to the list\n                    extracted_data.append([date_str, lat, lon, snow_cover_value])\n            \n            # Delete the merged GeoTIFF file\n    #         os.remove(merged_geotiff)\n            print(f\"Deleted {merged_geotiff}\")\n            \n            # Delete individual GeoTIFF files after a successful merge\n            #for geotiff_file in geotiff_files:\n            #    try:\n            #        os.remove(geotiff_file)\n            #        print(f\"Deleted {geotiff_file}\")\n            #    except Exception as e:\n            #        pass\n            \n            # Append the extracted data to the CSV file after each date\n            with open(output_csv_file, \"a\", newline=\"\") as csv_file:\n                csv_writer = csv.writer(csv_file)\n                csv_writer.writerows(extracted_data)\n            \n            print(f\"Extracted data appended to {output_csv_file}\")\n        \n        else:\n            print(\"Failed to fetch the HTML content.\")\n        \n        # Move to the next date\n        current_date += timedelta(days=1)\n\n\nif __name__ == \"__main__\":\n  generate_template()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "14bhpn",
  "name" : "download_modis_09",
  "description" : null,
  "code" : "# Write your first Python code in Geoweaver\n# Code to download 7 bands of MODIS (MOD09GA) to compute fSCA\n# Code first drafted by Millie Spencer and Ziheng Sun in UW Geohackweek 2024\n\n\nimport os\nimport sys\nimport subprocess\nimport threading\nfrom datetime import datetime, timedelta\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import date_to_julian, work_dir, homedir\nimport logging\n\nlog = logging.getLogger(__name__)\n\n# change directory before running the code\nshared_data_folder_path = f\"{homedir}/mod09\"\nos.makedirs(shared_data_folder_path, exist_ok=True)\nos.chdir(shared_data_folder_path)\n\n\n### Define the timeframe of interest: ###\n# to start with, we'll look at 1 month of data \n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \"h12v04\", \"h12v05\",\n             \"h13v04\", \"h13v05\", \"h15v04\", \"h16v03\", \"h16v04\", ]\n\n### Set up storage paths ###\n\n# The folder path where the HDF files will be temporarily stored.\ninput_folder = shared_data_folder_path + \"/temp/\"\n\n# The folder path where the GeoTIFF files will be stored after conversion from HDF.\noutput_folder = shared_data_folder_path + \"/output_folder/\"\n\n# The folder path where the final merged output GeoTIFF files will be stored.\nmodis_day_wise = shared_data_folder_path + \"/final_output/\"\n\n# Create necessary directories if they do not exist.\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\n\n# List of band names you want to download\nbands = [\n    \"MODIS_Grid_500m_2D:sur_refl_b01_1\",  # Band 1\n    \"MODIS_Grid_500m_2D:sur_refl_b02_1\",  # Band 2\n    \"MODIS_Grid_500m_2D:sur_refl_b03_1\",  # Band 3\n    \"MODIS_Grid_500m_2D:sur_refl_b04_1\",  # Band 4\n    \"MODIS_Grid_500m_2D:sur_refl_b05_1\",  # Band 5\n    \"MODIS_Grid_500m_2D:sur_refl_b06_1\",  # Band 6\n    \"MODIS_Grid_500m_2D:sur_refl_b07_1\",  # Band 7\n    \"MODIS_Grid_500m_2D:QC_500m_1\"        # Quality Control\n]\n\ndef get_band_path_name_from_band_original_name(original_band_name):\n    return original_band_name.split(':')[1]\n\ndef convert_hdf_to_geotiff(hdf_file, output_folder, force=False):\n    \"\"\"\n    Converts a specified HDF file to a GeoTIFF format.\n\n    Args:\n        hdf_file (str): The file path of the HDF file to be converted.\n        output_folder (str): The directory where the converted GeoTIFF file will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n    # Iterate through each band and download it\n    for target_subdataset_name in bands:\n        # Your existing code to download the data for each band\n        # print(f\"Exporting {target_subdataset_name} into geotiff\")\n        # (insert your downloading code here)\n        \n        band = get_band_path_name_from_band_original_name(target_subdataset_name)\n        # Create a name for the output file based on the HDF file name and subdataset\n        output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + f\"_{band}.tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n    \n        if os.path.exists(output_path) and not force:\n            pass\n        else:\n            try:\n                for subdataset in hdf_ds.GetSubDatasets():\n                    if target_subdataset_name in subdataset[0]:\n                        ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                        gdal.Translate(output_path, ds)\n                        ds = None\n                        break\n            except Exception as e:\n                log.exception(\"Something wrong with the downloaded HDF. Redownloading and try again..\")\n                \n    \n    hdf_ds = None\n\ndef convert_all_hdf_in_folder(folder_path, output_folder, force=False):\n    \"\"\"\n    Converts all HDF files in a given folder to GeoTIFF format.\n\n    Args:\n        folder_path (str): The directory containing HDF files to be converted.\n        output_folder (str): The directory where the converted GeoTIFF files will be saved.\n\n    Returns:\n        list: A list of file names that were found in the folder.\n    \"\"\"\n    file_lst = list()\n    for file in os.listdir(folder_path):\n        file_lst.append(file)\n        if file.lower().endswith(\".hdf\"):\n            hdf_file = os.path.join(folder_path, file)\n            convert_hdf_to_geotiff(hdf_file, output_folder, force=False)\n    return file_lst\n\ndef merge_tifs(folder_path, target_date, output_file, band_name):\n    \"\"\"\n    Merges multiple GeoTIFF files into a single GeoTIFF file for a specific date.\n\n    Args:\n        folder_path (str): The directory containing GeoTIFF files to be merged.\n        target_date (datetime): The date for which the GeoTIFF files should be merged.\n        output_file (str): The file path where the merged GeoTIFF file will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    julian_date = date_to_julian(target_date)\n    tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(f'{band_name}.tif') and julian_date in f]\n    print(\"tif_files = \", tif_files)\n    \n    if len(tif_files) == 0:\n        gdal_command = ['gdal_translate', '-b', '1', '-outsize', '100%', '100%', '-scale', '-100', '16000', '0', '255', \n                        f\"{homedir}/fsca/final_output/fsca_template.tif\", output_file]\n        print(gdal_command)\n        subprocess.run(gdal_command)\n    else:\n        gdal_command = ['gdalwarp', '-r', 'min'] + tif_files + [f\"{output_file}_500m.tif\"]\n        print(gdal_command)\n        subprocess.run(gdal_command)\n        \n        gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', \n                        f'{work_dir}/template.shp', \n                        '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n        print(gdal_command)\n        subprocess.run(gdal_command)\n\ndef list_files(directory):\n    \"\"\"\n    Lists all files in a specified directory. doc-string\n\n    Args:\n        directory (str): The directory from which to list files.\n\n    Returns:\n        list: A list of absolute file paths in the specified directory.\n    \"\"\"\n    return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n\ndef merge_tiles(date, hdf_files):\n    \"\"\"\n    Merges multiple tiles into a single GeoTIFF file for a specific date.\n\n    Args:\n        date (str): The date for which the tiles should be merged (format: YYYY-MM-DD).\n        hdf_files (list): A list of HDF file paths to be merged.\n\n    Returns:\n        None\n    \"\"\"\n    path = f\"data/{date}/\"\n    files = list_files(path)\n    merged_filename = f\"data/{date}/merged.tif\"\n    merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n    try:\n        subprocess.run(merge_command)\n        print(f\"Merged tiles into {merged_filename}\")\n    except subprocess.CalledProcessError as e:\n        print(f\"Error merging tiles: {e}\")\n\ndef download_url(date, url):\n    \"\"\"\n    Downloads a file from a specified URL to a local directory for a specific date.\n\n    Args:\n        date (str): The date for which the file is being downloaded (format: YYYY-MM-DD).\n        url (str): The URL from which to download the file.\n\n    Returns:\n        None\n    \"\"\"\n    file_name = url.split('/')[-1]\n    if os.path.exists(f'data/{date}/{file_name}'):\n        print(f'File: {file_name} already exists, SKIPPING')\n        return\n    try:\n        os.makedirs('data/', exist_ok=True)\n        os.makedirs(f'data/{date}', exist_ok=True)\n        response = requests.get(url, stream=True)\n        with open(f'data/{date}/{file_name}', 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n\n        print(f\"Downloaded {file_name}\")\n    except Exception as e:\n        print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__sur_refl_b01_1.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD09GA\", \n                                          cloud_hosted=True, \n                                        #   bounding_box=(-124.77, 24.52, -66.95, 49.38), # entire western US\n                                          bounding_box=(-125, 43, -120, 45), # just a piece of the oregon coast\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n        for band_name in bands:\n            band = get_band_path_name_from_band_original_name(band_name)\n            target_output_tif = f'{modis_day_wise}/{current_date}__{band}.tif'\n            merge_tifs(folder_path=output_folder, \n                       target_date = current_date, \n                       output_file=target_output_tif, band_name=band)\n        \n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\n    \nif __name__ == \"__main__\":\n  start_date = datetime(2018, 1, 1)\n  end_date = datetime(2018, 1, 31)\n  download_tiles_and_merge(start_date, end_date)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pyn9xn",
  "name" : "data_mod09_extract_csvs",
  "description" : null,
  "code" : "# ---\n# Millie \n\n# # code to convert MODIS09GA band geotiffs to a csv\n\n# +\nimport os\nimport pandas as pd\nimport rasterio\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport concurrent.futures\nfrom snowcast_utils import homedir, work_dir, train_start_date, train_end_date\n\nfrom datetime import datetime, timedelta\nimport dask.dataframe as dd\nimport numpy as np\n\n# +\nworking_dir = f\"{homedir}/mod09\"\nroot_dir = f\"{homedir}\"\nfolder_path = f\"{working_dir}/csvs/\"\n\nnew_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\ncell_to_modis_mapping = f\"{work_dir}/training_cell_to_modis_mapper_original_snotel_stations.csv\"\nnon_station_random_points_file = f\"{work_dir}/non_station_random_points_in_westus.csv\"\nonly_active_ghcd_station_in_west_conus_file = f\"{work_dir}/active_ghcnd_station_only_list.csv\"\nghcd_station_to_modis_mapper_file = f\"{work_dir}/active_ghcnd_mapper_modis.csv\"\nall_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\nmodis_day_wise = f\"{working_dir}/final_output/\"\nos.makedirs(modis_day_wise, exist_ok=True)\n\n\n# +\n### Define the timeframe of interest: ###\n# to start with, we'll look at 1 month of data \n\n# List of band names you want to download\nbands = [\n    \"MODIS_Grid_500m_2D:sur_refl_b01_1\",  # Band 1\n    \"MODIS_Grid_500m_2D:sur_refl_b02_1\",  # Band 2\n    \"MODIS_Grid_500m_2D:sur_refl_b03_1\",  # Band 3\n    \"MODIS_Grid_500m_2D:sur_refl_b04_1\",  # Band 4\n    \"MODIS_Grid_500m_2D:sur_refl_b05_1\",  # Band 5\n    \"MODIS_Grid_500m_2D:sur_refl_b06_1\",  # Band 6\n    \"MODIS_Grid_500m_2D:sur_refl_b07_1\",  # Band 7\n    \"MODIS_Grid_500m_2D:QC_500m_1\"        # Quality Control\n]\n\ndef get_band_path_name_from_band_original_name(original_band_name):\n    return original_band_name.split(':')[1]\n\n\n# +\ndef map_modis_to_station(row, src):\n  drow, dcol = src.index(row[\"lon\"], row[\"lat\"])\n  return drow, dcol\n\n\ndef generate_random_non_station_points():\n  # Load the GeoTIFF file\n  sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n  print(f\"loading geotiff {sample_modis_tif}\")\n  with rasterio.open(sample_modis_tif) as src:\n    # Get the raster metadata\n    bounds = src.bounds\n    transform = src.transform\n    width = src.width\n    height = src.height\n\n    # Read the raster values as a numpy array\n    raster_array = src.read(1)  # Assuming it's a single-band raster\n\n    # Generate random points\n    random_points = []\n    while len(random_points) < 4000:\n      # Generate random coordinates within the bounds of the raster\n      random_x = np.random.uniform(bounds.left, bounds.right)\n      random_y = np.random.uniform(bounds.bottom, bounds.top)\n\n      # Convert random coordinates to pixel coordinates\n      col, row = ~transform * (random_x, random_y)\n\n      # Ensure the generated pixel coordinates are within the raster bounds\n      if 0 <= row < height and 0 <= col < width:\n        # Get the value at the generated pixel coordinates\n        value = raster_array[int(row), int(col)]\n\n        # Check if the value is not 239\n        if value != 239 and value != 255:\n          # Append the coordinates to the list\n          random_points.append((random_x, random_y, col, row))\n\n    # Assuming random_points is a list of tuples where each tuple contains latitude and longitude\n    random_points = [(lat, lon, col, row) for lon, lat, col, row in random_points]  # Swap the order to (latitude, longitude)\n\n    # Create a DataFrame from the random_points list\n    random_points_df = pd.DataFrame(random_points, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n\n    # Save the DataFrame to a CSV file\n    random_points_df.to_csv(non_station_random_points_file, index=False)\n    print(f\"random points are saved to {non_station_random_points_file}\")\n\n    \n\n\ndef prepare_modis_grid_mapper_training():\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(cell_to_modis_mapping):\n    print(f\"The file {cell_to_modis_mapping} exists. skip.\")\n  else:\n    print(f\"start to generate {cell_to_modis_mapping}\")\n    station_df = pd.read_csv(new_base_station_list_file)\n    print(\"original station_df describe() = \", station_df.describe())\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"], \n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(cell_to_modis_mapping, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      \n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n\ndef merge_station_and_non_station_to_one_csv():\n  print(f\"new_base_station_list_file = {new_base_station_list_file}\")\n  print(f\"cell_to_modis_mapping = {cell_to_modis_mapping}\")\n  print(f\"non_station_random_points_file = {non_station_random_points_file}\")\n  df1 = pd.read_csv(cell_to_modis_mapping)\n  df2 = pd.read_csv(non_station_random_points_file)\n  combined_df = pd.concat([df1, df2], ignore_index=True)\n  combined_df.to_csv(all_training_points_with_station_and_non_station_file, index=False)\n\n  print(f\"Combined CSV saved to {all_training_points_with_station_and_non_station_file}\")\n\ndef merge_snotel_ghcnd_station_to_one_csv():\n  print(f\"new_base_station_list_file = {new_base_station_list_file}\")\n  print(f\"cell_to_modis_mapping = {cell_to_modis_mapping}\")\n  print(f\"ghcnd_to_modis_mapping = {ghcd_station_to_modis_mapper_file}\")\n  df1 = pd.read_csv(cell_to_modis_mapping)\n  df2 = pd.read_csv(ghcd_station_to_modis_mapper_file)\n  combined_df = pd.concat([df1, df2], ignore_index=True)\n  combined_df.to_csv(all_training_points_with_snotel_ghcnd_file, index=False)\n\n  print(f\"Combined CSV saved to {all_training_points_with_snotel_ghcnd_file}\")\n\ndef prepare_ghcnd_station_mapping_training():\n  if os.path.exists(ghcd_station_to_modis_mapper_file):\n    print(f\"The file {ghcd_station_to_modis_mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {ghcd_station_to_modis_mapper_file}\")\n    station_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n    station_df = station_df.rename(columns={'Latitude': 'latitude', \n                                            'Longitude': 'longitude'})\n    print(\"original station_df describe() = \", station_df.describe())\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"],\n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(ghcd_station_to_modis_mapper_file, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      print(f\"the new mapper to the ghcnd is saved to {ghcd_station_to_modis_mapper_file}\")\n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n  \ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n  if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n    return None\n  row, col = src.index(lat, lon)\n  if (0 <= row < src.height) and (0 <= col < src.width):\n    return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n  else:\n    return None\n\ndef get_band_value(row, src):\n  if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n    # print(\"src.height = \", src.height, \" - \", row[\"modis_y\"])\n    # print(\"src.width = \", src.width, \" - \", row[\"modis_x\"])\n    # print(row)\n    valid_value =  src.read(1, \n                            window=((int(row[\"modis_y\"]),\n                                     int(row[\"modis_y\"])+1), \n                                    (int(row[\"modis_x\"]),\n                                     int(row[\"modis_x\"])+1)))\n    # print(\"valid_value[0,0] = \", valid_value[0,0])\n    return valid_value[0,0]\n  else:\n    return None\n          \ndef process_file(file_path, current_date_str, outfile, band_path):\n  print(f\"processing {file_path}\")\n  station_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n  # print(\"station_df.head() = \", station_df.head())\n\n  # Apply get_band_value for each row in the DataFrame\n  with rasterio.open(file_path) as src:\n    # Apply get_band_value for each row in the DataFrame\n    # Get the affine transformation matrix\n    transform = src.transform\n\n    # Extract the spatial extent using the affine transformation\n    left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n    # Print the spatial extent\n    # print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n\n    station_df[band_path] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    \n  # Prepare final data\n  station_df['date'] = current_date_str\n  station_df.to_csv(outfile, index=False, \n                    columns=['date', 'latitude', 'longitude', band_path]) # replacing fsca with band\n  print(f\"Saved to csv: {outfile}\")\n\ndef merge_csv(start_date, end_date):\n  import glob\n\n  for band_name in bands:\n      band_path = get_band_path_name_from_band_original_name(band_name)\n      print(f\"Generating training csv for {band_path}\")\n      # Find CSV files within the specified date range\n      csv_files = glob.glob(folder_path + f'*_training_output_station*{band_path}*')\n      relevant_csv_files = []\n    \n      for c in csv_files:\n        # Extract the date from the file name\n        # print(\"c = \", c)\n        file_name = os.path.basename(c)\n        date_str = file_name.split('_')[0]  # Assuming the date is part of the file name\n        print(\"date_str = \", date_str)\n        file_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n        # Check if the file date is within the specified range\n        if start_date <= file_date <= end_date:\n          relevant_csv_files.append(c)\n    #       # Read and concatenate only relevant CSV files\n    #       df = []\n    #       for c in relevant_csv_files:\n    #         tmp = pd.read_csv(c, low_memory=False, usecols=['date', 'latitude', 'longitude', 'fsca'])\n    #         df.append(tmp)\n    \n    #         combined_df = pd.concat(df, ignore_index=True)\n    \n      # Initialize a Dask DataFrame\n      print(\"start to use dask to read all csv files\")\n      dask_df = dd.read_csv(relevant_csv_files)\n    \n      # Save the merged DataFrame to a CSV file\n      output_file = f'{working_dir}/{band_path}_final_training_all.csv'\n      # Write the Dask DataFrame to a single CSV file\n      print(f\"saving all csvs into one file: {output_file}\")\n      dask_df.to_csv(output_file, index=False, single_file=True)\n      #combined_df.to_csv(output_file, index=False)\n    \n      #print(combined_df.describe())\n      print(f\"Merged data saved to {output_file}\")\n  \ndef main():\n  \n  # The start date for downloading and processing MODIS tiles.\n  start_date = datetime(2018, 1, 1)\n\n  # The end date for downloading and processing MODIS tiles.\n  end_date = datetime(2018, 1, 31)\n  # -\n\n  # start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n  \n  # end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n  \n  prepare_modis_grid_mapper_training()\n  prepare_ghcnd_station_mapping_training()\n  # running this function will generate a new set of random points\n  # generate_random_non_station_points()\n  #merge_station_and_non_station_to_one_csv()\n  merge_snotel_ghcnd_station_to_one_csv()\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    for band_name in bands:\n        band_path = get_band_path_name_from_band_original_name(band_name)\n        #print(f\"extracting data for {current_date}\")\n        outfile = os.path.join(working_dir, f'csvs/{current_date}_training_output_station_with_ghcnd_{band_path}.csv')\n        if os.path.exists(outfile):\n          print(f\"The file {outfile} exists. skip.\")\n          pass\n        else:\n          process_file(f'{working_dir}/final_output/{current_date}__{band_path}.tif', current_date, outfile, band_path)\n  \n  merge_csv(start_date, end_date)\n\nif __name__ == \"__main__\":\n  main()\n  print(\"fsca Data extraction complete.\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "h1952i",
  "name" : "duplicated_feature_selection",
  "description" : "python",
  "code" : "import dask.dataframe as dd\n\n# Replace 'data.csv' with the path to your 50GB CSV file\ninput_csv = '/home/chetana/gridmet_test_run/model_training_data.csv'\n\n# List of columns you want to extract\nselected_columns = ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n                    'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n                    'elevation',\n                    'slope', 'curvature', 'aspect', 'eastness',\n                    'northness', 'Snow Water Equivalent (in) Start of Day Values']\n\n# Read the CSV file into a Dask DataFrame\ndf = dd.read_csv(input_csv, usecols=selected_columns)\n\n# Rename the column as you intended\ndf = df.rename(columns={\"Snow Water Equivalent (in) Start of Day Values\": \"swe_value\"})\n\n# Replace 'output.csv' with the desired output file name\noutput_csv = '/home/chetana/gridmet_test_run/model_training_cleaned.csv'\n\n# Write the selected columns to a new CSV file\ndf.to_csv(output_csv, index=False, single_file=True)  # single_file=True \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "k1aoz3",
  "name" : "train_self_attention_xgb_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"train_self_attention_xgb_slurm.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\n\n# CPU\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n\n# GPU\n#SBATCH --account=qtong\n#SBATCH --qos=gpu\n#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A100.80gb:1                # up to 8; only request what you need\n\n\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J self_attention_xgb_slurm       # Job name\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=40                 # number of cores needed\n#SBATCH --mem=50G\n#SBATCH --time=24:00:00         # walltime\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\nset echo\numask 0027\n\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\nexport CUDA_LAUNCH_BLOCKING=1\n\npython -u << INNER_EOF\nimport os\nimport numpy as np\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom datetime import datetime\n\nimport matplotlib\nmatplotlib.use('Agg')  # Use a non-GUI backend\n\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn, optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Define sequence length and target column\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\n# Base features (without lags)\nBASE_FEATURES = [\n#     'SWE', 'air_temperature_tmmn', 'relative_humidity_rmin',\n#     'precipitation_amount', 'wind_speed', 'fsca'\n    'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f', \n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', \n    'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', \n    'fsca', 'Slope', \n]\n\n# Define paths and create necessary directories\n# filepath = '/groups/ESS3/zsun/swe/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\n# Features to include in the sequence (including lagged features)\n# FEATURES = BASE_FEATURES + [f\"{feature}_{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# ['date', 'lat', 'lon', 'SWE', 'station_name', 'swe_value', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f', \n#  'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', \n#  'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', \n#  'fsca', 'Slope', 'SWE_1', 'air_temperature_tmmn_1', 'potential_evapotranspiration_1', 'mean_vapor_pressure_deficit_1', \n#  'relative_humidity_rmax_1', 'relative_humidity_rmin_1', 'precipitation_amount_1', 'air_temperature_tmmx_1', 'wind_speed_1', \n#  'fsca_1', 'SWE_2', 'air_temperature_tmmn_2', 'potential_evapotranspiration_2', 'mean_vapor_pressure_deficit_2', \n#  'relative_humidity_rmax_2', 'relative_humidity_rmin_2', 'precipitation_amount_2', 'air_temperature_tmmx_2', 'wind_speed_2', \n#  'fsca_2', 'SWE_3', 'air_temperature_tmmn_3', 'potential_evapotranspiration_3', 'mean_vapor_pressure_deficit_3', \n#  'relative_humidity_rmax_3', 'relative_humidity_rmin_3', 'precipitation_amount_3', 'air_temperature_tmmx_3', 'wind_speed_3', \n#  'fsca_3', 'SWE_4', 'air_temperature_tmmn_4', 'potential_evapotranspiration_4', 'mean_vapor_pressure_deficit_4', \n#  'relative_humidity_rmax_4', 'relative_humidity_rmin_4', 'precipitation_amount_4', 'air_temperature_tmmx_4', 'wind_speed_4', \n#  'fsca_4', 'SWE_5', 'air_temperature_tmmn_5', 'potential_evapotranspiration_5', 'mean_vapor_pressure_deficit_5', \n#  'relative_humidity_rmax_5', 'relative_humidity_rmin_5', 'precipitation_amount_5', 'air_temperature_tmmx_5', 'wind_speed_5', \n#  'fsca_5', 'SWE_6', 'air_temperature_tmmn_6', 'potential_evapotranspiration_6', 'mean_vapor_pressure_deficit_6', \n#  'relative_humidity_rmax_6', 'relative_humidity_rmin_6', 'precipitation_amount_6', 'air_temperature_tmmx_6', \n#  'wind_speed_6', 'fsca_6', 'SWE_7', 'air_temperature_tmmn_7', 'potential_evapotranspiration_7', 'mean_vapor_pressure_deficit_7', \n#  'relative_humidity_rmax_7', 'relative_humidity_rmin_7', 'precipitation_amount_7', 'air_temperature_tmmx_7', 'wind_speed_7', 'fsca_7', \n#  'water_year', 'cumulative_SWE', 'cumulative_air_temperature_tmmn', 'cumulative_potential_evapotranspiration', \n#  'cumulative_mean_vapor_pressure_deficit', 'cumulative_relative_humidity_rmax', 'cumulative_relative_humidity_rmin', \n#  'cumulative_precipitation_amount', 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'cumulative_fsca']\n\ndef load_data(filepath):\n    print(\"Loading dataset...\")\n    df = pd.read_csv(filepath)\n    print(\"df.columns: \", list(df.columns))\n    df = df[df['swe_value'] > 0]\n    df = df.sort_values(by=[\"station_name\", \"date\"])\n    \n    # Select the first 10,000 rows\n    # df = df.iloc[:100000]\n    # print(f\"Subset to first 10,000 rows. Data shape: {df.shape}\")\n    \n    return df\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, features, target):\n        self.data = data\n        self.sequence_length = sequence_length\n        self.features = features\n        self.target = target\n        self.scaler = StandardScaler()\n\n        # Normalize data\n        self.data_normalized = self.normalize_data()\n        \n        # Create sequences\n        self.sequences, self.targets = self.create_sequences()\n\n    def normalize_data(self):\n        # Normalize the features and target column\n        print(\"Normalizing data...\")\n        features_data = self.data[self.features]\n        target_data = self.data[self.target].values.reshape(-1, 1)\n        \n        # Normalize features\n        features_normalized = self.scaler.fit_transform(features_data)\n        \n        # Normalize target\n        target_normalized = self.scaler.fit_transform(target_data).flatten()\n\n        # Create normalized dataframe\n        normalized_data = self.data.copy()\n        normalized_data[self.features] = features_normalized\n        normalized_data[self.target] = target_normalized\n\n        return normalized_data\n\n    def create_sequences(self):\n        sequences = []\n        targets = []\n        print(f\"Creating sequences for {len(self.data['station_name'].unique())} unique stations.\")\n        for station in self.data['station_name'].unique():\n            station_data = self.data[self.data['station_name'] == station]\n            for i in range(len(station_data) - self.sequence_length):\n                seq = station_data.iloc[i:i + self.sequence_length][self.features].values\n                sequences.append(seq)\n                targets.append(station_data.iloc[i + self.sequence_length][self.target])\n        print(f\"Total sequences created: {len(sequences)}\")\n        return np.array(sequences), np.array(targets)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)\n\n# Define the Transformer Model\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n        super(TransformerModel, self).__init__()\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, SEQUENCE_LENGTH, hidden_dim))\n        self.transformer = nn.Transformer(\n            d_model=hidden_dim,\n            nhead=nhead,\n            num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers,\n            dim_feedforward=hidden_dim * 4,\n            dropout=0.1,\n            batch_first=True\n        )\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n#         print(f\"Input shape: {x.shape}\")\n        # Add positional encoding\n        x = self.input_embedding(x) + self.positional_encoding\n        x = self.transformer(x, x)\n        x = x.mean(dim=1)  # Pooling: Average across the sequence\n        output = self.output_layer(x)\n#         print(f\"Output shape: {output.shape}\")\n        return output\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(epochs):\n        start_time = time.time()  # Start timing the epoch\n        \n        model.train()\n        train_loss = 0\n        for x, y in train_loader:\n            optimizer.zero_grad()\n            output = model(x)\n            \n            loss = criterion(output.squeeze(), y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        val_loss = 0\n        model.eval()\n        with torch.no_grad():\n            for x, y in val_loader:\n                output = model(x)\n                loss = criterion(output.squeeze(), y)\n                val_loss += loss.item()\n        \n        # Calculate average losses\n        avg_train_loss = train_loss / len(train_loader)\n        avg_val_loss = val_loss / len(val_loader)\n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n\n        \n        # Calculate and print epoch time\n        epoch_time = time.time() - start_time\n        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Time: {epoch_time:.2f} seconds\")\n\n    # Save the model with a timestamped filename\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    model_path = os.path.join(\"/groups/ESS3/zsun/swe/\", f\"transformer_model_{timestamp}.pth\")\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return train_losses, val_losses\n\ndef evaluate_model(model, val_loader):\n    y_true = []\n    y_pred = []\n    model.eval()\n    with torch.no_grad():\n        for x, y in val_loader:\n            output = model(x)\n            y_true.extend(y.numpy())\n            y_pred.extend(output.squeeze().numpy())\n    \n    # Compute RMSE and R2\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f\"RMSE: {rmse:.4f}, R2: {r2:.4f}\")\n    \n    return y_true, y_pred\n\ndef plot_training_progress(train_losses, val_losses, plot_dir):\n    plt.figure(figsize=(12, 12))\n    plt.plot(train_losses, label='Train Loss')\n    plt.plot(val_losses, label='Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.legend()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(os.path.join(plot_dir, f'transformer_training_progress_{timestamp}.png'))\n\ndef plot_evaluation_results(y_true, y_pred, plot_dir):\n    plt.figure(figsize=(12, 12))\n    plt.scatter(y_true, y_pred, alpha=0.5)\n    plt.plot([min(y_true), max(y_true)], [min(y_true), max(y_true)], color='red', linestyle='--', lw=2)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('True vs Predicted SWE')\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    plt.savefig(os.path.join(plot_dir, f'transformer_predictions_vs_true_{timestamp}.png'))\n\n\ndef do_self_attention(filepath, plot_dir):\n    # Load data\n    df = load_data(filepath)\n\n    # Prepare dataset and dataloaders\n    print(\"Creating dataset and dataloaders...\")\n    print(\"input features: \", FEATURES)\n    print(\"SEQUENCE_LENGTH: \", SEQUENCE_LENGTH)\n    print(\"target column: \", TARGET_COLUMN)\n    dataset = TimeSeriesDataset(df, SEQUENCE_LENGTH, FEATURES, TARGET_COLUMN)\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n\n    train_loader = DataLoader(train_dataset, batch_size=32*20, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32*20, shuffle=False)\n\n    # Define model\n    input_dim = len(FEATURES)\n    hidden_dim = 64\n    nhead = 4\n    num_layers = 2\n    output_dim = 1\n\n    model = TransformerModel(input_dim, nhead, hidden_dim, num_layers, output_dim)\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    # Train the model\n    epochs = 1000\n    print(f\"Starting training for {epochs} epochs...\")\n    train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, epochs)\n\n    # Plot training progress\n    plot_training_progress(train_losses, val_losses, plot_dir)\n\n    # Evaluate the model\n    print(\"Evaluating the model...\")\n    y_true, y_pred = evaluate_model(model, val_loader)\n\n    # Plot evaluation results\n    plot_evaluation_results(y_true, y_pred, plot_dir)\n\n# Main Script\nif __name__ == \"__main__\":\n    filepath = '/groups/ESS3/zsun/swe/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n    # Define paths and create necessary directories\n    model_save_path = '/groups/ESS3/zsun/swe/transformer_model.pth'\n    plot_dir = '/groups/ESS3/zsun/swe/plots'\n    os.makedirs(plot_dir, exist_ok=True)\n    print(f\"Plot directory ensured at: {plot_dir}\")\n    do_self_attention(filepath, plot_dir)\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 100  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\necho \"job status $job_status\"\nif [[ $job_status == *\"COMPLETED\"* ]]; then\n    exit 0\nfi\n\nexit 1\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "i66nk8",
  "name" : "train_xgb_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"train_self_attention_xgb_slurm.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\n\n# CPU\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n\n# GPU\n#SBATCH --account=qtong\n#SBATCH --qos=gpu\n#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A100.80gb:1                # up to 8; only request what you need\n\n\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J self_attention_xgb_slurm       # Job name\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20                 # number of cores needed\n#SBATCH --mem=150G\n#SBATCH --time=24:00:00         # walltime\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\nset echo\numask 0027\n\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\nexport CUDA_LAUNCH_BLOCKING=1\n\npython -u << INNER_EOF\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport matplotlib\nmatplotlib.use('Agg')  # Use a non-GUI backend\n\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn, optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Define sequence length and target column\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\n# Base features (without lags)\nBASE_FEATURES = [\n#     'SWE', 'air_temperature_tmmn', 'relative_humidity_rmin',\n#     'precipitation_amount', 'wind_speed', 'fsca'\n    'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f', \n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', \n    'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', \n    'fsca', 'Slope', \n]\n\n# Define paths and create necessary directories\n# filepath = '/groups/ESS3/zsun/swe/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\n# Features to include in the sequence (including lagged features)\n# FEATURES = BASE_FEATURES + [f\"{feature}_{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Train XGBoost model\ndef train_xgboost(X_train, y_train, X_val, y_val):\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'max_depth': 8,\n        'learning_rate': 0.1,\n        'n_estimators': 200,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8\n    }\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=True)\n\n    # Save the model with a timestamped filename\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    model_path = os.path.join(\"/groups/ESS3/zsun/swe/\", f\"xgb_model_{timestamp}.json\")\n    model.save_model(model_path)\n    print(f\"Model saved to {model_path}\")\n\n    return model\n\n# Evaluate XGBoost model\ndef evaluate_xgboost(model, X_test, y_test, scaler_y, plot_dir):\n    predictions = model.predict(X_test)\n    predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).squeeze()\n    true_values = scaler_y.inverse_transform(y_test.reshape(-1, 1)).squeeze()\n\n    # Calculate metrics\n    mse = mean_squared_error(true_values, predictions)\n    r2 = r2_score(true_values, predictions)\n\n    print(f\"XGBoost Model Evaluation:\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n\n    # Plot Predictions vs True Values\n    plt.figure(figsize=(10, 10))\n    plt.scatter(true_values, predictions, alpha=0.6)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('XGBoost Predictions vs True Values')\n    plt.grid(True)\n    plt.tight_layout()\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    print(\"XGBoost model is saved to \", os.path.join(plot_dir, f'xgboost_predictions_vs_true_{timestamp}.png'))\n    plt.savefig(os.path.join(plot_dir, 'xgboost_predictions_vs_true.png'))\n    plt.close()\n\n    # Residuals plot\n    residuals = true_values - predictions\n    plt.figure(figsize=(10, 6))\n    sns.histplot(residuals, kde=True, bins=30, color='green', alpha=0.7)\n    plt.xlabel('Residuals')\n    plt.title('XGBoost Residuals Distribution')\n    plt.tight_layout()\n    plt.savefig(os.path.join(plot_dir, f'xgboost_residuals_{timestamp}.png'))\n    plt.close()\n\n\n# Load and preprocess data\ndef load_preprocess_data_xgboost(filepath, target_col='swe_value'):\n    df = pd.read_csv(filepath)\n    print(f\"Original dataset shape: {df.shape}\")\n    print(\"df.columns = \", list(df.columns))\n    df = df[df[target_col] > 0]  # Keep positive SWE values\n    # df = df.sample(n=500000, random_state=42)  # Downsample for speed\n    print(f\"Filtered dataset shape: {df.shape}\")\n\n    # Drop all columns with \"cumulative\" in their names\n    cumulative_columns = [col for col in df.columns if 'cumulative' in col.lower()]\n    X = df.drop(columns=[target_col, 'date', 'station_name'] + cumulative_columns)\n\n    print(f\"Dropped columns: {cumulative_columns}\")\n\n    # X = df.drop(columns=[target_col, 'date', 'station_name'])  # Drop unnecessary columns\n    y = df[target_col]\n    \n    scaler_X = StandardScaler()\n    scaler_y = StandardScaler()\n    \n    X_scaled = scaler_X.fit_transform(X)\n    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).squeeze()\n\n    return X_scaled, y_scaled, scaler_y\n\n\ndef do_xgboost(filepath, plot_dir):\n    # Load and preprocess data\n    print(f\"Loading and preprocessing data from: {filepath}\")\n    X, y, scaler_y = load_preprocess_data_xgboost(filepath)\n    \n    # Split data into train, validation, and test sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n    # Train XGBoost model\n    print(\"Training XGBoost model...\")\n    xgb_model = train_xgboost(X_train, y_train, X_val, y_val)\n\n    # Evaluate model\n    print(\"Evaluating XGBoost model...\")\n    evaluate_xgboost(xgb_model, X_test, y_test, scaler_y, plot_dir)\n\n# Main Script\nif __name__ == \"__main__\":\n    filepath = '/groups/ESS3/zsun/swe/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n    # Define paths and create necessary directories\n    model_save_path = '/groups/ESS3/zsun/swe/transformer_model.pth'\n    plot_dir = '/groups/ESS3/zsun/swe/plots'\n    os.makedirs(plot_dir, exist_ok=True)\n    print(f\"Plot directory ensured at: {plot_dir}\")\n    # do_self_attention(filepath, plot_dir)\n    do_xgboost(filepath, plot_dir)\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 100  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\necho \"job status $job_status\"\nif [[ $job_status == *\"COMPLETED\"* ]]; then\n    exit 0\nfi\n\nexit 1\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "sacl4k",
  "name" : "train_novel_transformerxgb",
  "description" : null,
  "code" : "#!/bin/bash\n\n#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"train_self_attention_xgb_slurm.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\n\n# CPU\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n\n# GPU\n#SBATCH --account=qtong\n#SBATCH --qos=gpu\n#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A100.80gb:1                # up to 8; only request what you need\n\n\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J combined_transformer_xgb_slurm       # Job name\n#SBATCH --account=qtong\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20                 # number of cores needed\n#SBATCH --mem=150G\n#SBATCH --time=24:00:00         # walltime\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\nset echo\numask 0027\n\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# export CUDA_LAUNCH_BLOCKING=1\n\npython -u << INNER_EOF\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib\n\nmatplotlib.use('Agg')\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Constants\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\nBASE_FEATURES = [\n    'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'fsca', 'Slope'\n]\n\n# Derived features include lagged values\n# FEATURES = BASE_FEATURES + [f\"{feature}_lag{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Data Preprocessing\ndef load_data(filepath):\n    print(\"Loading dataset...\")\n    df = pd.read_csv(filepath)\n    df = df[df[TARGET_COLUMN] > 0]\n    df = df.sort_values(by=[\"station_name\", \"date\"])\n    # df = df.iloc[:10000]  # Limit rows for faster processing\n    return df\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, features, target_feature, forecast_horizon=7):\n        \"\"\"\n        Initializes the TimeSeriesDataset object.\n\n        Args:\n            data (DataFrame): The data used for training.\n            sequence_length (int): The length of the historical time window (e.g., past 7 days).\n            features (list): List of feature columns to use as input.\n            target_feature (str): The specific target feature to predict (e.g., 'SWE').\n            forecast_horizon (int): The number of days to forecast for each day in the input sequence.\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.features = features\n        self.target_feature = target_feature\n        self.forecast_horizon = forecast_horizon\n        self.sequences, self.targets = self.create_sequences()\n\n    def create_sequences(self):\n        sequences = []\n        targets = []\n\n        # Iterate over each station (or other unique identifier in your dataset)\n        for station in self.data['station_name'].unique():\n            station_data = self.data[self.data['station_name'] == station]\n\n            # Ensure sufficient data for both input and output sequences\n            for i in range(len(station_data) - self.sequence_length - self.forecast_horizon):\n                # Input sequence: past `sequence_length` days\n                seq = station_data.iloc[i:i + self.sequence_length][self.features].values\n                sequences.append(seq)\n\n                # Target sequence: for each day in the input sequence, predict the next `forecast_horizon` days\n                targets.append(\n                    station_data.iloc[i + self.sequence_length:i + self.sequence_length + \\\n                                      self.forecast_horizon][self.target_feature].values\n                )\n\n        # Return sequences and targets as numpy arrays\n        return np.array(sequences), np.array(targets)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        # Fetch the input sequence and target sequence\n        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n        \n        # Print the shapes for debugging\n#         print(f\"Input sequence shape: {sequence.shape}, Target sequence shape: {target.shape}\")\n        \n        return sequence, target\n\n\n# Transformer Feature Extractor\nclass TransformerFeatureExtractor(nn.Module):\n    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n        super(TransformerFeatureExtractor, self).__init__()\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, SEQUENCE_LENGTH, hidden_dim))\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.1,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.feature_pool = nn.AdaptiveAvgPool1d(1)  # Pool over sequence length\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Add positional encoding to the input\n        x = self.input_embedding(x) + self.positional_encoding\n        # Pass through Transformer Encoder\n        x = self.transformer_encoder(x)\n        # Pool across the sequence dimension\n        x = self.feature_pool(x.transpose(1, 2)).squeeze(-1)\n        return self.output_layer(x)\n\n\n\n# Training and Feature Extraction\ndef train_transformer_feature_extractor(model, train_loader, val_loader, criterion, optimizer, epochs):\n    \"\"\"\n    Trains the transformer model with the given data and returns the training\n    and validation loss over epochs.\n    \"\"\"\n    train_loss_list = []\n    val_loss_list = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for batch_idx, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)  # Calculate average training loss\n        train_loss_list.append(avg_train_loss)  # Add to training loss list\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, (x, y) in enumerate(val_loader):\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n        val_loss_list.append(avg_val_loss)  # Add to validation loss list\n\n        # Print epoch results\n        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n\n    # Return epoch-wise training and validation loss\n    return train_loss_list, val_loss_list\n\n\n\ndef extract_features(model, data_loader):\n    model.eval()\n    features_list, targets_list = [], []\n    with torch.no_grad():\n        for x, y in data_loader:\n            features = model(x)\n            features_list.append(features.numpy())\n            targets_list.append(y.numpy())\n    return np.concatenate(features_list, axis=0), np.concatenate(targets_list, axis=0)\n\ndef plot_comparison_chart(test_targets, test_preds, plot_dir):\n    \"\"\"\n    Plots a comparison chart of predicted vs actual values and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.scatter(test_targets.flatten(), test_preds.flatten(), alpha=0.6, label=\"Predicted vs Actual\")\n    plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], \n             color='red', linestyle='--', label=\"Ideal Fit\")\n    plt.xlabel(\"Actual Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"Comparison of Predicted vs Actual Values\")\n    plt.legend()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    comparison_plot_path = f\"{plot_dir}/novel_comparison_plot_{timestamp}.png\"\n    plt.savefig(comparison_plot_path)\n    print(f\"Comparison chart saved at {comparison_plot_path}\")\n    # plt.show()\n    plt.close()\n\ndef plot_learning_curve(train_loss, val_loss, plot_dir):\n    \"\"\"\n    Plots the learning curve (training and validation loss over epochs) and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\")\n    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Learning Curve\")\n    plt.legend()\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    learning_curve_path = f\"{plot_dir}/novel_learning_curve_{timestamp}.png\"\n    plt.savefig(learning_curve_path)\n    print(f\"Learning curve saved at {learning_curve_path}\")\n    # plt.show()\n    plt.close()\n\n\n\n# Main Workflow\ndef do_ensemble_model(filepath, plot_dir):\n    \"\"\"\n    Performs ensemble modeling and visualizes results including:\n    - R2 score\n    - Comparison plots\n    - Learning curves\n    \"\"\"\n    print(\"Step 1: Loading and preprocessing data...\")\n    df = load_data(filepath)\n    print(f\"Data loaded successfully. Dataset contains {len(df)} records.\")\n\n    print(\"Step 2: Creating datasets and DataLoaders...\")\n    dataset = TimeSeriesDataset(df, SEQUENCE_LENGTH, FEATURES, TARGET_COLUMN)\n    print(f\"Dataset created with {len(dataset)} samples.\")\n\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=32*20, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32*20, shuffle=False)\n    print(f\"Train size: {train_size}, Validation size: {val_size}\")\n\n    print(\"Step 3: Initializing the transformer model...\")\n    transformer = TransformerFeatureExtractor(\n        input_dim=len(FEATURES), nhead=4, hidden_dim=64, num_layers=2, output_dim=7\n    )\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(transformer.parameters(), lr=1e-3)\n    print(\"Transformer model, loss function, and optimizer initialized.\")\n\n    print(\"Step 4: Training the transformer model...\")\n    train_loss, val_loss = train_transformer_feature_extractor(transformer, train_loader, \n                                                               val_loader, criterion, optimizer, \n                                                               epochs=2)\n    print(\"Transformer training completed.\")\n\n    print(\"Step 5: Extracting features from the transformer...\")\n    train_features, train_targets = extract_features(transformer, train_loader)\n    test_features, test_targets = extract_features(transformer, val_loader)\n    print(f\"Feature extraction complete. Train shape: {train_features.shape}, Test shape: {test_features.shape}\")\n\n    print(\"Step 6: Training the XGBoost model on extracted features...\")\n    print(f\"Training data shape for XGBoost - Features: {train_features.shape}, Targets: {train_targets.shape}\")\n    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", \n                                 n_estimators=100, \n                                 max_depth=6, \n                                 learning_rate=0.1)\n    xgb_model.fit(train_features, train_targets)\n    print(\"XGBoost training completed.\")\n\n    # Step 7: Saving the models\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_save_dir = os.path.join(plot_dir, \"models\")\n    os.makedirs(model_save_dir, exist_ok=True)\n    \n    # Save the transformer model\n    transformer_save_path = os.path.join(model_save_dir, f\"ensemble_transformer_model_{timestamp}.pth\")\n    torch.save(transformer.state_dict(), transformer_save_path)\n    print(f\"Transformer model saved at: {transformer_save_path}\")\n    \n    # Save the XGBoost model\n    xgb_model_save_path = os.path.join(model_save_dir, f\"ensemble_xgboost_model_{timestamp}.json\")\n    xgb_model.save_model(xgb_model_save_path)\n    print(f\"XGBoost model saved at: {xgb_model_save_path}\")\n\n    print(\"Step 8: Evaluating the XGBoost model...\")\n    print(f\"Test data shape for XGBoost prediction - Features: {test_features.shape}, Targets: {test_targets.shape}\")\n    test_preds = xgb_model.predict(test_features)\n\n    print(f\"Shape of predictions: {test_preds.shape}\")\n    rmse = np.sqrt(mean_squared_error(test_targets, test_preds))\n    r2 = r2_score(test_targets, test_preds)\n    print(f\"RMSE: {rmse:.4f} R2 Score: {r2:.4f}\")\n\n    print(\"Step 9: Generating plots...\")\n    plot_comparison_chart(test_targets, test_preds, plot_dir)\n    plot_learning_curve(train_loss, val_loss, plot_dir)\n    print(f\"Plots saved in: {plot_dir}\")\n\n\n\n# Main Script\nif __name__ == \"__main__\":\n    filepath = '/groups/ESS3/zsun/swe/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n    plot_dir = '/groups/ESS3/zsun/swe/plots'\n    os.makedirs(plot_dir, exist_ok=True)\n    do_ensemble_model(filepath, plot_dir)\n\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\necho \"job status $job_status\"\nif [[ $job_status == *\"COMPLETED\"* ]]; then\n    exit 0\nfi\n\nexit 1\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "f86ae7",
  "name" : "predict_transformerxgb",
  "description" : null,
  "code" : "#!/bin/bash\n\n#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"train_self_attention_xgb_slurm.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\n\n# CPU\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n\n# GPU\n#SBATCH --account=qtong\n#SBATCH --qos=gpu\n#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A100.80gb:1                # up to 8; only request what you need\n\n\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J combined_transformer_xgb_slurm       # Job name\n#SBATCH --account=qtong\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20                 # number of cores needed\n#SBATCH --mem=50G\n#SBATCH --time=24:00:00         # walltime\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\nset echo\numask 0027\n\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# export CUDA_LAUNCH_BLOCKING=1\n\npython -u << INNER_EOF\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib\n\n\nmatplotlib.use('Agg')\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Constants\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\nBASE_FEATURES = [\n    'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'fsca', 'Slope'\n]\n\n# Derived features include lagged values\n# FEATURES = BASE_FEATURES + [f\"{feature}_lag{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Data Preprocessing\ndef load_data(filepath):\n    print(\"Loading dataset...\")\n    df = pd.read_csv(filepath)\n    df = df[df[TARGET_COLUMN] > 0]\n    df = df.sort_values(by=[\"station_name\", \"date\"])\n    # df = df.iloc[:10000]  # Limit rows for faster processing\n    return df\n\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, features, target_column):\n        \"\"\"\n        Initializes the TimeSeriesDataset object for prediction.\n\n        Args:\n            data (numpy array): The data with shape (time_steps, stations, features).\n            sequence_length (int): The length of the historical time window (e.g., past 7 days).\n        \"\"\"\n        self.data = data  # Shape (7, 462204, 19)\n        self.sequence_length = sequence_length\n        self.sequences = self.create_sequences()\n        self.features = features\n        self.target_column = target_column\n\n    def create_sequences(self):\n        # reshape the input into (462204, 7, 19)\n        reshaped_array = np.transpose(self.data, (1, 0, 2))\n        data = np.nan_to_num(reshaped_array, nan=-1)\n        return data\n        # sequences = []\n\n        # # Extract dimensions\n        # num_time_steps, num_stations, num_features = self.data.shape\n\n        # # Ensure there are enough time steps for at least one sequence\n        # if num_time_steps < self.sequence_length:\n        #     print(f\"Warning: Insufficient time steps ({num_time_steps}) for the given sequence length \"\n        #           f\"({self.sequence_length}). No sequences will be created.\")\n        #     return []\n\n        # # Iterate over each station\n        # for station_idx in range(num_stations):\n        #     station_data = self.data[:, station_idx, :]  # Shape (time_steps, features)\n\n        #     # Use the last `sequence_length` time steps for prediction\n        #     seq = station_data[-self.sequence_length:, :]  # Shape (sequence_length, features)\n        #     sequences.append(seq)\n\n        # return np.array(sequences)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        # Fetch the input sequence\n        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)  # Shape (sequence_length, features)\n        return sequence\n\n\n# Transformer Feature Extractor\nclass TransformerFeatureExtractor(nn.Module):\n    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n        super(TransformerFeatureExtractor, self).__init__()\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, SEQUENCE_LENGTH, hidden_dim))\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.1,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.feature_pool = nn.AdaptiveAvgPool1d(1)  # Pool over sequence length\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Add positional encoding to the input\n        x = self.input_embedding(x) + self.positional_encoding\n        # Pass through Transformer Encoder\n        x = self.transformer_encoder(x)\n        # Pool across the sequence dimension\n        x = self.feature_pool(x.transpose(1, 2)).squeeze(-1)\n        return self.output_layer(x)\n\n\n\n# Training and Feature Extraction\ndef train_transformer_feature_extractor(model, train_loader, val_loader, criterion, optimizer, epochs):\n    \"\"\"\n    Trains the transformer model with the given data and returns the training\n    and validation loss over epochs.\n    \"\"\"\n    train_loss_list = []\n    val_loss_list = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for batch_idx, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)  # Calculate average training loss\n        train_loss_list.append(avg_train_loss)  # Add to training loss list\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, (x, y) in enumerate(val_loader):\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n        val_loss_list.append(avg_val_loss)  # Add to validation loss list\n\n        # Print epoch results\n        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n\n    # Return epoch-wise training and validation loss\n    return train_loss_list, val_loss_list\n\n\n\ndef extract_features(model, data_loader):\n    model.eval()\n    features_list, targets_list = [], []\n    with torch.no_grad():\n        for x, y in data_loader:\n            features = model(x)\n            features_list.append(features.numpy())\n            targets_list.append(y.numpy())\n    return np.concatenate(features_list, axis=0), np.concatenate(targets_list, axis=0)\n\ndef plot_comparison_chart(test_targets, test_preds, plot_dir):\n    \"\"\"\n    Plots a comparison chart of predicted vs actual values and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.scatter(test_targets.flatten(), test_preds.flatten(), alpha=0.6, label=\"Predicted vs Actual\")\n    plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], \n             color='red', linestyle='--', label=\"Ideal Fit\")\n    plt.xlabel(\"Actual Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"Comparison of Predicted vs Actual Values\")\n    plt.legend()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    comparison_plot_path = f\"{plot_dir}/novel_comparison_plot_{timestamp}.png\"\n    plt.savefig(comparison_plot_path)\n    print(f\"Comparison chart saved at {comparison_plot_path}\")\n    # plt.show()\n    plt.close()\n\ndef plot_learning_curve(train_loss, val_loss, plot_dir):\n    \"\"\"\n    Plots the learning curve (training and validation loss over epochs) and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\")\n    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Learning Curve\")\n    plt.legend()\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    learning_curve_path = f\"{plot_dir}/novel_learning_curve_{timestamp}.png\"\n    plt.savefig(learning_curve_path)\n    print(f\"Learning curve saved at {learning_curve_path}\")\n    # plt.show()\n    plt.close()\n\n\nclass EnsembleModelPredictor:\n    \"\"\"\n    Ensemble Model Predictor for processing the time series.\n    \"\"\"\n    def __init__(self, transformer_path, xgb_path, features, sequence_length, target_column, device='cpu'):\n        \"\"\"\n        Initializes the EnsembleModelPredictor with paths to the saved models.\n\n        Args:\n            transformer_path (str): Path to the saved Transformer model.\n            xgb_path (str): Path to the saved XGBoost model.\n            features (list): List of feature column names.\n            sequence_length (int): Sequence length for the Transformer input.\n            target_column (str): Name of the target column.\n            device (str): Device to load the Transformer model ('cpu' or 'cuda').\n        \"\"\"\n        self.transformer_path = transformer_path\n        self.xgb_path = xgb_path\n        self.features = features\n        self.sequence_length = sequence_length\n        self.target_column = target_column\n        self.device = device\n\n        # Load models\n        print(\"Loading Transformer model...\")\n        self.transformer = TransformerFeatureExtractor(\n            input_dim=len(features), nhead=4, hidden_dim=64, num_layers=2, output_dim=7\n        )\n        print(\"Transformer model configuration:\")\n        print(f\"  Input Dimension: {len(features)}\")\n        self.transformer.load_state_dict(torch.load(transformer_path, map_location=device))\n\n        # Print the loaded model's details\n        print(\"Model details after loading weights:\")\n        print(self.transformer)\n\n        # Optionally, print all parameter names and their shapes\n        print(\"\\nModel parameters after loading weights:\")\n        for name, param in self.transformer.named_parameters():\n            print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")\n\n        for name, param in self.transformer.named_parameters():\n            print(f\"{name}: min={torch.min(param)}, max={torch.max(param)}, mean={torch.mean(param)}\")\n\n\n        self.transformer.to(device)\n        self.transformer.eval()\n        print(\"Transformer model loaded successfully.\")\n\n        print(\"Loading XGBoost model...\")\n        self.xgb_model = xgb.XGBRegressor()\n        self.xgb_model.load_model(xgb_path)\n        print(\"XGBoost model loaded successfully.\")\n\n    def preprocess_data(self, data):\n        \"\"\"\n        Prepares the input data for prediction.\n\n        Args:\n            data (np.array): Input data as a NumPy array.\n\n        Returns:\n            DataLoader: DataLoader for the preprocessed data.\n        \"\"\"\n        print(\"Preprocessing input data...\")\n        dataset = TimeSeriesDataset(data, sequence_length=self.sequence_length, features=self.features, target_column=self.target_column)\n        dataloader = DataLoader(dataset, batch_size=640, shuffle=False)\n        print(f\"Data preprocessed. Total samples: {len(dataset)}\")\n        return dataloader\n\n    def predict(self, data):\n        \"\"\"\n        Performs the two-step prediction using Transformer and XGBoost models.\n\n        Args:\n            data (np.array): Input data as a NumPy array.\n\n        Returns:\n            np.array: Final predictions for the input data.\n        \"\"\"\n        # Step 1: Preprocess the data\n        dataloader = self.preprocess_data(data)\n\n        # Step 2: Extract features using the Transformer\n        print(\"Extracting features using the Transformer model...\")\n        transformer_features = []\n        past_sequences = []\n\n\n\n        with torch.no_grad():\n            for batch in dataloader:\n                # print(batch)\n                # Check if the batch contains only NaN values\n                if torch.isnan(batch).all():\n                    print(\"Error: Batch contains only NaN values. Terminating process.\")\n                    raise ValueError(\"Why batch only has nan, no values?\")\n\n                # print(\"batch shape: \", batch.shape)\n                batch = batch.to(self.device)\n                \n                # Extract past sequence features using the Transformer\n                future_sequence_features = self.transformer(batch)\n                # print(\"output: \", future_sequence_features)\n\n                if torch.isnan(future_sequence_features).all():\n                    print(\"Error: future_sequence_features contains only NaN values. Terminating process.\")\n                    raise ValueError(\"Why does future_sequence_features only have nan values?\")\n                transformer_features.append(future_sequence_features.cpu().numpy())\n                \n                # The future sequence (e.g., next time steps in the data) will be used as is\n                past_sequences.append(batch.cpu().numpy())\n\n                # break\n\n        transformer_features = np.concatenate(transformer_features, axis=0)\n        past_sequences = np.concatenate(past_sequences, axis=0)\n\n        # transformer_features_expanded = np.expand_dims(transformer_features, axis=-1)  \n\n        print(f\"Feature extraction complete. Shape: {transformer_features.shape}, Future sequences shape: {past_sequences.shape}\")\n\n        print(\"Flattening the last two dimensions of Transformer features and past sequences...\")\n        transformer_features_flat = transformer_features.reshape(transformer_features.shape[0], -1)\n        past_sequences_flat = past_sequences.reshape(past_sequences.shape[0], -1)\n        print(f\"Flattening complete. Transformer features shape: {transformer_features_flat.shape}, Past sequences shape: {past_sequences_flat.shape}\")\n\n        # Calculate and print statistics for transformer_features_flat\n        # print(\"\\nStatistics for `transformer_features_flat`:\")\n        # print(f\"  Shape: {transformer_features_flat.shape}\")\n        # print(f\"  Mean: {np.mean(transformer_features_flat):.4f}\")\n        # print(f\"  Standard Deviation: {np.std(transformer_features_flat):.4f}\")\n        # print(f\"  Min: {np.min(transformer_features_flat):.4f}\")\n        # print(f\"  Max: {np.max(transformer_features_flat):.4f}\")\n        # print(f\"  Number of elements: {transformer_features_flat.size}\")\n        # print(\"-\" * 50)\n\n        # # Calculate and print statistics for past_sequences_flat\n        # print(\"Statistics for `past_sequences_flat`:\")\n        # print(f\"  Shape: {past_sequences_flat.shape}\")\n        # print(f\"  Mean: {np.mean(past_sequences_flat):.4f}\")\n        # print(f\"  Standard Deviation: {np.std(past_sequences_flat):.4f}\")\n        # print(f\"  Min: {np.min(past_sequences_flat):.4f}\")\n        # print(f\"  Max: {np.max(past_sequences_flat):.4f}\")\n        # print(f\"  Number of elements: {past_sequences_flat.size}\")\n        # print(\"-\" * 50)\n\n        # Step 3: Combine flattened features for XGBoost input\n        print(\"Combining features for XGBoost input...\")\n        # xgb_input = np.concatenate([transformer_features_flat, past_sequences_flat], axis=-1)\n        xgb_input = transformer_features\n        print(f\"XGBoost input shape: {xgb_input.shape}\")\n\n        # Step 3: Combine past sequence features and future sequence data for XGBoost input\n        # xgb_input = np.concatenate([transformer_features_expanded, past_sequences], axis=-1)\n        # print(f\"XGBoost input shape: {xgb_input.shape}\")\n\n        # Step 4: Predict with the XGBoost model\n        # Print XGBoost model details\n        print(\"Inspecting the trained XGBoost model...\")\n        print(f\"XGBoost parameters: {self.xgb_model.get_params()}\")\n        # Retrieve the Booster object\n        booster = self.xgb_model.get_booster()\n        print(\"Booster object retrieved.\")\n        print(booster.get_dump())\n\n        # Number of features can be inferred from the input or the Booster\n        print(f\"Booster number of features: {booster.num_features()}\")\n        # Requires Booster object\n\n        # Step 4: Predict with the XGBoost model\n        print(\"Performing predictions with the XGBoost model...\")\n        np.concatenate(xgb_input, axis=0)\n        predictions = self.xgb_model.predict(xgb_input)\n        print(f\"Predictions complete. Output shape: {predictions.shape}\")\n        print(f\"First 5 inputs: {xgb_input[:5]}\")\n        print(f\"First 5 predictions: {predictions[:5]}\")\n        # print(\"Performing predictions with XGBoost model...\")\n        # predictions = self.xgb_model.predict(xgb_input)\n        # print(f\"Final predictions complete. Shape: {predictions.shape}\")\n\n        return predictions\n\n    def save_predictions(self, predictions, output_path):\n        \"\"\"\n        Saves the predictions to a CSV file.\n\n        Args:\n            predictions (np.array): Predicted values.\n            output_path (str): Path to save the predictions.\n        \"\"\"\n        print(f\"Saving predictions to {output_path}...\")\n        # the predictions contains all 7 future days. Only output 1 day\n        df = pd.DataFrame(predictions[:, 0], columns=[self.target_column])\n        print(df.describe())\n        df.to_csv(output_path, index=False)\n        print(\"Predictions saved successfully.\")\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef prepare_input_numpy_for_date(target_date=\"2024-11-20\"):\n    try:\n        # Debug: Start of function\n        print(\"Starting the process to prepare input NumPy array...\")\n        print(f\"Target date: {target_date}\")\n\n        # Parse the target date\n        target_date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n\n        # Generate the past 7 days including the target date\n        past_dates = [(target_date_obj - timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(7)]\n        print(f\"Past 7 days: {past_dates}\")\n\n        # File path and pattern\n        base_path = \"/groups/ESS3/zsun/swe/data/testing_input/\"  # Replace with actual path\n        file_pattern = \"testing_all_ready_{}.csv\"\n\n        # Initialize list to hold data for each day\n        daily_data = []\n\n        for date in reversed(past_dates):\n            file_name = file_pattern.format(date)\n            file_path = os.path.join(base_path, file_name)\n\n            if not os.path.exists(file_path):\n                print(f\"Warning: File for date {date} not found: {file_path}\")\n                continue\n\n            print(f\"Loading file: {file_path}\")\n            daily_df = pd.read_csv(file_path)\n            \n            # Ensure the 'date' column is consistent and matches the file's date\n            daily_df['date'] = date\n            print(\"original df columns: \", daily_df.columns)\n\n            # Append to the list\n            daily_data.append(daily_df)\n\n        if len(daily_data) < 7:\n            raise ValueError(f\"Insufficient data: Found data for {len(daily_data)} days, but 7 days are required.\")\n\n        # Combine all data into a single DataFrame\n        combined_df = pd.concat(daily_data, ignore_index=True)\n        print(f\"Combined data shape: {combined_df.shape}\")\n\n        # Rename columns to standardize\n        # column_mapping = {\n        #     'SWE': 'AMSR_SWE',\n        #     'air_temperature_observed_f': 'tmmx', \n        #     'precipitation_amount': 'pr',\n        #     'relative_humidity_rmin': 'rmin',\n        #     'potential_evapotranspiration': 'etr',\n        #     'air_temperature_tmmx': 'tmmx',\n        #     'relative_humidity_rmax': 'rmax',\n        #     'mean_vapor_pressure_deficit': 'vpd',\n        #     'air_temperature_tmmn': 'tmmn',\n        #     'wind_speed': 'vs',\n        # }\n        # combined_df = combined_df.rename(columns=column_mapping)\n\n        # [\n        #     'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n        #     'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n        #     'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n        #     'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n        #     'Northness', 'Eastness', 'fsca', 'Slope'\n        # ]\n\n        # Latitude,Longitude,rmin,vs,vpd,tmmn,pr,tmmx,rmax,etr,x,y,Elevation,Slope,Aspect,Curvature,Northness,Eastness,AMSR_SWE_2024-10-01,AMSR_Flag_2024-10-01,AMSR_SWE_2024-10-02,AMSR_Flag_2024-10-02,AMSR_SWE_2024-10-03,AMSR_Flag_2024-10-03,AMSR_SWE_2024-10-04,AMSR_Flag_2024-10-04,AMSR_SWE_2024-10-05,AMSR_Flag_2024-10-05,AMSR_SWE_2024-10-06,AMSR_Flag_2024-10-06,AMSR_SWE_2024-10-07,AMSR_Flag_2024-10-07,AMSR_SWE_2024-10-08,AMSR_Flag_2024-10-08,AMSR_SWE_2024-10-09,AMSR_Flag_2024-10-09,AMSR_SWE_2024-10-10,AMSR_Flag_2024-10-10,AMSR_SWE_2024-10-11,AMSR_Flag_2024-10-11,AMSR_SWE_2024-10-12,AMSR_Flag_2024-10-12,AMSR_SWE_2024-10-13,AMSR_Flag_2024-10-13,AMSR_SWE_2024-10-14,AMSR_Flag_2024-10-14,AMSR_SWE_2024-10-15,AMSR_Flag_2024-10-15,AMSR_SWE_2024-10-16,AMSR_Flag_2024-10-16,AMSR_SWE_2024-10-17,AMSR_Flag_2024-10-17,AMSR_SWE_2024-10-18,AMSR_Flag_2024-10-18,AMSR_SWE_2024-10-19,AMSR_Flag_2024-10-19,AMSR_SWE_2024-10-20,AMSR_Flag_2024-10-20,AMSR_SWE_2024-10-21,AMSR_Flag_2024-10-21,AMSR_SWE_2024-10-22,AMSR_Flag_2024-10-22,AMSR_SWE_2024-10-23,AMSR_Flag_2024-10-23,AMSR_SWE_2024-10-24,AMSR_Flag_2024-10-24,AMSR_SWE_2024-10-25,AMSR_Flag_2024-10-25,AMSR_SWE_2024-10-26,AMSR_Flag_2024-10-26,AMSR_SWE_2024-10-27,AMSR_Flag_2024-10-27,AMSR_SWE_2024-10-28,AMSR_Flag_2024-10-28,AMSR_SWE_2024-10-29,AMSR_Flag_2024-10-29,AMSR_SWE_2024-10-30,AMSR_Flag_2024-10-30,AMSR_SWE_2024-10-31,AMSR_Flag_2024-10-31,AMSR_SWE_2024-11-01,AMSR_Flag_2024-11-01,AMSR_SWE_2024-11-02,AMSR_Flag_2024-11-02,AMSR_SWE_2024-11-03,AMSR_Flag_2024-11-03,AMSR_SWE_2024-11-04,AMSR_Flag_2024-11-04,AMSR_SWE,AMSR_Flag,cumulative_AMSR_SWE,date,fsca_2024-10-01,fsca_2024-10-02,fsca_2024-10-03,fsca_2024-10-04,fsca_2024-10-05,fsca_2024-10-06,fsca_2024-10-07,fsca_2024-10-08,fsca_2024-10-09,fsca_2024-10-10,fsca_2024-10-11,fsca_2024-10-12,fsca_2024-10-13,fsca_2024-10-14,fsca_2024-10-15,fsca_2024-10-16,fsca_2024-10-17,fsca_2024-10-18,fsca_2024-10-19,fsca_2024-10-20,fsca_2024-10-21,fsca_2024-10-22,fsca_2024-10-23,fsca_2024-10-24,fsca_2024-10-25,fsca_2024-10-26,fsca_2024-10-27,fsca_2024-10-28,fsca_2024-10-29,fsca_2024-10-30,fsca_2024-10-31,fsca_2024-11-01,fsca_2024-11-02,fsca_2024-11-03,fsca_2024-11-04,fsca,cumulative_fsca,date,lc_prop3,water_year\n\n        column_mapping = {\n            'AMSR_SWE': 'SWE',\n            'tmmx': 'air_temperature_observed_f',  # this column is duplicated with tmmx, why?\n            'pr': 'precipitation_amount',\n            'rmin': 'relative_humidity_rmin',\n            'etr': 'potential_evapotranspiration',\n            'tmmx': 'air_temperature_tmmx',\n            'rmax': 'relative_humidity_rmax',\n            'vpd': 'mean_vapor_pressure_deficit',\n            'tmmn': 'air_temperature_tmmn',\n            'vs': 'wind_speed',\n        }\n        combined_df = combined_df.rename(columns=column_mapping)\n\n        # Filter necessary columns\n        # variable_columns = ['Latitude', 'Longitude', 'tmmn', 'etr', \n        #                     'rmin', 'vpd', 'tmmx', 'rmax', \n        #                     'vs', 'pr', 'Elevation', 'Slope', 'Aspect', 'Curvature', \n        #                     'Northness', 'Eastness', 'AMSR_SWE', 'cumulative_AMSR_SWE']\n        variable_columns = BASE_FEATURES\n        combined_df = combined_df[variable_columns + ['date', 'Latitude','Longitude',]]\n        print(\"Filtered columns to include necessary variables.\")\n\n        # Sort data by Latitude, Longitude, and date\n        combined_df['date'] = pd.to_datetime(combined_df['date'])\n        combined_df = combined_df.sort_values(by=['Latitude', 'Longitude', 'date'])\n\n        # Group data by grid points and reshape\n        grouped = combined_df.groupby(['Latitude', 'Longitude'])\n        numpy_array_list = []\n\n        print(combined_df.head())\n        print(\"the 19 columns: \", combined_df.columns)\n\n        for (lat, lon), group in grouped:\n            group_sorted = group.sort_values(by='date')\n            if len(group_sorted) == 7:\n                numpy_array_list.append(group_sorted[variable_columns].to_numpy())\n\n        if not numpy_array_list:\n            raise ValueError(\"No grids found with exactly 7 days of data.\")\n\n        # Stack arrays into final shape\n        final_array = np.stack(numpy_array_list, axis=1)  # Shape: (7, grids, variables)\n        print(f\"Final array shape: {final_array.shape}\")\n\n        # Save as .npy file\n        output_path = f\"/groups/ESS3/zsun/swe/data/testing_ready_input_bttf/combined_array_{target_date}.npy\"\n        np.save(output_path, final_array)\n        print(f\"Saved NumPy array at: {output_path}\")\n\n        return final_array\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise\n\ndef load_and_analyze_numpy(file_path):\n    try:\n        # Load the saved NumPy array\n        print(f\"Loading NumPy array from: {file_path}\")\n        data = np.load(file_path)\n        print(f\"Array shape: {data.shape} (days, grids, variables)\")\n\n        # Compute statistics\n        stats = {\n            'mean': np.mean(data, axis=(0, 1)),\n            'min': np.min(data, axis=(0, 1)),\n            'max': np.max(data, axis=(0, 1)),\n            'std': np.std(data, axis=(0, 1)),\n        }\n\n        # Print statistics for each variable\n        for i, var_stats in enumerate(zip(stats['mean'], stats['min'], stats['max'], stats['std'])):\n            mean, min_val, max_val, std = var_stats\n            print(f\"Variable {i+1}: Mean={mean:.3f}, Min={min_val:.3f}, Max={max_val:.3f}, Std={std:.3f}\")\n\n    except Exception as e:\n        print(f\"An error occurred while loading or analyzing the array: {e}\")\n\n\ndef do_prediction(target_date=\"2024-11-20\"):\n    # Paths to saved models\n    transformer_model_path = \"/groups/ESS3/zsun/swe/plots/models/ensemble_transformer_model_20241214_002501.pth\"\n    xgb_model_path = \"/groups/ESS3/zsun/swe/plots/models/ensemble_xgboost_model_20241214_002501.json\"\n\n    # Path to input CSV and output predictions\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # target_date = \"2024-11-20\"\n    # input_csv = \"/groups/ESS3/zsun/swe/data/testing_ready_input/input_2024-11-20.csv\"\n    input_np_path = f\"/groups/ESS3/zsun/swe/data/testing_ready_input_bttf/combined_array_{target_date}.npy\"\n    output_prediction_path = f\"/groups/ESS3/zsun/swe/data/testing_output/transformer_xgb_predictions_{timestamp}.csv\"\n\n    # Initialize the predictor\n    predictor = EnsembleModelPredictor(\n        transformer_path=transformer_model_path,\n        xgb_path=xgb_model_path,\n        features=FEATURES,\n        sequence_length=SEQUENCE_LENGTH,\n        target_column=TARGET_COLUMN\n    )\n\n    # Run predictions\n    data = np.load(input_np_path)  # Example of loading your data\n    predictions = predictor.predict(data)\n\n    # Save predictions\n    predictor.save_predictions(predictions, output_prediction_path)\n\nif __name__ == \"__main__\":\n    prepare_input_numpy_for_date(target_date=\"2024-11-11\")\n    # load_and_analyze_numpy(\"/groups/ESS3/zsun/swe/data/testing_ready_input_bttf/combined_array_2024-11-10.npy\")\n    # columns\n#     'Latitude', 'Longitude', 'tmmn', 'etr', 'rmin', 'vpd', 'tmmx', 'rmax',\n# >        'vs', 'pr', 'Elevation', 'Slope', 'Aspect', 'Aspect', 'Curvature',\n# >        'Northness', 'Eastness', 'AMSR_SWE', 'cumulative_AMSR_SWE', 'date']\n    do_prediction(target_date=\"2024-11-11\")\n    \n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(<\"${file_name}\")\nexit_code=0\nwhile true; do\n    # Capture the current content\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    current_content=$(<\"${file_name}\")\n\n    # Compare current content with previous content\n    diff_result=$(diff <(echo \"$previous_content\") <(echo \"$current_content\"))\n    # Check if there is new content\n    if [ -n \"$diff_result\" ]; then\n        echo \"$diff_result\"\n    fi\n    # Update previous content\n    previous_content=\"$current_content\"\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* || $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\necho \"job status $job_status\"\nif [[ $job_status == *\"COMPLETED\"* ]]; then\n    exit 0\nfi\n\nexit 1\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7ktwm9",
  "name" : "bttf_swe_predict",
  "description" : null,
  "code" : "\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib\nfrom snowcast_utils import homedir, work_dir, test_start_date, test_end_date\nfrom convert_results_to_images import convert_csvs_to_images_simple, convert_csv_to_geotiff\nimport geopandas as gpd\n\nmatplotlib.use('Agg')\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Constants\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\nPREDICTED_COLUMN = \"predicted_swe\"\n\nBASE_FEATURES = [\n    'SWE', \n    # 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f', # remove SNOTEL values\n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'fsca', 'Slope'\n]\n\n# Derived features include lagged values\n# FEATURES = BASE_FEATURES + [f\"{feature}_lag{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Data Preprocessing\ndef load_data(filepath):\n    print(\"Loading dataset...\")\n    df = pd.read_csv(filepath)\n    df = df[df[TARGET_COLUMN] > 0]\n    df = df.sort_values(by=[\"station_name\", \"date\"])\n    # df = df.iloc[:10000]  # Limit rows for faster processing\n    return df\n\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, features, target_column):\n        \"\"\"\n        Initializes the TimeSeriesDataset object for prediction.\n\n        Args:\n            data (numpy array): The data with shape (time_steps, stations, features).\n            sequence_length (int): The length of the historical time window (e.g., past 7 days).\n        \"\"\"\n        self.data = data  # Shape (7, 462204, 19)\n        self.sequence_length = sequence_length\n        self.sequences = self.create_sequences()\n        self.features = features\n        self.target_column = target_column\n\n    def create_sequences(self):\n        # reshape the input into (462204, 7, 19)\n        reshaped_array = np.transpose(self.data, (1, 0, 2))\n        data = np.nan_to_num(reshaped_array, nan=-1)\n        return data\n        # sequences = []\n\n        # # Extract dimensions\n        # num_time_steps, num_stations, num_features = self.data.shape\n\n        # # Ensure there are enough time steps for at least one sequence\n        # if num_time_steps < self.sequence_length:\n        #     print(f\"Warning: Insufficient time steps ({num_time_steps}) for the given sequence length \"\n        #           f\"({self.sequence_length}). No sequences will be created.\")\n        #     return []\n\n        # # Iterate over each station\n        # for station_idx in range(num_stations):\n        #     station_data = self.data[:, station_idx, :]  # Shape (time_steps, features)\n\n        #     # Use the last `sequence_length` time steps for prediction\n        #     seq = station_data[-self.sequence_length:, :]  # Shape (sequence_length, features)\n        #     sequences.append(seq)\n\n        # return np.array(sequences)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        # Fetch the input sequence\n        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)  # Shape (sequence_length, features)\n        return sequence\n\n\n# Transformer Feature Extractor\nclass TransformerFeatureExtractor(nn.Module):\n    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n        super(TransformerFeatureExtractor, self).__init__()\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, SEQUENCE_LENGTH, hidden_dim))\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.1,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.feature_pool = nn.AdaptiveAvgPool1d(1)  # Pool over sequence length\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Add positional encoding to the input\n        x = self.input_embedding(x) + self.positional_encoding\n        # Pass through Transformer Encoder\n        x = self.transformer_encoder(x)\n        # Pool across the sequence dimension\n        x = self.feature_pool(x.transpose(1, 2)).squeeze(-1)\n        return self.output_layer(x)\n\n\n\n# Training and Feature Extraction\ndef train_transformer_feature_extractor(model, train_loader, val_loader, criterion, optimizer, epochs):\n    \"\"\"\n    Trains the transformer model with the given data and returns the training\n    and validation loss over epochs.\n    \"\"\"\n    train_loss_list = []\n    val_loss_list = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for batch_idx, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)  # Calculate average training loss\n        train_loss_list.append(avg_train_loss)  # Add to training loss list\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, (x, y) in enumerate(val_loader):\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n        val_loss_list.append(avg_val_loss)  # Add to validation loss list\n\n        # Print epoch results\n        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n\n    # Return epoch-wise training and validation loss\n    return train_loss_list, val_loss_list\n\n\ndef extract_features(model, data_loader):\n    model.eval()\n    features_list, targets_list = [], []\n    with torch.no_grad():\n        for x, y in data_loader:\n            # Extract features from the model\n            features = model(x)  # Shape: [batch_size, seq_length]\n\n            # Reshape features to [batch_size, seq_length, 1]\n            features_reshaped = features.unsqueeze(2)  # Add a new dimension, resulting in [batch_size, seq_length, 1]\n\n            # Merge features with x along the feature dimension\n            combined_features = torch.cat((x, features_reshaped), dim=2)  # Shape: [batch_size, seq_length, feature_dim + 1]\n\n            # Add combined features to the list\n            features_list.append(combined_features.numpy())\n\n            # Use only the first day of the target list\n            targets_list.append(y[:, 0].numpy())  # Extract the first column\n\n    # Concatenate all features and targets into single arrays\n    concatenated_features = np.concatenate(features_list, axis=0)\n    concatenated_targets = np.concatenate(targets_list, axis=0)\n\n    # Flatten the last two dimensions of concatenated_features\n    flattened_features = concatenated_features.reshape(concatenated_features.shape[0], -1)\n\n    # Print shapes for debugging\n    print(f\"Original concatenated features shape: {concatenated_features.shape}\")\n    print(f\"Flattened features shape: {flattened_features.shape}\")\n    print(f\"Extracted targets shape: {concatenated_targets.shape}\")\n\n    return flattened_features, concatenated_targets\n\n\ndef plot_comparison_chart(test_targets, test_preds, plot_dir):\n    \"\"\"\n    Plots a comparison chart of predicted vs actual values and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.scatter(test_targets.flatten(), test_preds.flatten(), alpha=0.6, label=\"Predicted vs Actual\")\n    plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], \n             color='red', linestyle='--', label=\"Ideal Fit\")\n    plt.xlabel(\"Actual Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"Comparison of Predicted vs Actual Values\")\n    plt.legend()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    comparison_plot_path = f\"{plot_dir}/novel_comparison_plot_{timestamp}.png\"\n    plt.savefig(comparison_plot_path)\n    print(f\"Comparison chart saved at {comparison_plot_path}\")\n    # plt.show()\n    plt.close()\n\ndef plot_learning_curve(train_loss, val_loss, plot_dir):\n    \"\"\"\n    Plots the learning curve (training and validation loss over epochs) and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\")\n    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Learning Curve\")\n    plt.legend()\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    learning_curve_path = f\"{plot_dir}/novel_learning_curve_{timestamp}.png\"\n    plt.savefig(learning_curve_path)\n    print(f\"Learning curve saved at {learning_curve_path}\")\n    # plt.show()\n    plt.close()\n\n\nclass EnsembleModelPredictor:\n    \"\"\"\n    Ensemble Model Predictor for processing the time series.\n    \"\"\"\n    def __init__(self, transformer_path, xgb_path, features, sequence_length, target_column, device='cpu'):\n        \"\"\"\n        Initializes the EnsembleModelPredictor with paths to the saved models.\n\n        Args:\n            transformer_path (str): Path to the saved Transformer model.\n            xgb_path (str): Path to the saved XGBoost model.\n            features (list): List of feature column names.\n            sequence_length (int): Sequence length for the Transformer input.\n            target_column (str): Name of the target column.\n            device (str): Device to load the Transformer model ('cpu' or 'cuda').\n        \"\"\"\n        self.transformer_path = transformer_path\n        self.xgb_path = xgb_path\n        self.features = features\n        self.sequence_length = sequence_length\n        self.target_column = target_column\n        self.device = device\n\n        # Load models\n        print(\"Loading Transformer model...\")\n        self.transformer = TransformerFeatureExtractor(\n            input_dim=len(features), nhead=4, hidden_dim=64, num_layers=2, output_dim=7\n        )\n        print(\"Transformer model configuration:\")\n        print(f\"  Input Dimension: {len(features)}\")\n        self.transformer.load_state_dict(torch.load(transformer_path, map_location=device))\n\n        # Print the loaded model's details\n        # print(\"Model details after loading weights:\")\n        # print(self.transformer)\n\n        # Optionally, print all parameter names and their shapes\n        # print(\"\\nModel parameters after loading weights:\")\n        # for name, param in self.transformer.named_parameters():\n        #     print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")\n\n        # for name, param in self.transformer.named_parameters():\n        #     print(f\"{name}: min={torch.min(param)}, max={torch.max(param)}, mean={torch.mean(param)}\")\n\n        self.transformer.to(device)\n        self.transformer.eval()\n        print(\"Transformer model loaded successfully.\")\n\n        print(\"Loading XGBoost model...\")\n        self.xgb_model = xgb.XGBRegressor()\n        self.xgb_model.load_model(xgb_path)\n        print(\"XGBoost model loaded successfully.\")\n\n    def preprocess_data(self, data):\n        \"\"\"\n        Prepares the input data for prediction.\n\n        Args:\n            data (np.array): Input data as a NumPy array.\n\n        Returns:\n            DataLoader: DataLoader for the preprocessed data.\n        \"\"\"\n        print(\"Preprocessing input data...\")\n        dataset = TimeSeriesDataset(data, sequence_length=self.sequence_length, features=self.features, target_column=self.target_column)\n        dataloader = DataLoader(dataset, batch_size=640, shuffle=False)\n        print(f\"Data preprocessed. Total samples: {len(dataset)}\")\n        return dataloader\n\n    def predict(self, data):\n        \"\"\"\n        Performs the two-step prediction using Transformer and XGBoost models.\n\n        Args:\n            data (np.array): Input data as a NumPy array.\n\n        Returns:\n            np.array: Final predictions for the input data.\n        \"\"\"\n        print(\"data.shape: \", data.shape)\n        lat_lon = data[0, :, -2:] # get lat and lon\n        print(\"lat_lon shape: \", lat_lon.shape)\n        \n        # remove the last three columns\n        data = data[:, :, :-2]\n\n        # Step 1: Preprocess the data\n        dataloader = self.preprocess_data(data)\n\n        # Step 2: Extract features using the Transformer\n        print(\"Extracting features using the Transformer model...\")\n        transformer_features = []\n        past_sequences = []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                # print(batch)\n                # Check if the batch contains only NaN values\n                if torch.isnan(batch).all():\n                    print(\"Error: Batch contains only NaN values. Terminating process.\")\n                    raise ValueError(\"Why batch only has nan, no values?\")\n\n                # print(\"batch shape: \", batch.shape)\n                batch = batch.to(self.device)\n                \n                # Extract past sequence features using the Transformer\n                future_sequence_features = self.transformer(batch)\n                # print(\"output: \", future_sequence_features)\n\n                if torch.isnan(future_sequence_features).all():\n                    print(\"Error: future_sequence_features contains only NaN values. Terminating process.\")\n                    raise ValueError(\"Why does future_sequence_features only have nan values?\")\n                transformer_features.append(future_sequence_features.cpu().numpy())\n                \n                # The future sequence (e.g., next time steps in the data) will be used as is\n                past_sequences.append(batch.cpu().numpy())\n\n                # break\n\n        # Concatenate features\n        transformer_features = np.concatenate(transformer_features, axis=0)\n        past_sequences = np.concatenate(past_sequences, axis=0)\n\n        print(past_sequences.shape)\n        print(transformer_features.shape)\n\n        # Convert to PyTorch tensors\n        transformer_features_tensor = torch.tensor(transformer_features)\n\n        # Add a new dimension (unsqueeze) along the last axis, resulting in [batch_size, seq_length, 1]\n        transformer_features_reshaped = transformer_features_tensor.unsqueeze(2)  # Adds a new dimension\n\n        # Merge features with past_sequences along the feature dimension\n        # Ensure past_sequences is also a tensor if it's a NumPy array\n        past_sequences_tensor = torch.tensor(past_sequences)\n\n        # Concatenate along the feature dimension (dim=2)\n        combined_features = torch.cat((past_sequences_tensor, transformer_features_reshaped), dim=2)\n\n        # Print the shape of the combined features\n        print(combined_features.shape)\n\n        # Step 3: Combine flattened features for XGBoost input\n        print(\"Combining features for XGBoost input \", combined_features.shape)\n\n        # Flatten the combined features for XGBoost input\n        # Ensure combined_features is a PyTorch tensor before reshaping\n        flattened_features = combined_features.reshape(combined_features.shape[0], -1)\n        print(\"Flatten combined features: \", flattened_features.shape)\n\n        # Convert the flattened features to NumPy array (required by XGBoost)\n        xgb_input = flattened_features.cpu().numpy()  # Use .cpu() in case the tensor is on GPU\n        print(f\"XGBoost input shape: {xgb_input.shape}\")\n\n        # Step 3: Combine past sequence features and future sequence data for XGBoost input\n        # xgb_input = np.concatenate([transformer_features_expanded, past_sequences], axis=-1)\n        # print(f\"XGBoost input shape: {xgb_input.shape}\")\n\n        # Step 4: Predict with the XGBoost model\n        # Print XGBoost model details\n        print(\"Inspecting the trained XGBoost model...\")\n        print(f\"XGBoost parameters: {self.xgb_model.get_params()}\")\n        # Retrieve the Booster object\n        booster = self.xgb_model.get_booster()\n        print(\"Booster object retrieved.\")\n        # print(booster.get_dump())\n\n        # Number of features can be inferred from the input or the Booster\n        print(f\"Booster number of features: {booster.num_features()}\")\n        # Requires Booster object\n\n        # Step 4: Predict with the XGBoost model\n        print(\"Performing predictions with the XGBoost model...\")\n        predictions = self.xgb_model.predict(xgb_input)\n        print(f\"Predictions complete. Output shape: {predictions.shape}\")\n\n        \n        predictions = predictions.reshape(-1, 1)\n        print(\"predictions.shape: \", predictions.shape)\n        final_output = np.concatenate([lat_lon, predictions], axis=-1)\n        print(f\"Final output shape: {final_output.shape}\")\n        # print(f\"First 5 inputs: {xgb_input[:5]}\")\n        # print(f\"First 5 predictions: {predictions[:5]}\")\n        return final_output\n\n    def save_predictions(self, predictions, output_path):\n        \"\"\"\n        Saves the predictions to a CSV file.\n\n        Args:\n            predictions (np.array): Predicted values.\n            output_path (str): Path to save the predictions.\n        \"\"\"\n        print(f\"Saving predictions to {output_path}...\")\n        # the predictions contains all 7 future days. Only output 1 day\n        df = pd.DataFrame(predictions, columns=[\"Latitude\", \"Longitude\", PREDICTED_COLUMN])\n        print(df.describe())\n        df.to_csv(output_path, index=False)\n        print(\"Predictions saved successfully.\")\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef prepare_input_numpy_for_date(target_date=\"2024-11-20\", output_path: str = None, force=False):\n    try:\n        # Debug: Start of function\n        print(\"Starting the process to prepare input NumPy array...\")\n        print(f\"Target date: {target_date}\")\n\n        # Save as .npy file\n        if output_path is None:\n            output_path = f\"{homedir}/bttf_output/combined_array_{target_date}.npy\"\n        output_path_dir = os.path.dirname(output_path)\n        os.makedirs(output_path_dir, exist_ok=True)\n\n        if os.path.exists(output_path) and not force:\n            print(f\"File exists: {output_path}\")\n            return output_path\n\n        # Parse the target date\n        target_date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n\n        # Generate the past 7 days including the target date\n        past_dates = [(target_date_obj - timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(7)]\n        print(f\"Past 7 days: {past_dates}\")\n\n        # File path and pattern\n        base_path = f\"{work_dir}/\"  # Replace with actual path\n        file_pattern = \"testing_all_ready_{}.csv\"\n\n        # Initialize list to hold data for each day\n        daily_data = []\n        all_columns = []\n\n        for date in reversed(past_dates):\n            file_name = file_pattern.format(date)\n            file_path = os.path.join(base_path, file_name)\n\n            if not os.path.exists(file_path):\n                print(f\"Warning: File for date {date} not found: {file_path}\")\n                continue\n\n            print(f\"Loading file: {file_path}\")\n            daily_df = pd.read_csv(file_path)\n            \n            # Ensure the 'date' column is consistent and matches the file's date\n            daily_df['date'] = date\n            print(\"original df columns: \", daily_df.columns)\n            all_columns = daily_df.columns\n            # Append to the list\n            daily_data.append(daily_df)\n\n        if len(daily_data) < 7:\n            raise ValueError(f\"Insufficient data: Found data for {len(daily_data)} days, but 7 days are required.\")\n\n        # Combine all data into a single DataFrame\n        combined_df = pd.concat(daily_data, ignore_index=True)\n        print(f\"Combined data shape: {combined_df.shape}\", combined_df.columns)\n\n        column_mapping = {\n            'AMSR_SWE': 'SWE',\n            'tmmx': 'air_temperature_observed_f',  # this column is duplicated with tmmx, why?\n            'pr': 'precipitation_amount',\n            'rmin': 'relative_humidity_rmin',\n            'etr': 'potential_evapotranspiration',\n            'tmmx': 'air_temperature_tmmx',\n            'rmax': 'relative_humidity_rmax',\n            'vpd': 'mean_vapor_pressure_deficit',\n            'tmmn': 'air_temperature_tmmn',\n            'vs': 'wind_speed',\n        }\n        combined_df = combined_df.rename(columns=column_mapping)\n\n        combined_df = combined_df.fillna(-1)\n        print(\"Assign -1 to fsca and SWE column where values are >100..\")\n        combined_df.loc[combined_df['fsca'] > 100, 'fsca'] = -1\n        combined_df.loc[combined_df['SWE'] > 100, 'SWE'] = -1\n\n        # Filter necessary columns\n        # variable_columns = ['Latitude', 'Longitude', 'tmmn', 'etr', \n        #                     'rmin', 'vpd', 'tmmx', 'rmax', \n        #                     'vs', 'pr', 'Elevation', 'Slope', 'Aspect', 'Curvature', \n        #                     'Northness', 'Eastness', 'AMSR_SWE', 'cumulative_AMSR_SWE']\n        variable_columns = BASE_FEATURES\n        print(\"Only retain \", variable_columns)\n        all_variable_columns = variable_columns + ['date', 'Latitude','Longitude',] # must save the date and lat and lon into the numpy\n        final_variable_columns = variable_columns + ['Latitude','Longitude',]\n        combined_df = combined_df[all_variable_columns]\n        print(\"Filtered columns to include necessary variables.\")\n\n        # Sort data by Latitude, Longitude, and date\n        combined_df['date'] = pd.to_datetime(combined_df['date'])\n        combined_df = combined_df.sort_values(by=['Latitude', 'Longitude', 'date'])\n\n        # Group data by grid points and reshape\n        grouped = combined_df.groupby(['Latitude', 'Longitude'])\n        numpy_array_list = []\n\n        print(combined_df.head())\n        print(\"the 19 columns: \", combined_df.columns)\n\n        for (lat, lon), group in grouped:\n            group_sorted = group.sort_values(by='date')\n            if len(group_sorted) == 7:\n                numpy_array_list.append(group_sorted[final_variable_columns].to_numpy())\n\n        if not numpy_array_list:\n            raise ValueError(\"No grids found with exactly 7 days of data.\")\n\n        # Stack arrays into final shape\n        final_array = np.stack(numpy_array_list, axis=1)  # Shape: (7, grids, variables)\n        print(f\"Final array shape: {final_array.shape}\")\n        np.save(output_path, final_array)\n        print(f\"Saved NumPy array at: {output_path}\")\n\n        return final_array\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise\n\ndef load_and_analyze_numpy(file_path):\n    try:\n        # Load the saved NumPy array\n        print(f\"Loading NumPy array from: {file_path}\")\n        data = np.load(file_path)\n\n        print(f\"Array shape: {data.shape} (days, grids, variables)\")\n        column_names = BASE_FEATURES + [\"Latitude\", \"Longitude\"]\n\n        # Validate the number of columns matches the array shape\n        if len(column_names) != data.shape[2]:\n            print(\"Warning: Number of column names does not match the number of variables in the data.\")\n\n\n        # Compute statistics\n        stats = {\n            'mean': np.mean(data, axis=(0, 1)),\n            'min': np.min(data, axis=(0, 1)),\n            'max': np.max(data, axis=(0, 1)),\n            'std': np.std(data, axis=(0, 1)),\n        }\n\n        # Print statistics for each variable\n        for i, col_name in enumerate(column_names):\n            variable_data = data[:, :, i]\n            mean_val = np.nanmean(variable_data)  # Use nanmean to avoid NaNs in statistics\n            min_val = np.nanmin(variable_data)\n            max_val = np.nanmax(variable_data)\n            std_val = np.nanstd(variable_data)\n            print(f\"Variable {i+1} ({col_name}): Mean={mean_val:.3f}, Min={min_val:.3f}, Max={max_val:.3f}, Std={std_val:.3f}\")\n\n    except Exception as e:\n        print(f\"An error occurred while loading or analyzing the array: {e}\")\n\ndef do_prediction(target_date=\"2024-11-20\", output_prediction_path:str = None, force=False):\n    # Paths to saved models\n    transformer_model_path = f\"{homedir}/../models/ensemble_transformer_model_20250105_091448.pth\"\n    xgb_model_path = f\"{homedir}/../models/ensemble_xgboost_model_20250105_091448.json\"\n\n    print(\"Model used for prediction: \", transformer_model_path, xgb_model_path)\n\n# 50 epoch\n#     Transformer model saved at: /media/volume1/swe/data//../plots/models/ensemble_transformer_model_20250104_220829.pth\n# XGBoost model saved at: /media/volume1/swe/data//../plots/models/ensemble_xgboost_model_20250104_220829.json\n\n# 200 epoch\n# Transformer model saved at: /media/volume1/swe/data//../plots/models/ensemble_transformer_model_20250105_044829.pth\n# XGBoost model saved at: /media/volume1/swe/data//../plots/models/ensemble_xgboost_model_20250105_044829.json\n\n# 500 epoch\n# ransformer model saved at: /media/volume1/swe/data//../plots/models/ensemble_transformer_model_20250105_091448.pth\n# XGBoost model saved at: /media/volume1/swe/data//../plots/models/ensemble_xgboost_model_20250105_091448.json\n\n    # Path to input CSV and output predictions\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # target_date = \"2024-11-20\"\n    # input_csv = \"/groups/ESS3/zsun/swe/data/testing_ready_input/input_2024-11-20.csv\"\n    input_np_path = f\"{homedir}/bttf_output/combined_array_{target_date}.npy\"\n\n    if output_prediction_path is None:\n        output_prediction_path = f\"{homedir}/bttf_output/transformer_xgb_predictions_{target_date}.csv\"\n\n    if os.path.exists(output_prediction_path) and not force:\n        print(f\"File exists: {output_prediction_path}\")\n        return output_prediction_path\n\n    input_np_path_dir = os.path.dirname(input_np_path)\n    os.makedirs(input_np_path_dir, exist_ok=True)\n    output_np_path_dir = os.path.dirname(output_prediction_path)\n    os.makedirs(output_np_path_dir, exist_ok=True)\n\n    # Initialize the predictor\n    predictor = EnsembleModelPredictor(\n        transformer_path=transformer_model_path,\n        xgb_path=xgb_model_path,\n        features=FEATURES,\n        sequence_length=SEQUENCE_LENGTH,\n        target_column=TARGET_COLUMN\n    )\n\n    # Run predictions\n    data = np.load(input_np_path)  # Example of loading your data\n\n    predictions = predictor.predict(data)\n\n    # Save predictions\n    predictor.save_predictions(predictions, output_prediction_path,)\n\n\nif __name__ == \"__main__\":\n    target_date = \"2025-01-01\"\n    bttf_npy_file = f\"{homedir}/bttf_output/combined_array_{target_date}.npy\"\n    prepare_input_numpy_for_date(target_date=target_date, output_path = bttf_npy_file, force=True)\n    load_and_analyze_numpy(file_path = bttf_npy_file)\n    # columns\n#     'Latitude', 'Longitude', 'tmmn', 'etr', 'rmin', 'vpd', 'tmmx', 'rmax',\n# >        'vs', 'pr', 'Elevation', 'Slope', 'Aspect', 'Aspect', 'Curvature',\n# >        'Northness', 'Eastness', 'AMSR_SWE', 'cumulative_AMSR_SWE', 'date']\n    predicted_csv = f\"{homedir}/bttf_output/transformer_xgb_predictions_{target_date}.csv\"\n    do_prediction(\n        target_date=target_date, output_prediction_path=predicted_csv, force=True\n    )\n    # convert_csvs_to_images_simple(target_date = target_date)\n    convert_csvs_to_images_simple(\n        target_date=target_date, \n        column_name = \"predicted_swe\", \n        test_csv = predicted_csv,\n        res_png_path = f\"{homedir}/bttf_output/{target_date}.png\"\n    );\n    convert_csv_to_geotiff(\n        target_date, \n        test_csv=predicted_csv, \n        target_geotiff_file=f\"{homedir}/bttf_output/swe_predicted_{target_date}.tif\"\n    )\n    \n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "04fgyq",
  "name" : "bttf_train",
  "description" : null,
  "code" : "\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib\nfrom snowcast_utils import homedir, work_dir\n\nmatplotlib.use('Agg')\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Constants\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\nBASE_FEATURES = [\n    'SWE', \n    #'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'fsca', 'Slope'\n]\n\n# ET can reach 0.82 and 2.2\n# selected_columns = [\n#   'swe_value',\n#   'SWE',\n#   'fsca',\n#   'air_temperature_tmmx', \n#   'air_temperature_tmmn', \n#   'potential_evapotranspiration', \n#   'relative_humidity_rmax', \n#   'Elevation',\t\n#   'Slope',\t\n#   'Curvature',\t\n#   'Aspect',\t\n#   'Eastness',\t\n#   'Northness',\n# ]\n\n# Derived features include lagged values\n# FEATURES = BASE_FEATURES + [f\"{feature}_lag{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Data Preprocessing\ndef load_data(filepath):\n    print(\"Loading dataset...\")\n    df = pd.read_csv(filepath)\n    df = df[df[TARGET_COLUMN] > 0]\n    df = df.sort_values(by=[\"station_name\", \"date\"])\n\n    print(df.columns)\n    df = df.fillna(-1)\n    print(\"Assign -1 to fsca and SWE column where values are >100..\")\n    df.loc[df['fsca'] > 100, 'fsca'] = -1\n    df.loc[df['SWE'] > 100, 'SWE'] = -1\n    \n    # df = df.iloc[:10000]  # Limit rows for faster processing\n    return df\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, features, target_feature, forecast_horizon=7):\n        \"\"\"\n        Initializes the TimeSeriesDataset object.\n\n        Args:\n            data (DataFrame): The data used for training.\n            sequence_length (int): The length of the historical time window (e.g., past 7 days).\n            features (list): List of feature columns to use as input.\n            target_feature (str): The specific target feature to predict (e.g., 'SWE').\n            forecast_horizon (int): The number of days to forecast for each day in the input sequence.\n        \"\"\"\n        self.data = data\n        self.sequence_length = sequence_length\n        self.features = features\n        self.target_feature = target_feature\n        self.forecast_horizon = forecast_horizon\n        self.sequences, self.targets = self.create_sequences()\n\n    def create_sequences(self):\n        sequences = []\n        targets = []\n\n        # Iterate over each station (or other unique identifier in your dataset)\n        for station in self.data['station_name'].unique():\n            station_data = self.data[self.data['station_name'] == station]\n\n            # Ensure sufficient data for both input and output sequences\n            for i in range(len(station_data) - self.sequence_length - self.forecast_horizon):\n                # Input sequence: past `sequence_length` days\n                seq = station_data.iloc[i:i + self.sequence_length][self.features].values\n                sequences.append(seq)\n\n                # Target sequence: for each day in the input sequence, predict the next `forecast_horizon` days\n                targets.append(\n                    station_data.iloc[i + self.sequence_length:i + self.sequence_length + \\\n                                      self.forecast_horizon][self.target_feature].values\n                )\n\n        # Return sequences and targets as numpy arrays\n        return np.array(sequences), np.array(targets)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        # Fetch the input sequence and target sequence\n        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n        target = torch.tensor(self.targets[idx], dtype=torch.float32)\n        \n        # Print the shapes for debugging\n#         print(f\"Input sequence shape: {sequence.shape}, Target sequence shape: {target.shape}\")\n        \n        return sequence, target\n\n\n# Transformer Feature Extractor\nclass TransformerFeatureExtractor(nn.Module):\n    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n        super(TransformerFeatureExtractor, self).__init__()\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, SEQUENCE_LENGTH, hidden_dim))\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.1,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.feature_pool = nn.AdaptiveAvgPool1d(1)  # Pool over sequence length\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Add positional encoding to the input\n        x = self.input_embedding(x) + self.positional_encoding\n        # Pass through Transformer Encoder\n        x = self.transformer_encoder(x)\n        # Pool across the sequence dimension\n        x = self.feature_pool(x.transpose(1, 2)).squeeze(-1)\n        return self.output_layer(x)\n\n\n\n# Training and Feature Extraction\ndef train_transformer_feature_extractor(model, train_loader, val_loader, criterion, optimizer, epochs):\n    \"\"\"\n    Trains the transformer model with the given data and returns the training\n    and validation loss over epochs.\n    \"\"\"\n    train_loss_list = []\n    val_loss_list = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for batch_idx, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)  # Calculate average training loss\n        train_loss_list.append(avg_train_loss)  # Add to training loss list\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, (x, y) in enumerate(val_loader):\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n        val_loss_list.append(avg_val_loss)  # Add to validation loss list\n\n        # Print epoch results\n        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n\n    # Return epoch-wise training and validation loss\n    return train_loss_list, val_loss_list\n\ndef extract_features(model, data_loader):\n    model.eval()\n    features_list, targets_list = [], []\n    with torch.no_grad():\n        for x, y in data_loader:\n            # Extract features from the model\n            features = model(x)  # Shape: [batch_size, seq_length]\n\n            # Reshape features to [batch_size, seq_length, 1]\n            features_reshaped = features.unsqueeze(2)  # Add a new dimension, resulting in [batch_size, seq_length, 1]\n\n            # Merge features with x along the feature dimension\n            combined_features = torch.cat((x, features_reshaped), dim=2)  # Shape: [batch_size, seq_length, feature_dim + 1]\n\n            # Add combined features to the list\n            features_list.append(combined_features.numpy())\n\n            # Use only the first day of the target list\n            targets_list.append(y[:, 0].numpy())  # Extract the first column\n\n    # Concatenate all features and targets into single arrays\n    concatenated_features = np.concatenate(features_list, axis=0)\n    concatenated_targets = np.concatenate(targets_list, axis=0)\n\n    # Flatten the last two dimensions of concatenated_features\n    flattened_features = concatenated_features.reshape(concatenated_features.shape[0], -1)\n\n    # Print shapes for debugging\n    print(f\"Original concatenated features shape: {concatenated_features.shape}\")\n    print(f\"Flattened features shape: {flattened_features.shape}\")\n    print(f\"Extracted targets shape: {concatenated_targets.shape}\")\n\n    return flattened_features, concatenated_targets\n\n# def extract_features(model, data_loader):\n#     model.eval()\n#     features_list, targets_list = [], []\n#     with torch.no_grad():\n#         for x, y in data_loader:\n#             features = model(x)\n#             features_list.append(features.numpy())\n#             targets_list.append(y.numpy())\n#     return np.concatenate(features_list, axis=0), np.concatenate(targets_list, axis=0)\n\ndef plot_comparison_chart(test_targets, test_preds, plot_dir):\n    \"\"\"\n    Plots a comparison chart of predicted vs actual values and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.scatter(test_targets.flatten(), test_preds.flatten(), alpha=0.6, label=\"Predicted vs Actual\")\n    plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], \n             color='red', linestyle='--', label=\"Ideal Fit\")\n    plt.xlabel(\"Actual Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"Comparison of Predicted vs Actual Values\")\n    plt.legend()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    comparison_plot_path = f\"{plot_dir}/novel_comparison_plot_{timestamp}.png\"\n    plt.savefig(comparison_plot_path)\n    print(f\"Comparison chart saved at {comparison_plot_path}\")\n    # plt.show()\n    plt.close()\n\ndef plot_learning_curve(train_loss, val_loss, plot_dir):\n    \"\"\"\n    Plots the learning curve (training and validation loss over epochs) and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\")\n    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Learning Curve\")\n    plt.legend()\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    learning_curve_path = f\"{plot_dir}/novel_learning_curve_{timestamp}.png\"\n    plt.savefig(learning_curve_path)\n    print(f\"Learning curve saved at {learning_curve_path}\")\n    # plt.show()\n    plt.close()\n\n\n\n# Main Workflow\ndef do_ensemble_model(filepath, plot_dir):\n    \"\"\"\n    Performs ensemble modeling and visualizes results including:\n    - R2 score\n    - Comparison plots\n    - Learning curves\n    \"\"\"\n    print(\"Step 1: Loading and preprocessing data...\")\n    df = load_data(filepath)\n    print(f\"Data loaded successfully. Dataset contains {len(df)} records.\")\n\n    print(\"Step 2: Creating datasets and DataLoaders...\")\n    dataset = TimeSeriesDataset(df, SEQUENCE_LENGTH, FEATURES, TARGET_COLUMN)\n    print(f\"Dataset created with {len(dataset)} samples.\")\n\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n    train_loader = DataLoader(train_dataset, batch_size=32*20, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32*20, shuffle=False)\n    print(f\"Train size: {train_size}, Validation size: {val_size}\")\n\n    print(\"Step 3: Initializing the transformer model...\")\n    transformer = TransformerFeatureExtractor(\n        input_dim=len(FEATURES), nhead=4, hidden_dim=64, num_layers=2, output_dim=7\n    )\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(transformer.parameters(), lr=1e-3)\n    print(\"Transformer model, loss function, and optimizer initialized.\")\n\n    print(\"Step 4: Training the transformer model...\")\n    train_loss, val_loss = train_transformer_feature_extractor(transformer, train_loader, \n                                                               val_loader, criterion, optimizer, \n                                                               epochs=200)\n    print(\"Transformer training completed.\")\n\n    print(\"Step 5: Extracting features from the transformer...\")\n    train_features, train_targets = extract_features(transformer, train_loader)\n    test_features, test_targets = extract_features(transformer, val_loader)\n    print(f\"Feature extraction complete. Train shape: {train_features.shape}, Test shape: {test_features.shape}\")\n\n    print(\"Step 6: Training the XGBoost model on extracted features...\")\n    print(f\"Training data shape for XGBoost - Features: {train_features.shape}, Targets: {train_targets.shape}\")\n    xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", \n                                 n_estimators=100, \n                                 max_depth=6, \n                                 learning_rate=0.1)\n    xgb_model.fit(train_features, train_targets)\n    print(\"XGBoost training completed.\")\n\n    # Step 7: Saving the models\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_save_dir = os.path.join(plot_dir, \"models\")\n    os.makedirs(model_save_dir, exist_ok=True)\n    \n    # Save the transformer model\n    transformer_save_path = os.path.join(model_save_dir, f\"ensemble_transformer_model_{timestamp}.pth\")\n    torch.save(transformer.state_dict(), transformer_save_path)\n    print(f\"Transformer model saved at: {transformer_save_path}\")\n    \n    # Save the XGBoost model\n    xgb_model_save_path = os.path.join(model_save_dir, f\"ensemble_xgboost_model_{timestamp}.json\")\n    xgb_model.save_model(xgb_model_save_path)\n    print(f\"XGBoost model saved at: {xgb_model_save_path}\")\n\n    print(\"Step 8: Evaluating the XGBoost model...\")\n    print(f\"Test data shape for XGBoost prediction - Features: {test_features.shape}, Targets: {test_targets.shape}\")\n    test_preds = xgb_model.predict(test_features)\n\n    print(f\"Shape of predictions: {test_preds.shape}\")\n    rmse = np.sqrt(mean_squared_error(test_targets, test_preds))\n    r2 = r2_score(test_targets, test_preds)\n    print(f\"RMSE: {rmse:.4f} R2 Score: {r2:.4f}\")\n\n    print(\"Step 9: Generating plots...\")\n    plot_comparison_chart(test_targets, test_preds, plot_dir)\n    plot_learning_curve(train_loss, val_loss, plot_dir)\n    print(f\"Plots saved in: {plot_dir}\")\n\n\n\n# Main Script\nif __name__ == \"__main__\":\n    filepath = f'{homedir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n    plot_dir = f'{homedir}/../plots'\n    os.makedirs(plot_dir, exist_ok=True)\n    do_ensemble_model(filepath, plot_dir)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "dwa3fy",
  "name" : "xgb_train",
  "description" : null,
  "code" : "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom snowcast_utils import homedir, work_dir, data_dir, plot_dir, model_dir\n\nimport matplotlib\nmatplotlib.use('Agg')  # Use a non-GUI backend\n\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch import nn, optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport time\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Define sequence length and target column\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\n# Base features (without lags)\nBASE_FEATURES = [\n#     'SWE', 'air_temperature_tmmn', 'relative_humidity_rmin',\n#     'precipitation_amount', 'wind_speed', 'fsca'\n    'SWE', \n    # 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',  # these are from SNOTEL stations\n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', \n    'mean_vapor_pressure_deficit', 'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature', 'Northness', 'Eastness', \n    'fsca', 'Slope', \n]\n\n# Define paths and create necessary directories\n# filepath = '/groups/ESS3/zsun/swe/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\n\n# Features to include in the sequence (including lagged features)\n# FEATURES = BASE_FEATURES + [f\"{feature}_{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Train XGBoost model\ndef train_xgboost(X_train, y_train, X_val, y_val, model_save_path):\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'max_depth': 8,\n        'learning_rate': 0.1,\n        'n_estimators': 200,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8\n    }\n\n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True)\n\n    # Save the model with a timestamped filename\n    model_dir = os.path.dirname(model_save_path)\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    model_bkp_path = os.path.join(model_dir, f\"xgb_model_{timestamp}.json\")\n    model.save_model(model_bkp_path)\n    model.save_model(model_save_path)\n    print(f\"Model saved to {model_save_path} and backed up to {model_bkp_path}\")\n\n    return model\n\n# Evaluate XGBoost model\ndef evaluate_xgboost(model, X_test, y_test, scaler_y, plot_dir):\n    predictions = model.predict(X_test)\n    predictions = scaler_y.inverse_transform(predictions.reshape(-1, 1)).squeeze()\n    true_values = scaler_y.inverse_transform(y_test.reshape(-1, 1)).squeeze()\n\n    # Calculate metrics\n    mse = mean_squared_error(true_values, predictions)\n    r2 = r2_score(true_values, predictions)\n\n    print(f\"XGBoost Model Evaluation:\")\n    print(f\"Mean Squared Error: {mse}\")\n    print(f\"R-squared: {r2}\")\n\n    # Plot Predictions vs True Values\n    plt.figure(figsize=(10, 10))\n    plt.scatter(true_values, predictions, alpha=0.6)\n    plt.xlabel('True Values')\n    plt.ylabel('Predicted Values')\n    plt.title('XGBoost Predictions vs True Values')\n    plt.grid(True)\n    plt.tight_layout()\n    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n    print(\"XGBoost model is saved to \", os.path.join(plot_dir, f'xgboost_predictions_vs_true_{timestamp}.png'))\n    plt.savefig(os.path.join(plot_dir, 'xgboost_predictions_vs_true.png'))\n    plt.close()\n\n    # Residuals plot\n    residuals = true_values - predictions\n    plt.figure(figsize=(10, 6))\n    sns.histplot(residuals, kde=True, bins=30, color='green', alpha=0.7)\n    plt.xlabel('Residuals')\n    plt.title('XGBoost Residuals Distribution')\n    plt.tight_layout()\n    plt.savefig(os.path.join(plot_dir, f'xgboost_residuals_{timestamp}.png'))\n    plt.close()\n\n\n# Load and preprocess data\ndef load_preprocess_data_xgboost(filepath, target_col='swe_value'):\n    df = pd.read_csv(filepath)\n    print(f\"Original dataset shape: {df.shape}\")\n    print(\"df.columns = \", list(df.columns))\n    df = df.fillna(-1)\n    df = df[df[target_col] >= 0]  # Keep positive SWE values\n\n    print(\"Assign -1 to fsca and SWE column where values are >100..\")\n    df.loc[df['fsca'] > 100, 'fsca'] = -1\n    df.loc[df['SWE'] > 100, 'SWE'] = -1\n    # df = df.sample(n=500000, random_state=42)  # Downsample for speed\n    print(f\"Filtered dataset shape: {df.shape}\")\n\n    # Drop all columns with \"cumulative\" in their names\n    cumulative_columns = [col for col in df.columns if 'cumulative' in col.lower()]\n\n    precip_columns = [col for col in df.columns if 'precipitation_amount' in col.lower()]\n\n    columns_to_drop = [\n        'station_name', \n        # 'swe_value', \n        'change_in_swe_inch', \n        'snow_depth', \n        'air_temperature_observed_f', \n        # 'precipitation_amount'\n    ] + cumulative_columns + precip_columns\n\n    X = df.drop(columns=[target_col, 'date', ] + columns_to_drop)\n    print(f\"Dropped columns: {columns_to_drop}\")\n    print(\"Only choose the desired features, remove the duplicated time series columns\")\n    # print(FEATURES)\n    # X = X[FEATURES]\n    print(\"Current X columns: \", X.columns)\n    \n    # X = df.drop(columns=[target_col, 'date', 'station_name'])  # Drop unnecessary columns\n    y = df[target_col]\n    \n    scaler_X = StandardScaler()\n    scaler_y = StandardScaler()\n    \n    X_scaled = scaler_X.fit_transform(X)\n    y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).squeeze()\n\n    return X_scaled, y_scaled, scaler_y\n\n\ndef do_xgboost(filepath, plot_dir, model_save_path):\n    # Load and preprocess data\n    print(f\"Loading and preprocessing data from: {filepath}\")\n    X, y, scaler_y = load_preprocess_data_xgboost(filepath)\n    print(f\"Initial data shapes - X: {X.shape}, y: {y.shape}\")\n    \n    # Split data into train, validation, and test sets\n    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n    print(f\"After first split - X_train: {X_train.shape}, y_train: {y_train.shape}, X_temp: {X_temp.shape}, y_temp: {y_temp.shape}\")\n    \n    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n    print(f\"After second split - X_val: {X_val.shape}, y_val: {y_val.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n    \n    # Train XGBoost model\n    print(\"Training XGBoost model...\")\n    xgb_model = train_xgboost(X_train, y_train, X_val, y_val, model_save_path)\n\n    # Evaluate model\n    print(\"Evaluating XGBoost model...\")\n    evaluate_xgboost(xgb_model, X_test, y_test, scaler_y, plot_dir)\n\n# Main Script\nif __name__ == \"__main__\":\n    filepath = f'{work_dir}/all_points_final_merged_training.csv'\n    # Define paths and create necessary directories\n    model_save_path = f'{model_dir}/xgb_alone_model.pth'\n    model_dir = os.path.dirname(model_save_path)\n    os.makedirs(model_dir, exist_ok=True)\n    os.makedirs(plot_dir, exist_ok=True)\n    print(f\"Plot directory ensured at: {plot_dir}\")\n    # do_self_attention(filepath, plot_dir)\n    do_xgboost(filepath, plot_dir, model_save_path)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gz5syq",
  "name" : "xgb_predict",
  "description" : null,
  "code" : "\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib\n\n\nmatplotlib.use('Agg')\n\n# Ensure reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Constants\nSEQUENCE_LENGTH = 7\nTARGET_COLUMN = 'swe_value'\n\nBASE_FEATURES = [\n    'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n    'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n    'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n    'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n    'Northness', 'Eastness', 'fsca', 'Slope'\n]\n\n# Derived features include lagged values\n# FEATURES = BASE_FEATURES + [f\"{feature}_lag{i}\" for feature in BASE_FEATURES for i in range(1, SEQUENCE_LENGTH + 1)]\nFEATURES = BASE_FEATURES\n\n# Data Preprocessing\ndef load_data(filepath):\n    print(\"Loading dataset...\")\n    df = pd.read_csv(filepath)\n    df = df[df[TARGET_COLUMN] > 0]\n    df = df.sort_values(by=[\"station_name\", \"date\"])\n    # df = df.iloc[:10000]  # Limit rows for faster processing\n    return df\n\n\nclass TimeSeriesDataset(Dataset):\n    def __init__(self, data, sequence_length, features, target_column):\n        \"\"\"\n        Initializes the TimeSeriesDataset object for prediction.\n\n        Args:\n            data (numpy array): The data with shape (time_steps, stations, features).\n            sequence_length (int): The length of the historical time window (e.g., past 7 days).\n        \"\"\"\n        self.data = data  # Shape (7, 462204, 19)\n        self.sequence_length = sequence_length\n        self.sequences = self.create_sequences()\n        self.features = features\n        self.target_column = target_column\n\n    def create_sequences(self):\n        # reshape the input into (462204, 7, 19)\n        reshaped_array = np.transpose(self.data, (1, 0, 2))\n        data = np.nan_to_num(reshaped_array, nan=-1)\n        return data\n        # sequences = []\n\n        # # Extract dimensions\n        # num_time_steps, num_stations, num_features = self.data.shape\n\n        # # Ensure there are enough time steps for at least one sequence\n        # if num_time_steps < self.sequence_length:\n        #     print(f\"Warning: Insufficient time steps ({num_time_steps}) for the given sequence length \"\n        #           f\"({self.sequence_length}). No sequences will be created.\")\n        #     return []\n\n        # # Iterate over each station\n        # for station_idx in range(num_stations):\n        #     station_data = self.data[:, station_idx, :]  # Shape (time_steps, features)\n\n        #     # Use the last `sequence_length` time steps for prediction\n        #     seq = station_data[-self.sequence_length:, :]  # Shape (sequence_length, features)\n        #     sequences.append(seq)\n\n        # return np.array(sequences)\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        # Fetch the input sequence\n        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)  # Shape (sequence_length, features)\n        return sequence\n\n\n# Transformer Feature Extractor\nclass TransformerFeatureExtractor(nn.Module):\n    def __init__(self, input_dim, nhead, hidden_dim, num_layers, output_dim):\n        super(TransformerFeatureExtractor, self).__init__()\n        self.input_embedding = nn.Linear(input_dim, hidden_dim)\n        self.positional_encoding = nn.Parameter(torch.zeros(1, SEQUENCE_LENGTH, hidden_dim))\n        \n        # Transformer Encoder\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, \n            nhead=nhead, \n            dim_feedforward=hidden_dim * 4, \n            dropout=0.1,\n            batch_first=True\n        )\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        self.feature_pool = nn.AdaptiveAvgPool1d(1)  # Pool over sequence length\n        self.output_layer = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        # Add positional encoding to the input\n        x = self.input_embedding(x) + self.positional_encoding\n        # Pass through Transformer Encoder\n        x = self.transformer_encoder(x)\n        # Pool across the sequence dimension\n        x = self.feature_pool(x.transpose(1, 2)).squeeze(-1)\n        return self.output_layer(x)\n\n\n\n# Training and Feature Extraction\ndef train_transformer_feature_extractor(model, train_loader, val_loader, criterion, optimizer, epochs):\n    \"\"\"\n    Trains the transformer model with the given data and returns the training\n    and validation loss over epochs.\n    \"\"\"\n    train_loss_list = []\n    val_loss_list = []\n\n    for epoch in range(epochs):\n        print(f\"Epoch {epoch + 1}/{epochs}\")\n        \n        # Training phase\n        model.train()\n        train_loss = 0.0\n        for batch_idx, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            outputs = model(x)\n            loss = criterion(outputs, y)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        avg_train_loss = train_loss / len(train_loader)  # Calculate average training loss\n        train_loss_list.append(avg_train_loss)  # Add to training loss list\n\n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for batch_idx, (x, y) in enumerate(val_loader):\n                outputs = model(x)\n                loss = criterion(outputs, y)\n                val_loss += loss.item()\n\n        avg_val_loss = val_loss / len(val_loader)  # Calculate average validation loss\n        val_loss_list.append(avg_val_loss)  # Add to validation loss list\n\n        # Print epoch results\n        print(f\"  Training Loss: {avg_train_loss:.4f}\")\n        print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n\n    # Return epoch-wise training and validation loss\n    return train_loss_list, val_loss_list\n\n\n\ndef extract_features(model, data_loader):\n    model.eval()\n    features_list, targets_list = [], []\n    with torch.no_grad():\n        for x, y in data_loader:\n            features = model(x)\n            features_list.append(features.numpy())\n            targets_list.append(y.numpy())\n    return np.concatenate(features_list, axis=0), np.concatenate(targets_list, axis=0)\n\ndef plot_comparison_chart(test_targets, test_preds, plot_dir):\n    \"\"\"\n    Plots a comparison chart of predicted vs actual values and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.scatter(test_targets.flatten(), test_preds.flatten(), alpha=0.6, label=\"Predicted vs Actual\")\n    plt.plot([test_targets.min(), test_targets.max()], [test_targets.min(), test_targets.max()], \n             color='red', linestyle='--', label=\"Ideal Fit\")\n    plt.xlabel(\"Actual Values\")\n    plt.ylabel(\"Predicted Values\")\n    plt.title(\"Comparison of Predicted vs Actual Values\")\n    plt.legend()\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    comparison_plot_path = f\"{plot_dir}/novel_comparison_plot_{timestamp}.png\"\n    plt.savefig(comparison_plot_path)\n    print(f\"Comparison chart saved at {comparison_plot_path}\")\n    # plt.show()\n    plt.close()\n\ndef plot_learning_curve(train_loss, val_loss, plot_dir):\n    \"\"\"\n    Plots the learning curve (training and validation loss over epochs) and saves it to the plot directory.\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_loss) + 1), train_loss, label=\"Training Loss\")\n    plt.plot(range(1, len(val_loss) + 1), val_loss, label=\"Validation Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Learning Curve\")\n    plt.legend()\n    \n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    learning_curve_path = f\"{plot_dir}/novel_learning_curve_{timestamp}.png\"\n    plt.savefig(learning_curve_path)\n    print(f\"Learning curve saved at {learning_curve_path}\")\n    # plt.show()\n    plt.close()\n\n\nclass EnsembleModelPredictor:\n    \"\"\"\n    Ensemble Model Predictor for processing the time series.\n    \"\"\"\n    def __init__(self, transformer_path, xgb_path, features, sequence_length, target_column, device='cpu'):\n        \"\"\"\n        Initializes the EnsembleModelPredictor with paths to the saved models.\n\n        Args:\n            transformer_path (str): Path to the saved Transformer model.\n            xgb_path (str): Path to the saved XGBoost model.\n            features (list): List of feature column names.\n            sequence_length (int): Sequence length for the Transformer input.\n            target_column (str): Name of the target column.\n            device (str): Device to load the Transformer model ('cpu' or 'cuda').\n        \"\"\"\n        self.transformer_path = transformer_path\n        self.xgb_path = xgb_path\n        self.features = features\n        self.sequence_length = sequence_length\n        self.target_column = target_column\n        self.device = device\n\n        # Load models\n        print(\"Loading Transformer model...\")\n        self.transformer = TransformerFeatureExtractor(\n            input_dim=len(features), nhead=4, hidden_dim=64, num_layers=2, output_dim=7\n        )\n        print(\"Transformer model configuration:\")\n        print(f\"  Input Dimension: {len(features)}\")\n        self.transformer.load_state_dict(torch.load(transformer_path, map_location=device))\n\n        # Print the loaded model's details\n        print(\"Model details after loading weights:\")\n        print(self.transformer)\n\n        # Optionally, print all parameter names and their shapes\n        print(\"\\nModel parameters after loading weights:\")\n        for name, param in self.transformer.named_parameters():\n            print(f\"  {name}: shape={param.shape}, requires_grad={param.requires_grad}\")\n\n        for name, param in self.transformer.named_parameters():\n            print(f\"{name}: min={torch.min(param)}, max={torch.max(param)}, mean={torch.mean(param)}\")\n\n\n        self.transformer.to(device)\n        self.transformer.eval()\n        print(\"Transformer model loaded successfully.\")\n\n        print(\"Loading XGBoost model...\")\n        self.xgb_model = xgb.XGBRegressor()\n        self.xgb_model.load_model(xgb_path)\n        print(\"XGBoost model loaded successfully.\")\n\n    def preprocess_data(self, data):\n        \"\"\"\n        Prepares the input data for prediction.\n\n        Args:\n            data (np.array): Input data as a NumPy array.\n\n        Returns:\n            DataLoader: DataLoader for the preprocessed data.\n        \"\"\"\n        print(\"Preprocessing input data...\")\n        dataset = TimeSeriesDataset(data, sequence_length=self.sequence_length, features=self.features, target_column=self.target_column)\n        dataloader = DataLoader(dataset, batch_size=640, shuffle=False)\n        print(f\"Data preprocessed. Total samples: {len(dataset)}\")\n        return dataloader\n\n    def predict(self, data):\n        \"\"\"\n        Performs the two-step prediction using Transformer and XGBoost models.\n\n        Args:\n            data (np.array): Input data as a NumPy array.\n\n        Returns:\n            np.array: Final predictions for the input data.\n        \"\"\"\n        # Step 1: Preprocess the data\n        dataloader = self.preprocess_data(data)\n\n        # Step 2: Extract features using the Transformer\n        print(\"Extracting features using the Transformer model...\")\n        transformer_features = []\n        past_sequences = []\n\n\n\n        with torch.no_grad():\n            for batch in dataloader:\n                # print(batch)\n                # Check if the batch contains only NaN values\n                if torch.isnan(batch).all():\n                    print(\"Error: Batch contains only NaN values. Terminating process.\")\n                    raise ValueError(\"Why batch only has nan, no values?\")\n\n                # print(\"batch shape: \", batch.shape)\n                batch = batch.to(self.device)\n                \n                # Extract past sequence features using the Transformer\n                future_sequence_features = self.transformer(batch)\n                # print(\"output: \", future_sequence_features)\n\n                if torch.isnan(future_sequence_features).all():\n                    print(\"Error: future_sequence_features contains only NaN values. Terminating process.\")\n                    raise ValueError(\"Why does future_sequence_features only have nan values?\")\n                transformer_features.append(future_sequence_features.cpu().numpy())\n                \n                # The future sequence (e.g., next time steps in the data) will be used as is\n                past_sequences.append(batch.cpu().numpy())\n\n                # break\n\n        transformer_features = np.concatenate(transformer_features, axis=0)\n        past_sequences = np.concatenate(past_sequences, axis=0)\n\n        # transformer_features_expanded = np.expand_dims(transformer_features, axis=-1)  \n\n        print(f\"Feature extraction complete. Shape: {transformer_features.shape}, Future sequences shape: {past_sequences.shape}\")\n\n        print(\"Flattening the last two dimensions of Transformer features and past sequences...\")\n        transformer_features_flat = transformer_features.reshape(transformer_features.shape[0], -1)\n        past_sequences_flat = past_sequences.reshape(past_sequences.shape[0], -1)\n        print(f\"Flattening complete. Transformer features shape: {transformer_features_flat.shape}, Past sequences shape: {past_sequences_flat.shape}\")\n\n        # Calculate and print statistics for transformer_features_flat\n        # print(\"\\nStatistics for `transformer_features_flat`:\")\n        # print(f\"  Shape: {transformer_features_flat.shape}\")\n        # print(f\"  Mean: {np.mean(transformer_features_flat):.4f}\")\n        # print(f\"  Standard Deviation: {np.std(transformer_features_flat):.4f}\")\n        # print(f\"  Min: {np.min(transformer_features_flat):.4f}\")\n        # print(f\"  Max: {np.max(transformer_features_flat):.4f}\")\n        # print(f\"  Number of elements: {transformer_features_flat.size}\")\n        # print(\"-\" * 50)\n\n        # # Calculate and print statistics for past_sequences_flat\n        # print(\"Statistics for `past_sequences_flat`:\")\n        # print(f\"  Shape: {past_sequences_flat.shape}\")\n        # print(f\"  Mean: {np.mean(past_sequences_flat):.4f}\")\n        # print(f\"  Standard Deviation: {np.std(past_sequences_flat):.4f}\")\n        # print(f\"  Min: {np.min(past_sequences_flat):.4f}\")\n        # print(f\"  Max: {np.max(past_sequences_flat):.4f}\")\n        # print(f\"  Number of elements: {past_sequences_flat.size}\")\n        # print(\"-\" * 50)\n\n        # Step 3: Combine flattened features for XGBoost input\n        print(\"Combining features for XGBoost input...\")\n        # xgb_input = np.concatenate([transformer_features_flat, past_sequences_flat], axis=-1)\n        xgb_input = transformer_features\n        print(f\"XGBoost input shape: {xgb_input.shape}\")\n\n        # Step 3: Combine past sequence features and future sequence data for XGBoost input\n        # xgb_input = np.concatenate([transformer_features_expanded, past_sequences], axis=-1)\n        # print(f\"XGBoost input shape: {xgb_input.shape}\")\n\n        # Step 4: Predict with the XGBoost model\n        # Print XGBoost model details\n        print(\"Inspecting the trained XGBoost model...\")\n        print(f\"XGBoost parameters: {self.xgb_model.get_params()}\")\n        # Retrieve the Booster object\n        booster = self.xgb_model.get_booster()\n        print(\"Booster object retrieved.\")\n        print(booster.get_dump())\n\n        # Number of features can be inferred from the input or the Booster\n        print(f\"Booster number of features: {booster.num_features()}\")\n        # Requires Booster object\n\n        # Step 4: Predict with the XGBoost model\n        print(\"Performing predictions with the XGBoost model...\")\n        np.concatenate(xgb_input, axis=0)\n        predictions = self.xgb_model.predict(xgb_input)\n        print(f\"Predictions complete. Output shape: {predictions.shape}\")\n        print(f\"First 5 inputs: {xgb_input[:5]}\")\n        print(f\"First 5 predictions: {predictions[:5]}\")\n        # print(\"Performing predictions with XGBoost model...\")\n        # predictions = self.xgb_model.predict(xgb_input)\n        # print(f\"Final predictions complete. Shape: {predictions.shape}\")\n\n        return predictions\n\n    def save_predictions(self, predictions, output_path):\n        \"\"\"\n        Saves the predictions to a CSV file.\n\n        Args:\n            predictions (np.array): Predicted values.\n            output_path (str): Path to save the predictions.\n        \"\"\"\n        print(f\"Saving predictions to {output_path}...\")\n        # the predictions contains all 7 future days. Only output 1 day\n        df = pd.DataFrame(predictions[:, 0], columns=[self.target_column])\n        print(df.describe())\n        df.to_csv(output_path, index=False)\n        print(\"Predictions saved successfully.\")\n\n\nimport os\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\ndef prepare_input_numpy_for_date(target_date=\"2024-11-20\"):\n    try:\n        # Debug: Start of function\n        print(\"Starting the process to prepare input NumPy array...\")\n        print(f\"Target date: {target_date}\")\n\n        # Parse the target date\n        target_date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n\n        # Generate the past 7 days including the target date\n        past_dates = [(target_date_obj - timedelta(days=i)).strftime(\"%Y-%m-%d\") for i in range(7)]\n        print(f\"Past 7 days: {past_dates}\")\n\n        # File path and pattern\n        base_path = \"/groups/ESS3/zsun/swe/data/testing_input/\"  # Replace with actual path\n        file_pattern = \"testing_all_ready_{}.csv\"\n\n        # Initialize list to hold data for each day\n        daily_data = []\n\n        for date in reversed(past_dates):\n            file_name = file_pattern.format(date)\n            file_path = os.path.join(base_path, file_name)\n\n            if not os.path.exists(file_path):\n                print(f\"Warning: File for date {date} not found: {file_path}\")\n                continue\n\n            print(f\"Loading file: {file_path}\")\n            daily_df = pd.read_csv(file_path)\n            \n            # Ensure the 'date' column is consistent and matches the file's date\n            daily_df['date'] = date\n            print(\"original df columns: \", daily_df.columns)\n\n            # Append to the list\n            daily_data.append(daily_df)\n\n        if len(daily_data) < 7:\n            raise ValueError(f\"Insufficient data: Found data for {len(daily_data)} days, but 7 days are required.\")\n\n        # Combine all data into a single DataFrame\n        combined_df = pd.concat(daily_data, ignore_index=True)\n        print(f\"Combined data shape: {combined_df.shape}\")\n\n        # Rename columns to standardize\n        # column_mapping = {\n        #     'SWE': 'AMSR_SWE',\n        #     'air_temperature_observed_f': 'tmmx', \n        #     'precipitation_amount': 'pr',\n        #     'relative_humidity_rmin': 'rmin',\n        #     'potential_evapotranspiration': 'etr',\n        #     'air_temperature_tmmx': 'tmmx',\n        #     'relative_humidity_rmax': 'rmax',\n        #     'mean_vapor_pressure_deficit': 'vpd',\n        #     'air_temperature_tmmn': 'tmmn',\n        #     'wind_speed': 'vs',\n        # }\n        # combined_df = combined_df.rename(columns=column_mapping)\n\n        # [\n        #     'SWE', 'change_in_swe_inch', 'snow_depth', 'air_temperature_observed_f',\n        #     'precipitation_amount', 'relative_humidity_rmin', 'potential_evapotranspiration',\n        #     'air_temperature_tmmx', 'relative_humidity_rmax', 'mean_vapor_pressure_deficit',\n        #     'air_temperature_tmmn', 'wind_speed', 'Elevation', 'Aspect', 'Curvature',\n        #     'Northness', 'Eastness', 'fsca', 'Slope'\n        # ]\n\n        # Latitude,Longitude,rmin,vs,vpd,tmmn,pr,tmmx,rmax,etr,x,y,Elevation,Slope,Aspect,Curvature,Northness,Eastness,AMSR_SWE_2024-10-01,AMSR_Flag_2024-10-01,AMSR_SWE_2024-10-02,AMSR_Flag_2024-10-02,AMSR_SWE_2024-10-03,AMSR_Flag_2024-10-03,AMSR_SWE_2024-10-04,AMSR_Flag_2024-10-04,AMSR_SWE_2024-10-05,AMSR_Flag_2024-10-05,AMSR_SWE_2024-10-06,AMSR_Flag_2024-10-06,AMSR_SWE_2024-10-07,AMSR_Flag_2024-10-07,AMSR_SWE_2024-10-08,AMSR_Flag_2024-10-08,AMSR_SWE_2024-10-09,AMSR_Flag_2024-10-09,AMSR_SWE_2024-10-10,AMSR_Flag_2024-10-10,AMSR_SWE_2024-10-11,AMSR_Flag_2024-10-11,AMSR_SWE_2024-10-12,AMSR_Flag_2024-10-12,AMSR_SWE_2024-10-13,AMSR_Flag_2024-10-13,AMSR_SWE_2024-10-14,AMSR_Flag_2024-10-14,AMSR_SWE_2024-10-15,AMSR_Flag_2024-10-15,AMSR_SWE_2024-10-16,AMSR_Flag_2024-10-16,AMSR_SWE_2024-10-17,AMSR_Flag_2024-10-17,AMSR_SWE_2024-10-18,AMSR_Flag_2024-10-18,AMSR_SWE_2024-10-19,AMSR_Flag_2024-10-19,AMSR_SWE_2024-10-20,AMSR_Flag_2024-10-20,AMSR_SWE_2024-10-21,AMSR_Flag_2024-10-21,AMSR_SWE_2024-10-22,AMSR_Flag_2024-10-22,AMSR_SWE_2024-10-23,AMSR_Flag_2024-10-23,AMSR_SWE_2024-10-24,AMSR_Flag_2024-10-24,AMSR_SWE_2024-10-25,AMSR_Flag_2024-10-25,AMSR_SWE_2024-10-26,AMSR_Flag_2024-10-26,AMSR_SWE_2024-10-27,AMSR_Flag_2024-10-27,AMSR_SWE_2024-10-28,AMSR_Flag_2024-10-28,AMSR_SWE_2024-10-29,AMSR_Flag_2024-10-29,AMSR_SWE_2024-10-30,AMSR_Flag_2024-10-30,AMSR_SWE_2024-10-31,AMSR_Flag_2024-10-31,AMSR_SWE_2024-11-01,AMSR_Flag_2024-11-01,AMSR_SWE_2024-11-02,AMSR_Flag_2024-11-02,AMSR_SWE_2024-11-03,AMSR_Flag_2024-11-03,AMSR_SWE_2024-11-04,AMSR_Flag_2024-11-04,AMSR_SWE,AMSR_Flag,cumulative_AMSR_SWE,date,fsca_2024-10-01,fsca_2024-10-02,fsca_2024-10-03,fsca_2024-10-04,fsca_2024-10-05,fsca_2024-10-06,fsca_2024-10-07,fsca_2024-10-08,fsca_2024-10-09,fsca_2024-10-10,fsca_2024-10-11,fsca_2024-10-12,fsca_2024-10-13,fsca_2024-10-14,fsca_2024-10-15,fsca_2024-10-16,fsca_2024-10-17,fsca_2024-10-18,fsca_2024-10-19,fsca_2024-10-20,fsca_2024-10-21,fsca_2024-10-22,fsca_2024-10-23,fsca_2024-10-24,fsca_2024-10-25,fsca_2024-10-26,fsca_2024-10-27,fsca_2024-10-28,fsca_2024-10-29,fsca_2024-10-30,fsca_2024-10-31,fsca_2024-11-01,fsca_2024-11-02,fsca_2024-11-03,fsca_2024-11-04,fsca,cumulative_fsca,date,lc_prop3,water_year\n\n        column_mapping = {\n            'AMSR_SWE': 'SWE',\n            'tmmx': 'air_temperature_observed_f',  # this column is duplicated with tmmx, why?\n            'pr': 'precipitation_amount',\n            'rmin': 'relative_humidity_rmin',\n            'etr': 'potential_evapotranspiration',\n            'tmmx': 'air_temperature_tmmx',\n            'rmax': 'relative_humidity_rmax',\n            'vpd': 'mean_vapor_pressure_deficit',\n            'tmmn': 'air_temperature_tmmn',\n            'vs': 'wind_speed',\n        }\n        combined_df = combined_df.rename(columns=column_mapping)\n\n        # Filter necessary columns\n        # variable_columns = ['Latitude', 'Longitude', 'tmmn', 'etr', \n        #                     'rmin', 'vpd', 'tmmx', 'rmax', \n        #                     'vs', 'pr', 'Elevation', 'Slope', 'Aspect', 'Curvature', \n        #                     'Northness', 'Eastness', 'AMSR_SWE', 'cumulative_AMSR_SWE']\n        variable_columns = BASE_FEATURES\n        combined_df = combined_df[variable_columns + ['date', 'Latitude','Longitude',]]\n        print(\"Filtered columns to include necessary variables.\")\n\n        # Sort data by Latitude, Longitude, and date\n        combined_df['date'] = pd.to_datetime(combined_df['date'])\n        combined_df = combined_df.sort_values(by=['Latitude', 'Longitude', 'date'])\n\n        # Group data by grid points and reshape\n        grouped = combined_df.groupby(['Latitude', 'Longitude'])\n        numpy_array_list = []\n\n        print(combined_df.head())\n        print(\"the 19 columns: \", combined_df.columns)\n\n        for (lat, lon), group in grouped:\n            group_sorted = group.sort_values(by='date')\n            if len(group_sorted) == 7:\n                numpy_array_list.append(group_sorted[variable_columns].to_numpy())\n\n        if not numpy_array_list:\n            raise ValueError(\"No grids found with exactly 7 days of data.\")\n\n        # Stack arrays into final shape\n        final_array = np.stack(numpy_array_list, axis=1)  # Shape: (7, grids, variables)\n        print(f\"Final array shape: {final_array.shape}\")\n\n        # Save as .npy file\n        output_path = f\"/groups/ESS3/zsun/swe/data/testing_ready_input_bttf/combined_array_{target_date}.npy\"\n        np.save(output_path, final_array)\n        print(f\"Saved NumPy array at: {output_path}\")\n\n        return final_array\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise\n\ndef load_and_analyze_numpy(file_path):\n    try:\n        # Load the saved NumPy array\n        print(f\"Loading NumPy array from: {file_path}\")\n        data = np.load(file_path)\n        print(f\"Array shape: {data.shape} (days, grids, variables)\")\n\n        # Compute statistics\n        stats = {\n            'mean': np.mean(data, axis=(0, 1)),\n            'min': np.min(data, axis=(0, 1)),\n            'max': np.max(data, axis=(0, 1)),\n            'std': np.std(data, axis=(0, 1)),\n        }\n\n        # Print statistics for each variable\n        for i, var_stats in enumerate(zip(stats['mean'], stats['min'], stats['max'], stats['std'])):\n            mean, min_val, max_val, std = var_stats\n            print(f\"Variable {i+1}: Mean={mean:.3f}, Min={min_val:.3f}, Max={max_val:.3f}, Std={std:.3f}\")\n\n    except Exception as e:\n        print(f\"An error occurred while loading or analyzing the array: {e}\")\n\n\ndef do_prediction(target_date=\"2024-11-20\"):\n    # Paths to saved models\n    transformer_model_path = \"/groups/ESS3/zsun/swe/plots/models/ensemble_transformer_model_20241214_002501.pth\"\n    xgb_model_path = \"/groups/ESS3/zsun/swe/plots/models/ensemble_xgboost_model_20241214_002501.json\"\n\n    # Path to input CSV and output predictions\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n    # target_date = \"2024-11-20\"\n    # input_csv = \"/groups/ESS3/zsun/swe/data/testing_ready_input/input_2024-11-20.csv\"\n    input_np_path = f\"/groups/ESS3/zsun/swe/data/testing_ready_input_bttf/combined_array_{target_date}.npy\"\n    output_prediction_path = f\"/groups/ESS3/zsun/swe/data/testing_output/transformer_xgb_predictions_{timestamp}.csv\"\n\n    # Initialize the predictor\n    predictor = EnsembleModelPredictor(\n        transformer_path=transformer_model_path,\n        xgb_path=xgb_model_path,\n        features=FEATURES,\n        sequence_length=SEQUENCE_LENGTH,\n        target_column=TARGET_COLUMN\n    )\n\n    # Run predictions\n    data = np.load(input_np_path)  # Example of loading your data\n    predictions = predictor.predict(data)\n\n    # Save predictions\n    predictor.save_predictions(predictions, output_prediction_path)\n\nif __name__ == \"__main__\":\n    prepare_input_numpy_for_date(target_date=\"2024-11-11\")\n    # load_and_analyze_numpy(\"/groups/ESS3/zsun/swe/data/testing_ready_input_bttf/combined_array_2024-11-10.npy\")\n    # columns\n#     'Latitude', 'Longitude', 'tmmn', 'etr', 'rmin', 'vpd', 'tmmx', 'rmax',\n# >        'vs', 'pr', 'Elevation', 'Slope', 'Aspect', 'Aspect', 'Curvature',\n# >        'Northness', 'Eastness', 'AMSR_SWE', 'cumulative_AMSR_SWE', 'date']\n    do_prediction(target_date=\"2024-11-11\")\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "3b7jqb",
  "name" : "snodas_test",
  "description" : null,
  "code" : "import os\nimport requests\nimport tarfile\nimport gzip\nimport shutil\nfrom datetime import datetime, timedelta\nfrom osgeo import gdal\nfrom snowcast_utils import test_start_date, test_end_date, data_dir, plot_dir, process_dates_in_range\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nfrom scipy.spatial import cKDTree\n\nsample_nc_file = f\"{data_dir}/snodas/snodas_sample.nc\"\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\nmapper_file = os.path.join(data_dir, f'snodas/snodas_to_dem_mapper.csv')\n\ndef prepare_snodas_grid_mapper(\n    snodas_netcdf=sample_nc_file, \n    target_grid_csv = western_us_coords,\n):\n    \"\"\"\n    Prepares a mapper file to map coordinates between SNODAS grid coordinates and target grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads target grid coordinates from a CSV file (`target_grid_csv`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample SNODAS NetCDF file (`snodas_netcdf`) to map SNODAS grid coordinates ('snodas_x' and 'snodas_y') to target grid coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'snodas_x', and 'snodas_y'.\n\n    Args:\n    - snodas_netcdf (str): Path to the SNODAS NetCDF file.\n    - target_grid_csv (str): Path to the CSV file containing target grid coordinates with 'Longitude' and 'Latitude'.\n    - mapper_file (str): Path to save the generated mapper CSV file.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n    \"\"\"\n    \n    if os.path.exists(mapper_file):\n        # print(f\"The file {mapper_file} exists. Skipping generation.\")\n        pass\n    else:\n        print(f\"Starting to generate {mapper_file}\")\n        \n        # Read target grid coordinates from the CSV file\n        target_grid_df = pd.read_csv(target_grid_csv, usecols=['Longitude', 'Latitude'])\n\n        # Open SNODAS NetCDF file\n        snodas_data = xr.open_dataset(snodas_netcdf)\n\n        # Get the SNODAS grid coordinates (longitude and latitude)\n        snodas_lon = snodas_data['lon'].values\n        snodas_lat = snodas_data['lat'].values\n\n        # Create a grid of SNODAS coordinates\n        snodas_coords = np.array(np.meshgrid(snodas_lon, snodas_lat)).T.reshape(-1, 2)\n\n        # Build a KDTree for SNODAS coordinates\n        snodas_kdtree = cKDTree(snodas_coords)\n\n        # Get target coordinates from the dataframe\n        target_coords = target_grid_df[['Longitude', 'Latitude']].values\n\n        # Apply nearest neighbor search using KDTree\n        _, indices = snodas_kdtree.query(target_coords)\n\n        # Map target grid coordinates to SNODAS grid coordinates\n        target_grid_df['snodas_lon'] = snodas_coords[indices, 0]\n        target_grid_df['snodas_lat'] = snodas_coords[indices, 1]\n\n        # Save the mapper file as CSV\n        print(f\"Saving mapper CSV file: {mapper_file}\")\n        target_grid_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'snodas_lon', 'snodas_lat'])\n\ndef create_envi_header(dat_file_path):\n    \"\"\"\n    Creates an ENVI header file for the corresponding .dat file.\n    \"\"\"\n    hdr_file_path = dat_file_path.replace(\".dat\", \".hdr\")\n    if not os.path.exists(hdr_file_path):\n        header_content = \"\"\"ENVI\nsamples = 6935\nlines = 3351\nbands = 1\nheader offset = 0\nfile type = ENVI Standard\ndata type = 2\ninterleave = bsq\nbyte order = 1\n\"\"\"\n        with open(hdr_file_path, \"w\") as hdr_file:\n            hdr_file.write(header_content)\n        print(f\"Created header file: {hdr_file_path}\")\n    else:\n        print(f\"Header file {hdr_file_path} already exists.\")\n\ndef download_and_extract_tar(url, download_dir, extract_dir):\n    tar_file_name = url.split(\"/\")[-1]\n    tar_file_path = os.path.join(download_dir, tar_file_name)\n\n    if not os.path.exists(tar_file_path):\n        print(f\"Downloading {url} to {tar_file_path}\")\n        response = requests.get(url)\n        response.raise_for_status()\n        with open(tar_file_path, \"wb\") as f:\n            f.write(response.content)\n        print(f\"Downloaded {tar_file_name}\")\n    # else:\n    #     print(f\"Tar file {tar_file_name} already exists, skipping download.\")\n    \n    extracted_folder = os.path.join(extract_dir, tar_file_name.replace(\".tar\", \"\"))\n    if not os.path.exists(extracted_folder):\n        with tarfile.open(tar_file_path, \"r\") as tar_ref:\n            tar_ref.extractall(extracted_folder)\n        print(f\"Extracted {tar_file_name} to {extracted_folder}\")\n    # else:\n    #     print(f\"Folder {extracted_folder} already exists, skipping extraction.\")\n    return extracted_folder\n\ndef decompress_gz(file_path, output_dir):\n    file_name = os.path.basename(file_path).replace(\".gz\", \"\")\n    output_path = os.path.join(output_dir, file_name)\n    if not os.path.exists(output_path):\n        with gzip.open(file_path, \"rb\") as gz_file:\n            with open(output_path, \"wb\") as out_file:\n                shutil.copyfileobj(gz_file, out_file)\n        print(f\"Decompressed {file_path} to {output_path}\")\n    # else:\n    #     print(f\"File {output_path} already exists, skipping decompression.\")\n    return output_path\n\ndef convert_to_netcdf(dat_file_path, output_dir):\n    netcdf_file_path = os.path.join(output_dir, os.path.basename(dat_file_path).replace(\".dat\", \".nc\"))\n    if os.path.exists(netcdf_file_path):\n        # print(f\"{netcdf_file_path} exists. skip.\")\n        pass\n    else:\n        dataset = gdal.Open(dat_file_path)\n        if dataset is None:\n            print(f\"Failed to open {dat_file_path}. File format not supported by GDAL.\")\n            return\n        # Define spatial extent and projection\n        spatial_extent = \"-124.73333333333333 52.87500000000000 -66.94166666666667 24.95000000000000\"\n        projection = \"EPSG:4326\"\n        gdal.Translate(\n            netcdf_file_path, \n            dataset, \n            format=\"NetCDF\",\n            outputSRS=projection,\n            noData=-9999,\n            outputBounds=(\n                float(spatial_extent.split()[0]), \n                float(spatial_extent.split()[1]),\n                float(spatial_extent.split()[2]), \n                float(spatial_extent.split()[3])\n            )\n        )\n        print(f\"Converted {dat_file_path} to {netcdf_file_path}\")\n    return netcdf_file_path\n\ndef process_snodas_file(netcdf_file, mapper_file, current_date, output_dir):\n    \"\"\"\n    Process a SNODAS NetCDF file, extract values for specific coordinates based on the provided mapper CSV file,\n    and save the result in a new CSV file.\n\n    Parameters:\n    - netcdf_file (str): Path to the SNODAS NetCDF file to be processed.\n    - mapper_file (str): Path to the mapper CSV file that contains target grid coordinates and SNODAS grid coordinates.\n    - current_date (str): Current date to be associated with the processed data.\n    - output_dir (str): Directory to save the resulting CSV file.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n    # Output file path\n    output_file = os.path.join(output_dir, f\"{current_date}_snodas_output.csv\")\n    print(f\"Saving processed data to: {output_file}\")\n    if os.path.exists(output_file):\n        # print(f\"{output_file} exists. skip.\")\n        pass\n    else:\n        # Read the target grid coordinates and SNODAS grid mapping from the mapper CSV file\n        station_df = pd.read_csv(mapper_file)\n        print(f\"Opening SNODAS NetCDF file: {netcdf_file}\")\n        \n        # Open the SNODAS NetCDF file using xarray\n        snodas_data = xr.open_dataset(netcdf_file)\n        \n        # Extract SNODAS grid coordinates (lon and lat) and the snow cover data (Band1)\n        snodas_lon = snodas_data['lon'].values\n        snodas_lat = snodas_data['lat'].values\n        snodas_values = snodas_data['Band1'].values  # assuming Band1 contains snow cover data\n        \n        # Function to map SNODAS grid coordinates to the extracted data\n        def get_snodas_value(row):\n            # Target grid coordinates from mapper\n            target_lon = row['Longitude']\n            target_lat = row['Latitude']\n            \n            # Find the closest matching SNODAS grid coordinate for the target grid\n            lon_idx = (np.abs(snodas_lon - target_lon)).argmin()\n            lat_idx = (np.abs(snodas_lat - target_lat)).argmin()\n            \n            # Extract the snodas value at the closest coordinates\n            snodas_value = snodas_values[lat_idx, lon_idx]\n            \n            # Handle cases where the value is the NoData value (-9999) and convert it to NaN\n            if snodas_value == -9999:\n                return np.nan\n            return snodas_value\n\n        # Apply the function to get snow cover data for each row in the target grid\n        station_df['snodas'] = station_df.apply(get_snodas_value, axis=1)\n        \n        # Filter out rows with missing or NaN snodas values\n        valid_data = station_df[station_df['snodas'].notna()]\n        \n        # Add the current date to the final dataset\n        valid_data['date'] = current_date\n        \n        # Save the result to a CSV\n        valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'snodas'])\n    \n    return output_file\n\ndef snodas_callback(current_date):\n    current_date_str = current_date.strftime(\"%Y-%m-%d\")\n\n    # Define directories\n    download_dir = os.path.join(data_dir, \"snodas/raw/\")\n    extract_dir = os.path.join(data_dir, \"snodas/extracted/\")\n    output_dir = os.path.join(data_dir, \"snodas/netcdf/\")\n    csv_dir = os.path.join(data_dir, \"snodas/csv/\")\n\n    os.makedirs(download_dir, exist_ok=True)\n    os.makedirs(extract_dir, exist_ok=True)\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(csv_dir, exist_ok=True)\n\n    final_output_csv_file = os.path.join(output_dir, f\"{current_date_str}_snodas_output.csv\")\n    if os.path.exists(final_output_csv_file):\n        return final_output_csv_file\n\n    prepare_snodas_grid_mapper()\n\n    # URL for tar files\n    # Extract the year and month in the required format\n    day = current_date.day\n    month_num = current_date.month  # Numeric month (1-12)\n    month = current_date.strftime(\"%b\")  # Month abbreviated (e.g., Jan, Feb, Mar)\n    \n    # Construct the updated URL with day and month swapped\n    base_url = f\"https://noaadata.apps.nsidc.org/NOAA/G02158/masked/{current_date.year}/{month_num:02d}_{month.capitalize()}/\"\n    \n    \n    # Dates for the past 7 days\n    dates = [(current_date - timedelta(days=i)).strftime(\"%Y%m%d\") for i in range(7)]\n    tar_file = f\"SNODAS_{current_date.strftime('%Y%m%d')}.tar\"\n\n    tar_url = os.path.join(base_url, tar_file)\n    extracted_folder = download_and_extract_tar(tar_url, download_dir, extract_dir)\n\n    # Process .gz files inside the extracted folder\n    for root, _, files in os.walk(extracted_folder):\n        for file in files:\n            if file.endswith(\".dat.gz\") and \"1034\" in file:\n                gz_file_path = os.path.join(root, file)\n                decompressed_file = decompress_gz(gz_file_path, root)\n                if decompressed_file.endswith(\".dat\") and \"1034\" in decompressed_file:\n                    create_envi_header(decompressed_file)\n                    netcdf_file = convert_to_netcdf(\n                        decompressed_file, output_dir\n                    )\n                    process_snodas_file(\n                        netcdf_file, mapper_file, current_date_str, csv_dir\n                    )\n    return final_output_csv_file\n\nif __name__ == \"__main__\":\n    process_dates_in_range(\n        # start_date=test_start_date,\n        # end_date=test_end_date,\n        start_date=\"2000-01-01\",\n        end_date=\"2015-12-31\",\n        callback=snodas_callback,\n    )\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "j7shdk",
  "name" : "transformer_snodas_train",
  "description" : null,
  "code" : "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport psutil\nfrom snowcast_utils import model_dir, data_dir\nimport random\nimport subprocess\n\n# Function to get system memory usage\ndef get_system_memory():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 ** 2)  # Return in MB\n\n# Function to get GPU memory usage\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / (1024 ** 2)  # Return in MB\n    return 0\n\ndef print_memory_usage(step_description):\n    system_memory = get_system_memory()\n    gpu_memory = get_gpu_memory()\n    print(f\"Memory usage ({step_description}):\")\n    print(f\"System Memory: {system_memory:.2f} MB\")\n    print(f\"GPU Memory: {gpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\ndef get_first_available_device():\n    \"\"\"\n    Returns the first available GPU device based on utilization or falls back to CPU.\n    \n    Returns:\n        torch.device: The device to use for computations.\n    \"\"\"\n    try:\n        # Fetch the list of GPUs and their UUIDs\n        gpu_list = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,uuid\", \"--format=csv,noheader,nounits\"],\n            universal_newlines=True\n        )\n        \n        gpu_id_to_uuid = {}\n        for line in gpu_list.strip().split(\"\\n\"):\n            if line:\n                gpu_id, gpu_uuid = line.split(\", \")\n                gpu_id_to_uuid[gpu_uuid] = int(gpu_id)\n        \n        # Fetch processes using GPUs\n        gpu_usage = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-compute-apps=gpu_uuid\", \"--format=csv,noheader\"],\n            universal_newlines=True\n        )\n        \n        # Track GPUs in use\n        used_gpus = set(gpu_usage.strip().split(\"\\n\"))\n        \n        # Find the first GPU not in use\n        for gpu_uuid, gpu_id in gpu_id_to_uuid.items():\n            if gpu_uuid not in used_gpus:\n                print(f\"Using GPU: {gpu_id} - {torch.cuda.get_device_properties(gpu_id).name}\")\n                return torch.device(f\"cuda:{gpu_id}\")\n        \n        print(\"No free GPUs available. Using CPU.\")\n    except Exception as e:\n        print(f\"Error checking GPU usage: {e}. Falling back to CPU.\")\n    \n    return torch.device(\"cpu\")\n\nclass TransformerModel(nn.Module):\n    def __init__(self, input_dim, d_model, num_heads, num_layers, output_dim):\n        super(TransformerModel, self).__init__()\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n        \n        # Linear layers to project input to d_model\n        self.src_linear = nn.Linear(input_dim, d_model)\n        \n        # Define the transformer layer (using only the encoder here)\n        self.transformer = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads),\n            num_layers=num_layers\n        )\n        \n        # Output layer to predict SWE value\n        self.fc_out = nn.Linear(d_model, output_dim)\n\n    def forward(self, src):\n        \"\"\"\n        Forward pass of the Transformer model with shape printing.\n\n        Args:\n            src (Tensor): Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            Tensor: Output tensor of shape (batch_size, output_dim).\n        \"\"\"\n        # print(f\"Input shape: {src.shape}\")  # Shape: (batch_size, input_dim)\n\n        # Project inputs to d_model\n        src = self.src_linear(src)  # Shape: (batch_size, d_model)\n        # print(f\"After src_linear shape: {src.shape}\")\n\n        # Transformer expects (seq_len, batch, d_model), so reshape inputs accordingly\n        # src = src.unsqueeze(0)  # Add sequence dimension, shape: (1, batch_size, d_model)\n        # print(f\"Before transformer shape: {src.shape}\")\n\n        # Pass through the transformer\n        output = self.transformer(src)  # Shape: (1, batch_size, d_model)\n        # print(f\"After transformer shape: {output.shape}\")\n\n        # Use the output of the first sequence token for prediction\n        output = output.squeeze(0)  # Remove sequence dimension, shape: (batch_size, d_model)\n        # print(f\"After squeeze shape: {output.shape}\")\n\n        # Predict the SWE value\n        output = self.fc_out(output)  # Shape: (batch_size, output_dim)\n        # print(f\"Output shape: {output.shape}\")\n\n        return output\n\n\n\n# Load and preprocess data for a specific batch (5 days)\ndef load_batch(files):\n    batch_data = []\n    \n    # Iterate over the files and filter data by date range\n    for file in files:\n        if not os.path.exists(file):\n            continue\n        \n        df = pd.read_csv(file)\n        \n        # Convert date column to datetime\n        df['date'] = pd.to_datetime(df['date'])\n        \n        # Filter data for the date range of this batch (start_date to end_date)\n        batch_data.append(df)\n    \n    # Concatenate all data for the batch\n    batch_df = pd.concat(batch_data)\n    \n    # Feature engineering: Extract day of year and month\n    batch_df['day_of_year'] = batch_df['date'].dt.dayofyear\n    batch_df['month'] = batch_df['date'].dt.month\n    batch_df['year'] = batch_df['date'].dt.year\n    \n    # print(batch_df[\"snodas\"].describe())\n\n    # scale snodas to deal with bias\n    batch_df['snodas'] = np.log10(batch_df['snodas'] + 1)  # +1 to avoid log(0)\n    # print(batch_df[\"snodas\"].describe())\n\n    # Features (Latitude, Longitude, day_of_year, month)\n    X = batch_df[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n    \n    # Target: SWE (snodas)\n    y = batch_df['snodas'].values\n    \n    return X, y\n\ndef new_batch_generator(files, start_date, end_date, batch_size):\n    \"\"\"\n    Generate batches of data from files.\n    \n    Args:\n        files (list): List of file paths containing the dataset.\n        start_date (str): Start date for filtering data.\n        end_date (str): End date for filtering data.\n        batch_size (int): Number of rows per batch.\n    \n    Yields:\n        (X_batch, y_batch): Tuple of feature and target batches.\n    \"\"\"\n    # Sort the files by date (assuming file names are in the format 'YYYY-MM-DD')\n    files.sort()  # Adjust if necessary to ensure files are in chronological order\n\n    # Filter files based on the provided date range\n    # filtered_files = []\n    # for file in files:\n    #     # Extract date from the file name (assuming the date is part of the file name)\n    #     # Adjust this if your file name format differs\n    #     file_date_str = os.path.basename(file).split('_')[0]  # Example: \"2025-01-01_data.csv\"\n    #     file_date = datetime.strptime(file_date_str, '%Y-%m-%d').date()\n\n    #     # Add file to list if it's within the date range\n    #     if start_date <= file_date <= end_date:\n    #         filtered_files.append(file)\n    \n    num_files = len(files)\n    current_index = 0\n\n    # Loop through the filtered files and create batches\n    while current_index < num_files:\n\n        current_file = files[current_index]\n        # print(\"current_file = \", current_file)\n        \n        # Load the data for this batch (you need to implement load_batch to handle files)\n        X_batch, y_batch = load_batch([current_file])\n\n        num_samples = X_batch.shape[0]\n        for start_idx in range(0, num_samples, batch_size):\n            end_idx = min(start_idx + batch_size, num_samples)\n\n            # Slice the batch into smaller chunks\n            X_chunk = X_batch[start_idx:end_idx]\n            y_chunk = y_batch[start_idx:end_idx]\n\n            # Yield the batch\n            yield X_chunk, y_chunk\n\n        current_index += 1\n\ndef get_model():\n    model = TransformerModel(\n        input_dim=5, d_model=32, num_heads=4, num_layers=2, output_dim=1\n    )\n    return model\n\ndef train_model_in_batches(\n    files, start_date, end_date, epochs=100, batch_size=50000,\n    model_save_path=f\"{model_dir}/snodas_transformer_model.pth\"\n):\n    model = get_model()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.MSELoss()\n    print(\"Transformer is set, ready to train\")\n\n    # Send model to GPUs\n    device = get_first_available_device()\n    model = model.to(device)\n    \n    print(\"Starting model training...\")\n    print_memory_usage(\"start training\")\n\n    for epoch in range(epochs):\n        print(f\"Training epoch {epoch} / {epochs}\")\n        model.train()  # Set model to training mode\n        \n        big_batch_num = 0\n        epoch_loss = 0\n        for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n            big_batch_num += 1\n            # print(f\"Epoch {epoch+1}/{epochs}, File Batch {big_batch_num}...\")\n            \n            # print_memory_usage(f\"Training epoch {epoch} file {big_batch_num}\")\n            \n            # Convert to torch tensors\n            X_batch = torch.tensor(X_batch, dtype=torch.float32)\n            y_batch = torch.tensor(y_batch, dtype=torch.float32).view(-1, 1)\n\n            # Ensure chunks are on the correct device\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            # Add sequence dimension and permute for transformer\n            src = X_batch.unsqueeze(1)  # Add sequence length dimension\n            src = src.permute(1, 0, 2)  # Change to (seq_len, batch_size, input_dim)\n\n            # Forward pass for each smaller chunk\n            optimizer.zero_grad()\n            y_pred = model(src)  # Pass the source to the model\n            # print(\"y_pred.shape: \", y_pred.shape)\n\n            # Compute the loss\n            loss = criterion(y_pred, y_batch)\n            epoch_loss += loss.item()\n\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch+1} completed. Loss: {epoch_loss / big_batch_num:.4f}\")\n\n    print(\"Model training completed.\")\n    # Save the trained model to a file\n    torch.save(model.state_dict(), model_save_path)\n    print(f\"Model saved to {model_save_path}.\")\n    return model\n\n\n# Evaluate the model\ndef evaluate_model(model_file_path, files, start_date, end_date, batch_size=10000):\n    \"\"\"\n    Evaluate the model on the given data files between start_date and end_date.\n    \n    Args:\n        model_file_path (str): Path to the saved model file.\n        files (list): List of file paths containing the dataset.\n        start_date (str): Start date for filtering data.\n        end_date (str): End date for filtering data.\n        batch_size (int): Number of rows per batch.\n    \"\"\"\n    print(\"Loading model from:\", model_file_path)\n    \n    model = get_model()\n    model.load_state_dict(torch.load(model_file_path))\n\n    # Move model to GPU if available\n    device = get_first_available_device()\n    model = model.to(device)\n    model.eval()  # Set model to evaluation mode\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n\n    all_y_true = []\n    all_y_pred = []\n\n    # Start batch processing\n    batch_count = 0\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        # print(f\"Processing batch {batch_count + 1}...\")\n        \n        # Convert to torch tensors and move to the appropriate device\n        X_batch = torch.tensor(X_batch, dtype=torch.float32).to(device)\n        y_batch = torch.tensor(y_batch, dtype=torch.float32).view(-1, 1).to(device)\n\n        # print(f\"Batch {batch_count + 1} - X_batch shape: {X_batch.shape}, y_batch shape: {y_batch.shape}\")\n\n        # Add sequence dimensions for transformer input\n        if X_batch.dim() == 2:\n            X_batch = X_batch.unsqueeze(1)  # Add a sequence length dimension: (batch_size, 1, input_dim)\n        \n        src = X_batch.permute(1, 0, 2)  # Shape: (seq_len, batch_size, d_model)\n\n        with torch.no_grad():\n            # Predict (no target sequence provided during evaluation)\n            y_pred = model(src).squeeze(1)  # Shape: (batch_size, output_dim)\n        \n        # Check the shape of the prediction\n        # print(f\"Batch {batch_count + 1} - y_pred shape: {y_pred.shape}\")\n\n        # Collect predictions and true values\n        all_y_true.append(y_batch.cpu().numpy())\n        all_y_pred.append(y_pred.cpu().numpy())\n\n        # print(f\"Batch {batch_count + 1} processed.\")\n        \n        # Free up GPU memory\n        torch.cuda.empty_cache()\n\n        # Increment batch counter\n        batch_count += 1\n\n    # Flatten arrays\n    all_y_true = np.concatenate(all_y_true)\n    all_y_pred = np.concatenate(all_y_pred)\n\n    print(\"all_y_true.shape = \", all_y_true.shape)\n    print(\"all_y_pred.shape = \", all_y_pred.shape)\n\n    # Ensure that the shapes match\n    if all_y_true.shape[0] != all_y_pred.shape[0]:\n        print(f\"Warning: Mismatch in number of samples: {all_y_true.shape[0]} vs {all_y_pred.shape[0]}\")\n\n    # Calculate evaluation metrics\n    rmse = mean_squared_error(all_y_true, all_y_pred, squared=False)\n    r2 = r2_score(all_y_true, all_y_pred)\n\n    print(f\"Evaluation completed.\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"R2 Score: {r2:.4f}\")\n\n\n# Main function to execute the training process\ndef main(data_folder, start_date_str, end_date_str):\n    # Convert date strings to datetime objects\n    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n    \n    # List all CSV files in the data folder\n    files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.csv')]\n    # Shuffle the files to ensure randomness\n    random.shuffle(files)\n\n    # Split the files: 80% for training and 20% for evaluation\n    train_files = files[:int(0.8 * len(files))]\n    eval_files = files[int(0.8 * len(files)):]\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_file_path = f\"{model_dir}/snodas_transformer_{timestamp}.pth\"\n\n    batch_size = 200000\n    epochs = 500\n    \n    # Train the model using batches of 5 days\n    train_model_in_batches(\n        train_files, start_date, end_date, epochs=epochs,\n        batch_size=batch_size,\n        model_save_path = model_file_path,\n    )\n\n    # Evaluate model on the last batch of data\n    evaluate_model(\n        model_file_path, eval_files, end_date - timedelta(days=4), end_date, batch_size=batch_size,\n    )\n    \n    return model_file_path\n\nif __name__ == \"__main__\":\n    # Folder containing the data files\n    data_folder = f\"{data_dir}/snodas/csv/\"\n\n    # Define start and end date for training\n    start_date_str = '2024-10-01'\n    end_date_str = '2025-01-15'\n\n    # Run the training process\n    model = main(data_folder, start_date_str, end_date_str)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ijolp2",
  "name" : "snodas_tabnet",
  "description" : null,
  "code" : "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport psutil\nfrom snowcast_utils import model_dir, data_dir\nimport random\nimport subprocess\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport warnings\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_tabnet.callbacks\")\n\n\n# Function to get system memory usage\ndef get_system_memory():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 ** 2)  # Return in MB\n\n# Function to get GPU memory usage\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / (1024 ** 2)  # Return in MB\n    return 0\n\ndef print_memory_usage(step_description):\n    system_memory = get_system_memory()\n    gpu_memory = get_gpu_memory()\n    print(f\"Memory usage ({step_description}):\")\n    print(f\"System Memory: {system_memory:.2f} MB\")\n    print(f\"GPU Memory: {gpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\ndef get_first_available_device():\n    \"\"\"\n    Returns the first available GPU device based on utilization or falls back to CPU.\n    \n    Returns:\n        torch.device: The device to use for computations.\n    \"\"\"\n    try:\n        # Fetch the list of GPUs and their UUIDs\n        gpu_list = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,uuid\", \"--format=csv,noheader,nounits\"],\n            universal_newlines=True\n        )\n        \n        gpu_id_to_uuid = {}\n        for line in gpu_list.strip().split(\"\\n\"):\n            if line:\n                gpu_id, gpu_uuid = line.split(\", \")\n                gpu_id_to_uuid[gpu_uuid] = int(gpu_id)\n        \n        # Fetch processes using GPUs\n        gpu_usage = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-compute-apps=gpu_uuid\", \"--format=csv,noheader\"],\n            universal_newlines=True\n        )\n        \n        # Track GPUs in use\n        used_gpus = set(gpu_usage.strip().split(\"\\n\"))\n        \n        # Find the first GPU not in use\n        for gpu_uuid, gpu_id in gpu_id_to_uuid.items():\n            if gpu_uuid not in used_gpus:\n                print(f\"Using GPU: {gpu_id} - {torch.cuda.get_device_properties(gpu_id).name}\")\n                return torch.device(f\"cuda:{gpu_id}\")\n        \n        print(\"No free GPUs available. Using CPU.\")\n    except Exception as e:\n        print(f\"Error checking GPU usage: {e}. Falling back to CPU.\")\n    \n    return torch.device(\"cpu\")\n\n# Replace TransformerModel with TabNet model\nclass TabNet(nn.Module):\n    def __init__(self,input_dim,output_dim,n_d=64,n_a=64,\nn_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n        super().__init__()\n        if n_shared>0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(input_dim,2*(n_d+n_a)))\n            for x in range(n_shared-1):\n                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n        else:\n            self.shared=None\n        self.first_step = FeatureTransformer(input_dim,n_d+n_a,self.shared,n_ind) \n        self.steps = nn.ModuleList()\n        for x in range(n_steps-1):\n            self.steps.append(DecisionStep(input_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n        self.fc = nn.Linear(n_d,output_dim)\n        self.bn = nn.BatchNorm1d(input_dim)\n        self.n_d = n_d\n\n    def forward(self,x):\n        x = self.bn(x)\n        x_a = self.first_step(x)[:,self.n_d:]\n        sparse_loss = torch.zeros(1).to(x.device)\n        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n        priors = torch.ones(x.shape).to(x.device)\n        for step in self.steps:\n            x_te,l = step(x,x_a,priors)\n            out += F.relu(x_te[:,:self.n_d])\n            x_a = x_te[:,self.n_d:]\n            sparse_loss += l\n        return self.fc(out),sparse_loss\n\n# Load and preprocess data for a specific batch (5 days)\ndef load_batch(files):\n    batch_data = []\n    \n    # Iterate over the files and filter data by date range\n    for file in files:\n        if not os.path.exists(file):\n            continue\n        \n        df = pd.read_csv(file)\n        \n        # Convert date column to datetime\n        df['date'] = pd.to_datetime(df['date'])\n        \n        # Filter data for the date range of this batch (start_date to end_date)\n        batch_data.append(df)\n    \n    # Concatenate all data for the batch\n    batch_df = pd.concat(batch_data)\n    \n    # Feature engineering: Extract day of year and month\n    batch_df['day_of_year'] = batch_df['date'].dt.dayofyear\n    batch_df['month'] = batch_df['date'].dt.month\n    batch_df['year'] = batch_df['date'].dt.year\n    \n    # scale snodas to deal with bias\n    batch_df['snodas'] = np.log10(batch_df['snodas'] + 1)  # +1 to avoid log(0)\n\n    # Features (Latitude, Longitude, day_of_year, month)\n    X = batch_df[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n    \n    # Target: SWE (snodas)\n    y = batch_df['snodas'].values\n    \n    return X, y\n\ndef new_batch_generator(files, start_date, end_date, batch_size):\n    num_files = len(files)\n    current_index = 0\n    X_accumulated = []\n    y_accumulated = []\n\n    while current_index < num_files:\n        current_file = files[current_index]\n        \n        # Load the data for this file\n        X_file, y_file = load_batch([current_file])\n\n        # Accumulate rows from this file\n        X_accumulated.extend(X_file)\n        y_accumulated.extend(y_file)\n\n        # If the accumulated data size exceeds or reaches batch_size, yield batches\n        while len(X_accumulated) >= batch_size:\n            X_chunk = X_accumulated[:batch_size]\n            y_chunk = y_accumulated[:batch_size]\n            \n            # Remove yielded data from accumulation\n            X_accumulated = X_accumulated[batch_size:]\n            y_accumulated = y_accumulated[batch_size:]\n\n            # Convert lists to numpy arrays and reshape y_chunk\n            X_chunk = np.array(X_chunk)\n            y_chunk = np.array(y_chunk).reshape(-1, 1)  # Ensure y_chunk is a 2D array\n            \n            yield X_chunk, y_chunk\n\n        current_index += 1\n\n    # Yield any remaining data if it's less than batch_size\n    if X_accumulated:\n        X_remaining = np.array(X_accumulated)\n        y_remaining = np.array(y_accumulated).reshape(-1, 1)\n        yield X_remaining, y_remaining\n\ndef get_model(device):\n    # model = TabNet(\n    #     input_dim=5, output_dim=1\n    # )\n    # model = TabNetRegressor()\n    model = TabNetRegressor(\n        input_dim=5,\n        output_dim=1,\n        n_d=16,\n        n_a=16,\n        n_steps=9,\n        gamma=1.5,\n        lambda_sparse=1e-5,\n        device_name = str(device)\n    )\n    return model\n\ndef train_model_in_batches(\n    files, start_date, end_date, epochs=100, batch_size=50000,\n    model_save_path=f\"{model_dir}/snodas_tabnet_model.pth\"\n):\n    device = get_first_available_device()\n    model = get_model(device)\n\n    # params_dict = {k: torch.tensor(v) if isinstance(v, (int, float)) else v for k, v in model.optimizer_params.items()}\n    # optimizer = optim.Adam(params_dict, lr=0.001)\n    criterion = nn.MSELoss()\n    print(\"TabNet is set, ready to train\")\n\n    # Send model to GPUs\n    \n    # model = model.to(device)\n    # model.device_name = device\n    \n    print(\"Starting model training...\")\n    print_memory_usage(\"start training\")\n    \n    big_batch_num = 0\n    epoch_loss = 0\n    # for epoch in range(epochs):\n        # print(f\"Big wrap epoch {epoch} / {epochs}\")\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        big_batch_num += 1\n        print(\"processing \", big_batch_num, \" - \", len(X_batch))\n        # Split the batch into training and validation sets (e.g., 80% train, 20% validation)\n        split_idx = int(0.8 * len(X_batch))  # 80% for training, 20% for validation\n        \n        X_train, y_train = X_batch[:split_idx], y_batch[:split_idx]\n        X_val, y_val = X_batch[split_idx:], y_batch[split_idx:]\n\n        # Reshape targets to 2D for regression\n        y_train = y_train.reshape(-1, 1)\n        y_val = y_val.reshape(-1, 1)\n\n        # Training the model using the fit method\n        model.fit(\n            X_train=X_train,\n            y_train=y_train,\n            eval_set=[(X_val, y_val)],\n            eval_name=[\"val\"],\n            eval_metric=[\"rmse\"],\n            max_epochs=epochs,\n            patience=0,\n            batch_size=batch_size,\n            virtual_batch_size=128,\n            num_workers=1,\n            drop_last=False,\n            warm_start=True,  # Ensures continued training from the last state\n            loss_fn=torch.nn.MSELoss()  # Use MSELoss for regression tasks\n        )\n\n        # After training, make predictions on the validation set\n        # y_val_pred = model.predict(X_val)\n\n        # Get the minimum value of the validation loss (logits_ll)\n        # score = np.min(model.history[\"val_logits_ll\"])\n\n        # print(f\"Validation score: {score}\")\n\n        # Compute the loss\n        # Ensure that y_val_pred and y_val are tensors\n        # y_val_pred = torch.tensor(y_val_pred, dtype=torch.float32)\n        # y_val = torch.tensor(y_val, dtype=torch.float32)\n        # loss = criterion(y_val_pred, y_val)\n        # epoch_loss += loss.item()\n\n        # Backward pass and optimization\n        # loss.backward()\n        # optimizer.step()\n\n    # print(f\"Epoch {epoch+1} completed.\")\n\n    print(\"Model training completed.\")\n    # Save the trained model to a file\n    model.save_model(model_save_path)\n    print(f\"Model saved to {model_save_path}.\")\n    return model\n\n\n# Evaluate the model\ndef evaluate_model(model_file_path, files, start_date, end_date, batch_size=10000):\n    print(\"Loading model from:\", model_file_path)\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(f\"{model_file_path}.zip\")\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n\n    all_y_true = []\n    all_y_pred = []\n\n    # Start batch processing\n    batch_count = 0\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        y_pred = model.predict(X_batch)  # Get model predictions\n        all_y_true.extend(y_batch)\n        all_y_pred.extend(y_pred)\n\n    # Calculate evaluation metrics\n    y_true = np.array(all_y_true)\n    y_pred = np.array(all_y_pred)\n    \n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"Evaluation Results: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n\ndef predict_swe_map(target_date: str):\n    \n    print(\"Loading model from:\", model_file_path)\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(f\"{model_file_path}.zip\")\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n\n    all_y_true = []\n    all_y_pred = []\n\n    # Start batch processing\n    batch_count = 0\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        y_pred = model.predict(X_batch)  # Get model predictions\n        all_y_true.extend(y_batch)\n        all_y_pred.extend(y_pred)\n\n    # Calculate evaluation metrics\n    y_true = np.array(all_y_true)\n    y_pred = np.array(all_y_pred)\n    \n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"Evaluation Results: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n\n# Main function to execute the training process\ndef main(data_folder, start_date_str, end_date_str):\n    # Convert date strings to datetime objects\n    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n    \n    # List all CSV files in the data folder\n    files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.csv')]\n    # Shuffle the files to ensure randomness\n    # random.shuffle(files)\n\n    files = files[-5:]\n\n    # Split the files: 80% for training and 20% for evaluation\n    train_files = files[:int(0.8 * len(files))]\n    print(\"Train files: \", train_files)\n    eval_files = files[int(0.8 * len(files)):]\n    print(\"Evaluation files: \", eval_files)\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_file_path = f\"{model_dir}/snodas_tabnet_{timestamp}.pth\"\n\n    batch_size = 100000\n    epochs = 100\n    \n    # Train the model using batches of 5 days\n    train_model_in_batches(\n        train_files, start_date, end_date, epochs=epochs,\n        batch_size=batch_size,\n        model_save_path = model_file_path,\n    )\n\n    # Evaluate model on the last batch of data\n    evaluate_model(\n        model_file_path, eval_files, end_date - timedelta(days=4), end_date, batch_size=batch_size,\n    )\n    \n    return model_file_path\n\nif __name__ == \"__main__\":\n    # Folder containing the data files\n    data_folder = f\"{data_dir}/snodas/csv/\"\n\n    # Define start and end date for training\n    start_date_str = '2024-10-01'\n    end_date_str = '2025-01-15'\n\n    # Run the training process\n    model = main(\n        data_folder, \n        start_date_str, \n        end_date_str\n    )\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6nca3k",
  "name" : "snodas_testing_realtime",
  "description" : null,
  "code" : "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport psutil\nfrom snowcast_utils import model_dir, data_dir, plot_dir\nimport random\nimport subprocess\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport warnings\nimport matplotlib.pyplot as plt\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_tabnet.callbacks\")\n\n\n# Function to get system memory usage\ndef get_system_memory():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 ** 2)  # Return in MB\n\n# Function to get GPU memory usage\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / (1024 ** 2)  # Return in MB\n    return 0\n\ndef print_memory_usage(step_description):\n    system_memory = get_system_memory()\n    gpu_memory = get_gpu_memory()\n    print(f\"Memory usage ({step_description}):\")\n    print(f\"System Memory: {system_memory:.2f} MB\")\n    print(f\"GPU Memory: {gpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\ndef get_first_available_device():\n    \"\"\"\n    Returns the first available GPU device based on utilization or falls back to CPU.\n    \n    Returns:\n        torch.device: The device to use for computations.\n    \"\"\"\n    try:\n        # Fetch the list of GPUs and their UUIDs\n        gpu_list = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,uuid\", \"--format=csv,noheader,nounits\"],\n            universal_newlines=True\n        )\n        \n        gpu_id_to_uuid = {}\n        for line in gpu_list.strip().split(\"\\n\"):\n            if line:\n                gpu_id, gpu_uuid = line.split(\", \")\n                gpu_id_to_uuid[gpu_uuid] = int(gpu_id)\n        \n        # Fetch processes using GPUs\n        gpu_usage = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-compute-apps=gpu_uuid\", \"--format=csv,noheader\"],\n            universal_newlines=True\n        )\n        \n        # Track GPUs in use\n        used_gpus = set(gpu_usage.strip().split(\"\\n\"))\n        \n        # Find the first GPU not in use\n        for gpu_uuid, gpu_id in gpu_id_to_uuid.items():\n            if gpu_uuid not in used_gpus:\n                print(f\"Using GPU: {gpu_id} - {torch.cuda.get_device_properties(gpu_id).name}\")\n                return torch.device(f\"cuda:{gpu_id}\")\n        \n        print(\"No free GPUs available. Using CPU.\")\n    except Exception as e:\n        print(f\"Error checking GPU usage: {e}. Falling back to CPU.\")\n    \n    return torch.device(\"cpu\")\n\n# Replace TransformerModel with TabNet model\nclass TabNet(nn.Module):\n    def __init__(self,input_dim,output_dim,n_d=64,n_a=64,\nn_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n        super().__init__()\n        if n_shared>0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(input_dim,2*(n_d+n_a)))\n            for x in range(n_shared-1):\n                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n        else:\n            self.shared=None\n        self.first_step = FeatureTransformer(input_dim,n_d+n_a,self.shared,n_ind) \n        self.steps = nn.ModuleList()\n        for x in range(n_steps-1):\n            self.steps.append(DecisionStep(input_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n        self.fc = nn.Linear(n_d,output_dim)\n        self.bn = nn.BatchNorm1d(input_dim)\n        self.n_d = n_d\n\n    def forward(self,x):\n        x = self.bn(x)\n        x_a = self.first_step(x)[:,self.n_d:]\n        sparse_loss = torch.zeros(1).to(x.device)\n        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n        priors = torch.ones(x.shape).to(x.device)\n        for step in self.steps:\n            x_te,l = step(x,x_a,priors)\n            out += F.relu(x_te[:,:self.n_d])\n            x_a = x_te[:,self.n_d:]\n            sparse_loss += l\n        return self.fc(out),sparse_loss\n\n# Load and preprocess data for a specific batch (5 days)\ndef load_batch(files):\n    batch_data = []\n    \n    # Iterate over the files and filter data by date range\n    for file in files:\n        if not os.path.exists(file):\n            continue\n        \n        df = pd.read_csv(file)\n        \n        # Convert date column to datetime\n        df['date'] = pd.to_datetime(df['date'])\n        \n        # Filter data for the date range of this batch (start_date to end_date)\n        batch_data.append(df)\n    \n    # Concatenate all data for the batch\n    batch_df = pd.concat(batch_data)\n    \n    # Feature engineering: Extract day of year and month\n    batch_df['day_of_year'] = batch_df['date'].dt.dayofyear\n    batch_df['month'] = batch_df['date'].dt.month\n    batch_df['year'] = batch_df['date'].dt.year\n    \n    # scale snodas to deal with bias\n    batch_df['snodas'] = np.log10(batch_df['snodas'] + 1)  # +1 to avoid log(0)\n\n    # Features (Latitude, Longitude, day_of_year, month)\n    X = batch_df[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n    \n    # Target: SWE (snodas)\n    y = batch_df['snodas'].values\n    \n    return X, y\n\ndef new_batch_generator(files, start_date, end_date, batch_size):\n    num_files = len(files)\n    current_index = 0\n    X_accumulated = []\n    y_accumulated = []\n\n    while current_index < num_files:\n        current_file = files[current_index]\n        \n        # Load the data for this file\n        X_file, y_file = load_batch([current_file])\n\n        # Accumulate rows from this file\n        X_accumulated.extend(X_file)\n        y_accumulated.extend(y_file)\n\n        # If the accumulated data size exceeds or reaches batch_size, yield batches\n        while len(X_accumulated) >= batch_size:\n            X_chunk = X_accumulated[:batch_size]\n            y_chunk = y_accumulated[:batch_size]\n            \n            # Remove yielded data from accumulation\n            X_accumulated = X_accumulated[batch_size:]\n            y_accumulated = y_accumulated[batch_size:]\n\n            # Convert lists to numpy arrays and reshape y_chunk\n            X_chunk = np.array(X_chunk)\n            y_chunk = np.array(y_chunk).reshape(-1, 1)  # Ensure y_chunk is a 2D array\n            \n            yield X_chunk, y_chunk\n\n        current_index += 1\n\n    # Yield any remaining data if it's less than batch_size\n    if X_accumulated:\n        X_remaining = np.array(X_accumulated)\n        y_remaining = np.array(y_accumulated).reshape(-1, 1)\n        yield X_remaining, y_remaining\n\ndef get_model(device):\n    # model = TabNet(\n    #     input_dim=5, output_dim=1\n    # )\n    # model = TabNetRegressor()\n    model = TabNetRegressor(\n        input_dim=5,\n        output_dim=1,\n        n_d=8,\n        n_a=8,\n        n_steps=3,\n        gamma=1.5,\n        lambda_sparse=1e-5,\n        device_name = str(device)\n    )\n    return model\n\ndef train_model_in_batches(\n    files, start_date, end_date, epochs=100, batch_size=50000,\n    model_save_path=f\"{model_dir}/snodas_tabnet_model.pth\"\n):\n    device = get_first_available_device()\n    model = get_model(device)\n\n    # params_dict = {k: torch.tensor(v) if isinstance(v, (int, float)) else v for k, v in model.optimizer_params.items()}\n    # optimizer = optim.Adam(params_dict, lr=0.001)\n    criterion = nn.MSELoss()\n    print(\"TabNet is set, ready to train\")\n\n    # Send model to GPUs\n    \n    # model = model.to(device)\n    # model.device_name = device\n    \n    print(\"Starting model training...\")\n    print_memory_usage(\"start training\")\n    \n    big_batch_num = 0\n    epoch_loss = 0\n    # for epoch in range(epochs):\n        # print(f\"Big wrap epoch {epoch} / {epochs}\")\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        big_batch_num += 1\n        print(\"processing \", big_batch_num, \" - \", len(X_batch))\n        # Split the batch into training and validation sets (e.g., 80% train, 20% validation)\n        split_idx = int(0.8 * len(X_batch))  # 80% for training, 20% for validation\n        \n        X_train, y_train = X_batch[:split_idx], y_batch[:split_idx]\n        X_val, y_val = X_batch[split_idx:], y_batch[split_idx:]\n\n        # Reshape targets to 2D for regression\n        y_train = y_train.reshape(-1, 1)\n        y_val = y_val.reshape(-1, 1)\n\n        # Training the model using the fit method\n        model.fit(\n            X_train=X_train,\n            y_train=y_train,\n            eval_set=[(X_val, y_val)],\n            eval_name=[\"val\"],\n            eval_metric=[\"rmse\"],\n            max_epochs=epochs,\n            patience=20,\n            batch_size=batch_size,\n            virtual_batch_size=128,\n            num_workers=1,\n            drop_last=False,\n            warm_start=True,  # Ensures continued training from the last state\n            loss_fn=torch.nn.MSELoss()  # Use MSELoss for regression tasks\n        )\n\n        # After training, make predictions on the validation set\n        # y_val_pred = model.predict(X_val)\n\n        # Get the minimum value of the validation loss (logits_ll)\n        # score = np.min(model.history[\"val_logits_ll\"])\n\n        # print(f\"Validation score: {score}\")\n\n        # Compute the loss\n        # Ensure that y_val_pred and y_val are tensors\n        # y_val_pred = torch.tensor(y_val_pred, dtype=torch.float32)\n        # y_val = torch.tensor(y_val, dtype=torch.float32)\n        # loss = criterion(y_val_pred, y_val)\n        # epoch_loss += loss.item()\n\n        # Backward pass and optimization\n        # loss.backward()\n        # optimizer.step()\n\n    # print(f\"Epoch {epoch+1} completed.\")\n\n    print(\"Model training completed.\")\n    # Save the trained model to a file\n    model.save_model(model_save_path)\n    print(f\"Model saved to {model_save_path}.\")\n    return model\n\n\n# Evaluate the model\ndef evaluate_model(model_file_path, files, start_date, end_date, batch_size=10000):\n    print(\"Loading model from:\", model_file_path)\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(f\"{model_file_path}.zip\")\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n\n    all_y_true = []\n    all_y_pred = []\n\n    # Start batch processing\n    batch_count = 0\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        y_pred = model.predict(X_batch)  # Get model predictions\n        all_y_true.extend(y_batch)\n        all_y_pred.extend(y_pred)\n\n    # Calculate evaluation metrics\n    y_true = np.array(all_y_true)\n    y_pred = np.array(all_y_pred)\n    \n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"Evaluation Results: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n\ndef predict_swe_map(data_folder: str, target_date: str, model_path: str):\n    print(\"Loading model from:\", model_path)\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(model_path)\n\n    model_file_name = os.path.basename(model_path)\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n    lat_lon_csv_path = \"/home/chetana/data/snodas/2025-01-19_snodas_output.csv\"\n    mapper_df = pd.read_csv(lat_lon_csv_path)\n    # Ensure required columns exist\n    if 'Latitude' not in mapper_df.columns or 'Longitude' not in mapper_df.columns:\n        raise ValueError(\"The CSV file must contain 'Latitude' and 'Longitude' columns.\")\n    \n    # Step 2: Parse the date string to extract year, month, and day_of_year\n    date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n    year = date_obj.year\n    month = date_obj.month\n    day_of_year = date_obj.timetuple().tm_yday  # Get day of the year\n\n    print(year, month, day_of_year)\n    \n    # Step 3: Add the time-related columns to the DataFrame\n    mapper_df['year'] = year\n    mapper_df['month'] = month\n    mapper_df['day_of_year'] = day_of_year\n    \n    # Step 4: Extract the features (Latitude, Longitude, day_of_year, month, year)\n    X = mapper_df[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n    \n    print(X)\n\n    y = model.predict(X)\n\n    # Flatten y if it has an extra dimension\n    if y.ndim > 1:\n        y = y.flatten()\n\n    # Convert to a pandas Series for convenience\n    y_series = pd.Series(y)\n\n    # Print basic statistics\n    print(\"Statistics of predictions (y):\")\n    print(f\"Mean: {y_series.mean()}\")\n    print(f\"Median: {y_series.median()}\")\n    print(f\"Standard Deviation: {y_series.std()}\")\n    print(f\"Minimum: {y_series.min()}\")\n    print(f\"Maximum: {y_series.max()}\")\n\n    swe_predictions = (10 ** y) - 0.1  # Reverse log10 scaling\n\n    # Add predictions to the DataFrame\n    mapper_df['SWE_predicted'] = swe_predictions\n\n    print(mapper_df['SWE_predicted'].describe())\n\n    mapper_df = mapper_df[(mapper_df['SWE_predicted'] >= 0) & (mapper_df['SWE_predicted'] <= 3000)]\n    \n    # Plotting the SWE results\n    plt.figure(figsize=(10, 8))\n    scatter = plt.scatter(\n        mapper_df['Longitude'], \n        mapper_df['Latitude'], \n        c=mapper_df['SWE_predicted'], \n        cmap='viridis', \n        s=10\n    )\n    plt.colorbar(scatter, label=\"SWE (mm)\")\n    plt.title(f\"Predicted SWE Map for {target_date}\")\n    plt.xlabel(\"Longitude\")\n    plt.ylabel(\"Latitude\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(f\"{plot_dir}/swe_map_{target_date}_{model_file_name}.png\")\n    plt.show()\n\n    print(f\"SWE map saved to {plot_dir}/swe_map_{target_date}_{model_file_name}.png\")\n\n\n# Main function to execute the training process\ndef main(data_folder, start_date_str, end_date_str, model_path):\n    # Convert date strings to datetime objects\n    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n    \n    # List all CSV files in the data folder\n    # Loop through dates from start_date to end_date (inclusive)\n    current_date = start_date\n    while current_date <= end_date:\n        # Convert the current date back to a string (if needed)\n        current_date_str = current_date.strftime(\"%Y-%m-%d\")\n        \n        # Perform your processing for the current date\n        print(f\"Processing data for date: {current_date_str}\")\n        \n        # Example: Placeholder for data processing function\n        predict_swe_map(data_folder, current_date_str, model_path)\n        \n        # Increment the date by 1 day\n        current_date += timedelta(days=1)\n\nif __name__ == \"__main__\":\n    # Folder containing the data files\n    data_folder = f\"{data_dir}/snodas/csv/\"\n\n    # Define start and end date for training\n    start_date_str = '2025-01-29'\n    end_date_str = '2025-01-29'\n    # epoch 2\n    # model_path = f\"/home/chetana/models/snodas_tabnet_20250122_074129.pth.zip\" \n    # epoch 10\n    # model_path = f\"/home/chetana/models/snodas_tabnet_20250122_075325.pth.zip\"\n    # epoch 1000 on 1 file\n    # model_path = f\"/home/chetana/models/snodas_tabnet_20250128_132543.pth.zip\"\n    # epoch 100 on 5 files\n    model_path = f\"/home/chetana/models/snodas_tabnet_20250128_145240.pth.zip\"\n\n    # Run the training process\n    model = main(data_folder, start_date_str, end_date_str, model_path)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6cfy6n",
  "name" : "snodas_tabnet_slurm",
  "description" : null,
  "code" : "#!/bin/bash\n# Specify the name of the script you want to submit\nSCRIPT_NAME=\"train_tabnet_on_snodas.sh\"\necho \"write the slurm script into ${SCRIPT_NAME}\"\n\n# CPU\n#SBATCH --qos=qtong             #\n#SBATCH --partition=contrib     # partition (queue): debug, interactive, contrib, normal, orc-test\n\n# GPU\n#SBATCH --account=qtong\n#SBATCH --qos=gpu\n#SBATCH --partition=gpuq\n#SBATCH --gres=gpu:A100.80gb:1                # up to 8; only request what you need\n\n\ncat > ${SCRIPT_NAME} << EOF\n#!/bin/bash\n#SBATCH -J tabnet_snodas       # Job name\n#SBATCH --account=qtong\n#SBATCH --qos=gpu\n#SBATCH --partition=contrib-gpuq\n#SBATCH --gres=gpu:A100.80gb:1                # up to 8; only request what you need\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20                 # number of cores needed\n#SBATCH --mem=20G\n#SBATCH --time=24:00:00         # walltime\n#SBATCH --mail-user=zsun@gmu.edu    #Email account\n#SBATCH --mail-type=FAIL           #When to email\n#SBATCH --output=/scratch/%u/%x-%N-%j.out  # Output file`\n#SBATCH --error=/scratch/%u/%x-%N-%j.err   # Error file`\n\nset echo\numask 0027\n\n\n# Activate your customized virtual environment\nsource /home/zsun/anaconda3/bin/activate\n\n# export CUDA_LAUNCH_BLOCKING=1\n\npython -u << INNER_EOF\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport psutil\nfrom snowcast_utils import model_dir, data_dir\nimport random\nimport subprocess\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport warnings\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_tabnet.callbacks\")\n\n\n# Function to get system memory usage\ndef get_system_memory():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 ** 2)  # Return in MB\n\n# Function to get GPU memory usage\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / (1024 ** 2)  # Return in MB\n    return 0\n\ndef print_memory_usage(step_description):\n    system_memory = get_system_memory()\n    gpu_memory = get_gpu_memory()\n    print(f\"Memory usage ({step_description}):\")\n    print(f\"System Memory: {system_memory:.2f} MB\")\n    print(f\"GPU Memory: {gpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\ndef get_first_available_device():\n    \"\"\"\n    Returns the first available GPU device based on utilization or falls back to CPU.\n    \n    Returns:\n        torch.device: The device to use for computations.\n    \"\"\"\n    try:\n        # Fetch the list of GPUs and their UUIDs\n        gpu_list = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,uuid\", \"--format=csv,noheader,nounits\"],\n            universal_newlines=True\n        )\n        \n        gpu_id_to_uuid = {}\n        for line in gpu_list.strip().split(\"\\n\"):\n            if line:\n                gpu_id, gpu_uuid = line.split(\", \")\n                gpu_id_to_uuid[gpu_uuid] = int(gpu_id)\n        \n        # Fetch processes using GPUs\n        gpu_usage = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-compute-apps=gpu_uuid\", \"--format=csv,noheader\"],\n            universal_newlines=True\n        )\n        \n        # Track GPUs in use\n        used_gpus = set(gpu_usage.strip().split(\"\\n\"))\n        \n        # Find the first GPU not in use\n        for gpu_uuid, gpu_id in gpu_id_to_uuid.items():\n            if gpu_uuid not in used_gpus:\n                print(f\"Using GPU: {gpu_id} - {torch.cuda.get_device_properties(gpu_id).name}\")\n                return torch.device(f\"cuda:{gpu_id}\")\n        \n        print(\"No free GPUs available. Using CPU.\")\n    except Exception as e:\n        print(f\"Error checking GPU usage: {e}. Falling back to CPU.\")\n    \n    return torch.device(\"cpu\")\n\n# Replace TransformerModel with TabNet model\nclass TabNet(nn.Module):\n    def __init__(self,input_dim,output_dim,n_d=64,n_a=64,\nn_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n        super().__init__()\n        if n_shared>0:\n            self.shared = nn.ModuleList()\n            self.shared.append(nn.Linear(input_dim,2*(n_d+n_a)))\n            for x in range(n_shared-1):\n                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n        else:\n            self.shared=None\n        self.first_step = FeatureTransformer(input_dim,n_d+n_a,self.shared,n_ind) \n        self.steps = nn.ModuleList()\n        for x in range(n_steps-1):\n            self.steps.append(DecisionStep(input_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n        self.fc = nn.Linear(n_d,output_dim)\n        self.bn = nn.BatchNorm1d(input_dim)\n        self.n_d = n_d\n\n    def forward(self,x):\n        x = self.bn(x)\n        x_a = self.first_step(x)[:,self.n_d:]\n        sparse_loss = torch.zeros(1).to(x.device)\n        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n        priors = torch.ones(x.shape).to(x.device)\n        for step in self.steps:\n            x_te,l = step(x,x_a,priors)\n            out += F.relu(x_te[:,:self.n_d])\n            x_a = x_te[:,self.n_d:]\n            sparse_loss += l\n        return self.fc(out),sparse_loss\n\n# Load and preprocess data for a specific batch (5 days)\ndef load_batch(files):\n    batch_data = []\n    \n    # Iterate over the files and filter data by date range\n    for file in files:\n        if not os.path.exists(file):\n            continue\n        \n        df = pd.read_csv(file)\n        \n        # Convert date column to datetime\n        df['date'] = pd.to_datetime(df['date'])\n        \n        # Filter data for the date range of this batch (start_date to end_date)\n        batch_data.append(df)\n    \n    # Concatenate all data for the batch\n    batch_df = pd.concat(batch_data)\n    \n    # Feature engineering: Extract day of year and month\n    batch_df['day_of_year'] = batch_df['date'].dt.dayofyear\n    batch_df['month'] = batch_df['date'].dt.month\n    batch_df['year'] = batch_df['date'].dt.year\n    \n    # scale snodas to deal with bias\n    batch_df['snodas'] = np.log10(batch_df['snodas'] + 1)  # +1 to avoid log(0)\n\n    # Features (Latitude, Longitude, day_of_year, month)\n    X = batch_df[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n    \n    # Target: SWE (snodas)\n    y = batch_df['snodas'].values\n    \n    return X, y\n\ndef new_batch_generator(files, start_date, end_date, batch_size):\n    num_files = len(files)\n    current_index = 0\n    X_accumulated = []\n    y_accumulated = []\n\n    while current_index < num_files:\n        current_file = files[current_index]\n        \n        # Load the data for this file\n        X_file, y_file = load_batch([current_file])\n\n        # Accumulate rows from this file\n        X_accumulated.extend(X_file)\n        y_accumulated.extend(y_file)\n\n        # If the accumulated data size exceeds or reaches batch_size, yield batches\n        while len(X_accumulated) >= batch_size:\n            X_chunk = X_accumulated[:batch_size]\n            y_chunk = y_accumulated[:batch_size]\n            \n            # Remove yielded data from accumulation\n            X_accumulated = X_accumulated[batch_size:]\n            y_accumulated = y_accumulated[batch_size:]\n\n            # Convert lists to numpy arrays and reshape y_chunk\n            X_chunk = np.array(X_chunk)\n            y_chunk = np.array(y_chunk).reshape(-1, 1)  # Ensure y_chunk is a 2D array\n            \n            yield X_chunk, y_chunk\n\n        current_index += 1\n\n    # Yield any remaining data if it's less than batch_size\n    if X_accumulated:\n        X_remaining = np.array(X_accumulated)\n        y_remaining = np.array(y_accumulated).reshape(-1, 1)\n        yield X_remaining, y_remaining\n\ndef get_model(device):\n    # model = TabNet(\n    #     input_dim=5, output_dim=1\n    # )\n    # model = TabNetRegressor()\n    model = TabNetRegressor(\n        input_dim=5,\n        output_dim=1,\n        n_d=8,\n        n_a=8,\n        n_steps=3,\n        gamma=1.5,\n        lambda_sparse=1e-5,\n        device_name = str(device)\n    )\n    return model\n\ndef train_model_in_batches(\n    files, start_date, end_date, epochs=100, batch_size=50000,\n    model_save_path=f\"{model_dir}/snodas_tabnet_model.pth\"\n):\n    device = get_first_available_device()\n    model = get_model(device)\n\n    # params_dict = {k: torch.tensor(v) if isinstance(v, (int, float)) else v for k, v in model.optimizer_params.items()}\n    # optimizer = optim.Adam(params_dict, lr=0.001)\n    criterion = nn.MSELoss()\n    print(\"TabNet is set, ready to train\")\n\n    # Send model to GPUs\n    \n    # model = model.to(device)\n    # model.device_name = device\n    \n    print(\"Starting model training...\")\n    print_memory_usage(\"start training\")\n    \n    big_batch_num = 0\n    epoch_loss = 0\n    # for epoch in range(epochs):\n        # print(f\"Big wrap epoch {epoch} / {epochs}\")\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        big_batch_num += 1\n        print(\"processing \", big_batch_num, \" - \", len(X_batch))\n        # Split the batch into training and validation sets (e.g., 80% train, 20% validation)\n        split_idx = int(0.8 * len(X_batch))  # 80% for training, 20% for validation\n        \n        X_train, y_train = X_batch[:split_idx], y_batch[:split_idx]\n        X_val, y_val = X_batch[split_idx:], y_batch[split_idx:]\n\n        # Reshape targets to 2D for regression\n        y_train = y_train.reshape(-1, 1)\n        y_val = y_val.reshape(-1, 1)\n\n        # Training the model using the fit method\n        model.fit(\n            X_train=X_train,\n            y_train=y_train,\n            eval_set=[(X_val, y_val)],\n            eval_name=[\"val\"],\n            eval_metric=[\"rmse\"],\n            max_epochs=epochs,\n            patience=20,\n            batch_size=batch_size,\n            virtual_batch_size=128,\n            num_workers=1,\n            drop_last=False,\n            warm_start=True,  # Ensures continued training from the last state\n            loss_fn=torch.nn.MSELoss()  # Use MSELoss for regression tasks\n        )\n\n        # After training, make predictions on the validation set\n        # y_val_pred = model.predict(X_val)\n\n        # Get the minimum value of the validation loss (logits_ll)\n        # score = np.min(model.history[\"val_logits_ll\"])\n\n        # print(f\"Validation score: {score}\")\n\n        # Compute the loss\n        # Ensure that y_val_pred and y_val are tensors\n        # y_val_pred = torch.tensor(y_val_pred, dtype=torch.float32)\n        # y_val = torch.tensor(y_val, dtype=torch.float32)\n        # loss = criterion(y_val_pred, y_val)\n        # epoch_loss += loss.item()\n\n        # Backward pass and optimization\n        # loss.backward()\n        # optimizer.step()\n\n    # print(f\"Epoch {epoch+1} completed.\")\n\n    print(\"Model training completed.\")\n    # Save the trained model to a file\n    model.save_model(model_save_path)\n    print(f\"Model saved to {model_save_path}.\")\n    return model\n\n\n# Evaluate the model\ndef evaluate_model(model_file_path, files, start_date, end_date, batch_size=10000):\n    print(\"Loading model from:\", model_file_path)\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(f\"{model_file_path}.zip\")\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n\n    all_y_true = []\n    all_y_pred = []\n\n    # Start batch processing\n    batch_count = 0\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        y_pred = model.predict(X_batch)  # Get model predictions\n        all_y_true.extend(y_batch)\n        all_y_pred.extend(y_pred)\n\n    # Calculate evaluation metrics\n    y_true = np.array(all_y_true)\n    y_pred = np.array(all_y_pred)\n    \n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"Evaluation Results: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n\ndef predict_swe_map(target_date: str):\n    \n    print(\"Loading model from:\", model_file_path)\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(f\"{model_file_path}.zip\")\n\n    print(f\"Model loaded successfully. Running evaluation on {device}...\")\n\n    all_y_true = []\n    all_y_pred = []\n\n    # Start batch processing\n    batch_count = 0\n    for X_batch, y_batch in new_batch_generator(files, start_date, end_date, batch_size):\n        y_pred = model.predict(X_batch)  # Get model predictions\n        all_y_true.extend(y_batch)\n        all_y_pred.extend(y_pred)\n\n    # Calculate evaluation metrics\n    y_true = np.array(all_y_true)\n    y_pred = np.array(all_y_pred)\n    \n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n\n    print(f\"Evaluation Results: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n\n# Main function to execute the training process\ndef main(data_folder, start_date_str, end_date_str):\n    # Convert date strings to datetime objects\n    start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n    end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n    \n    # List all CSV files in the data folder\n    files = [os.path.join(data_folder, f) for f in os.listdir(data_folder) if f.endswith('.csv')]\n    # Shuffle the files to ensure randomness\n    random.shuffle(files)\n\n    # files = files[:50]\n\n    # Split the files: 80% for training and 20% for evaluation\n    train_files = files[:int(0.8 * len(files))]\n    eval_files = files[int(0.8 * len(files)):]\n\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    model_file_path = f\"{model_dir}/snodas_tabnet_{timestamp}.pth\"\n\n    batch_size = 2500000\n    epochs = 4\n    \n    # Train the model using batches of 5 days\n    train_model_in_batches(\n        train_files, start_date, end_date, epochs=epochs,\n        batch_size=batch_size,\n        model_save_path = model_file_path,\n    )\n\n    # Evaluate model on the last batch of data\n    evaluate_model(\n        model_file_path, eval_files, end_date - timedelta(days=4), end_date, batch_size=batch_size,\n    )\n    \n    return model_file_path\n\nif __name__ == \"__main__\":\n    # Folder containing the data files\n    data_folder = f\"{data_dir}/snodas/csv/\"\n\n    # Define start and end date for training\n    start_date_str = '2024-10-01'\n    end_date_str = '2025-01-15'\n\n    # Run the training process\n    model = main(data_folder, start_date_str, end_date_str)\n\n\nINNER_EOF\n\nEOF\n\n# Submit the Slurm job and wait for it to finish\necho \"sbatch ${SCRIPT_NAME}\"\n\n\n# Submit the Slurm job\njob_id=$(sbatch ${SCRIPT_NAME} | awk '{print $4}')\necho \"job_id=\"${job_id}\n\nif [ -z \"${job_id}\" ]; then\n    echo \"job id is empty. something wrong with the slurm job submission.\"\n    exit 1\nfi\n\n# Wait for the Slurm job to finish\n# Wait for the Slurm job to finish\nfile_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\nprevious_content=$(cat file_name)\nexit_code=0\nwhile true; do\n    file_name=$(find /scratch/zsun -name '*'${job_id}'.out' -print -quit)\n    if [[ -n \"$file_name\" ]]; then\n        current_content=$(<\"${file_name}\")\n        # Compare current content with previous content\n        if [ \"$previous_content\" != \"$current_content\" ]; then\n            diff <(echo \"$previous_content\") <(echo \"$current_content\")\n            previous_content=\"$current_content\"\n        fi\n    fi\n\n    job_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\n    if [[ $job_status == *\"COMPLETED\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        break;\n    elif [[ $job_status == *\"CANCELLED\"* || $job_status == *\"FAILED\"* || $job_status == *\"TIMEOUT\"* || $job_status == *\"NODE_FAIL\"* || $job_status == *\"PREEMPTED\"* || $job_status == *\"OUT_OF_MEMORY\"* ]]; then\n        echo \"Job $job_id has finished with state: $job_status\"\n        exit_code=1\n        break;\n    fi\n    sleep 10  # Adjust the sleep interval as needed\ndone\n\necho \"Slurm job ($job_id) has finished.\"\n\necho \"Print the job's output logs\"\nsacct --format=JobID,JobName,State,ExitCode,MaxRSS,Start,End -j $job_id\n#find /scratch/zsun/ -type f -name \"*${job_id}.out\" -exec cat {} \\;\n\necho \"All slurm job for ${SCRIPT_NAME} finishes.\"\n\njob_status=$(scontrol show job ${job_id} | awk '/JobState=/{print $1}')\necho \"job status $job_status\"\nif [[ $job_status == *\"COMPLETED\"* ]]; then\n    exit 0\nfi\n\nexit 1\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "4q1uy4",
  "name" : "add_snodas_mask_column",
  "description" : null,
  "code" : "import pandas as pd\nimport torch\nfrom torch import nn, optim\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import datetime\nimport numpy as np\nfrom snowcast_utils import data_dir, work_dir\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport psutil\nfrom snowcast_utils import model_dir, data_dir, test_start_date, test_end_date\nimport random\nimport subprocess\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nimport warnings\nimport matplotlib.pyplot as plt\nfrom snodas_dnn_new import SNODAS_DNN_Model, SNODASDNNHole\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_tabnet.callbacks\")\n\n\n# Function to get system memory usage\ndef get_system_memory():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 ** 2)  # Return in MB\n\n# Function to get GPU memory usage\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / (1024 ** 2)  # Return in MB\n    return 0\n\ndef print_memory_usage(step_description):\n    system_memory = get_system_memory()\n    gpu_memory = get_gpu_memory()\n    print(f\"Memory usage ({step_description}):\")\n    print(f\"System Memory: {system_memory:.2f} MB\")\n    print(f\"GPU Memory: {gpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\ndef get_first_available_device():\n    \"\"\"\n    Returns the first available GPU device based on utilization or falls back to CPU.\n    \n    Returns:\n        torch.device: The device to use for computations.\n    \"\"\"\n    try:\n        # Fetch the list of GPUs and their UUIDs\n        gpu_list = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,uuid\", \"--format=csv,noheader,nounits\"],\n            universal_newlines=True\n        )\n        \n        gpu_id_to_uuid = {}\n        for line in gpu_list.strip().split(\"\\n\"):\n            if line:\n                gpu_id, gpu_uuid = line.split(\", \")\n                gpu_id_to_uuid[gpu_uuid] = int(gpu_id)\n        \n        # Fetch processes using GPUs\n        gpu_usage = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-compute-apps=gpu_uuid\", \"--format=csv,noheader\"],\n            universal_newlines=True\n        )\n        \n        # Track GPUs in use\n        used_gpus = set(gpu_usage.strip().split(\"\\n\"))\n        \n        # Find the first GPU not in use\n        for gpu_uuid, gpu_id in gpu_id_to_uuid.items():\n            if gpu_uuid not in used_gpus:\n                print(f\"Using GPU: {gpu_id} - {torch.cuda.get_device_properties(gpu_id).name}\")\n                return torch.device(f\"cuda:{gpu_id}\")\n        \n        print(\"No free GPUs available. Using CPU.\")\n    except Exception as e:\n        print(f\"Error checking GPU usage: {e}. Falling back to CPU.\")\n    \n    return torch.device(\"cpu\")\n\ndevice = get_first_available_device()\n\ndef get_model(device):\n    model = SNODAS_DNN_Model(norm = True).to(device)\n    # Calculate the total number of trainable parameters\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"Total trainable parameters: {total_params}\")\n    return model\n\ndef add_snodas_mask_column_for_training(\n    model_path: str, \n    training_csv, \n    new_training_csv,\n    lat_col_name = \"lat\", \n    lon_col_name = \"lon\", \n    date_col_name = \"date\"\n):\n    # the training must be added from AI-derived swe maps. This is because if the batch size changes, the results will change. Must maintain the same input and batch size as the training. \n    training_df = pd.read_csv(training_csv)\n    print(training_df.columns)\n    unique_dates = training_df[[\"date\"]].drop_duplicates()\n    print(\"unique dates: \", len(unique_dates))\n\n    hole_model = SNODASDNNHole(\n        start_date_str = test_start_date,\n        end_date_str = test_end_date,\n        batch_size = 310000,\n        epochs = 10,\n        train_ratio = 0.8,\n        test_ratio = 0.2,\n        val_ratio = 0.01,\n        normalization = True,\n        retrain = True,\n        model_path = \"/home/chetana/models//SNODASDNNHole_e10_nTrue_20253101055710.model\",\n        # base_model_path = \"/home/chetana/models//SNODASDNNHole_e10_nTrue_20253101055710.model\"\n    )\n    \n    # hole_model.preprocessing(verbose=True, semimonth_only = False)\n    # hole_model.train()\n    # hole_model.save()\n    # hole_model.evaluate()\n    for date in unique_dates[\"date\"]:\n        print(\"Generating for \", date)\n        df = hole_model.predict(date, save_csv=False, tiff=False, skip_exists=False)\n\n        print(df.describe())\n        return\n\n    \n\n    \n\n# Main function for prediction on the large CSV\ndef add_snodas_mask_column_for_testing(\n    model_path: str, \n    testing_csv, \n    new_testing_csv,\n    lat_col_name = \"lat\", \n    lon_col_name = \"lon\", \n    date_col_name = \"date\"\n):\n    print(f\"Loading model from: {model_path}\")\n    device = get_first_available_device()\n    model = get_model(device)\n    model.load_model(model_path)\n    \n    print(\"Model loaded successfully. Preparing the dataset...\")\n    \n    # Read large CSV efficiently in chunks\n    chunk_size = 310000  # Set a chunk size that your system can handle\n    # this chunk size must match the batch size when training the model. Different chunk size will cause problems. Cannot change this!!! Be aware.\n    lat_lon_csv_path = testing_csv\n    \n    # Placeholder for predictions (to add them to the dataframe)\n    predictions_list = []\n    \n    model.eval()\n    for chunk in pd.read_csv(lat_lon_csv_path, chunksize=chunk_size):\n        # print(chunk.columns)\n        # Extract required columns\n        current_chunk = chunk[[date_col_name, lat_col_name, lon_col_name]].copy()\n        \n        # Parse the date and extract year, month, day of year\n        print(\"Parsing date and extracting features...\")\n        current_chunk[date_col_name] = pd.to_datetime(current_chunk[date_col_name], format=\"%Y-%m-%d\")\n        current_chunk['year'] = current_chunk[date_col_name].dt.year\n        current_chunk['month'] = current_chunk[date_col_name].dt.month\n        current_chunk['day_of_year'] = current_chunk[date_col_name].dt.dayofyear\n        \n        # Extract features (Latitude, Longitude, day_of_year, month, year)\n        X = current_chunk[[lat_col_name, lon_col_name, 'day_of_year', 'month', 'year']].values\n\n        # y = model.predict(X)\n        X = torch.tensor(X, dtype=torch.float32).to(device)\n        y = model(X).detach().cpu().numpy()\n\n        # Flatten y if it has an extra dimension\n        if y.ndim > 1:\n            y = y.flatten()\n\n        # Convert to a pandas Series for convenience\n        y_series = pd.Series(y)\n\n        snodas_mask = (10 ** y) - 1  # Reverse log10 scaling, should be 0.1\n\n        # Add predictions to the chunk\n        chunk['snodas_mask'] = snodas_mask\n        \n        # Append the chunk with predictions to the list\n        predictions_list.append(chunk)\n\n        print(f\"Processed {len(chunk)} rows, predictions added.\")\n    \n    # Concatenate all the chunks with predictions\n    final_df = pd.concat(predictions_list, ignore_index=True)\n    \n    # Save the final dataframe with predictions\n    output_file = new_testing_csv\n    final_df.to_csv(output_file, index=False)\n    print(f\"Predictions saved to {output_file}\")\n\nif __name__ == \"__main__\":\n    # Call the function with appropriate paths\n    # WARNING: This is not right, because batch matters. The start of each batch will matter. The correct way is to first generate all years of data, and then use the output to generate the training.csv.\n    training_csv = f\"{work_dir}/all_points_final_merged_training.csv\"\n    new_training_csv = f\"{work_dir}/all_points_final_merged_training_snodas_mask_resnet.csv\"\n    model_path = f\"/home/chetana/models//SNODASDNNHole_e10_nTrue_20253101055710.model\"\n    add_snodas_mask_column_for_training(\n        model_path, \n        training_csv, \n        new_training_csv, \n        lat_col_name = \"lat\", \n        lon_col_name = \"lon\", \n        date_col_name = \"date\"\n    )\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xj0045",
  "name" : "snodas_fttransformer",
  "description" : null,
  "code" : "# This doesn't work, only gives noise\n\nimport torch\nimport torch.nn as nn\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport random\nimport psutil\nfrom snowcast_utils import model_dir, data_dir, plot_dir, output_dir, test_start_date, test_end_date\nfrom datetime import datetime, timedelta\nimport time\nimport subprocess\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom base_nn_hole import BaseNNHole\n\n# Function to define the FT-Transformer model\nimport torch\nimport torch.nn as nn\n\nclass FTTransformer(nn.Module):\n    def __init__(\n        self, input_dim, output_dim, \n        embed_dim=16, num_heads=4, num_layers=2\n    ):\n        super(FTTransformer, self).__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)\n        \n        # Define the transformer encoder layer\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim * 2\n        )\n        \n        # Use TransformerEncoder with batch_first=True for (batch_size, seq_len, embed_dim)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        \n        # Output regressor layer\n        self.regressor = nn.Linear(embed_dim, output_dim)\n\n    def forward(self, x):\n        # Input embedding: [batch_size, sequence_length, embed_dim]\n        x = self.embedding(x)\n        # print(\"after embedding: \", x.shape)\n        \n        # Add a sequence length dimension of 1, resulting in shape [batch_size, 1, embed_dim]\n        x = x.unsqueeze(1)  # Adds a dimension for seq_len, so now it's [batch_size, 1, embed_dim]\n        \n        # Transformer expects inputs with shape (seq_len, batch_size, embed_dim)\n        x = x.permute(1, 0, 2)  # [seq_len, batch_size, embed_dim]\n        \n        # Pass through the transformer encoder\n        x = self.transformer(x)\n        # print(\"after transformer: \", x.shape)\n        \n        # Pooling the sequence output (mean over the sequence length)\n        x = torch.mean(x, dim=0)  # Mean pooling over the sequence length\n        # print(\"after mean: \", x.shape)\n\n        # Remove sequence length dimension\n        x = x.squeeze(0)  # [batch_size, embed_dim]\n        \n        # Regress to the final output\n        return self.regressor(x)\n\n    def load_model(self, model_path):\n        \"\"\"Loads the model weights from a file.\"\"\"\n        try:\n            # Load the state dict from the file\n            state_dict = torch.load(model_path)\n            # Load the state dict into the model\n            self.load_state_dict(state_dict)\n            print(f\"Model loaded from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n\n\nclass SNODAS_FTTransformer_Hole(BaseNNHole):\n\n    def get_model(self):\n        model = FTTransformer(\n            input_dim=5, output_dim=1, embed_dim=64, num_heads=4, num_layers=5\n        ).to(self.device)\n        # Calculate the total number of trainable parameters\n        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        print(f\"Total trainable parameters: {total_params}\")\n        return model\n\n\nif __name__ == \"__main__\":\n    \n    hole_model = SNODAS_FTTransformer_Hole(\n        start_date_str = test_start_date,\n        end_date_str = test_end_date,\n        batch_size = 300000,\n        epochs = 50,\n        train_ratio = 0.1,\n        test_ratio = 0.1,\n        val_ratio = 0.2,\n        retrain = False,\n        normalization = False,\n    )\n    \n    hole_model.preprocessing(verbose=True)\n    hole_model.train()\n    hole_model.save()\n    hole_model.evaluate()\n    hole_model.predict(\"2025-01-29\", save_csv=True, tiff=True)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "05h6l5",
  "name" : "snodas_dnn_new",
  "description" : null,
  "code" : "from base_nn_hole import BaseNNHole\nimport torch\nimport torch.nn as nn\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport random\nimport psutil\nfrom snowcast_utils import model_dir, data_dir, plot_dir, test_start_date, test_end_date\nfrom datetime import datetime, timedelta\nimport time\nimport subprocess\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom base_hole import BaseHole\nimport shutil\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SNODAS_DNN_Model(nn.Module):\n    def __init__(self, norm):\n        self.norm = norm\n        super(SNODAS_DNN_Model, self).__init__()\n\n        self.input_layer = nn.Sequential(\n            nn.Linear(5, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128)\n        )\n\n        self.hidden_layers = nn.Sequential(\n            self._residual_block(128, 256),\n            self._residual_block(256, 256),\n            self._residual_block(256, 256),\n            self._residual_block(256, 256),\n            self._residual_block(256, 128)\n        )\n\n        self.output_layer = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def _residual_block(self, in_features, out_features):\n        \"\"\"Creates a residual block for faster training\"\"\"\n        return nn.Sequential(\n            nn.Linear(in_features, out_features),\n            nn.ReLU(),\n            nn.BatchNorm1d(out_features),\n            nn.Linear(out_features, out_features),\n            nn.ReLU(),\n            nn.BatchNorm1d(out_features)\n        )\n\n    def forward(self, X):\n        X = self.input_layer(X)\n        X = self.hidden_layers(X) + X  # Residual Connection\n        return self.output_layer(X)\n    \n    def load_model(self, model_path):\n        \"\"\"Loads the model weights from a file.\"\"\"\n        try:\n            # Load the state dict from the file\n            state_dict = torch.load(model_path)\n            # Load the state dict into the model\n            self.load_state_dict(state_dict)\n            print(f\"Model loaded from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n\nclass SNODASDNNHole(BaseNNHole):\n\n    def normalize(self, X):\n        return X\n\n    def preprocessing(self, verbose=False, semimonth_only = False):\n        \"\"\"\n        Preprocesses the data for training and testing.\n\n        Returns:\n            None\n        \"\"\"\n        \n        # List all CSV files that end with -01.csv or -15.csv\n        if semimonth_only:\n            files = [\n                os.path.join(self.SNODAS_CSV_FOLDER, f)\n                for f in os.listdir(self.SNODAS_CSV_FOLDER)\n                if f.endswith('.csv') and ('-01_snodas' in f or '-15_snodas' in f)\n            ]\n        else:\n            files = [\n                os.path.join(self.SNODAS_CSV_FOLDER, f)\n                for f in os.listdir(self.SNODAS_CSV_FOLDER)\n                if f.endswith('.csv')\n            ]\n\n        # Shuffle the files to ensure randomness\n        random.shuffle(files)\n\n        # Split the files: 80% for training and 20% for evaluation\n        train_size = int(self.train_ratio * len(files))\n        test_size = int(self.test_ratio * len(files))\n        self.train_files = files[:train_size]\n        self.eval_files = files[-test_size:]\n\n        if verbose:\n            print(\"Train files:\", self.train_files)\n            print(\"Test files:\", self.eval_files)\n\n    def get_model(self):\n        model = SNODAS_DNN_Model(norm = self.normalization).to(self.device)\n        # Calculate the total number of trainable parameters\n        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        print(f\"Total trainable parameters: {total_params}\")\n        return model\n\n\n\nif __name__ == \"__main__\":\n\n    # Define start and end date for training\n    start_date_str = '2024-10-01'\n    end_date_str = '2025-01-15'\n    \n    hole_model = SNODASDNNHole(\n        start_date_str = test_start_date,\n        end_date_str = test_end_date,\n        batch_size = 310000,\n        epochs = 10,\n        train_ratio = 0.8,\n        test_ratio = 0.2,\n        val_ratio = 0.01,\n        normalization = True,\n        retrain = True,\n        # model_path = \"/home/chetana/models//SNODASDNNHole_e10_nTrue_20253101221511.model\",\n        base_model_path = \"/home/chetana/models//SNODASDNNHole_e10_nTrue_20253101221511.model\"\n    )\n    \n    hole_model.preprocessing(verbose=True, semimonth_only = False)\n    hole_model.train()\n    hole_model.save()\n    hole_model.evaluate()\n    hole_model.predict(\"2024-12-10\")\n    hole_model.predict(\"2024-11-10\")\n    hole_model.predict(\"2024-10-10\")\n    hole_model.predict(\"2024-09-10\")\n    hole_model.predict(\"2024-08-10\")\n    hole_model.predict(\"2024-07-10\")\n    hole_model.predict(\"2024-06-10\")\n    hole_model.predict(\"2024-05-10\")\n    hole_model.predict(\"2024-04-10\")\n    hole_model.predict(\"2024-03-10\")\n    hole_model.predict(\"2024-02-10\")\n    hole_model.predict(\"2024-01-10\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "u5x64m",
  "name" : "base_nn_hole",
  "description" : null,
  "code" : "\nimport torch\nimport torch.nn as nn\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport random\nimport psutil\nfrom snowcast_utils import model_dir, data_dir, plot_dir, output_dir, test_start_date, test_end_date\nfrom datetime import datetime, timedelta\nimport time\nimport subprocess\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom base_hole import BaseHole\nimport shutil\nfrom sklearn.utils import shuffle\n\nimport os\nimport torch\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom tqdm import tqdm\n\nimport rasterio\nfrom rasterio.transform import from_origin\nfrom scipy.interpolate import griddata\n\n\nclass SNODASModel(nn.Module):\n    def __init__(self):\n        super(SNODASModel, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(5, 128),  # Increased capacity\n            nn.ReLU(),\n\n            nn.Linear(128, 256),\n            nn.ReLU(),\n\n            # Repeated block for consistency\n            self._build_block(256),\n            self._build_block(256),\n            self._build_block(256),\n            self._build_block(256),\n            self._build_block(256),\n            self._build_block(256),\n\n            nn.Linear(256, 128),\n            nn.ReLU(),\n\n            nn.Linear(128, 1)\n        )\n\n    def _build_block(self, input_size):\n        \"\"\"Helper function to define a common block for reuse\"\"\"\n        return nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n        )\n    \n    def forward(self, x):\n        return self.network(x)\n\n    def load_model(self, model_path):\n        \"\"\"Loads the model weights from a file.\"\"\"\n        try:\n            # Load the state dict from the file\n            state_dict = torch.load(model_path)\n            # Load the state dict into the model\n            self.load_state_dict(state_dict)\n            print(f\"Model loaded from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n    \n\n# Function to get system memory usage\ndef get_system_memory():\n    process = psutil.Process(os.getpid())\n    memory_info = process.memory_info()\n    return memory_info.rss / (1024 ** 2)  # Return in MB\n\n# Function to get GPU memory usage\ndef get_gpu_memory():\n    if torch.cuda.is_available():\n        return torch.cuda.memory_allocated() / (1024 ** 2)  # Return in MB\n    return 0\n\ndef print_memory_usage(step_description):\n    system_memory = get_system_memory()\n    gpu_memory = get_gpu_memory()\n    print(f\"Memory usage ({step_description}):\")\n    print(f\"System Memory: {system_memory:.2f} MB\")\n    print(f\"GPU Memory: {gpu_memory:.2f} MB\")\n    print(\"-\" * 40)\n\ndef get_first_available_device():\n    \"\"\"\n    Returns the first available GPU device based on utilization or falls back to CPU.\n    \n    Returns:\n        torch.device: The device to use for computations.\n    \"\"\"\n    try:\n        # Fetch the list of GPUs and their UUIDs\n        gpu_list = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-gpu=index,uuid\", \"--format=csv,noheader,nounits\"],\n            universal_newlines=True\n        )\n        \n        gpu_id_to_uuid = {}\n        for line in gpu_list.splitlines():\n            if line:\n                gpu_id, gpu_uuid = line.split(\", \")\n                gpu_id_to_uuid[gpu_uuid] = int(gpu_id)\n        \n        # Fetch processes using GPUs\n        gpu_usage = subprocess.check_output(\n            [\"nvidia-smi\", \"--query-compute-apps=gpu_uuid\", \"--format=csv,noheader\"],\n            universal_newlines=True\n        )\n        \n        # Track GPUs in use\n        used_gpus = set(gpu_usage.splitlines())\n        \n        # Find the first GPU not in use\n        for gpu_uuid, gpu_id in gpu_id_to_uuid.items():\n            if gpu_uuid not in used_gpus:\n                print(f\"Using GPU: {gpu_id} - {torch.cuda.get_device_properties(gpu_id).name}\")\n                return torch.device(f\"cuda:{gpu_id}\")\n        \n        print(\"No free GPUs available. Using CPU.\")\n    except Exception as e:\n        print(f\"Error checking GPU usage: {e}. Falling back to CPU.\")\n    \n    return torch.device(\"cpu\")\n\ndef csv_to_geotiff(csv_file, output_tiff, value_column=\"SWE_predicted\", resolution_km=1):\n    \"\"\"\n    Convert CSV with latitude, longitude, and value data into a GeoTIFF raster with a 1 km resolution.\n\n    Parameters:\n    - csv_file (str): Path to the input CSV file.\n    - output_tiff (str): Path to save the output GeoTIFF.\n    - value_column (str): Column in the CSV file to rasterize.\n    - resolution_km (float): Desired grid resolution in kilometers (default: 1 km).\n\n    Returns:\n    - None\n    \"\"\"\n\n    # Load CSV\n    df = pd.read_csv(csv_file)\n\n    # Drop rows with invalid lat/lon\n    df = df[(df[\"Latitude\"] != -999) & (df[\"Longitude\"] != -999)]\n\n    # Extract coordinate and value columns\n    lat, lon, values = df[\"Latitude\"].values, df[\"Longitude\"].values, df[value_column].values\n\n    # Calculate degree resolution based on 1 km spacing\n    lat_res = resolution_km / 111.32  # Approximate for latitude\n    lon_res = resolution_km / (111.32 * np.cos(np.radians(lat.mean())))  # Adjust for longitude\n\n    # Define grid extent\n    min_lon, max_lon = lon.min(), lon.max()\n    min_lat, max_lat = lat.min(), lat.max()\n\n    # Create grid with 1 km resolution\n    grid_x, grid_y = np.meshgrid(\n        np.arange(min_lon, max_lon, lon_res),\n        np.arange(min_lat, max_lat, lat_res)\n    )\n\n    # Interpolate scattered points to a grid\n    grid_z = griddata((lon, lat), values, (grid_x, grid_y), method=\"linear\")\n\n    # ✅ Flip the grid along the latitude axis\n    grid_z = np.flipud(grid_z)\n\n    # Define transform (fix top-left origin)\n    transform = from_origin(min_lon, max_lat, lon_res, lat_res)\n\n\n    if os.path.exists(output_tiff):\n        os.remove(output_tiff)\n    \n    # Write to GeoTIFF\n    with rasterio.open(\n        output_tiff, \"w\",\n        driver=\"GTiff\",\n        height=grid_z.shape[0],\n        width=grid_z.shape[1],\n        count=1,\n        dtype=grid_z.dtype,\n        crs=\"EPSG:4326\",  # WGS84 projection\n        transform=transform\n    ) as dst:\n        dst.write(grid_z, 1)\n\n    print(f\"GeoTIFF saved to {output_tiff}\")\n\nclass BaseNNHole(BaseHole):\n    '''\n    Base class for snowcast_wormhole neural network predictors.\n\n    Attributes:\n        all_ready_file (str): The path to the CSV file containing the data for training.\n        classifier: The machine learning model used for prediction.\n        holename (str): The name of the wormhole class.\n        train_x (numpy.ndarray): The training input data.\n        train_y (numpy.ndarray): The training target data.\n        test_x (numpy.ndarray): The testing input data.\n        test_y (numpy.ndarray): The testing target data.\n        test_y_results (numpy.ndarray): The predicted results on the test data.\n        save_file (str): The path to save the trained model.\n    '''\n\n    \n    # Folder containing the data files\n    SNODAS_CSV_FOLDER = f\"{data_dir}/snodas/csv/\"\n\n    LAT_LON_CSV_PATH = \"/home/chetana/data/snodas/2025-01-19_snodas_output.csv\"\n\n    def __init__(\n        self, \n        start_date_str, end_date_str, \n        model_path=None, \n        base_model_path=None,\n        epochs=5, \n        batch_size=100000,\n        train_ratio = 0.8,\n        val_ratio = 0.2,\n        test_ratio = 0.2,\n        retrain: bool = False, \n        normalization: bool = False,\n    ):\n        '''\n        Initializes a new instance of the BaseHole class.\n        '''\n        self.holename = self.__class__.__name__ \n\n        self.train_ratio = 0.8\n\n        # Convert date strings to datetime objects\n        self.start_date = datetime.strptime(start_date_str, '%Y-%m-%d')\n        self.end_date = datetime.strptime(end_date_str, '%Y-%m-%d')\n        self.device = get_first_available_device()\n        self.epochs = epochs\n        self.batch_size = batch_size\n\n        self.train_ratio = train_ratio\n        self.val_ratio = val_ratio\n        self.test_ratio = test_ratio\n\n\n        self.train_x = None\n        self.train_y = None\n        self.test_x = None\n        self.test_y = None\n        self.test_y_results = None\n\n        self.train_files = []\n        self.retrain = retrain\n        self.normalization = normalization\n        self.base_model_path = base_model_path\n        \n        self.batch_size = batch_size\n        self.save_file = None\n        if model_path is None:\n            now = datetime.now()\n            date_time = now.strftime(\"%Y%d%m%H%M%S\")\n            model_path = f\"{model_dir}/{self.holename}_e{epochs}_n{self.normalization}_{date_time}.model\"\n        else:\n            print(\"Provided model and will continue to use: \", model_path)\n        self.model_path = model_path\n        self.latest_model_file = f\"{model_dir}/{self.holename}_e{epochs}_n{self.normalization}_latest.model\"\n\n\n        self.model = self.get_model()\n        if self.retrain:\n            print(\"Current base model: \", self.base_model_path)\n            self.load_model(model_path = self.base_model_path)\n\n    \n    def save(self):\n        '''\n        Save the trained model to a joblib file with a timestamp.\n\n        Returns:\n            None\n        '''\n        torch.save(self.model.state_dict(), self.model_path)\n        print(f\"Model saved to {self.model_path}\")\n\n        shutil.copy(self.model_path, self.latest_model_file)\n        print(f\"a copy of the model is saved to {self.latest_model_file}\")\n  \n    def preprocessing(self, verbose=False):\n        '''\n        Preprocesses the data for training and testing.\n\n        Returns:\n            None\n        '''\n        \n        # List all CSV files in the data folder\n        files = [\n            os.path.join(self.SNODAS_CSV_FOLDER, f) \n            for f in os.listdir(self.SNODAS_CSV_FOLDER) if f.endswith('.csv')\n        ]\n        # Shuffle the files to ensure randomness\n        # random.shuffle(files)\n        # Split the files: 80% for training and 20% for evaluation\n        self.train_files = files[:int(self.train_ratio * len(files))]\n        self.eval_files = files[-int(self.test_ratio * len(files)):]\n        if verbose:\n            print(\"Train files: \", self.train_files)\n            print(\"Test files: \", self.eval_files)\n  \n    def train(self):\n        '''\n        Trains the machine learning model.\n\n        Returns:\n            None\n        '''\n        # Train the model using batches of 5 days\n        self.train_model_in_batches(\n            self.train_files, \n            self.start_date, \n            self.end_date, \n            epochs=self.epochs,\n            batch_size=self.batch_size,\n            model_save_path = self.model_path,\n            val_ratio = self.val_ratio\n        )\n  \n    # Evaluate the model\n    def evaluate_model(\n        self, model_file_path, files, start_date, end_date, batch_size=10000\n    ):\n        self.model = self.load_model()\n\n        print(f\"Model loaded successfully. Running evaluation on {self.device}...\")\n\n        all_y_true = []\n        all_y_pred = []\n\n        # Start batch processing\n        self.model.eval()\n        batch_count = 0\n        for X_batch, y_batch in self.new_batch_generator(files, batch_size):\n            # X_batch = feature_scaler.fit_transform(X_batch)\n            X_batch = self.normalize(X_batch)\n            X_batch_tensor = torch.tensor(X_batch, dtype=torch.float32).to(self.device)\n            y_pred = self.model(X_batch_tensor).detach().cpu().numpy()  # Get model predictions\n            all_y_true.extend(y_batch)\n            all_y_pred.extend(y_pred)\n\n        # Calculate evaluation metrics\n        y_true = np.array(all_y_true)\n        y_pred = np.array(all_y_pred)\n        \n        mse = mean_squared_error(y_true, y_pred)\n        rmse = np.sqrt(mse)\n        r2 = r2_score(y_true, y_pred)\n\n        print(f\"Evaluation Results: RMSE = {rmse:.4f}, R2 = {r2:.4f}\")\n\n    def evaluate(self):\n        '''\n        Evaluates the performance of the machine learning model.\n\n        Returns:\n            None\n        '''\n        # Evaluate model on the last batch of data\n        self.evaluate_model(\n            self.model_path, \n            self.eval_files, \n            self.end_date - timedelta(days=4), \n            self.end_date,\n            batch_size=self.batch_size,\n        )\n  \n    def load_model(self, model_path = None):\n        self.model = self.get_model()\n        if model_path is None:\n            model_path = self.model_path\n        print(\"Loading model from:\", model_path)\n        self.model.load_model(f\"{model_path}\")\n        return self.model\n\n    # Load and preprocess data for a specific batch\n    def load_batch(self, files):\n        batch_data = []\n        for file in files:\n            if not os.path.exists(file):\n                continue\n            df = pd.read_csv(file)\n            df['date'] = pd.to_datetime(df['date'])\n            batch_data.append(df)\n        batch_df = pd.concat(batch_data)\n\n        batch_df['day_of_year'] = batch_df['date'].dt.dayofyear\n        batch_df['month'] = batch_df['date'].dt.month\n        batch_df['year'] = batch_df['date'].dt.year\n\n        X = batch_df[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n        \n        y = np.log10(batch_df['snodas'] + 1)  # +1 to avoid log(0)\n        return X, y\n\n    def new_batch_generator(self, files, batch_size):\n        num_files = len(files)\n        current_index = 0\n        X_accumulated, y_accumulated = [], []\n\n        while current_index < num_files:\n            current_file = files[current_index]\n            X_file, y_file = self.load_batch([current_file])\n            X_accumulated.extend(X_file)\n            y_accumulated.extend(y_file)\n\n            while len(X_accumulated) >= batch_size:\n                X_chunk = np.array(X_accumulated[:batch_size])\n                y_chunk = np.array(y_accumulated[:batch_size]).reshape(-1, 1)\n                X_accumulated, y_accumulated = X_accumulated[batch_size:], y_accumulated[batch_size:]\n                \n                yield X_chunk, y_chunk\n\n            current_index += 1\n\n        if X_accumulated:\n            yield np.array(X_accumulated), np.array(y_accumulated).reshape(-1, 1)\n\n    def get_model(self):\n        model = SNODASModel().to(self.device)\n        # Calculate the total number of trainable parameters\n        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        print(f\"Total trainable parameters: {total_params}\")\n        return model\n\n    # Define RMSE calculation function\n    def calculate_rmse(self, predictions, targets):\n        return torch.sqrt(((predictions - targets) ** 2).mean())\n\n    def normalize(self, X):\n        if self.normalization:\n            print(\"do norm\")\n            pass\n\n        return X\n\n    def train_model_in_batches(\n        self, \n        files, \n        start_date, end_date, \n        epochs=100, batch_size=50000, \n        model_save_path=\"ft_transformer_model.pth\", \n        val_ratio=0.2\n    ):\n        # optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n\n        optimizer = torch.optim.AdamW(\n            self.model.parameters(), lr=1e-3, weight_decay=1e-4\n        )\n        scheduler = torch.optim.lr_scheduler.StepLR(\n            optimizer, step_size=10, gamma=0.5\n        )\n\n        criterion = nn.MSELoss()\n\n        self.model.train()\n        for epoch in range(self.epochs):\n            start_time = time.time()  # Record start time for the epoch\n            epoch_loss = 0\n            epoch_rmse = 0  # To track RMSE for the whole epoch\n            batch_num = 0\n\n            # Training loop\n            for X_batch, y_batch in self.new_batch_generator(files, self.batch_size):\n                batch_num += 1\n                # X and y must be shuffled together to ensure the row correspondance\n                # X_batch, y_batch = shuffle(X_batch, y_batch, random_state=42)\n\n                # shuffle the X_batch indices\n                indices = np.random.permutation(X_batch.shape[0])  # Generate shuffled indices\n                X_batch = X_batch[indices]  # Shuffle X\n                y_batch = y_batch[indices]  # Shuffle y\n\n                X_batch = self.normalize(X_batch)\n                \n                # Split batch into training and validation sets based on val_ratio\n                split_idx = int((1 - val_ratio) * X_batch.shape[0])  # 80% for training\n                X_train_batch, X_val_batch = X_batch[:split_idx], X_batch[split_idx:]\n                y_train_batch, y_val_batch = y_batch[:split_idx], y_batch[split_idx:]\n\n                # Convert to torch tensors and move to device\n                X_train_batch = torch.tensor(X_train_batch, dtype=torch.float32).to(self.device)\n                y_train_batch = torch.tensor(y_train_batch, dtype=torch.float32).to(self.device)\n                X_val_batch = torch.tensor(X_val_batch, dtype=torch.float32).to(self.device)\n                y_val_batch = torch.tensor(y_val_batch, dtype=torch.float32).to(self.device)\n\n                # Training step\n                optimizer.zero_grad()\n                train_predictions = self.model(X_train_batch)\n                train_loss = criterion(train_predictions, y_train_batch)\n                train_loss.backward()\n                optimizer.step()\n                scheduler.step()\n\n                # Calculate training loss\n                epoch_loss += train_loss.item()\n\n                # Validation step\n                val_predictions = self.model(X_val_batch)\n                val_rmse = self.calculate_rmse(val_predictions, y_val_batch).item()\n                epoch_rmse += val_rmse  # Accumulate RMSE for this batch\n\n            # Time cost for the epoch\n            epoch_time = time.time() - start_time\n\n            # Print epoch statistics\n            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss/batch_num:.4f}, Validation RMSE: {epoch_rmse/batch_num:.4f}, Time: {epoch_time:.2f} seconds\")\n\n        return self.model\n\n    def test(self):\n        '''\n        Tests the machine learning model on the testing data.\n\n        Returns:\n            numpy.ndarray: The predicted results on the testing data.\n        '''\n        pass\n\n    def save_png(self, final_mapper_df, target_date, model_file_name):\n        # Plotting the SWE results\n        plt.figure(figsize=(10, 8))\n        scatter = plt.scatter(\n            final_mapper_df['Longitude'], \n            final_mapper_df['Latitude'], \n            c=final_mapper_df['SWE_predicted'], \n            cmap='viridis', \n            s=10\n        )\n        plt.colorbar(scatter, label=\"SWE (mm)\")\n        plt.title(f\"SWE {target_date} {model_file_name}\")\n        plt.xlabel(\"Longitude\")\n        plt.ylabel(\"Latitude\")\n        plt.grid(True)\n        plt.tight_layout()\n\n        # Save plot\n        plot_path = f\"{plot_dir}/swe_map_{target_date}_{model_file_name}.png\"\n        plt.savefig(plot_path)\n        # plt.show()\n\n        print(f\"SWE map saved to {plot_path}\")\n\n    def predict_swe_map(\n        self, target_date: str, model_path: str = None, \n        batch_size: int = -1\n    ):\n        if model_path is None:\n            model_path = self.model_path\n        self.model = self.load_model(model_path)\n\n        if batch_size == -1:\n            batch_size = self.batch_size\n\n        model_file_name = os.path.basename(self.model_path)\n\n        print(f\"Model loaded successfully. Running evaluation on {self.device}...\")\n        print(\"reading: \", self.LAT_LON_CSV_PATH, \" - batch_size: \", batch_size)\n        \n        mapper_df = pd.read_csv(self.LAT_LON_CSV_PATH, chunksize=batch_size)  # Read in chunks\n\n        # Step 2: Parse the date string to extract year, month, and day_of_year\n        date_obj = datetime.strptime(target_date, \"%Y-%m-%d\")\n        year, month, day_of_year = date_obj.year, date_obj.month, date_obj.timetuple().tm_yday\n\n        print(year, month, day_of_year)\n\n        result_df_list = []  # List to store processed batches\n\n        # Process in batches using tqdm for progress tracking\n        for batch in tqdm(mapper_df, desc=\"Processing batches\", unit=\"batch\"):\n            if 'Latitude' not in batch.columns or 'Longitude' not in batch.columns:\n                raise ValueError(\"The CSV file must contain 'Latitude' and 'Longitude' columns.\")\n\n            # Step 3: Add time-related columns to the batch\n            batch['year'], batch['month'], batch['day_of_year'] = year, month, day_of_year\n\n            # Step 4: Extract the features\n            X = batch[['Latitude', 'Longitude', 'day_of_year', 'month', 'year']].values\n            X_tensor = torch.tensor(X, dtype=torch.float32).to(self.device)\n\n            # Run inference\n            with torch.no_grad():\n                y = self.model(X_tensor).cpu().numpy()\n\n            # Flatten if necessary\n            if y.ndim > 1:\n                y = y.flatten()\n\n            # Apply transformation\n            batch['SWE_predicted'] = (10 ** y) - 1  # Reverse log10 scaling\n            if batch.empty or len(batch) == 0:\n                raise ValueError(\"Encountered an empty batch. Please check the data source.\")\n            \n            # Correctly print the first and last Latitude/Longitude\n            print(f\"Batch first row: {batch.iloc[0]['Latitude']}, {batch.iloc[0]['Longitude']}\")\n            print(f\"Batch last row: {batch.iloc[-1]['Latitude']}, {batch.iloc[-1]['Longitude']}\")\n            \n            result_df_list.append(batch)\n\n        # Concatenate all processed batches\n        final_mapper_df = pd.concat(result_df_list, ignore_index=True)\n        if final_mapper_df.isna().any().any():\n            raise ValueError(\"DataFrame contains NaN values. Please check the input data.\")\n\n        self.save_png(final_mapper_df, target_date, model_file_name)\n\n        return final_mapper_df\n\n\n    def predict(\n        self, \n        date, \n        csv_path: str = None, \n        save_csv: bool = False, \n        tiff: bool = False,\n        skip_exists: bool = False,\n    ):\n        '''\n        Makes predictions using the trained model on new input data.\n\n        Args:\n            input_x (numpy.ndarray): The input data for prediction.\n\n        Returns:\n            numpy.ndarray: The predicted results.\n        '''\n        tif_path = f\"{output_dir}/{self.holename}_e{self.epochs}_n{self.normalization}_nn_swe_predicted_{date}.tif\"\n        if csv_path is None:\n            csv_path = f\"{output_dir}/{self.holename}_e{self.epochs}_n{self.normalization}_nn_swe_predicted_{date}.csv\"\n        \n        if skip_exists:\n            if os.path.exists(tif_path):\n                print(f\"Skip as {tif_path} exists\")\n                return\n            if os.path.exists(csv_path):\n                print(f\"Skip as {csv_path} exists\")\n                return\n\n        final_predicted_df = self.predict_swe_map(\n            date, \n            model_path = self.model_path,\n        )\n        if save_csv:\n            final_predicted_df.to_csv(csv_path, index=False)\n            print(\"saved to csv: \", csv_path)\n\n            if tiff:\n                # Save geotiff\n                csv_to_geotiff(\n                    csv_file = csv_path, \n                    output_tiff=tif_path,\n                )\n        return final_predicted_df\n\n    def post_processing(self):\n        '''\n        Perform post-processing on the model's predictions.\n\n        Returns:\n            None\n        '''\n        pass\n\n\nif __name__ == \"__main__\":\n\n    # Define start and end date for training\n    start_date_str = '2024-10-01'\n    end_date_str = '2025-01-15'\n    \n    hole_model = BaseNNHole(\n        start_date_str = test_start_date,\n        end_date_str = test_end_date,\n        batch_size = 310000,\n        epochs = 1,\n        train_ratio = 0.01,\n        test_ratio = 0.2,\n        val_ratio = 0.2,\n        retrain = False,\n        normalization = False,\n    )\n    \n    hole_model.preprocessing(verbose=True)\n    hole_model.train()\n    hole_model.save()\n    hole_model.evaluate()\n    hole_model.predict(\"2025-01-29\", save_csv=True, tiff=True)\n\n\n\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "3kurwn",
  "name" : "clip_basins_for_eval",
  "description" : "python",
  "code" : "# Write your first Python code in Geoweaver",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "76xv33",
  "name" : "snodas_resnet",
  "description" : null,
  "code" : "from base_nn_hole import BaseNNHole\nimport torch\nimport torch.nn as nn\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\nimport random\nimport psutil\nfrom snowcast_utils import model_dir, data_dir, plot_dir, test_start_date, test_end_date\nfrom datetime import datetime, timedelta\nimport time\nimport subprocess\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom base_hole import BaseHole\nimport shutil\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(ResidualBlock, self).__init__()\n        self.fc1 = nn.Linear(in_features, out_features)\n        self.bn1 = nn.BatchNorm1d(out_features)\n        self.fc2 = nn.Linear(out_features, out_features)\n        self.bn2 = nn.BatchNorm1d(out_features)\n        self.relu = nn.ReLU()\n\n        # Shortcut connection\n        self.shortcut = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n\n    def forward(self, x):\n        identity = self.shortcut(x)\n        out = self.relu(self.bn1(self.fc1(x)))\n        out = self.bn2(self.fc2(out)) + identity\n        return self.relu(out)\n\nclass SNODAS_NewResNet_Model(nn.Module):\n    def __init__(self, input_dim=8):\n        super(SNODAS_NewResNet_Model, self).__init__()\n        self.input_layer = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.BatchNorm1d(128)\n        )\n\n        self.hidden_layers = nn.Sequential(\n            ResidualBlock(128, 256),\n            ResidualBlock(256, 256),\n            ResidualBlock(256, 256),\n            ResidualBlock(256, 128)\n        )\n\n        self.output_layer = nn.Sequential(\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, X):\n        X = self.input_layer(X)\n        X = self.hidden_layers(X)\n        return self.output_layer(X)\n    \n    def load_model(self, model_path):\n        \"\"\"Loads the model weights from a file.\"\"\"\n        try:\n            # Load the state dict from the file\n            state_dict = torch.load(model_path)\n            # Load the state dict into the model\n            self.load_state_dict(state_dict)\n            print(f\"Model loaded from {model_path}\")\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n\nclass SNODASNewResNetHole(BaseNNHole):\n\n    def normalize(self, X):\n        return X\n\n    def preprocessing(self, verbose=False, semimonth_only = False):\n        \"\"\"\n        Preprocesses the data for training and testing.\n\n        Returns:\n            None\n        \"\"\"\n        \n        # List all CSV files that end with -01.csv or -15.csv\n        if semimonth_only:\n            files = [\n                os.path.join(self.SNODAS_CSV_FOLDER, f)\n                for f in os.listdir(self.SNODAS_CSV_FOLDER)\n                if f.endswith('.csv') and ('-01_snodas' in f or '-15_snodas' in f)\n            ]\n        else:\n            files = [\n                os.path.join(self.SNODAS_CSV_FOLDER, f)\n                for f in os.listdir(self.SNODAS_CSV_FOLDER)\n                if f.endswith('.csv')\n            ]\n\n        # Shuffle the files to ensure randomness\n        random.shuffle(files)\n\n        # Split the files: 80% for training and 20% for evaluation\n        train_size = int(self.train_ratio * len(files))\n        test_size = int(self.test_ratio * len(files))\n        self.train_files = files[:train_size]\n        self.eval_files = files[-test_size:]\n\n        if verbose:\n            print(\"Train files:\", self.train_files)\n            print(\"Test files:\", self.eval_files)\n    \n    def encode_features(self, df):\n        \"\"\"\n        Converts spatial and temporal features into an 8D representation:\n        - Uses sin transformations for Latitude and Longitude (to maintain continuity)\n        - Uses sin/cos transformations for periodic variables (year, month, day_of_year)\n        \"\"\"\n        # Encode Latitude and Longitude using sin transformation for generalization\n        df['lat_enc'] = np.sin(np.pi * df['Latitude'] / 180)\n        df['lon_enc'] = np.sin(np.pi * df['Longitude'] / 180)\n\n        # Encode Year (assuming range ~[2000, 2100])\n        df['year_sin'] = np.sin(2 * np.pi * (df['year'] - 2000) / 100)\n        df['year_cos'] = np.cos(2 * np.pi * (df['year'] - 2000) / 100)\n\n        # Encode Month (12-month cycle)\n        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n\n        # Encode Day of Year (365-day cycle)\n        df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n        df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n\n        # Select final 8D input features\n        return df[['lat_enc', 'lon_enc', 'year_sin', 'year_cos', \n                'month_sin', 'month_cos', 'day_sin', 'day_cos']].values\n\n    def load_batch(self, files):\n        batch_data = []\n        \n        for file in files:\n            if not os.path.exists(file):\n                continue\n            df = pd.read_csv(file)\n            df['date'] = pd.to_datetime(df['date'])\n            batch_data.append(df)\n        \n        batch_df = pd.concat(batch_data, ignore_index=True)\n\n        # Extract date components\n        batch_df['day_of_year'] = batch_df['date'].dt.dayofyear\n        batch_df['month'] = batch_df['date'].dt.month\n        batch_df['year'] = batch_df['date'].dt.year\n\n        # Transform into 8D input features\n        X = self.encode_features(batch_df)\n\n        # Log-transform the target variable\n        y = np.log10(batch_df['snodas'] + 1)  # +1 to avoid log(0)\n        \n        return X, y\n\n    def get_model(self):\n        model = SNODAS_NewResNet_Model().to(self.device)\n        # Calculate the total number of trainable parameters\n        total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n        print(f\"Total trainable parameters: {total_params}\")\n        return model\n\n\n\nif __name__ == \"__main__\":\n\n    # Define start and end date for training\n    start_date_str = '2024-10-01'\n    end_date_str = '2025-01-15'\n    \n    hole_model = SNODASNewResNetHole(\n        start_date_str = test_start_date,\n        end_date_str = test_end_date,\n        batch_size = 310000,\n        epochs = 1,\n        train_ratio = 0.8,\n        test_ratio = 0.2,\n        val_ratio = 0.01,\n        normalization = True,\n        retrain = False,\n        # model_path = \"/home/chetana/models//SNODASNewResNetHole_e10_nTrue_20253101221511.model\",\n        # base_model_path = \"/home/chetana/models//SNODASNewResNetHole_e10_nTrue_20253101221511.model\"\n    )\n    \n    hole_model.preprocessing(verbose=True, semimonth_only = False)\n    hole_model.train()\n    hole_model.save()\n    hole_model.evaluate()\n    hole_model.predict(\"2024-12-10\")\n    hole_model.predict(\"2024-11-10\")\n    hole_model.predict(\"2024-10-10\")\n    hole_model.predict(\"2024-09-10\")\n    hole_model.predict(\"2024-08-10\")\n    hole_model.predict(\"2024-07-10\")\n    hole_model.predict(\"2024-06-10\")\n    hole_model.predict(\"2024-05-10\")\n    hole_model.predict(\"2024-04-10\")\n    hole_model.predict(\"2024-03-10\")\n    hole_model.predict(\"2024-02-10\")\n    hole_model.predict(\"2024-01-10\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
