[{
  "history_id" : "jm434otjjy0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933437,
  "history_end_time" : 1701230952327,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d5jztn89ght",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933438,
  "history_end_time" : 1701230952327,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kmpq7jc41a7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933439,
  "history_end_time" : 1701230952328,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vjpwlb1nb4d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933440,
  "history_end_time" : 1701230952328,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7x00gi6iesr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933441,
  "history_end_time" : 1701230952328,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ufcrj5le8nn",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1701230952329,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e3253ye3u87",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933443,
  "history_end_time" : 1701230952337,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "33gtavj74t9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933444,
  "history_end_time" : 1701230952337,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bb94mjs1h0d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933445,
  "history_end_time" : 1701230952338,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1jgr1e4nuea",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933446,
  "history_end_time" : 1701230952338,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a23pd4oxybc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933446,
  "history_end_time" : 1701230952338,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qdy02azxwet",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933447,
  "history_end_time" : 1701230952338,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6ggrcd966m0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933448,
  "history_end_time" : 1701230952338,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qfhdlkr81mv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933448,
  "history_end_time" : 1701230952339,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bvlifnqvo9i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933449,
  "history_end_time" : 1701230952339,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pf0rbornws1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933450,
  "history_end_time" : 1701230952339,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l0bpuuzd7sm",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1701230952339,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5qj10ztzv1p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933451,
  "history_end_time" : 1701230952341,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l54kcwzp03r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933452,
  "history_end_time" : 1701230952341,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gam1r4swbli",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933452,
  "history_end_time" : 1701230952341,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4bf3zgpau08",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933453,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s32bva1w5le",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933454,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "65sds4f3m2l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933455,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "br0dqbwcfk2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933455,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "szqurtaf4id",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933456,
  "history_end_time" : 1701230952342,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3h22tg24mos",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933457,
  "history_end_time" : 1701230952343,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4hel3zid26y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933458,
  "history_end_time" : 1701230952343,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "29qr8uq5p2i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933458,
  "history_end_time" : 1701230952343,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "backl5pjf4x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933459,
  "history_end_time" : 1701230952343,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a28xpk9hoz0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933460,
  "history_end_time" : 1701230952343,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3yyiegod1f7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933461,
  "history_end_time" : 1701230952343,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sdju9puizvm",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1701230952344,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7dccf0kawls",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933462,
  "history_end_time" : 1701230952345,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5ww5hykca14",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933463,
  "history_end_time" : 1701230952346,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hhz89v8khjd",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1701230952346,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w4aoozcqpag",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1701230952348,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8n6gibo6axs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933465,
  "history_end_time" : 1701230952349,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x4b6rggmuov",
  "history_input" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    \n    prepare_amsr_grid_mapper()\n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\n    \ndef get_cumulative_amsr_data(target_date = test_start_date):\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n      past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Traverse and print every day from past October 1 to the specific date\n    current_date = past_october_1\n\n    date_keyed_objects = {}\n    data_dict = {}\n    new_df = None\n    while current_date <= selected_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      \n      data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n      current_df = pd.read_csv(data_dict[current_date_str])\n      current_df = current_df.rename(columns={\n        \"AMSR_SWE\": f'AMSR_SWE_{current_date_str}',\n        \"AMSR_Flag\": f'AMSR_Flag_{current_date_str}',\n        \"date\": f\"data_{current_date_str}\",\n      })\n      \n      if new_df is None:\n        new_df = current_df\n      else:\n        new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n      \n      current_date += timedelta(days=1)\n    \n    print(new_df.describe())\n    \n    # add all the columns together and save to new csv\n    # Adding all columns except latitude and longitude\n    new_df = new_df.apply(pd.to_numeric, errors='coerce')\n    \n    #new_df = new_df.head(2000)\n    \n    swe_cols = [col for col in new_df.columns if col.startswith('AMSR_SWE')]\n    print(\"swe_cols are: \", swe_cols)\n    def interpolate_row(row):\n        values = row.values\n        if np.all(np.isnan(values)):\n          values[:] = 0  # Set all elements to zero\n        else:\n          mask = (values > 240) | np.isnan(values)\n          x = np.arange(len(values))\n          values = np.interp(x, x[~mask], values[~mask])\n\n        if np.any(values > 240) or np.any(np.isnan(values)):\n          raise ValueError(\"Single group: shouldn't have values > 240 here\")\n\n        # Replace missing values with interpolated values\n        row[:] = values\n        return row\n    \n    for swe_col in swe_cols:\n        new_df.loc[(new_df[swe_col] > 240), swe_col] = np.nan\n        new_df.loc[new_df[swe_col].isnull(), swe_col] = np.nan\n        \n    print(\"start to interpolate the values..\")\n    new_df[swe_cols].to_csv(f\"{data_dict[target_date]}_before_interpolated.csv\", index=False)\n    new_df[swe_cols] = new_df[swe_cols].apply(interpolate_row, axis=1)\n    print(\"finished interpolating the values..\")\n    #print(new_df[swe_cols].head())\n    new_df[swe_cols].to_csv(f\"{data_dict[target_date]}_interpolated.csv\", index=False)\n    \n    new_df['cumulative_AMSR_SWE'] = new_df[swe_cols].sum(axis=1)\n    \n    flag_cols = [col for col in new_df.columns if col.startswith('AMSR_Flag')]\n    print(\"flag_cols are: \", flag_cols)\n    new_df['cumulative_AMSR_Flag'] = new_df[flag_cols].sum(axis=1)\n    \n    new_df.to_csv(f\"{data_dict[target_date]}_cumulative_all_columns.csv\", index=False)\n    \n    new_df = new_df.loc[:, ['gridmet_lat', 'gridmet_lon', f'AMSR_SWE_{target_date}', 'cumulative_AMSR_SWE', f'AMSR_Flag_{target_date}', 'cumulative_AMSR_Flag']]\n    new_df[\"date\"] = target_date\n    \n    new_df = new_df.rename(columns={\n      f'AMSR_SWE_{target_date}': \"AMSR_SWE\",\n      f'AMSR_Flag_{target_date}': \"AMSR_Flag\",\n    })\n    \n    print(\"new_df final shape: \", new_df.head())\n    new_df.to_csv(f\"{data_dict[target_date]}_cumulative.csv\", index=False)\n    print(f\"new df is saved to {data_dict[target_date]}_cumulative.csv\")\n    \n    print(new_df[f'AMSR_SWE'].describe(include='all'))\n    \n    \n      \n    \nif __name__ == \"__main__\":\n    # Run the download and conversion function\n    #prepare_amsr_grid_mapper()\n    \n#     download_amsr_and_convert_grid()\n    get_cumulative_amsr_data()\n",
  "history_output" : "today date = 2023-11-29\ntest start date:  2022-11-17\ntest end date:  2023-10-07\n/home/chetana\n2022275\n2022-11-17 00:00:00\n2022-10-01\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.01.csv already exists, skipping..\n2022-10-02\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.02.csv already exists, skipping..\n2022-10-03\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.03.csv already exists, skipping..\n2022-10-04\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.04.csv already exists, skipping..\n2022-10-05\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.05.csv already exists, skipping..\n2022-10-06\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.06.csv already exists, skipping..\n2022-10-07\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.07.csv already exists, skipping..\n2022-10-08\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.08.csv already exists, skipping..\n2022-10-09\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.09.csv already exists, skipping..\n2022-10-10\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.10.csv already exists, skipping..\n2022-10-11\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.11.csv already exists, skipping..\n2022-10-12\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.12.csv already exists, skipping..\n2022-10-13\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.13.csv already exists, skipping..\n2022-10-14\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.14.csv already exists, skipping..\n2022-10-15\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.15.csv already exists, skipping..\n2022-10-16\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.16.csv already exists, skipping..\n2022-10-17\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.17.csv already exists, skipping..\n2022-10-18\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2022.10.18.csv already exists, skipping..\n",
  "history_begin_time" : 1701230936297,
  "history_end_time" : 1701230952383,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mo6vaj3ug3a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933467,
  "history_end_time" : 1701230952352,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6o1zk3af34o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933468,
  "history_end_time" : 1701230952352,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nbc6xpio8aq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933468,
  "history_end_time" : 1701230952352,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cl0v0vv255k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933469,
  "history_end_time" : 1701230952352,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "etaqg9yxpoj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933470,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x341dc0nqf0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933471,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "d4zcq6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xasrg74udyc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933472,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "6x6myw",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u8mgrsm3pu7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933473,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "697skzy9pya",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933473,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "27xmtv8d2cy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933474,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "ee5ur4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "raqx44rug9p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933475,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "f03i7p",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1ychsgb92s4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933476,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8ew9e887gso",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933477,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "j8swco",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "n598zqv31lk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933478,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "pnr64x",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "i779asabat8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933479,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "qg80lj",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9xmdfcqvzvi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933480,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "ggy7gf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "meiljzdc9nq",
  "history_input" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\n\n\n# change directory before running the code\nos.chdir(f\"{homedir}/fsca/\")\n\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \"h12v04\", \"h12v05\",\n             \"h13v04\", \"h13v05\", \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n  hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n  # Specific subdataset name you're interested in\n  target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n  # Create a name for the output file based on the HDF file name and subdataset\n  output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n  output_path = os.path.join(output_folder, output_file_name)\n\n  if os.path.exists(output_path):\n    pass\n    #print(f\"The file {output_path} exists. skip.\")\n  else:\n    for subdataset in hdf_ds.GetSubDatasets():\n      # Check if the subdataset is the one we want to convert\n      if target_subdataset_name in subdataset[0]:\n        ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n        # Convert to GeoTIFF\n        gdal.Translate(output_path, ds)\n        ds = None\n        break  # Exit the loop after converting the target subdataset\n\n  hdf_ds = None\n\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n  file_lst = list()\n  for file in os.listdir(folder_path):\n    file_lst.append(file)\n    if file.lower().endswith(\".hdf\"):\n      hdf_file = os.path.join(folder_path, file)\n      convert_hdf_to_geotiff(hdf_file, output_folder)\n      #print(f\"Converted {file} to GeoTIFF\")\n  return file_lst\n\n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['gdal_translate', '-b', '1', '-outsize', '100%', '100%', '-scale', '0', '255', '200', '200', f\"{modis_day_wise}/fsca_template.tif\", output_file]\n    print(\"Running \", gdal_command)\n    subprocess.run(gdal_command)\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", gdal_command)\n    subprocess.run(gdal_command)\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", gdal_command)\n    subprocess.run(gdal_command)\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command)\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    #date = file_name.split('__')[0]\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Openning {file_path}\")\n\n    with rasterio.open(file_path) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n    \n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving csv file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    return output_file\n\ndef merge_cumulative_csv(start_date, end_date):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    print(current_date.strftime('%Y-%m-%d'))\n    current_date_str = current_date.strftime('%Y-%m-%d')\n\n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df = current_df.rename(columns={\n      \"fsca\": f'fsca_{current_date_str}',\n    })\n    print(\"current_df describe: \", current_df.describe())\n\n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.describe())\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  new_df = new_df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in new_df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  def interpolate_row(row):\n    values = row.values\n    if np.all(np.isnan(values)):\n      values[:] = 0  # Set all elements to zero\n    else:\n      mask = (values == 200) | np.isnan(values)\n      x = np.arange(len(values))\n      values = np.interp(x, x[~mask], values[~mask])\n\n      if np.any(values == 200) or np.any(np.isnan(values)):\n        raise ValueError(\"Single group: shouldn't have values > 240 here\")\n\n    # Replace missing values with interpolated values\n    row[:] = values\n    return row\n\n  for fsca_col in fsca_cols:\n    new_df.replace(255, np.nan, inplace=True)\n    new_df.loc[new_df[fsca_col].isnull(), fsca_col] = np.nan\n\n  #print(\"start to interpolate the values..\")\n  #new_df[fsca_cols].to_csv(f\"{data_dict[target_date.strftime('%Y-%m-%d')]}_before_interpolated.csv\", index=False)\n  #new_df[fsca_cols] = new_df[fsca_cols].apply(interpolate_row, axis=1)\n  #print(\"finished interpolating the values..\")\n  #new_df[fsca_cols].to_csv(f\"{data_dict[end_date.strftime('%Y-%m-%d')]}_interpolated.csv\", index=False)\n  \n  new_df['cumulative_fsca'] = new_df[fsca_cols].sum(axis=1)\n  new_df.to_csv(f\"{data_dict[end_date.strftime('%Y-%m-%d')]}_cumulative_all_columns.csv\", index=False)\n\n  new_df = new_df.loc[:, ['Latitude', 'Longitude', f\"fsca_{target_date.strftime('%Y-%m-%d')}\", 'cumulative_fsca']]\n  new_df[\"date\"] = end_date\n\n  new_df = new_df.rename(columns={\n    f\"fsca_{end_date.strftime('%Y-%m-%d')}\": \"fsca\",\n  })\n\n  print(\"new_df final shape: \", new_df.head())\n  new_df.to_csv(f\"{data_dict[end_date.strftime('%Y-%m-%d')]}_cumulative.csv\", index=False)\n  print(f\"new df is saved to {data_dict[end_date.strftime('%Y-%m-%d')]}_cumulative.csv\")\n\n  print(new_df[f'fsca'].describe(include='all'))\n    \n    \ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  merge_cumulative_csv(start_date, end_date)\n\n\nextract_data_for_testing()\n\n# SnowCover is missing from 10-12 to 10-23\n#download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/meiljzdc9nq/fsCA_testing.py\", line 13\n    from snowcast_utils import work_dir, homedir test_start_date, date_to_julian\n                                                 ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1701230935858,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "c2qa9u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "aqs8tex46lt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933481,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "lnrsop",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ufbv9973dfd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933482,
  "history_end_time" : 1701230952355,
  "history_notes" : null,
  "history_process" : "c8isgf",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]
