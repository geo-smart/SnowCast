[{
  "history_id" : "voc9ycv71uc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999697,
  "history_end_time" : 1720505076153,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ml62nvzrh4o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999740,
  "history_end_time" : 1720505076155,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sm1ih0hg4ab",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999743,
  "history_end_time" : 1720505076157,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wbp6d8y4p7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999746,
  "history_end_time" : 1720505076157,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "od2coh5pnon",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999748,
  "history_end_time" : 1720505076158,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7u9em4tv8hn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999768,
  "history_end_time" : 1720505076159,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ndy8nlc44wv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999782,
  "history_end_time" : 1720505076159,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "02g2mflfk3y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999786,
  "history_end_time" : 1720505076160,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tag1f5x6dnc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999790,
  "history_end_time" : 1720505076160,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ggn0vfzm5qx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999794,
  "history_end_time" : 1720505076161,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ii6d03w3uu8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999797,
  "history_end_time" : 1720505076162,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yl6g9u3838k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999801,
  "history_end_time" : 1720505076162,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2amlg466x18",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999804,
  "history_end_time" : 1720505076163,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8vcaol96xzk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999808,
  "history_end_time" : 1720505076163,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5sky76cyhte",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999812,
  "history_end_time" : 1720505076164,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rjjhnp5icy2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999834,
  "history_end_time" : 1720505076165,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wuflmb8qtt0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999845,
  "history_end_time" : 1720505076165,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r9vk65x6n2m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999854,
  "history_end_time" : 1720505076166,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hh9j0l7moir",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999866,
  "history_end_time" : 1720505076167,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4iwy2isha0n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999877,
  "history_end_time" : 1720505076167,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "o9f9is29mt1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999889,
  "history_end_time" : 1720505076168,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "y62n1west85",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999940,
  "history_end_time" : 1720505076168,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kcheexrbgsd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999950,
  "history_end_time" : 1720505076169,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s0pug45ksvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999959,
  "history_end_time" : 1720505076169,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h4ogng2j4u5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999968,
  "history_end_time" : 1720505076170,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w23khn3h0g2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999978,
  "history_end_time" : 1720505076170,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oz81dcwxpag",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999987,
  "history_end_time" : 1720505076171,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4d9l78c27z8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720503999996,
  "history_end_time" : 1720505076171,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r1ugd5g88m1",
  "history_input" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, work_dir\nimport matplotlib.pyplot as plt\n\n# Define the folder to store downloaded files\ngridmet_folder_name = f'{work_dir}/gridmet_climatology'\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\n# Define your value ranges for color mapping\n#value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n#value_ranges = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.5, 3]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n  if value_ranges == None:\n    max_value = df_col.max()\n    min_value = df_col.min()\n    if min_value < 0:\n      min_value = 0\n    step_size = (max_value - min_value) / 12\n\n    # Create 10 periods\n    new_value_ranges = [min_value + i * step_size for i in range(12)]\n  # Define your custom function to map data values to colors\n  def map_value_to_color(value):\n    # Iterate through the value ranges to find the appropriate color index\n    for i, range_max in enumerate(new_value_ranges):\n      if value <= range_max:\n        return colors[i]\n\n      # If the value is greater than the largest range, return the last color\n      return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n  color_mapping = [map_value_to_color(value) for value in df_col.values]\n  return color_mapping, new_value_ranges\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path, current_year):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef create_gridmet_to_dem_mapper(nc_file):\n    western_us_dem_df = pd.read_csv(western_us_coords)\n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n    \n      # Use the apply function to apply the custom function to each row\n      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n      \n    # print(western_us_dem_df.head())\n    \n    # Save the new converted AMSR to CSV file\n    western_us_dem_df.to_csv(target_csv_path, index=False)\n    \n    return western_us_dem_df\n  \n  \ndef get_nc_csv_by_coords_and_variable(nc_file,\n                                      var_name,\n                                      target_date=test_start_date):\n    \n    create_gridmet_to_dem_mapper(nc_file)\n  \t\n    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      var_col = nc_file.variables[long_var_name][:]\n      #print(\"val_col.shape: \", var_col.shape)\n      \n      # Calculate the day of the year\n      day_of_year = selected_date.timetuple().tm_yday\n      day_index = day_of_year - 1\n      #print('day_index:', day_index)\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        lat_index = int(row[\"gridmet_lat_idx\"])\n        lon_index = int(row[\"gridmet_lon_idx\"])\n        var_value = var_col[day_index, lat_index, lon_index]\n        \n        return var_value\n    \n      # Use the apply function to apply the custom function to each row\n      # print(mapper_df.columns)\n      # print(mapper_df.head())\n      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n      \n      # print(\"mapper_df[var_name]: \", mapper_df[var_name].describe())\n      \n      # drop useless columns\n      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n                               \"dem_lon\": \"Longitude\"}, inplace=True)\n\n      \n    # print(mapper_df.head())\n    return mapper_df\n\n\ndef turn_gridmet_nc_to_csv(target_date=test_start_date):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    generated_csvs = []\n    for root, dirs, files in os.walk(gridmet_folder_name):\n        for file_name in files:\n            \n            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n                print(f\"Checking file: {file_name}\")\n                var_name = get_var_from_file_name(file_name)\n                # print(\"Variable name:\", var_name)\n                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n\n                if os.path.exists(res_csv):\n                    #os.remove(res_csv)\n                    print(f\"{res_csv} already exists. Skipping..\")\n                    generated_csvs.append(res_csv)\n                    continue\n\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n                                                       var_name, target_date)\n                df.replace('--', pd.NA, inplace=True)\n                df.to_csv(res_csv, index=False)\n                print(\"gridmet var saved: \", res_csv)\n                generated_csvs.append(res_csv)\n    return generated_csvs   \n\ndef plot_gridmet(target_date=test_start_date):\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  var_name = \"pr\"\n  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n  gridmet_var_df = pd.read_csv(test_csv)\n  gridmet_var_df.replace('--', pd.NA, inplace=True)\n  gridmet_var_df.dropna(inplace=True)\n  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n  #print(gridmet_var_df.head())\n  #print(gridmet_var_df[\"Latitude\"].describe())\n  #print(gridmet_var_df[\"Longitude\"].describe())\n  #print(gridmet_var_df[\"pr\"].describe())\n  \n  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n  \n  # Create a scatter plot\n  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n              gridmet_var_df[\"Latitude\"].values, \n              label='Pressure', \n              color=colormaplist, \n              marker='o')\n\n  # Add labels and a legend\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Scatter Plot Example')\n  plt.legend()\n  \n  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n  plt.savefig(res_png_path)\n  print(f\"test image is saved at {res_png_path}\")\n                \ndef prepare_folder_and_get_year_list(target_date=test_start_date):\n  # Check if the folder exists, if not, create it\n  if not os.path.exists(gridmet_folder_name):\n      os.makedirs(gridmet_folder_name)\n\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n  year_list = [selected_date.year, past_october_1.year]\n\n  # Remove any existing files in the folder\n  if selected_date.year == datetime.now().year:\n    # check if the current year's netcdf contains the selected date\n    # get etr netcdf and read\n    nc_file = f\"{gridmet_folder_name}/tmmx_{selected_date.year}.nc\"\n    ifremove = False\n    if os.path.exists(nc_file):\n      with nc.Dataset(nc_file) as ncd:\n        day = ncd.variables['day'][:]\n        # Calculate the day of the year\n        day_of_year = selected_date.timetuple().tm_yday\n        day_index = day_of_year - 1\n        if len(day) <= day_index:\n          ifremove = True\n    \n    if ifremove:\n      print(\"The current year netcdf has new data. Redownloading..\")\n      remove_files_in_folder(gridmet_folder_name, selected_date.year)  # only redownload when the year is the current year\n    else:\n      print(\"The existing netcdf already covers the selected date. Avoid downloading..\")\n  return year_list\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].sum()\n  return df\n    \n\ndef prepare_cumulative_history_csvs(target_date=test_start_date, force=False):\n  \"\"\"\n    Prepare cumulative history CSVs for a specified target date.\n\n    Parameters:\n    - target_date (str, optional): The target date in the format 'YYYY-MM-DD'. Default is 'test_start_date'.\n    - force (bool, optional): If True, forcefully regenerate cumulative CSVs even if they already exist. Default is False.\n\n    Returns:\n    None\n\n    This function generates cumulative history CSVs for a specified target date. It traverses the date range from the past\n    October 1 to the target date, downloads gridmet data, converts it to CSV, and merges it into a big DataFrame.\n    The cumulative values are calculated and saved in new CSV files.\n\n    Example:\n    ```python\n    prepare_cumulative_history_csvs(target_date='2023-01-01', force=True)\n    ```\n\n    Note: This function assumes the existence of the following helper functions:\n    - download_gridmet_of_specific_variables\n    - prepare_folder_and_get_year_list\n    - turn_gridmet_nc_to_csv\n    - add_cumulative_column\n    - process_group_value_filling\n    ```\n\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n        past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n        past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Rest of the function logic...\n\n    filled_data = filled_data.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n    print(\"new_df final shape: \", filled_data.head())\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    print(filled_data.describe())\n    ```\nNote: This docstring includes placeholders such as \"download_gridmet_of_specific_variables\" and \"prepare_folder_and_get_year_list\" for the assumed existence of related helper functions. You should replace these placeholders with actual documentation for those functions.\n  \"\"\"\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  \n  date_keyed_objects = {}\n  \n  download_gridmet_of_specific_variables(\n    prepare_folder_and_get_year_list(target_date=target_date)\n  )\n  \n  while current_date <= selected_date:\n    print(current_date.strftime('%Y-%m-%d'))\n    current_date_str = current_date.strftime('%Y-%m-%d')\n    \n    \n    generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n    \n    # read the csv into dataframe and merge to the big dataframe\n    date_keyed_objects[current_date_str] = generated_csvs\n    \n    current_date += timedelta(days=1)\n    \n  print(\"date_keyed_objects: \", date_keyed_objects)\n  target_generated_csvs = date_keyed_objects[target_date]\n  for index, single_csv in enumerate(target_generated_csvs):\n    # traverse the variables of gridmet here\n    # each variable is a loop\n    print(f\"creating cumulative for {single_csv}\")\n    \n    cumulative_target_path = f\"{single_csv}_cumulative.csv\"\n    print(\"cumulative_target_path = \", cumulative_target_path)\n    \n    if os.path.exists(cumulative_target_path) and not force:\n      print(f\"{cumulative_target_path} already exists, skipping..\")\n      continue\n    \n    # Extract the file name without extension\n    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n    gap_filled_csv = f\"{cumulative_target_path}_gap_filled.csv\"\n\n\t# Split the file name using underscores\n    var_name = file_name.split('_')[1]\n    print(f\"Found variable name {var_name}\")\n    current_date = past_october_1\n    new_df = pd.read_csv(single_csv)\n    print(new_df.head())\n    \n    all_df = pd.read_csv(f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\")\n    all_df[\"date\"] = target_date\n    all_df[var_name] = pd.to_numeric(all_df[var_name], errors='coerce')\n    \n    filled_data = all_df\n    filled_data = filled_data[(filled_data['date'] == target_date)]\n    \n    filled_data.fillna(0, inplace=True)\n    \n    print(\"Finished correctly \", filled_data.head())\n    \n    filled_data = filled_data[['Latitude', 'Longitude', \n                               var_name, \n#                                f'cumulative_{var_name}'\n                              ]]\n    print(filled_data.shape)\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    print(filled_data.describe())\n\n\nif __name__ == \"__main__\":\n  # Run the download function\n#   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n#   turn_gridmet_nc_to_csv()\n#   plot_gridmet()\n\n  # prepare testing data with cumulative variables\n  prepare_cumulative_history_csvs(force=True)\n\n",
  "history_output" : "today date = 2024-07-09\nDate value from environment variable: 2023-10-28\ntest start date:  2023-10-28\ntest end date:  2024-5-19\n/home/chetana\n2023-10-28 00:00:00\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmn_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/tmmx_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/pr_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vpd_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/etr_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmax_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/rmin_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2023.nc exists\nFile /home/chetana/gridmet_test_run/gridmet_climatology/vs_2023.nc exists\n2023-10-01\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-01.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-01.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-01.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-01.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-01.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-01.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-01.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-01.csv already exists. Skipping..\n2023-10-02\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-02.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-02.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-02.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-02.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-02.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-02.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-02.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-02.csv already exists. Skipping..\n2023-10-03\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-03.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-03.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-03.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-03.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-03.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-03.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-03.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-03.csv already exists. Skipping..\n2023-10-04\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-04.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-04.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-04.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-04.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-04.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-04.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-04.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-04.csv already exists. Skipping..\n2023-10-05\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-05.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-05.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-05.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-05.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-05.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-05.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-05.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-05.csv already exists. Skipping..\n2023-10-06\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-06.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-06.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-06.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-06.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-06.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-06.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-06.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-06.csv already exists. Skipping..\n2023-10-07\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-07.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-07.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-07.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-07.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-07.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-07.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-07.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-07.csv already exists. Skipping..\n2023-10-08\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-08.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-08.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-08.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-08.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-08.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-08.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-08.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-08.csv already exists. Skipping..\n2023-10-09\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-09.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-09.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-09.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-09.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-09.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-09.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-09.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-09.csv already exists. Skipping..\n2023-10-10\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-10.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-10.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-10.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-10.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-10.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-10.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-10.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-10.csv already exists. Skipping..\n2023-10-11\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-11.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-11.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-11.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-11.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-11.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-11.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-11.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-11.csv already exists. Skipping..\n2023-10-12\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-12.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-12.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-12.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-12.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-12.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-12.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-12.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-12.csv already exists. Skipping..\n2023-10-13\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-13.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-13.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-13.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-13.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-13.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-13.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-13.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-13.csv already exists. Skipping..\n2023-10-14\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-14.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-14.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-14.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-14.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-14.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-14.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-14.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-14.csv already exists. Skipping..\n2023-10-15\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-15.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-15.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-15.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-15.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-15.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-15.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-15.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-15.csv already exists. Skipping..\n2023-10-16\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-16.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-16.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-16.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-16.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-16.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-16.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-16.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-16.csv already exists. Skipping..\n2023-10-17\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-17.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-17.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-17.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-17.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-17.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-17.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-17.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-17.csv already exists. Skipping..\n2023-10-18\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-18.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-18.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-18.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-18.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-18.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-18.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-18.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-18.csv already exists. Skipping..\n2023-10-19\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-19.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-19.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-19.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-19.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-19.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-19.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-19.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-19.csv already exists. Skipping..\n2023-10-20\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-20.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-20.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-20.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-20.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-20.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-20.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-20.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-20.csv already exists. Skipping..\n2023-10-21\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-21.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-21.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-21.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-21.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-21.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-21.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-21.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-21.csv already exists. Skipping..\n2023-10-22\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-22.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-22.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-22.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-22.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-22.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-22.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-22.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-22.csv already exists. Skipping..\n2023-10-23\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-23.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-23.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-23.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-23.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-23.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-23.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-23.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-23.csv already exists. Skipping..\n2023-10-24\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-24.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-24.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-24.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-24.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-24.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-24.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-24.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-24.csv already exists. Skipping..\n2023-10-25\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-25.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-25.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-25.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-25.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-25.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-25.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-25.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-25.csv already exists. Skipping..\n2023-10-26\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-26.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-26.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-26.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-26.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-26.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-26.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-26.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-26.csv already exists. Skipping..\n2023-10-27\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-27.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-27.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-27.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-27.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-27.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-27.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-27.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-27.csv already exists. Skipping..\n2023-10-28\nChecking file: rmax_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-28.csv already exists. Skipping..\nChecking file: etr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-28.csv already exists. Skipping..\nChecking file: rmin_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-28.csv already exists. Skipping..\nChecking file: pr_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-28.csv already exists. Skipping..\nChecking file: tmmn_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-28.csv already exists. Skipping..\nChecking file: vpd_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-28.csv already exists. Skipping..\nChecking file: vs_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-28.csv already exists. Skipping..\nChecking file: tmmx_2023.nc\n/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-28.csv already exists. Skipping..\ndate_keyed_objects:  {'2023-10-01': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-01.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-01.csv'], '2023-10-02': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-02.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-02.csv'], '2023-10-03': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-03.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-03.csv'], '2023-10-04': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-04.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-04.csv'], '2023-10-05': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-05.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-05.csv'], '2023-10-06': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-06.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-06.csv'], '2023-10-07': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-07.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-07.csv'], '2023-10-08': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-08.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-08.csv'], '2023-10-09': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-09.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-09.csv'], '2023-10-10': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-10.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-10.csv'], '2023-10-11': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-11.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-11.csv'], '2023-10-12': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-12.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-12.csv'], '2023-10-13': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-13.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-13.csv'], '2023-10-14': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-14.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-14.csv'], '2023-10-15': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-15.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-15.csv'], '2023-10-16': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-16.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-16.csv'], '2023-10-17': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-17.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-17.csv'], '2023-10-18': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-18.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-18.csv'], '2023-10-19': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-19.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-19.csv'], '2023-10-20': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-20.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-20.csv'], '2023-10-21': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-21.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-21.csv'], '2023-10-22': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-22.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-22.csv'], '2023-10-23': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-23.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-23.csv'], '2023-10-24': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-24.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-24.csv'], '2023-10-25': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-25.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-25.csv'], '2023-10-26': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-26.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-26.csv'], '2023-10-27': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-27.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-27.csv'], '2023-10-28': ['/home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-28.csv', '/home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-28.csv']}\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-28.csv_cumulative.csv\nFound variable name rmax\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\nFinished correctly     Latitude  Longitude  rmax        date\n0      49.0   -125.000   0.0  2023-10-28\n1      49.0   -124.964   0.0  2023-10-28\n2      49.0   -124.928   0.0  2023-10-28\n3      49.0   -124.892   0.0  2023-10-28\n4      49.0   -124.856   0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_rmax_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude           rmax\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600      58.630787\nstd         6.921275       7.21226      43.511805\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600      78.200000\n75%        43.024000    -106.28000     100.000000\nmax        49.000000    -100.05200     100.000000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-28.csv_cumulative.csv\nFound variable name etr\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\nFinished correctly     Latitude  Longitude  etr        date\n0      49.0   -125.000  0.0  2023-10-28\n1      49.0   -124.964  0.0  2023-10-28\n2      49.0   -124.928  0.0  2023-10-28\n3      49.0   -124.892  0.0  2023-10-28\n4      49.0   -124.856  0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_etr_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude            etr\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600       1.648861\nstd         6.921275       7.21226       2.110744\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600       0.700000\n75%        43.024000    -106.28000       2.400000\nmax        49.000000    -100.05200      10.200000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-28.csv_cumulative.csv\nFound variable name rmin\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\nFinished correctly     Latitude  Longitude  rmin        date\n0      49.0   -125.000   0.0  2023-10-28\n1      49.0   -124.964   0.0  2023-10-28\n2      49.0   -124.928   0.0  2023-10-28\n3      49.0   -124.892   0.0  2023-10-28\n4      49.0   -124.856   0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_rmin_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude           rmin\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600      30.085385\nstd         6.921275       7.21226      27.465551\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600      28.300000\n75%        43.024000    -106.28000      50.600000\nmax        49.000000    -100.05200     100.000000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-28.csv_cumulative.csv\nFound variable name pr\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\nFinished correctly     Latitude  Longitude   pr        date\n0      49.0   -125.000  0.0  2023-10-28\n1      49.0   -124.964  0.0  2023-10-28\n2      49.0   -124.928  0.0  2023-10-28\n3      49.0   -124.892  0.0  2023-10-28\n4      49.0   -124.856  0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_pr_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude             pr\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600       0.550619\nstd         6.921275       7.21226       2.535537\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600       0.000000\n75%        43.024000    -106.28000       0.000000\nmax        49.000000    -100.05200      62.500000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-28.csv_cumulative.csv\nFound variable name tmmn\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\nFinished correctly     Latitude  Longitude  tmmn        date\n0      49.0   -125.000   0.0  2023-10-28\n1      49.0   -124.964   0.0  2023-10-28\n2      49.0   -124.928   0.0  2023-10-28\n3      49.0   -124.892   0.0  2023-10-28\n4      49.0   -124.856   0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_tmmn_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude           tmmn\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600     180.274405\nstd         6.921275       7.21226     126.154664\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600     261.900000\n75%        43.024000    -106.28000     270.300000\nmax        49.000000    -100.05200     297.300000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-28.csv_cumulative.csv\nFound variable name vpd\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\nFinished correctly     Latitude  Longitude  vpd        date\n0      49.0   -125.000  0.0  2023-10-28\n1      49.0   -124.964  0.0  2023-10-28\n2      49.0   -124.928  0.0  2023-10-28\n3      49.0   -124.892  0.0  2023-10-28\n4      49.0   -124.856  0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_vpd_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude            vpd\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600       0.274671\nstd         6.921275       7.21226       0.436559\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600       0.060000\n75%        43.024000    -106.28000       0.340000\nmax        49.000000    -100.05200       2.600000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-28.csv_cumulative.csv\nFound variable name vs\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\nFinished correctly     Latitude  Longitude   vs        date\n0      49.0   -125.000  0.0  2023-10-28\n1      49.0   -124.964  0.0  2023-10-28\n2      49.0   -124.928  0.0  2023-10-28\n3      49.0   -124.892  0.0  2023-10-28\n4      49.0   -124.856  0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_vs_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude             vs\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600       2.473730\nstd         6.921275       7.21226       2.020614\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600       2.800000\n75%        43.024000    -106.28000       4.000000\nmax        49.000000    -100.05200      12.100000\ncreating cumulative for /home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-28.csv\ncumulative_target_path =  /home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-28.csv_cumulative.csv\nFound variable name tmmx\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\nFinished correctly     Latitude  Longitude  tmmx        date\n0      49.0   -125.000   0.0  2023-10-28\n1      49.0   -124.964   0.0  2023-10-28\n2      49.0   -124.928   0.0  2023-10-28\n3      49.0   -124.892   0.0  2023-10-28\n4      49.0   -124.856   0.0  2023-10-28\n(462204, 3)\nnew df is saved to /home/chetana/gridmet_test_run/testing_output/2023_tmmx_2023-10-28.csv_cumulative.csv\n            Latitude     Longitude           tmmx\ncount  462204.000000  462204.00000  462204.000000\nmean       37.030000    -112.52600     188.305546\nstd         6.921275       7.21226     131.853079\nmin        25.060000    -125.00000       0.000000\n25%        31.036000    -118.77200       0.000000\n50%        37.030000    -112.52600     271.100000\n75%        43.024000    -106.28000     281.800000\nmax        49.000000    -100.05200     308.800000\n",
  "history_begin_time" : 1720504236400,
  "history_end_time" : 1720505076172,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2wvancgdcv0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000022,
  "history_end_time" : 1720505076173,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dxs9itawida",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000034,
  "history_end_time" : 1720505076174,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5hlbfc3gnfp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000076,
  "history_end_time" : 1720505076175,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9027s1wjltb",
  "history_input" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\nimport sys\nfrom convert_results_to_images import plot_all_variables_in_one_csv\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef is_binary(file_path):\n    try:\n        with open(file_path, 'rb') as file:\n            # Read a chunk of bytes from the file\n            chunk = file.read(1024)\n\n            # Check for null bytes, a common indicator of binary data\n            if b'\\x00' in chunk:\n                return True\n\n            # Check for a high percentage of non-printable ASCII characters\n            text_characters = \"\".join(chr(byte) for byte in chunk if 32 <= byte <= 126)\n            if not text_characters:\n                return True\n\n            # If none of the binary indicators are found, assume it's a text file\n            return False\n\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found.\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n  \ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    parent_directory = os.path.dirname(target_amsr_hdf_path)\n    if not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n        print(f\"Parent directory '{parent_directory}' created successfully.\")\n    \n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    \n    \n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n        # Check the exit code\n        if result.returncode != 0:\n            print(f\"Command failed with exit code {result.returncode}.\")\n            if os.path.exists(target_amsr_hdf_path):\n              os.remove(target_amsr_hdf_path)\n              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n    \n    # Read the HDF\n    print(f\"Reading {target_amsr_hdf_path}\")\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\ndef add_cumulative_column(df, column_name):\n    df[f'cumulative_{column_name}'] = df[column_name].sum()\n    return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_all_key = row.index\n  \n  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n  #print(\"x_subset_key = \", x_subset_key)\n#   x = np.arange(len(x_subset_key))\n\n#   # Extract Y series (values from the first row)\n#   y = row[x_subset_key]\n# #   print(\"start row: \", y)\n  \n#   # Create a mask for missing values\n#   if column_name == \"AMSR_SWE\":\n#     mask = (y > 240) | y.isnull()\n#   elif column_name == \"fsca\":\n#     mask = (y > 100) | y.isnull() \n#   else:\n#     mask = y.isnull()\n\n#   # Check if all elements in the mask array are True\n#   all_true = np.all(mask)\n\n#   if all_true or len(np.where(~mask)[0]) == 1:\n#     row[x_subset_key] = 0\n# #     print(\"Final all columns: \", row)\n#   else:\n#     # Perform interpolation\n#     #new_y = np.interp(x, x[~mask], y[~mask])\n#     # Replace missing values with interpolated values\n#     #df[column_name] = new_y\n    \n#     try:\n#       # Coefficients of the polynomial fit\n#       #coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n#       # Perform polynomial interpolation\n#       #interpolated_values = np.polyval(coefficients, x)\n\n#       # Merge using np.where\n#       #merged_array = np.where(mask, interpolated_values, y)\n\n#       #row.loc[x_subset_key] = merged_array\n# #       print(\"after assign: \", row)\n#       #print(\"don't interpolate and check the original data\")\n#       pass\n#     except Exception as e:\n#       # Print the error message and traceback\n#       import traceback\n#       traceback.print_exc()\n#       print(\"x:\", x)\n#       print(\"y:\", y)\n#       print(\"mask:\", mask)\n#       print(f\"Error: {e}\")\n#       raise e\n      \n#     if column_name == \"AMSR_SWE\":\n#       row[x_subset_key] = row[x_subset_key].clip(upper=240, lower=0)\n#     elif column_name == \"fsca\":\n#       row[x_subset_key] = row[x_subset_key].clip(upper=100, lower=0)\n#     else:\n#       row[x_subset_key] = row[x_subset_key].clip(upper=240, lower=0)\n      \n#     print(\"after clip: \", row)\n      \n#     if row[x_subset_key].isnull().any():\n#       print(\"x:\", x)\n#       print(\"y:\", y)\n#       print(\"mask:\", mask)\n#       print(\"why row still has values > 100\", row)\n#       raise ValueError(\"Single group: shouldn't have null values here\")\n\n  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n  if are_all_values_between_0_and_240:\n    print(\"row[x_subset_key] = \", row[x_subset_key])\n    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n  return row\n    \n    \ndef get_cumulative_amsr_data(target_date = test_start_date, force=False):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n      past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Traverse and print every day from past October 1 to the specific date\n    current_date = past_october_1\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n\n    columns_to_be_cumulated = [\"AMSR_SWE\"]\n    \n    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n    if os.path.exists(gap_filled_csv) and not force:\n      print(f\"{gap_filled_csv} already exists, skipping..\")\n      df = pd.read_csv(gap_filled_csv)\n      print(df[\"AMSR_SWE\"].describe())\n    else:\n      date_keyed_objects = {}\n      data_dict = {}\n      new_df = None\n      while current_date <= selected_date:\n        print(current_date.strftime('%Y-%m-%d'))\n        current_date_str = current_date.strftime('%Y-%m-%d')\n\n        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n        current_df = pd.read_csv(data_dict[current_date_str])\n        current_df.drop(columns=[\"date\"], inplace=True)\n\n        if current_date != selected_date:\n          current_df.rename(columns={\n            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n          }, inplace=True)\n        #print(current_df.head())\n\n        if new_df is None:\n          new_df = current_df\n        else:\n          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n          #new_df = new_df.append(current_df, ignore_index=True)\n\n        current_date += timedelta(days=1)\n\n      print(\"new_df.columns = \", new_df.columns)\n      print(\"new_df.head = \", new_df.head())\n      df = new_df\n\n      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n      print(\"All current head: \", df.head())\n      print(\"the new_df.shape: \", df.shape)\n\n      print(\"Start to fill in the missing values\")\n      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n      filled_data = pd.DataFrame()\n\n      # Apply the function to each group\n      for column_name in columns_to_be_cumulated:\n        start_time = time.time()\n        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n        #alike_columns = filled_data.filter(like=column_name)\n        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n        print(\"filled_data.columns = \", filled_data.columns)\n        filtered_columns = df.filter(like=column_name)\n        print(filtered_columns.columns)\n        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n        filtered_columns.fillna(0, inplace=True)\n        \n        sum_column = filtered_columns.sum(axis=1)\n        # Define a specific name for the new column\n        df[f'cumulative_{column_name}'] = sum_column\n        df[filtered_columns.columns] = filtered_columns\n        \n        if filtered_columns.isnull().any().any():\n          print(\"filtered_columns :\", filtered_columns)\n          raise ValueError(\"Single group: shouldn't have null values here\")\n        \n        \n        \n\n        # Concatenate the original DataFrame with the Series containing the sum\n        #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n#         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n#         filled_data[f'cumulative_{column_name}'] = cumulative_column\n        #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n        print(\"filled_data.columns: \", filled_data.columns)\n        end_time = time.time()\n        # Calculate the elapsed time\n        elapsed_time = end_time - start_time\n        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n\n#       if any(filled_data['AMSR_SWE'] > 240):\n#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n      filled_data = df\n      filled_data[\"date\"] = target_date\n      print(\"Finished correctly \", filled_data.head())\n      filled_data.to_csv(gap_filled_csv, index=False)\n      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n      df = filled_data\n    \n    result = df\n    print(\"result.head = \", result.head())\n    # fill in the rest NA as 0\n    if result.isnull().any().any():\n      print(\"result :\", result)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    # only retain the rows of the target date\n    print(result['date'].unique())\n    print(result.shape)\n    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n    result.to_csv(target_csv_path, index=False)\n    print(f\"New data is saved to {target_csv_path}\")\n    \n      \n    \nif __name__ == \"__main__\":\n    # Run the download and conversion function\n    #prepare_amsr_grid_mapper()\n    prepare_amsr_grid_mapper()\n#     download_amsr_and_convert_grid()\n    \n    get_cumulative_amsr_data(force=False)\n    input_time_series_file = f'{work_dir}/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n\n    #plot_all_variables_in_one_csv(input_time_series_file, f\"{input_time_series_file}.png\")\n",
  "history_output" : "today date = 2024-07-09\nDate value from environment variable: 2023-10-28\ntest start date:  2023-10-28\ntest end date:  2024-5-19\n/home/chetana\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\n2023-10-28 00:00:00\n2023-10-01\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.01.csv already exists, skipping..\n2023-10-02\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.02.csv already exists, skipping..\n2023-10-03\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.03.csv already exists, skipping..\n2023-10-04\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.04.csv already exists, skipping..\n2023-10-05\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.05.csv already exists, skipping..\n2023-10-06\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.06.csv already exists, skipping..\n2023-10-07\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.07.csv already exists, skipping..\n2023-10-08\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.08.csv already exists, skipping..\n2023-10-09\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.09.csv already exists, skipping..\n2023-10-10\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.10.csv already exists, skipping..\n2023-10-11\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.11.csv already exists, skipping..\n2023-10-12\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.12.csv already exists, skipping..\n2023-10-13\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.13.csv already exists, skipping..\n2023-10-14\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.14.csv already exists, skipping..\n2023-10-15\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.15.csv already exists, skipping..\n2023-10-16\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.16.csv already exists, skipping..\n2023-10-17\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.17.csv already exists, skipping..\n2023-10-18\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.18.csv already exists, skipping..\n2023-10-19\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.19.csv already exists, skipping..\n2023-10-20\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.20.csv already exists, skipping..\n2023-10-21\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.21.csv already exists, skipping..\n2023-10-22\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.22.csv already exists, skipping..\n2023-10-23\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.23.csv already exists, skipping..\n2023-10-24\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.24.csv already exists, skipping..\n2023-10-25\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.25.csv already exists, skipping..\n2023-10-26\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.26.csv already exists, skipping..\n2023-10-27\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.27.csv already exists, skipping..\n2023-10-28\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2023.10.28.csv already exists, skipping..\nnew_df.columns =  Index(['gridmet_lat', 'gridmet_lon', 'AMSR_SWE_2023-10-01',\n       'AMSR_Flag_2023-10-01', 'AMSR_SWE_2023-10-02', 'AMSR_Flag_2023-10-02',\n       'AMSR_SWE_2023-10-03', 'AMSR_Flag_2023-10-03', 'AMSR_SWE_2023-10-04',\n       'AMSR_Flag_2023-10-04', 'AMSR_SWE_2023-10-05', 'AMSR_Flag_2023-10-05',\n       'AMSR_SWE_2023-10-06', 'AMSR_Flag_2023-10-06', 'AMSR_SWE_2023-10-07',\n       'AMSR_Flag_2023-10-07', 'AMSR_SWE_2023-10-08', 'AMSR_Flag_2023-10-08',\n       'AMSR_SWE_2023-10-09', 'AMSR_Flag_2023-10-09', 'AMSR_SWE_2023-10-10',\n       'AMSR_Flag_2023-10-10', 'AMSR_SWE_2023-10-11', 'AMSR_Flag_2023-10-11',\n       'AMSR_SWE_2023-10-12', 'AMSR_Flag_2023-10-12', 'AMSR_SWE_2023-10-13',\n       'AMSR_Flag_2023-10-13', 'AMSR_SWE_2023-10-14', 'AMSR_Flag_2023-10-14',\n       'AMSR_SWE_2023-10-15', 'AMSR_Flag_2023-10-15', 'AMSR_SWE_2023-10-16',\n       'AMSR_Flag_2023-10-16', 'AMSR_SWE_2023-10-17', 'AMSR_Flag_2023-10-17',\n       'AMSR_SWE_2023-10-18', 'AMSR_Flag_2023-10-18', 'AMSR_SWE_2023-10-19',\n       'AMSR_Flag_2023-10-19', 'AMSR_SWE_2023-10-20', 'AMSR_Flag_2023-10-20',\n       'AMSR_SWE_2023-10-21', 'AMSR_Flag_2023-10-21', 'AMSR_SWE_2023-10-22',\n       'AMSR_Flag_2023-10-22', 'AMSR_SWE_2023-10-23', 'AMSR_Flag_2023-10-23',\n       'AMSR_SWE_2023-10-24', 'AMSR_Flag_2023-10-24', 'AMSR_SWE_2023-10-25',\n       'AMSR_Flag_2023-10-25', 'AMSR_SWE_2023-10-26', 'AMSR_Flag_2023-10-26',\n       'AMSR_SWE_2023-10-27', 'AMSR_Flag_2023-10-27', 'AMSR_SWE', 'AMSR_Flag'],\n      dtype='object')\nnew_df.head =     gridmet_lat  gridmet_lon  ...  AMSR_SWE  AMSR_Flag\n0         49.0     -125.000  ...         0        241\n1         49.0     -124.964  ...         0        241\n2         49.0     -124.928  ...         0        241\n3         49.0     -124.892  ...         0        241\n4         49.0     -124.856  ...         0        241\n[5 rows x 58 columns]\nAll current head:     gridmet_lat  gridmet_lon  ...  AMSR_SWE  AMSR_Flag\n0         49.0     -125.000  ...         0        241\n1         49.0     -124.964  ...         0        241\n2         49.0     -124.928  ...         0        241\n3         49.0     -124.892  ...         0        241\n4         49.0     -124.856  ...         0        241\n[5 rows x 58 columns]\nthe new_df.shape:  (462204, 58)\nStart to fill in the missing values\nfilled_data.columns =  Index([], dtype='object')\nIndex(['AMSR_SWE_2023-10-01', 'AMSR_SWE_2023-10-02', 'AMSR_SWE_2023-10-03',\n       'AMSR_SWE_2023-10-04', 'AMSR_SWE_2023-10-05', 'AMSR_SWE_2023-10-06',\n       'AMSR_SWE_2023-10-07', 'AMSR_SWE_2023-10-08', 'AMSR_SWE_2023-10-09',\n       'AMSR_SWE_2023-10-10', 'AMSR_SWE_2023-10-11', 'AMSR_SWE_2023-10-12',\n       'AMSR_SWE_2023-10-13', 'AMSR_SWE_2023-10-14', 'AMSR_SWE_2023-10-15',\n       'AMSR_SWE_2023-10-16', 'AMSR_SWE_2023-10-17', 'AMSR_SWE_2023-10-18',\n       'AMSR_SWE_2023-10-19', 'AMSR_SWE_2023-10-20', 'AMSR_SWE_2023-10-21',\n       'AMSR_SWE_2023-10-22', 'AMSR_SWE_2023-10-23', 'AMSR_SWE_2023-10-24',\n       'AMSR_SWE_2023-10-25', 'AMSR_SWE_2023-10-26', 'AMSR_SWE_2023-10-27',\n       'AMSR_SWE'],\n      dtype='object')\nfilled_data.columns:  Index([], dtype='object')\ncalculate column AMSR_SWE elapsed time: 33.129554986953735 seconds\nFinished correctly     gridmet_lat  gridmet_lon  ...  cumulative_AMSR_SWE        date\n0         49.0     -125.000  ...                  0.0  2023-10-28\n1         49.0     -124.964  ...                  0.0  2023-10-28\n2         49.0     -124.928  ...                  0.0  2023-10-28\n3         49.0     -124.892  ...                  0.0  2023-10-28\n4         49.0     -124.856  ...                  0.0  2023-10-28\n[5 rows x 60 columns]\nNew filled values csv is saved to /home/chetana/gridmet_test_run/testing_ready_amsr_2023-10-28_cumulative.csv_gap_filled.csv\nresult.head =     gridmet_lat  gridmet_lon  ...  cumulative_AMSR_SWE        date\n0         49.0     -125.000  ...                  0.0  2023-10-28\n1         49.0     -124.964  ...                  0.0  2023-10-28\n2         49.0     -124.928  ...                  0.0  2023-10-28\n3         49.0     -124.892  ...                  0.0  2023-10-28\n4         49.0     -124.856  ...                  0.0  2023-10-28\n[5 rows x 60 columns]\n['2023-10-28']\n(462204, 60)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean        0.783561     246.595220\nstd         3.486175       6.520449\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%         0.000000     241.000000\n75%         0.000000     254.000000\nmax        40.000000     255.000000\nNew data is saved to /home/chetana/gridmet_test_run/testing_ready_amsr_2023-10-28_cumulative.csv\n",
  "history_begin_time" : 1720504102107,
  "history_end_time" : 1720505076176,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "88x3685d2y4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000123,
  "history_end_time" : 1720505076177,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ey68nscefoj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000156,
  "history_end_time" : 1720505076179,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z667eoh8zui",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000185,
  "history_end_time" : 1720505076179,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wc3oxoaw4ed",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000199,
  "history_end_time" : 1720505076180,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "edmqxbwyhd7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000214,
  "history_end_time" : 1720505076181,
  "history_notes" : null,
  "history_process" : "d4zcq6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w22q104y4f2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000228,
  "history_end_time" : 1720505076181,
  "history_notes" : null,
  "history_process" : "6x6myw",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ajwrj2zkhah",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000241,
  "history_end_time" : 1720505076182,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "omz3s9hvsm7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000254,
  "history_end_time" : 1720505076183,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ka6lprhlu6y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000267,
  "history_end_time" : 1720505076184,
  "history_notes" : null,
  "history_process" : "ee5ur4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tftm0fswr7c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000281,
  "history_end_time" : 1720505076184,
  "history_notes" : null,
  "history_process" : "f03i7p",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rqc0u2d27v1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000293,
  "history_end_time" : 1720505076185,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5ww7ba3129o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000304,
  "history_end_time" : 1720505076186,
  "history_notes" : null,
  "history_process" : "j8swco",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "puzioog6vr5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000315,
  "history_end_time" : 1720505076187,
  "history_notes" : null,
  "history_process" : "pnr64x",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ab52r95yezf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000328,
  "history_end_time" : 1720505076187,
  "history_notes" : null,
  "history_process" : "qg80lj",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5vm7a7nzff6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000343,
  "history_end_time" : 1720505076188,
  "history_notes" : null,
  "history_process" : "ggy7gf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "332mwo5n3tu",
  "history_input" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nos.chdir(f\"{homedir}/fsca/\")\n\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  add_time_series_columns(start_date, end_date, force=False)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n",
  "history_output" : "today date = 2024-07-09\nDate value from environment variable: 2023-10-28\ntest start date:  2023-10-28\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2023-10-28\n2023-10-28 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2023-10-01__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2023-10-02__snow_cover.tif exists. skip.\nfile_size_bytes: 509484\nThe file /home/chetana/fsca/final_output//2023-10-03__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-04__snow_cover.tif exists. skip.\nfile_size_bytes: 509484\nThe file /home/chetana/fsca/final_output//2023-10-05__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-06__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-07__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-08__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-09__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-10__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-11__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-12__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-13__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-14__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-15__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-16__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-17__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-18__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-19__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-20__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-21__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2023-10-22__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2023-10-23__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-24__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-25__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-26__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-27__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-28__snow_cover.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/fsca/final_output/2023-10-01_output.csv exists. skip.\nextracting data for 2023-10-02\nThe file /home/chetana/fsca/final_output/2023-10-02_output.csv exists. skip.\nextracting data for 2023-10-03\nThe file /home/chetana/fsca/final_output/2023-10-03_output.csv exists. skip.\nextracting data for 2023-10-04\nThe file /home/chetana/fsca/final_output/2023-10-04_output.csv exists. skip.\nextracting data for 2023-10-05\nThe file /home/chetana/fsca/final_output/2023-10-05_output.csv exists. skip.\nextracting data for 2023-10-06\nThe file /home/chetana/fsca/final_output/2023-10-06_output.csv exists. skip.\nextracting data for 2023-10-07\nThe file /home/chetana/fsca/final_output/2023-10-07_output.csv exists. skip.\nextracting data for 2023-10-08\nThe file /home/chetana/fsca/final_output/2023-10-08_output.csv exists. skip.\nextracting data for 2023-10-09\nThe file /home/chetana/fsca/final_output/2023-10-09_output.csv exists. skip.\nextracting data for 2023-10-10\nThe file /home/chetana/fsca/final_output/2023-10-10_output.csv exists. skip.\nextracting data for 2023-10-11\nThe file /home/chetana/fsca/final_output/2023-10-11_output.csv exists. skip.\nextracting data for 2023-10-12\nThe file /home/chetana/fsca/final_output/2023-10-12_output.csv exists. skip.\nextracting data for 2023-10-13\nThe file /home/chetana/fsca/final_output/2023-10-13_output.csv exists. skip.\nextracting data for 2023-10-14\nThe file /home/chetana/fsca/final_output/2023-10-14_output.csv exists. skip.\nextracting data for 2023-10-15\nThe file /home/chetana/fsca/final_output/2023-10-15_output.csv exists. skip.\nextracting data for 2023-10-16\nThe file /home/chetana/fsca/final_output/2023-10-16_output.csv exists. skip.\nextracting data for 2023-10-17\nThe file /home/chetana/fsca/final_output/2023-10-17_output.csv exists. skip.\nextracting data for 2023-10-18\nThe file /home/chetana/fsca/final_output/2023-10-18_output.csv exists. skip.\nextracting data for 2023-10-19\nThe file /home/chetana/fsca/final_output/2023-10-19_output.csv exists. skip.\nextracting data for 2023-10-20\nThe file /home/chetana/fsca/final_output/2023-10-20_output.csv exists. skip.\nextracting data for 2023-10-21\nThe file /home/chetana/fsca/final_output/2023-10-21_output.csv exists. skip.\nextracting data for 2023-10-22\nThe file /home/chetana/fsca/final_output/2023-10-22_output.csv exists. skip.\nextracting data for 2023-10-23\nThe file /home/chetana/fsca/final_output/2023-10-23_output.csv exists. skip.\nextracting data for 2023-10-24\nThe file /home/chetana/fsca/final_output/2023-10-24_output.csv exists. skip.\nextracting data for 2023-10-25\nThe file /home/chetana/fsca/final_output/2023-10-25_output.csv exists. skip.\nextracting data for 2023-10-26\nThe file /home/chetana/fsca/final_output/2023-10-26_output.csv exists. skip.\nextracting data for 2023-10-27\nThe file /home/chetana/fsca/final_output/2023-10-27_output.csv exists. skip.\nextracting data for 2023-10-28\nThe file /home/chetana/fsca/final_output/2023-10-28_output.csv exists. skip.\nadd_time_series_columns target csv: /home/chetana/fsca/final_output//2023-10-28_output_with_time_series.csv\nnew_df.columns =  Index(['Latitude', 'Longitude', 'fsca_2023-10-01', 'fsca_2023-10-02',\n       'fsca_2023-10-03', 'fsca_2023-10-04', 'fsca_2023-10-05',\n       'fsca_2023-10-06', 'fsca_2023-10-07', 'fsca_2023-10-08',\n       'fsca_2023-10-09', 'fsca_2023-10-10', 'fsca_2023-10-11',\n       'fsca_2023-10-12', 'fsca_2023-10-13', 'fsca_2023-10-14',\n       'fsca_2023-10-15', 'fsca_2023-10-16', 'fsca_2023-10-17',\n       'fsca_2023-10-18', 'fsca_2023-10-19', 'fsca_2023-10-20',\n       'fsca_2023-10-21', 'fsca_2023-10-22', 'fsca_2023-10-23',\n       'fsca_2023-10-24', 'fsca_2023-10-25', 'fsca_2023-10-26',\n       'fsca_2023-10-27', 'fsca'],\n      dtype='object')\n   Latitude  Longitude  fsca_2023-10-01  ...  fsca_2023-10-26  fsca_2023-10-27  fsca\n0      49.0   -125.000              239  ...              239               61   239\n1      49.0   -124.964              239  ...              239              239   239\n2      49.0   -124.928              239  ...              239              239   239\n3      49.0   -124.892                0  ...                0                0     0\n4      49.0   -124.856                0  ...                0               10    13\n[5 rows x 30 columns]\nAll current columns:  Index(['Latitude', 'Longitude', 'fsca_2023-10-01', 'fsca_2023-10-02',\n       'fsca_2023-10-03', 'fsca_2023-10-04', 'fsca_2023-10-05',\n       'fsca_2023-10-06', 'fsca_2023-10-07', 'fsca_2023-10-08',\n       'fsca_2023-10-09', 'fsca_2023-10-10', 'fsca_2023-10-11',\n       'fsca_2023-10-12', 'fsca_2023-10-13', 'fsca_2023-10-14',\n       'fsca_2023-10-15', 'fsca_2023-10-16', 'fsca_2023-10-17',\n       'fsca_2023-10-18', 'fsca_2023-10-19', 'fsca_2023-10-20',\n       'fsca_2023-10-21', 'fsca_2023-10-22', 'fsca_2023-10-23',\n       'fsca_2023-10-24', 'fsca_2023-10-25', 'fsca_2023-10-26',\n       'fsca_2023-10-27', 'fsca'],\n      dtype='object')\nStart to fill in the missing values\nall the df shape:  (462204, 30)\nIndex(['fsca_2023-10-01', 'fsca_2023-10-02', 'fsca_2023-10-03',\n       'fsca_2023-10-04', 'fsca_2023-10-05', 'fsca_2023-10-06',\n       'fsca_2023-10-07', 'fsca_2023-10-08', 'fsca_2023-10-09',\n       'fsca_2023-10-10', 'fsca_2023-10-11', 'fsca_2023-10-12',\n       'fsca_2023-10-13', 'fsca_2023-10-14', 'fsca_2023-10-15',\n       'fsca_2023-10-16', 'fsca_2023-10-17', 'fsca_2023-10-18',\n       'fsca_2023-10-19', 'fsca_2023-10-20', 'fsca_2023-10-21',\n       'fsca_2023-10-22', 'fsca_2023-10-23', 'fsca_2023-10-24',\n       'fsca_2023-10-25', 'fsca_2023-10-26', 'fsca_2023-10-27', 'fsca'],\n      dtype='object')\nFinished correctly     Latitude  Longitude  fsca_2023-10-01  ...  fsca  cumulative_fsca        date\n0      49.0   -125.000              0.0  ...  61.0            671.0  2023-10-28\n1      49.0   -124.964              0.0  ...   0.0              0.0  2023-10-28\n2      49.0   -124.928              0.0  ...   0.0              0.0  2023-10-28\n3      49.0   -124.892              0.0  ...   0.0              0.0  2023-10-28\n4      49.0   -124.856              0.0  ...  13.0             44.0  2023-10-28\n[5 rows x 32 columns]\nNew filled values csv is saved to /home/chetana/fsca/final_output//2023-10-28_output_with_time_series.csv_gap_filled.csv\n['2023-10-28']\n(462204, 32)\ncount    462204.000000\nmean          6.452722\nstd          19.385145\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           0.000000\nmax          99.000000\nName: fsca, dtype: float64\nNew data is saved to /home/chetana/fsca/final_output//2023-10-28_output_with_time_series.csv\nFile is backed up to /home/chetana/fsca/final_output//2023-10-28_output_with_time_series_backup.csv\n",
  "history_begin_time" : 1720504001050,
  "history_end_time" : 1720505076189,
  "history_notes" : null,
  "history_process" : "c2qa9u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x9zqxq7mxcz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000387,
  "history_end_time" : 1720505076190,
  "history_notes" : null,
  "history_process" : "lnrsop",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ireqle8p4n1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000407,
  "history_end_time" : 1720505076191,
  "history_notes" : null,
  "history_process" : "c8isgf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jfetiar51pq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000419,
  "history_end_time" : 1720505076191,
  "history_notes" : null,
  "history_process" : "16qpco",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "f726rpddqul",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing(test_start_date):\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing(\"2023-10-01\")\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-07-09\nDate value from environment variable: 2023-10-28\ntest start date:  2023-10-28\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2023-10-01\n2023-10-01 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1720504001050,
  "history_end_time" : 1720505076192,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ppchuykifnw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1720504000441,
  "history_end_time" : 1720505076193,
  "history_notes" : null,
  "history_process" : "uw1w1u",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]
