[{
  "history_id" : "x4fm08vl50j",
  "history_input" : "# Data Preparation for Sentinel 2\n\nprint(\"Not ready yet..Prepare sentinel 2 into .csv\")\n\n",
  "history_output" : "Not ready yet..Prepare sentinel 2 into .csv\n",
  "history_begin_time" : 1646604599910,
  "history_end_time" : 1646604600105,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3b6f1yzn486",
  "history_input" : "# Create LSTM model\n\nprint(\"Create LSTM\")\n\n",
  "history_output" : "Create LSTM\n",
  "history_begin_time" : 1646604591236,
  "history_end_time" : 1646604591348,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ora6zash5fg",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#pd.set_option('display.max_columns', None)\n",
  "history_output" : "",
  "history_begin_time" : 1646604590996,
  "history_end_time" : 1646604598978,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1rmvv195irq",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1646604591652,
  "history_end_time" : 1646604591849,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6v3lpvxf7p6",
  "history_input" : "# Find the best model\n\n",
  "history_output" : "[]\n",
  "history_begin_time" : 1646604645365,
  "history_end_time" : 1646604646386,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ujpl6lk1idc",
  "history_input" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom datetime import datetime\n\n\nprint(\"integrating datasets into one dataset\")\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\n#example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\n#print(training_feature_pd.head())\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n#print(station_cell_mapper_pd.head())\n\n#example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n#print(example_mod_pd.shape)\n\n\ndef integrate_modis():\n  \"\"\"\n  Integrate all MODIS data into mod_all.csv\n  \"\"\"\n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  mod_all_df = pd.DataFrame(columns=[\"date\"])\n  mod_all_df['date'] = dates\n  all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n  #print(mod_all_df.head())\n  for ind in station_cell_mapper_pd.index:\n    current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n    print(current_cell_id)\n    mod_single_file = f\"{github_dir}/data/modis/mod10a1_ndsi_{current_cell_id}.csv\"\n    if os.path.isfile(mod_single_file):\n      mod_single_pd = pd.read_csv(mod_single_file, header=0)\n      mod_single_pd = mod_single_pd[[\"date\", \"mod10a1_ndsi\"]]\n      mod_single_pd = mod_single_pd.rename(columns={\"mod10a1_ndsi\": current_cell_id})\n      mod_single_pd['date'] = pd.to_datetime(mod_single_pd['date']).astype(str)\n      print(mod_all_df.shape)\n      mod_all_df = pd.merge(mod_all_df, mod_single_pd, how='left', on=\"date\")\n  mod_all_df.to_csv(all_mod_file)\n\n  \ndef integrate_sentinel1():\n  \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  \"\"\"\n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  sentinel1_all_df = pd.DataFrame(columns=[\"date\"])\n  sentinel1_all_df['date'] = dates\n  #print(mod_all_df.head())\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n\n  def getDateStr(x):\n    return x.split(\" \")[0]\n\n  for ind in station_cell_mapper_pd.index:\n    current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n    print(current_cell_id)\n    sentinel1_single_file = f\"{github_dir}/data/sentinel1/s1_grd_vv_{current_cell_id}.csv\"\n    if os.path.isfile(sentinel1_single_file) and current_cell_id not in sentinel1_all_df :\n      sentinel1_single_pd = pd.read_csv(sentinel1_single_file, header=0)\n      sentinel1_single_pd = sentinel1_single_pd[[\"date\", \"s1_grd_vv\"]]\n      sentinel1_single_pd = sentinel1_single_pd.rename(columns={\"s1_grd_vv\": current_cell_id})\n      #sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n      sentinel1_single_pd['date'] = pd.to_datetime(sentinel1_single_pd['date']).dt.round(\"D\").astype(str)\n      print(\"sentinel1_single_pd: \", sentinel1_single_pd.head())\n      print(\"sentinel1_single_pd check value: \", sentinel1_single_pd[sentinel1_single_pd[\"date\"]==\"2015-04-01\"])\n      sentinel1_single_pd = sentinel1_single_pd.drop_duplicates(subset=['date'], keep='first') # this will remove all the other values of the same day\n      \n      sentinel1_all_df = pd.merge(sentinel1_all_df, sentinel1_single_pd, how='left', on=\"date\")\n      print(\"sentinel1_all_df check value: \", sentinel1_all_df[sentinel1_all_df[\"date\"]==\"2015-04-01\"])\n      print(\"sentinel1_all_df: \", sentinel1_all_df.shape)\n      \n\n  print(sentinel1_all_df.shape)\n  sentinel1_all_df.to_csv(all_sentinel1_file)\n\ndef prepare_training_csv():\n  \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  \"\"\"\n  all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n  modis_all_pd = pd.read_csv(all_mod_file, header=0)\n  print(\"modis_all_size: \", modis_all_pd.shape)\n  print(\"station size: \", station_cell_mapper_pd.shape)\n  print(\"training_feature_pd size: \", training_feature_pd.shape)\n  print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n  \n  ndsi_training_pd = pd.DataFrame(columns=[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\"])\n  modis_all_pd = modis_all_pd.reset_index()\n  for index, row in modis_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    month = dt.month\n    year = dt.year\n    doy = dt.timetuple().tm_yday\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i][:-2]\n      ndsi = row.values[i]\n      if cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        if not np.isnan(swe):\n          ndsi_training_pd.loc[len(ndsi_training_pd.index)] = [year, month, doy, ndsi, swe]\n  \n  print(ndsi_training_pd.shape)\n  ndsi_training_pd.to_csv(f\"{github_dir}/data/ready_for_training/modis_ready.csv\")\n  \n  all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n  sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0)\n  grd_all_pd = pd.DataFrame(columns=[\"year\", \"m\", \"doy\", \"grd\", \"swe\"])\n  grd_all_pd = grd_all_pd.reset_index()\n  for index, row in sentinel1_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    year = dt.year\n    month = dt.month\n    doy = dt.timetuple().tm_yday\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i]\n      grd = row.values[i]\n      if not np.isnan(grd) and cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        if not np.isnan(swe):\n          print([month, doy, grd, swe])\n          grd_all_pd = grd_all_pd.append({\"year\": year, \"m\":month, \"doy\": doy, \"grd\": grd, \"swe\": swe}, ignore_index = True)\n  \n  print(grd_all_pd.shape)\n  grd_all_pd.to_csv(f\"{github_dir}/data/ready_for_training/sentinel1_ready.csv\")\n  \nexit() # done already\n\nintegrate_modis()\nintegrate_sentinel1()\nprepare_training_csv()\n\n\n  \n  \n  \n",
  "history_output" : "integrating datasets into one dataset\n/Users/joe\n",
  "history_begin_time" : 1646604613196,
  "history_end_time" : 1646604615221,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "3buye4neevn",
  "history_input" : "# Deploy model to service\n\nprint(\"deploy model to service\")\n",
  "history_output" : "deploy model to service\n",
  "history_begin_time" : 1646604648073,
  "history_end_time" : 1646604648235,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0mng6wg8pce",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit()  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "",
  "history_begin_time" : 1646604649427,
  "history_end_time" : 1646604651060,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "00ek6y19e5w",
  "history_input" : "# Train Model\n\nprint(\"Train Models\")\n",
  "history_output" : "Train Models\n",
  "history_begin_time" : 1646604617080,
  "history_end_time" : 1646604617219,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "els34q7rcji",
  "history_input" : "# Test models\n\n# Random Forest model creation and save to file\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_train_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\nbase_accuracy = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_v2.joblib\")\nrandom_accuracy = evaluate(best_random, all_features, all_labels, \"Optimized\")\n\n\n\n",
  "history_output" : "/Users/joe\nThe Base Model model performance for testing set\n--------------------------------------\nMAE is 3.962166141092642\nMSE is 47.73350021329197\nR2 score is 0.6715537637787654\nRMSE is 6.908943494724209\nThe Optimized model performance for testing set\n--------------------------------------\nMAE is 4.599956188723821\nMSE is 56.6941020141579\nR2 score is 0.6098973605688434\nRMSE is 7.529548592987358\n",
  "history_begin_time" : 1646604619256,
  "history_end_time" : 1646604644033,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "wilzqzf096q",
  "history_input" : "# Load dependencies\nimport geopandas as gpd\nimport json\nimport geojson\nfrom pystac_client import Client\nimport planetary_computer\nimport xarray\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyproj import Proj, transform\nimport os\n\nhome_dir = os.path.expanduser('~')\nsnowcast_github_dir = f\"{home_dir}/Documents/GitHub/SnowCast/\"\n\nexit() # this process no longer need to execute, we need to make Geoweaver to specify which process doesn't need to run\n\n# user-defined paths for data-access\ndata_dir = f'{snowcast_github_dir}data/station_gridcell/'\ngridcells_file = data_dir+'grid_cells.geojson'\nstations_file = data_dir+'ground_measures_metadata.csv'\ngridcells_outfile = data_dir+'gridcells_terrainData.csv'\nstations_outfile = data_dir+'station_terrainData.csv'\n\n# setup client for handshaking and data-access\nclient = Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    ignore_conformance=True,\n)\n\n# Load metadata\ngridcellsGPD = gpd.read_file(gridcells_file)\ngridcells = geojson.load(open(gridcells_file))\nstations = pd.read_csv(stations_file)\n\n# instantiate output panda dataframes\ndf_gridcells = df = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                          \"Elevation [m]\",\"Aspect [deg]\",\n                                          \"Curvature [ratio]\",\"Slope [deg]\"))\ndf_station = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                   \"Elevation [m]\",\"Elevation_30 [m]\",\"Elevation_1000 [m]\",\n                                   \"Aspect_30 [deg]\",\"Aspect_1000 [deg]\",\n                                   \"Curvature_30 [ratio]\",\"Curvature_1000 [ratio]\",\n                                   \"Slope_30 [deg]\",\"Slope_1000 [deg]\"))\n\n# Calculate gridcell characteristics using Copernicus DEM data\nfor idx,cell in enumerate(gridcells['features']):\n    print(idx)\n    search = client.search(\n        collections=[\"cop-dem-glo-30\"],\n        intersects={\"type\":\"Polygon\", \"coordinates\":cell['geometry']['coordinates']},\n    )\n    items = list(search.get_items())   \n    \n    try:\n        signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n        data = (\n            xarray.open_rasterio(signed_asset.href)\n            .squeeze()\n            .drop(\"band\")\n            .coarsen({\"y\": 1, \"x\": 1})\n            .mean()\n        )\n        cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n    except:\n      \tprint(\"fail to open data using xarray\")\n      \t\"\"\"\n        signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n        data = (\n            xarray.open_rasterio(signed_asset.href)\n            .squeeze()\n            .drop(\"band\")\n            .coarsen({\"y\": 1, \"x\": 1})\n            .mean()\n        )\n        cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n        \"\"\"\n    \n    longitude = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n    latitude = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n    \n    cropped_data = cropped_data.rio.reproject(\"EPSG:32612\")\n        \n    mean_elev = cropped_data.mean().values\n    print(mean_elev)\n    \n    aspect = xrspatial.aspect(cropped_data)\n    aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n    aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n    mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n    if mean_aspect < 0:\n        mean_aspect = 360 + mean_aspect\n    print(mean_aspect)\n    \n    # Positive curvature = upward convex\n    curvature = xrspatial.curvature(cropped_data)\n    mean_curvature = curvature.mean().values\n    print(mean_curvature)\n    \n    slope = xrspatial.slope(cropped_data)\n    mean_slope = slope.mean().values\n    print(mean_slope)\n    \n    df_gridcells.loc[idx] = [longitude,latitude,\n                             mean_elev,mean_aspect,\n                             mean_curvature,mean_slope]\n    \n    if idx % 250 == 0:\n        df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n        df_gridcells.to_csv(gridcells_outfile)\n\n# Save output data into csv format\ndf_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\ndf_gridcells.to_csv(gridcells_outfile)\n\n# Calculate terrain characteristics of stations, and surrounding regions using COP 30\nfor idx,station in stations.iterrows():\n    search = client.search(\n        collections=[\"cop-dem-glo-30\"],\n        intersects={\"type\":\"Point\", \"coordinates\":[station['longitude'],station['latitude']]},\n    )\n    items = list(search.get_items())\n    print(f\"Returned {len(items)} items\")\n    \n    try:\n        signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n        data = (\n            xarray.open_rasterio(signed_asset.href)\n            .squeeze()\n            .drop(\"band\")\n            .coarsen({\"y\": 1, \"x\": 1})\n            .mean()\n        )\n        xdiff = np.abs(data.x-station['longitude'])\n        ydiff = np.abs(data.y-station['latitude'])\n        xdiff = np.where(xdiff == xdiff.min())[0][0]\n        ydiff = np.where(ydiff == ydiff.min())[0][0]\n        data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n    except:\n        print(\"Fail to open data using xarray\")\n        \"\"\"\n        signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n        data = (\n            xarray.open_rasterio(signed_asset.href)\n            .squeeze()\n            .drop(\"band\")\n            .coarsen({\"y\": 1, \"x\": 1})\n            .mean()\n        )\n        xdiff = np.abs(data.x-station['longitude'])\n        ydiff = np.abs(data.y-station['latitude'])\n        xdiff = np.where(xdiff == xdiff.min())[0][0]\n        ydiff = np.where(ydiff == ydiff.min())[0][0]\n        data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n    \t\"\"\"\n    \n    inProj = Proj(init='epsg:4326')\n    outProj = Proj(init='epsg:32612')\n    new_x,new_y = transform(inProj,outProj,station['longitude'],station['latitude'])\n    \n    mean_elevation = data.mean().values\n    elevation = data.sel(x=new_x,y=new_y,method='nearest')\n    print(elevation.values)\n    \n    aspect = xrspatial.aspect(data)\n    aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n    aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n    mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n    if mean_aspect < 0:\n        mean_aspect = 360 + mean_aspect\n    print(mean_aspect)\n    aspect = aspect.sel(x=new_x,y=new_y,method='nearest')\n    print(aspect.values)\n    \n    # Positive curvature = upward convex\n    curvature = xrspatial.curvature(data)\n    mean_curvature = curvature.mean().values\n    curvature = curvature.sel(x=new_x,y=new_y,method='nearest')\n    print(curvature.values)\n    \n    slope = xrspatial.slope(data)\n    mean_slope = slope.mean().values\n    slope = slope.sel(x=new_x,y=new_y,method='nearest')\n    print(slope.values)\n    \n    df_station.loc[idx] = [station['longitude'],station['latitude'],\n                           station['elevation_m'],elevation.values,mean_elevation,\n                           aspect.values,mean_aspect,\n                           curvature.values,mean_curvature,\n                           slope.values,mean_slope]\n    \n    if idx % 250 == 0:\n        df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n        df_station.to_csv(stations_outfile)\n\n# Save output data into CSV format\ndf_station.set_index(stations['station_id'][0:idx+1],inplace=True)\ndf_station.to_csv(stations_outfile)\n\n",
  "history_output" : "",
  "history_begin_time" : 1646604599915,
  "history_end_time" : 1646604611732,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "a3sxzgny4h5",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n\n#org_name = 'sentinel1'\n#product_name = 'COPERNICUS/S1_GRD'\n#var_name = 'VV'\n#column_name = 's1_grd_vv'\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n  \t  single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/modis/{column_name}_{current_cell_id}.csv\"\n\n  \t  if os.path.exists(single_csv_file):\n  \t    print(\"exists skipping..\")\n  \t    continue\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate('2013-01-01','2017-12-31')\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df1 = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  viirs2 = ee.ImageCollection(product_name).filterDate('2018-01-01','2021-12-31')\n  \t  poi_reduced_imgs2 = viirs2.map(poi_mean)\n  \t  nested_list2 = poi_reduced_imgs2.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df2 = pd.DataFrame(nested_list2.getInfo(), columns=['date',column_name])\n      \n\n  \t  df = pd.concat([df1, df2])\n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "history_output" : "\nStream closed",
  "history_begin_time" : 1646604600099,
  "history_end_time" : 1646604600136,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "z1e8pbsgf4o",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}_{current_cell_id}.csv\"\n\n      if os.path.exists(single_csv_file):\n          print(\"exists skipping..\")\n          continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      viirs = ee.ImageCollection(product_name).filterDate('2013-01-01','2021-12-31').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "history_output" : "",
  "history_begin_time" : 1646604601010,
  "history_end_time" : 1646604602553,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6t9bge0mh6e",
  "history_input" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\n\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n\nready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\nresult_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\n\nif os.path.exists(result_mapping_file):\n    exit()\n\n\ngridcells = geojson.load(open(gridcells_file))\ntraining_df = pd.read_csv(training_feature_file, header=0)\ntesting_df = pd.read_csv(testing_feature_file, header=0)\nground_measure_metadata_df = pd.read_csv(ground_measure_metadata_file, header=0)\ntrain_labels_df = pd.read_csv(train_labels_file, header=0)\n\nprint(\"training: \", training_df.head())\nprint(\"testing: \", testing_df.head())\nprint(\"ground measure metadata: \", ground_measure_metadata_df.head())\nprint(\"training labels: \", train_labels_df.head())\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1-lat2)**2 + (lon1-lon2)**2)\n  \n# prepare the training data\n\nstation_cell_mapper_df = pd.DataFrame(columns = [\"station_id\", \"cell_id\", \"lat\", \"lon\"])\n\nground_measure_metadata_df = ground_measure_metadata_df.reset_index()  # make sure indexes pair with number of rows\nfor index, row in ground_measure_metadata_df.iterrows():\n  \t\n    print(row['station_id'], row['name'], row['latitude'], row['longitude'])\n    station_lat = row['latitude']\n    station_lon = row['longitude']\n    \n    shortest_dis = 999\n    associated_cell_id = None\n    associated_lat = None\n    associated_lon = None\n    \n    for idx,cell in enumerate(gridcells['features']):\n    \n      current_cell_id = cell['properties']['cell_id']\n\n      #print(\"collecting \", current_cell_id)\n      cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      dist = calculateDistance(station_lat, station_lon, cell_lat, cell_lon)\n\n      if dist < shortest_dis:\n        associated_cell_id = current_cell_id\n        shortest_dis = dist\n        associated_lat = cell_lat\n        associated_lon = cell_lon\n    \n    station_cell_mapper_df.loc[len(station_cell_mapper_df.index)] = [row['station_id'], associated_cell_id, associated_lat, associated_lon]\n    \nprint(station_cell_mapper_df.head())\nstation_cell_mapper_df.to_csv(f\"{ready_for_training_folder}station_cell_mapping.csv\")\n    \n\n\n      \n",
  "history_output" : "/Users/joe\n",
  "history_begin_time" : 1646604591399,
  "history_end_time" : 1646604598978,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "xskryb7jtjp",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nfrom all_dependencies import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\nstart_date = '2022-02-20'\nend_date = '2022-02-27'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\nif os.path.exists(final_csv_file):\n     print(\"exists exiting..\")\n     exit()\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate(start_date, end_date)\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  #df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(final_csv_file)  \n\n\n",
  "history_output" : "",
  "history_begin_time" : 1646604591677,
  "history_end_time" : 1646604591858,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6itb54mis7l",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nfrom all_dependencies import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nstart_date = '2022-02-20'\nend_date = '2022-02-27'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\n\nif os.path.exists(final_csv_file):\n    print(\"exists skipping..\")\n    exit()\n\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      print(\"collecting \", current_cell_id)\n      \n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n\n      viirs = ee.ImageCollection(product_name) \\\n          \t.filterDate(start_date, end_date) \\\n            .filterBounds(poi) \\\n          \t.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n      \t\t.select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nall_cell_df.to_csv(final_csv_file)  \n\n\n",
  "history_output" : "",
  "history_begin_time" : 1646604593363,
  "history_end_time" : 1646604598996,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Done"
}]
