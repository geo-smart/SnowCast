[{
  "history_id" : "s855qglth5u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106430993,
  "history_end_time" : 1677106430993,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "q9gtxx3csnm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106430995,
  "history_end_time" : 1677106430995,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "xml0rd1rv3n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106430998,
  "history_end_time" : 1677106430998,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "utanyr5u0b9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106430999,
  "history_end_time" : 1677106430999,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ep3yfubuq9a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431001,
  "history_end_time" : 1677106431001,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "kon20335aho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431010,
  "history_end_time" : 1677106431010,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "4exwoys5etf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431014,
  "history_end_time" : 1677106431014,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "b81vc4ryyxf",
  "history_input" : "from model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\n\nprint(\"Train Models\")\n\nworm_holes = [RandomForestHole(), XGBoostHole()]\n\nfor hole in worm_holes:\n  hole.preprocessing()\n  print(hole.train_x.shape)\n  print(hole.train_y.shape)\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  \nprint(\"Finished training and validating all the models.\")\n",
  "history_output" : "Train Models\nTraceback (most recent call last):\n  File \"model_train_validate.py\", line 6, in <module>\n    worm_holes = [RandomForestHole(), XGBoostHole()]\n  File \"/home/chetana/gw-workspace/b81vc4ryyxf/base_hole.py\", line 21, in __init__\n    self.classifier = self.get_model()\n  File \"/home/chetana/gw-workspace/b81vc4ryyxf/model_creation_xgboost.py\", line 48, in get_model\n    random_state=123, verbose=0, warm_start=False)\nTypeError: __init__() got an unexpected keyword argument 'ccp_alpha'\n",
  "history_begin_time" : 1677106441077,
  "history_end_time" : 1677106445292,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "0hgsq7anp1i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431024,
  "history_end_time" : 1677106431024,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zi4d66dre0e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431026,
  "history_end_time" : 1677106431026,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zknvw1cyo0h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431028,
  "history_end_time" : 1677106431028,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "kwwli1mz9u6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431030,
  "history_end_time" : 1677106431030,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "a45g53n5nc8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431031,
  "history_end_time" : 1677106431031,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "qhod8c08kyc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431034,
  "history_end_time" : 1677106431034,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "prikpxbya48",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431036,
  "history_end_time" : 1677106431036,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "vbhx909lcvx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431039,
  "history_end_time" : 1677106431039,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "9tqtwq1a1tc",
  "history_input" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\n# exit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = os.path.join(homedir, 'Documents', 'GitHub', 'SnowCast')\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\nfor var in var_list:\n\n    var_name = var\n    column_name = var\n\n    dfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n    if not os.path.exists(dfolder):\n        os.makedirs(dfolder)\n\n    all_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\n    for ind in station_cell_mapper_df.index:\n\n        try:\n\n          current_cell_id = station_cell_mapper_df['cell_id'][ind]\n          print(\"collecting \", current_cell_id)\n          single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n          if os.path.exists(single_csv_file):\n              print(\"exists skipping..\")\n              continue\n\n          longitude = station_cell_mapper_df['lon'][ind]\n          latitude = station_cell_mapper_df['lat'][ind]\n\n          # identify a 500 meter buffer around our Point Of Interest (POI)\n          poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n          viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_name)\n\n          def poi_mean(img):\n              reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n              mean = reducer.get(var_name)\n              return img.set('date', img.date().format()).set(column_name,mean)\n\n\n          poi_reduced_imgs = viirs.map(poi_mean)\n\n          nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n          # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n          df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n          df['date'] = pd.to_datetime(df['date'])\n          df = df.set_index('date')\n\n          df['cell_id'] = current_cell_id\n          df['latitude'] = latitude\n          df['longitude'] = longitude\n          df.to_csv(single_csv_file)\n\n          df_list = [all_cell_df, df]\n          all_cell_df = pd.concat(df_list) # merge into big dataframe\n\n        except Exception as e:\n\n          print(e)\n          pass\n    \n    all_cell_df.to_csv(f\"{dfolder}/{column_name}.csv\")  ",
  "history_output" : "Traceback (most recent call last):\n  File \"data_gee_gridmet_station_only.py\", line 3, in <module>\n    import ee\n  File \"/home/chetana/.local/lib/python3.6/site-packages/ee/__init__.py\", line 16, in <module>\n    from . import batch\n  File \"/home/chetana/.local/lib/python3.6/site-packages/ee/batch.py\", line 15, in <module>\n    from . import _cloud_api_utils\n  File \"/home/chetana/.local/lib/python3.6/site-packages/ee/_cloud_api_utils.py\", line 21, in <module>\n    from googleapiclient import discovery\n  File \"/home/chetana/.local/lib/python3.6/site-packages/googleapiclient/discovery.py\", line 43, in <module>\n    from google.auth.exceptions import MutualTLSChannelError\nImportError: cannot import name 'MutualTLSChannelError'\n",
  "history_begin_time" : 1677106431693,
  "history_end_time" : 1677106434982,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "3klzmvp1x6u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431046,
  "history_end_time" : 1677106431046,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "bzqpqxnb58b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431048,
  "history_end_time" : 1677106431048,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "uqjizh1dh0e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431050,
  "history_end_time" : 1677106431050,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "i66d6hcwg3w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431053,
  "history_end_time" : 1677106431053,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gbeygyk1xeo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431055,
  "history_end_time" : 1677106431055,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "9nk1s6vukha",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431057,
  "history_end_time" : 1677106431057,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "984cef7hbxi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431060,
  "history_end_time" : 1677106431060,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "rhwg4jtksjp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431062,
  "history_end_time" : 1677106431062,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "gg8u0qvb4lj",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "",
  "history_begin_time" : 1677106431833,
  "history_end_time" : 1677106435001,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "wi6e1e44p19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677106431070,
  "history_end_time" : 1677106431070,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
}]
