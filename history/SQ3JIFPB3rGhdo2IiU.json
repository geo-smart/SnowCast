[{
  "history_id" : "17gfihy1u73",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887053,
  "history_end_time" : 1694897887053,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aaida6hh12v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887060,
  "history_end_time" : 1694897887060,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pfka548nzvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887061,
  "history_end_time" : 1694897887061,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vkuoov8rhrr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887062,
  "history_end_time" : 1694897887062,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0kdto7lqwp8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887064,
  "history_end_time" : 1694897887064,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2r19rd0ef22",
  "history_input" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\n\ndef load_model(model_path):\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.rename(columns={'Latitude': 'lat', 'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', 'pr': 'precipitation_amount',\n                         'rmax': 'relative_humidity', 'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                        'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                        'AMSR_SWE': 'SWE',\n                        'AMSR_Flag': 'Flag',\n                        'Elevation': 'elevation',\n                        'Slope': 'slope',\n                        'Aspect': 'aspect',\n                        'Curvature': 'curvature',\n                        'Northness': 'northness',\n                        'Eastness': 'eastness'\n                        }, inplace=True)\n\n    numerical_columns = ['lat', 'lon', 'mean_vapor_pressure_deficit', 'wind_speed', 'precipitation_amount', 'relative_humidity', 'potential_evapotranspiration', 'air_temperature_tmmn', 'air_temperature_tmmx', 'relative_humidity_rmin', 'relative_humidity_rmax', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    #data[numerical_columns] = data[numerical_columns].apply(pd.to_numeric, errors='coerce')\n\n    columns_to_delete = [0, 4, 6, 8, 10, 12, 14, 16, 18, 19]\n    data.drop(data.columns[columns_to_delete], axis=1, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    data = data.reindex(columns=desired_order)\n    print('features:', data.columns)\n    data.dropna(inplace=True)\n    return data\n\ndef predict_swe(model, data):\n    predictions = model.predict(data)\n    print(predictions[:10])\n    data['predicted_swe'] = predictions\n    return data\n\ndef merge_data(original_data, predicted_data):\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    return merged_df\n\ndef save_data(data, file_path):\n    data.to_csv(file_path, index=False)\n\ndef main():\n  height = 666\n  width = 694\n  model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109162144.joblib'\n  new_data_path = f'{work_dir}/testing_all_ready.csv'\n  #new_data_path = f\"{work_dir}/test_predict_sample.csv\"\n  output_path = f'{work_dir}/test_data_predicted.csv'\n\n  model = load_model(model_path)\n  new_data = load_data(new_data_path)\n\n  preprocessed_data = preprocess_data(new_data)\n  print('data preprocessing completed.')\n  print(f'model used: {model_path}')\n  predicted_data = predict_swe(model, preprocessed_data)\n  print('data prediction completed.')\n  final_data = merge_data(new_data, predicted_data)\n\n  save_data(final_data, output_path)\n\n  print(\"Prediction successfully done\")\n\n  if len(final_data) == height * width:\n    print(f\"The image width, height match with the number of rows in the csv. {len(final_data)} rows\")\n  else:\n    raise Exception(\"The total number of rows do not match\")\n\nmain()\n",
  "history_output" : "/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator ExtraTreeRegressor from version 1.2.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n  warnings.warn(\n/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator ExtraTreesRegressor from version 1.2.2 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n  warnings.warn(\ntoday date = 2023-09-16\ntest start date:  2023-06-01\ntest end date:  2023-09-16\n/home/chetana\nfeatures: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109162144.joblib\n[1.857 1.887 1.938 1.894 1.894 1.907 1.88  1.907 1.907 1.908]\ndata prediction completed.\nColumns after merge: Index(['lat', 'lon', 'mean_vapor_pressure_deficit', 'wind_speed',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'potential_evapotranspiration', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'relative_humidity_rmin', 'elevation', 'slope',\n       'aspect', 'curvature', 'northness', 'eastness', 'date', 'SWE', 'Flag',\n       'predicted_swe'],\n      dtype='object')\nPrediction successfully done\nThe image width, height match with the number of rows in the csv. 462204 rows\n",
  "history_begin_time" : 1694901452137,
  "history_end_time" : 1694901474377,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5p06lrzuzhx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887100,
  "history_end_time" : 1694897887100,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uczv8hrmq82",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887102,
  "history_end_time" : 1694897887102,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dssz6afg4hk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887104,
  "history_end_time" : 1694897887104,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s3d9toqkhsl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887105,
  "history_end_time" : 1694897887105,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y4dl8nmck3z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887106,
  "history_end_time" : 1694897887106,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e8r0hvuukdp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887107,
  "history_end_time" : 1694897887107,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ddyj2kqetfo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887108,
  "history_end_time" : 1694897887108,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "803w2dq967e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887109,
  "history_end_time" : 1694897887109,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g27njygl4x8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887110,
  "history_end_time" : 1694897887110,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ilgl51jc0yu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887111,
  "history_end_time" : 1694897887111,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w49289ay56a",
  "history_input" : "import os\nimport pandas as pd\nimport netCDF4 as nc\nimport csv\nfrom datetime import datetime\nfrom snowcast_utils import day_index, work_dir\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n\n\ndem_csv = f\"{work_dir}/dem_all.csv\"\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef get_nc_csv_by_coords_and_variable(nc_file, coordinates, var_name):\n    coordinates = get_coordinates_of_template_tif()\n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    new_lat_data = []\n    new_lon_data = []\n    new_var_data = []\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n\n      # Print the variables and their shapes\n      for variable in variables:\n        shape = nc_file.variables[variable].shape\n        print(f\"Variable: {variable}, Shape: {shape}\")\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      print(\"long var name: \", long_var_name)\n      var_col = nc_file.variables[long_var_name][:]\n      \n      print(f\"latitudes shape: {latitudes.shape}\")\n      print(f\"longitudes shape: {longitudes.shape}\")\n      print(f\"day shape: {day.shape}\")\n      print(f\"val col shape: {var_col.shape}\")\n      \n      #day_index = convert_date_to_1900(test_start_date)\n      #day_index = day[day.shape[0]-1]\n      #day_index = 44998\n      print('day_index:', day_index)\n      \n      for coord in coordinates:\n        lon, lat = coord\n        new_lat_data.append(lat)\n        new_lon_data.append(lon)\n        # Access the variables in the NetCDF file\n        # Find the nearest indices for the given coordinates\n        lon_index = find_nearest_index(longitudes, lon)\n        lat_index = find_nearest_index(latitudes, lat)\n        #day_index = find_nearest_index(day, day[day.shape[0]-1])\n        #print(f\"last day: {day_index}\")\n\n        # Get the value at the specified coordinates\n        the_value = var_col[day.shape[0]-1, lat_index, lon_index]  # Assuming data_variable is a 3D variable (time, lat, lon)\n        if the_value == \"--\":\n          the_value = -9999\n        new_var_data.append(the_value)\n        #print(f\"lon - {lon} lat - {lat} lon-index {lon_index} lat-index {lat_index} day-index {day_index} value - {the_value}\")\n    # Create the DataFrame\n    data = { 'Latitude': new_lat_data, 'Longitude': new_lon_data, var_name: new_var_data}\n    df = pd.DataFrame(data)\n    return df\n\ndef turn_gridmet_nc_to_csv(folder_path, dem_all_csv, testing_all_csv):\n    coordinates = get_coordinates_of_template_tif()\n    current_year = get_current_year()\n    for root, dirs, files in os.walk(folder_path):\n        for file_name in files:\n            var_name = get_var_from_file_name(file_name)\n            print(\"Variable name:\", var_name)\n            res_csv = f\"{work_dir}/testing_output/{str(current_year)}_{var_name}.csv\"\n            if os.path.exists(res_csv):\n                os.remove(res_csv)\n                print(f\"remove old {res_csv}\")\n            \n            if str(current_year) in file_name :\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n                print(\"File Name:\", file_name)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, coordinates, var_name)\n                df.to_csv(res_csv)\n            \ndef merge_all_gridmet_amsr_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv):\n    # List of file paths for the CSV files\n    csv_files = []\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('.csv'):\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8', index_col=False)\n        dfs.append(df)\n\n    dem_df = pd.read_csv(dem_all_csv, encoding='utf-8', index_col=False)\n    dfs.append(dem_df)\n    \n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n    amsr_df = pd.read_csv(f'{work_dir}/testing_ready_amsr.csv', index_col=False)\n    amsr_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n    merged_df = pd.merge(merged_df, amsr_df, on=['Latitude', 'Longitude'])\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input csv files are merged to {testing_all_csv}\")\n    print(merged_df.head())\n\n    \n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = f\"{work_dir}/gridmet_climatology/\"\n    #turn_gridmet_nc_to_csv(gridmet_csv_folder)\n    merge_all_gridmet_amsr_csv_into_one(f\"{work_dir}/testing_output/\",\n                                        f\"{work_dir}/dem_all.csv\",\n                                        f\"{work_dir}/testing_all_ready.csv\")\n\n",
  "history_output" : "/home/chetana/gw-workspace/w49289ay56a/testing_data_integration.py:153: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/w49289ay56a/testing_data_integration.py:153: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/w49289ay56a/testing_data_integration.py:153: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\ntoday date = 2023-09-16\ntest start date:  2023-06-01\ntest end date:  2023-09-16\n/home/chetana\nAll input csv files are merged to /home/chetana/gridmet_test_run/testing_all_ready.csv\n   Unnamed: 0_x  Latitude  Longitude  ...        date  AMSR_SWE AMSR_Flag\n0             0      49.0   -125.000  ...  2023-01-01         0       241\n1             1      49.0   -124.964  ...  2023-01-01         0       241\n2             2      49.0   -124.928  ...  2023-01-01         0       241\n3             3      49.0   -124.892  ...  2023-01-01         0       241\n4             4      49.0   -124.856  ...  2023-01-01         0       241\n[5 rows x 29 columns]\n",
  "history_begin_time" : 1694901437135,
  "history_end_time" : 1694901450256,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "vdjmipdx5kc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887116,
  "history_end_time" : 1694897887116,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mfaogpv6gno",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887117,
  "history_end_time" : 1694897887117,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ptjm9d7pa3y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887118,
  "history_end_time" : 1694897887118,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gcssx027o3b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887119,
  "history_end_time" : 1694897887119,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fep2k4sj164",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887120,
  "history_end_time" : 1694897887120,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tull47ezkik",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887121,
  "history_end_time" : 1694897887121,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9jrqt33jlvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887122,
  "history_end_time" : 1694897887122,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2siy4l8vzvl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887123,
  "history_end_time" : 1694897887123,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vf77kpvxr3o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887124,
  "history_end_time" : 1694897887124,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1xdooorlj05",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887125,
  "history_end_time" : 1694897887125,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c6xgzz6f0s0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887126,
  "history_end_time" : 1694897887126,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ae9hyow29i8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887127,
  "history_end_time" : 1694897887127,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dzkk0pd1vvy",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport warnings\nimport rasterio\nimport csv\nfrom rasterio.transform import Affine\nfrom scipy.ndimage import sobel, gaussian_filter\n\n# Set the warning filter globally to ignore the FutureWarning\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ndef lat_lon_to_pixel(lat, lon, geotransform):\n    x = int((lon - geotransform[0]) / geotransform[1])\n    y = int((lat - geotransform[3]) / geotransform[5])\n    return x, y\n\n\ndef calculate_slope_aspect_for_single(elevation_data, pixel_size_x, pixel_size_y):\n    # Calculate slope using the Sobel operator\n    slope_x = np.gradient(elevation_data, pixel_size_x, axis=1)\n    slope_y = np.gradient(elevation_data, pixel_size_y, axis=0)\n    slope_rad = np.arctan(np.sqrt(slope_x ** 2 + slope_y ** 2))\n    slope_deg = np.degrees(slope_rad)\n\n    # Calculate aspect (direction of the steepest descent)\n    aspect_rad = np.arctan2(slope_y, -slope_x)\n    aspect_deg = (np.degrees(aspect_rad) + 360) % 360\n\n    return slope_deg, aspect_deg\n\n\ndef save_as_geotiff(data, output_file, src_file):\n    with rasterio.open(src_file) as src_dataset:\n        profile = src_dataset.profile\n        transform = src_dataset.transform\n\n        # Update the data type, count, and set the transform for the new dataset\n        profile.update(dtype=rasterio.float32, count=1, transform=transform)\n\n        # Create the new GeoTIFF file\n        with rasterio.open(output_file, 'w', **profile) as dst_dataset:\n            # Write the data to the new GeoTIFF\n            dst_dataset.write(data, 1)\n  \n\ndef calculate_slope_aspect(dem_file):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Calculate the slope and aspect using numpy\n        dx, dy = np.gradient(dem_data, transform[0], transform[4])\n        slope = np.arctan(np.sqrt(dx ** 2 + dy ** 2)) * (180.0 / np.pi)\n        aspect = np.arctan2(-dy, dx) * (180.0 / np.pi)\n\n        # Adjust aspect values to range from 0 to 360 degrees\n        aspect[aspect < 0] += 360\n        print(f\"slope shape: {slope.shape}\")\n        print(f\"aspect shape: {aspect.shape}\")\n        \n        \n    return slope, aspect\n  \ndef calculate_curvature(elevation_data, pixel_size_x, pixel_size_y):\n    # Calculate curvature using the Laplacian operator\n    curvature_x = np.gradient(np.gradient(elevation_data, pixel_size_x, axis=1), pixel_size_x, axis=1)\n    curvature_y = np.gradient(np.gradient(elevation_data, pixel_size_y, axis=0), pixel_size_y, axis=0)\n    curvature = curvature_x + curvature_y\n\n    return curvature\n\n  \ndef calculate_curvature(dem_file, sigma=1):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradient using the Sobel filter\n        dx = sobel(dem_data, axis=1, mode='constant')\n        dy = sobel(dem_data, axis=0, mode='constant')\n\n        # Calculate the second derivatives using the Sobel filter\n        dxx = sobel(dx, axis=1, mode='constant')\n        dyy = sobel(dy, axis=0, mode='constant')\n\n        # Calculate the curvature using the second derivatives\n        curvature = dxx + dyy\n\n        # Smooth the curvature using Gaussian filtering (optional)\n        curvature = gaussian_filter(curvature, sigma)\n\n    return curvature\n  \ndef calculate_gradients(dem_file):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradients along the North and East directions\n        dy, dx = np.gradient(dem_data, dataset.res[0], dataset.res[1])\n\n        # Calculate the Northness and Eastness\n        northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n        eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\n\n    return northness, eastness\n  \n  \ndef geotiff_to_csv(geotiff_file, csv_file, column_name):\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_file) as dataset:\n        # Get the pixel values as a 2D array\n        data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Get the width and height of the GeoTIFF\n        height, width = data.shape\n\n        # Open the CSV file for writing\n        with open(csv_file, 'w', newline='') as csvfile:\n            csvwriter = csv.writer(csvfile)\n\n            # Write the CSV header\n            csvwriter.writerow(['Latitude', 'Longitude', 'x', 'y', column_name])\n\n            # Loop through each pixel and extract latitude, longitude, and image value\n            for y in range(height):\n                for x in range(width):\n                    # Get the pixel value\n                    image_value = data[y, x]\n\n                    # Convert pixel coordinates to geographic coordinates\n                    lon, lat = transform * (x, y)\n\n                    # Write the data to the CSV file\n                    csvwriter.writerow([lat, lon, x, y, image_value])\n\n  \ndef read_elevation_data(file_path, result_dem_csv_path, result_dem_feature_csv_path):\n    neighborhood_size=4\n    df = pd.read_csv(file_path)\n    \n    dataset = rasterio.open(geotiff_file)\n    data = dataset.read(1)\n\n    # Get the width and height of the GeoTIFF\n    height, width = data.shape\n    \n    # Create an empty DataFrame with column names\n    columns = ['lat', 'lon', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    all_df = pd.DataFrame(columns=columns)\n    \n    all_df.to_csv(result_dem_feature_csv_path)\n    print(f\"DEM and other columns are saved to file {result_dem_feature_csv_path}\")\n    return all_df\n\n\n  \n# Usage example:\nresult_dem_csv_path = \"/home/chetana/gridmet_test_run/dem_template.csv\"\nresult_dem_feature_csv_path = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\n\ndem_file = \"/home/chetana/gridmet_test_run/dem_file.tif\"\nslope_file = '/home/chetana/gridmet_test_run/slope_file.tif'\naspect_file = '/home/chetana/gridmet_test_run/aspect_file.tif'\ncurvature_file = '/home/chetana/gridmet_test_run/curvature_file.tif'\nnorthness_file = '/home/chetana/gridmet_test_run/northness_file.tif'\neastness_file = '/home/chetana/gridmet_test_run/eastness_file.tif'\n\n\nslope, aspect = calculate_slope_aspect(dem_file)\ncurvature = calculate_curvature(dem_file)\nnorthness, eastness = calculate_gradients(dem_file)\n\n# Save the slope and aspect as new GeoTIFF files\nsave_as_geotiff(slope, slope_file, dem_file)\nsave_as_geotiff(aspect, aspect_file, dem_file)\nsave_as_geotiff(curvature, curvature_file, dem_file)\nsave_as_geotiff(northness, northness_file, dem_file)\nsave_as_geotiff(eastness, eastness_file, dem_file)\n\ngeotiff_to_csv(dem_file, dem_file+\".csv\", \"Elevation\")\ngeotiff_to_csv(slope_file, slope_file+\".csv\", \"Slope\")\ngeotiff_to_csv(aspect_file, aspect_file+\".csv\", \"Aspect\")\ngeotiff_to_csv(curvature_file, curvature_file+\".csv\", \"Curvature\")\ngeotiff_to_csv(northness_file, northness_file+\".csv\", \"Northness\")\ngeotiff_to_csv(eastness_file, eastness_file+\".csv\", \"Eastness\")\n\n# List of file paths for the CSV files\ncsv_files = [dem_file+\".csv\", slope_file+\".csv\", aspect_file+\".csv\", \n             curvature_file+\".csv\", northness_file+\".csv\", eastness_file+\".csv\"]\n\n# Initialize an empty list to store all dataframes\ndfs = []\n\n# Read each CSV file into separate dataframes\nfor file in csv_files:\n    df = pd.read_csv(file, encoding='utf-8')\n    dfs.append(df)\n\n# Merge the dataframes based on the latitude and longitude columns\nmerged_df = dfs[0]  # Start with the first dataframe\nfor i in range(1, len(dfs)):\n    merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude', 'x', 'y'])\n\n# Save the merged dataframe to a new CSV file\nmerged_df.to_csv(result_dem_feature_csv_path, index=False)\n",
  "history_output" : "/home/chetana/gw-workspace/dzkk0pd1vvy/western_us_dem.py:107: RuntimeWarning: invalid value encountered in true_divide\n  northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n/home/chetana/gw-workspace/dzkk0pd1vvy/western_us_dem.py:108: RuntimeWarning: invalid value encountered in true_divide\n  eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\nslope shape: (666, 694)\naspect shape: (666, 694)\n",
  "history_begin_time" : 1694897888115,
  "history_end_time" : 1694897919598,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "hvhonoq8gpm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887134,
  "history_end_time" : 1694897887134,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kve0a1lkqm1",
  "history_input" : "import os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, work_dir\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n#year_list = [get_current_year()]\nselected_yr = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n\n\nyear_list = [selected_yr.year]\n\ndef remove_files_in_folder(folder_path):\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\n\ndef download_gridmet_of_specific_variables():\n    # make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\nfolder_name = f'{work_dir}/gridmet_climatology'\nif not os.path.exists(folder_name):\n    os.makedirs(folder_name)\nremove_files_in_folder(folder_name)\ndownload_gridmet_of_specific_variables()\n\n",
  "history_output" : "",
  "history_begin_time" : 1694897888090,
  "history_end_time" : 1694897919621,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "wont9r6ukil",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887139,
  "history_end_time" : 1694897887139,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iy5gz7x9xh8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887140,
  "history_end_time" : 1694897887140,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5065bawxe44",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport uuid\nfrom snowcast_utils import day_index\nimport matplotlib.colors as mcolors\nfrom matplotlib.patches import Patch\n\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\ncmap_name = 'custom_snow_colormap'\ncustom_cmap = mcolors.ListedColormap(colors)\n\n# Define the lat_lon_to_map_coordinates function\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    x, y = m(lon, lat)\n    return x, y\n\n# Define your value ranges for color mapping\nvalue_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n\ndef convert_csvs_to_images():\n    data = pd.read_csv(\"/home/chetana/gridmet_test_run/test_data_predicted.csv\")\n    data['predicted_swe'].fillna(0, inplace=True)\n    data['predicted_swe'] = data['predicted_swe'].apply(lambda x: 0 if x < 2 else x)\n    print(data[:5])\n\n    lon_min, lon_max = -125, -100\n    lat_min, lat_max = 25, 49.5\n\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    x, y = m(data['lon'].values, data['lat'].values)\n\n    # Create a figure with a white background\n    fig = plt.figure(facecolor='white')\n\n    # Define your custom function to map data values to colors\n    def map_value_to_color(value):\n        # Iterate through the value ranges to find the appropriate color index\n        for i, range_max in enumerate(value_ranges):\n            if value <= range_max:\n                return colors[i]\n        \n        # If the value is greater than the largest range, return the last color\n        return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n    color_mapping = [map_value_to_color(value) for value in data['predicted_swe']]\n    \n    # Plot the data using the custom colormap\n    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n\n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_date = datetime(1900, 1, 1)\n    day_value = day_index\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n    \n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}°W\" if lon < 0 else f\"{lon:.1f}°E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    # Create custom legend elements using the same colormap\n    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n\n    # Create the legend outside the map\n    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n    legend.set_bbox_to_anchor((1.01, 1)) \n\n    # Remove the color bar\n    plt.colorbar().remove()\n\n    plt.text(0.98, 0.02, 'Copyright © SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Set the aspect ratio to 'equal' to keep the plot at the center\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n    # Show the plot or save it to a file\n    plt.savefig(f'/home/chetana/gridmet_test_run/predicted_swe-{timestamp_string}-{uuid.uuid4().hex}.png')\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\nconvert_csvs_to_images()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/5065bawxe44/convert_results_to_images.py\", line 3, in <module>\n    from mpl_toolkits.basemap import Basemap\nModuleNotFoundError: No module named 'mpl_toolkits.basemap'\n",
  "history_begin_time" : 1694901476272,
  "history_end_time" : 1694901478889,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "o1cy22ikpuo",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit(1)  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/o1cy22ikpuo/service_prediction.py\", line 17, in <module>\n    import geopandas as gpd\nModuleNotFoundError: No module named 'geopandas'\n",
  "history_begin_time" : 1694901480397,
  "history_end_time" : 1694901489001,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "w8ebe5677m3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887147,
  "history_end_time" : 1694897887147,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8ark2qqcdpk",
  "history_input" : "import h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\n\nwestern_us_coords = '/home/chetana/gridmet_test_run/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n    #print(lat_idx, lon_idx)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n  \ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Query the KD-Tree to find the nearest point\n    distance, index = tree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\nif __name__ == \"__main__\":\n    df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE', 'AMSR_Flag'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    cmd = f\"curl --output {work_dir}/amsr_testing/testing_amsr_file_2.he5 -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n    print(f'running command {cmd}')\n    #subprocess.run(cmd, shell=True)\n    file = h5py.File(f'{work_dir}/amsr_testing/testing_amsr_file.he5', 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n  \n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n    \n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        df.loc[len(df.index)] = [date,\n                                 target_lat, target_lon,\n                                 closest_swe, closest_flag]\n    df.to_csv(f'{work_dir}/testing_ready_amsr_2.csv', index=False)\n  \n    print('completed amsr testing data collection.')\n",
  "history_output" : "today date = 2023-09-16\ntest start date:  2023-06-01\ntest end date:  2023-09-16\n/home/chetana\nrunning command curl --output /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_file_2.he5 -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2023.06.01/AMSR_U2_L3_DailySnow_B02_20230601.he5\ncompleted amsr testing data collection.\n",
  "history_begin_time" : 1694897888050,
  "history_end_time" : 1694901435275,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "sz0nhokgifq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887152,
  "history_end_time" : 1694897887152,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pkhi09xyjvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887153,
  "history_end_time" : 1694897887153,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "68kkcsza87z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887154,
  "history_end_time" : 1694897887154,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0e1vx0ofokp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887155,
  "history_end_time" : 1694897887155,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ta50tdf6l7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887156,
  "history_end_time" : 1694897887156,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Skipped"
}]
