[{
  "history_id" : "azur07tomeb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741721,
  "history_end_time" : 1736926334409,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "czxrh8ah6ho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741724,
  "history_end_time" : 1736926334411,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "i6pvrdjn57o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741740,
  "history_end_time" : 1736926334411,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6ig5oelv9ez",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741743,
  "history_end_time" : 1736926334412,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6c6umhcad7t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741748,
  "history_end_time" : 1736926334413,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "itw0zjvqnx8",
  "history_input" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, model_dir, plot_dir, output_dir, month_to_season, test_start_date, test_end_date\nimport os\nimport random\nimport string\nimport shutil\nfrom model_creation_et import selected_columns\nfrom datetime import datetime, timedelta\n# from interpret_model_results import explain_predictions\n\nimport shap\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport traceback\n\ndef generate_random_string(length):\n    # Define the characters that can be used in the random string\n    characters = string.ascii_letters + string.digits  # You can customize this to include other characters if needed\n\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n\n    return random_string\n  \n\ndef load_model(model_path):\n    \"\"\"\n    Load a machine learning model from a file.\n\n    Args:\n        model_path (str): Path to the saved model file.\n\n    Returns:\n        model: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file containing the data.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the loaded data.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data, is_model_input: bool = True):\n    \"\"\"\n    Preprocess the input data for model prediction.\n\n    Args:\n        data (pd.DataFrame): Input data in the form of a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: Preprocessed data ready for prediction.\n    \"\"\"\n    \n    #print(\"check date format: \", data.head())\n    #data['date'] = data['date'].dt.strftime('%j').astype(int)\n    #data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    \n    #data = data.apply(pd.to_numeric, errors='coerce')\n\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n#                          'Elevation': 'elevation',\n#                          'Slope': 'Slope',\n#                          'Aspect': 'Aspect',\n#                          'Curvature': 'Curvature',\n#                          'Northness': 'Northness',\n#                          'Eastness': 'Eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n#                          'relative_humidity_rmin': '',\n#                          'cumulative_rmin',\n#                          'mean_vapor_pressure_deficit', \n#                          'cumulative_vpd', \n#                          'wind_speed',\n#                          'cumulative_vs', \n#                          'relative_humidity_rmax', 'cumulative_rmax',\n\n# 'precipitation_amount', 'cumulative_pr', 'air_temperature_tmmx',\n\n# 'cumulative_tmmx', 'potential_evapotranspiration', 'cumulative_etr',\n\n# 'air_temperature_tmmn', 'cumulative_tmmn', 'x', 'y', 'elevation',\n\n# 'slope', 'aspect', 'curvature', 'northness', 'eastness', 'AMSR_SWE',\n\n# 'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag',\n                        }, inplace=True)\n\n    print(data.head())\n    print(data.columns)\n    \n    # filter out three days for final visualization to accelerate the process\n    #dates_to_match = ['2018-03-15', '2018-04-15', '2018-05-15']\n    #mask = data['date'].dt.strftime('%Y-%m-%d').isin(dates_to_match)\n    # Filter the DataFrame based on the mask\n    #data = data[mask]\n    if is_model_input:\n        data['date'] = pd.to_datetime(data['date'])\n        if \"swe_value\" in selected_columns:\n            selected_columns.remove(\"swe_value\")\n        desired_order = selected_columns + ['lat', 'lon',]\n        \n        data = data[desired_order]\n        data = data.reindex(columns=desired_order)\n        \n        print(\"reorganized columns: \", data.columns)\n    \n    return data\n\n\ndef explain_predictions(model, input_data, feature_names, output_csv, output_plots_dir):\n    \"\"\"\n    Explains predictions using SHAP, saves the explanations into a CSV file,\n    and generates SHAP plots for each row.\n\n    Parameters:\n    - model: Trained tree-based model (e.g., RandomForest, LightGBM, XGBoost)\n    - input_data: Input data as a numpy array or pandas DataFrame\n    - feature_names: List of feature names\n    - output_csv: Path to save the explanation report as a CSV\n    - output_plots_dir: Directory to save the SHAP plots\n    \"\"\"\n    print(\"Starting the explanation process...\")\n    \n    # Ensure input_data is a DataFrame\n    if not isinstance(input_data, pd.DataFrame):\n        print(\"Converting input data to a DataFrame...\")\n        input_data = pd.DataFrame(input_data, columns=feature_names)\n\n    # Choose 100 random samples from input_data\n    print(\"Selecting 100 random samples for SHAP explanations...\")\n    sampled_indices = random.sample(range(len(input_data)), min(1, len(input_data)))\n    input_data_sampled = input_data.iloc[sampled_indices] if isinstance(input_data, pd.DataFrame) else input_data[sampled_indices]\n\n    print(\"Pick sample: \", input_data_sampled)\n\n    print(\"Initializing SHAP explainer...\")\n    explainer = shap.TreeExplainer(model)\n\n    # Calculate SHAP values for the sampled data\n    print(f\"Calculating SHAP values for {len(input_data_sampled)} samples...\")\n    shap_values = explainer.shap_values(input_data_sampled)\n\n    # Store explanations\n    explanations = []\n\n    print(f\"Processing {len(input_data)} rows...\")\n    for i in range(len(input_data_sampled)):\n        print(f\"Explaining row {i + 1}/{len(input_data_sampled)}...\")\n        \n        # Get SHAP values for the current row\n        shap_row = shap_values[i]\n        prediction = model.predict([input_data_sampled.iloc[i]])[0]\n\n        # Determine the most important feature\n        top_feature_idx = abs(shap_row).argmax()\n        top_feature = feature_names[top_feature_idx]\n        top_contribution = shap_row[top_feature_idx]\n\n        # Append explanation to the list\n        explanations.append({\n            \"Row\": i,\n            \"Prediction\": prediction,\n            \"Most_Important_Feature\": top_feature,\n            \"Feature_Contribution\": top_contribution\n        })\n\n        # Plot and save the SHAP summary plot for the current row\n        plot_path = f\"{plot_dir}/shap_row_{i}.png\"\n        try:\n            print(f\"Generating plot for row {i + 1}...\")\n            plt.figure()\n            shap.waterfall_plot(shap.Explanation(values=shap_row, \n                                                 base_values=explainer.expected_value, \n                                                 data=input_data_sampled.iloc[i].values,\n                                                 feature_names=feature_names))\n            plt.title(f\"SHAP Explanation for Row {i}\")\n            plt.savefig(plot_path)\n            plt.close()\n            print(f\"Plot saved: {plot_path}\")\n        except Exception as e:\n            print(f\"Failed to generate plot for row {i + 1}: {e}\")\n\n    # Save explanations to a CSV file\n    explanations_df = pd.DataFrame(explanations)\n    try:\n        print(f\"Saving explanations to {output_csv}...\")\n        explanations_df.to_csv(output_csv, index=False)\n        print(f\"Explanations successfully saved to {output_csv}\")\n    except Exception as e:\n        print(f\"Failed to save explanations CSV: {e}\")\n\n    print(f\"Process completed. Explanations saved to {output_csv}. Plots saved in {output_plots_dir}.\")\n\n\ndef predict_swe(model, data):\n    \"\"\"\n    Predict Snow Water Equivalent (SWE) values using a pre-trained model.\n\n    This function takes in a machine learning model and a DataFrame containing \n    meteorological and geospatial data, preprocesses the data by handling missing \n    values and dropping unnecessary columns, and applies the model to predict SWE values. \n    The predicted SWE values are then added to the original DataFrame as a new column \n    called 'predicted_swe'.\n\n    Args:\n        model (object): A pre-trained machine learning model with a `predict` method.\n        data (pd.DataFrame): A pandas DataFrame containing input data for prediction.\n            It is expected to have columns including 'lat', 'lon', and other relevant \n            features for the model.\n\n    Returns:\n        pd.DataFrame: The original DataFrame with an additional column 'predicted_swe' \n        containing the predicted SWE values.\n    \"\"\"\n    data = data.fillna(-1)\n    input_data = data\n    input_data = data.drop([\"lat\", \"lon\"], axis=1)\n\n    print(\"Assign -1 to fsca column..\")\n    input_data.loc[input_data['fsca'] > 100, 'fsca'] = -1 \n    #input_data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin',], axis=1)\n    #scaler = StandardScaler()\n\n    # Fit the scaler on the training data and transform both training and testing data\n    #input_data_scaled = scaler.fit_transform(input_data)\n    print(\"Start to predict\", input_data.shape)\n    predictions = model.predict(input_data)\n    data['predicted_swe'] = predictions\n\n    print(\"Explain the prediction: \")\n    explain_predictions(model, input_data, input_data.columns, f\"{output_dir}/explain_ai.csv\", f\"{plot_dir}\")\n    return data\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge predicted SWE data with the original data.\n\n    Args:\n        original_data (pd.DataFrame): Original input data.\n        predicted_data (pd.DataFrame): Dataframe with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged dataframe.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    if \"date\" not in predicted_data:\n    \tpredicted_data[\"date\"] = test_start_date\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    print(\"original_data.columns: \", original_data.columns)\n    print(\"new_data_extracted.columns: \", new_data_extracted.columns)\n    print(\"new prediction statistics: \", new_data_extracted[\"predicted_swe\"].describe())\n    #merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"first merged df: \", merged_df.columns)\n\n    merged_df.loc[merged_df['fsca'] == 237, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['fsca'] == 239, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['fsca'] == 225, 'predicted_swe'] = 0\n    #merged_df.loc[merged_df['cumulative_fsca'] == 0, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['fsca'] == 0, 'predicted_swe'] = 0\n    \n    merged_df.loc[merged_df['air_temperature_tmmx'].isnull(), \n                  'predicted_swe'] = 0\n\n    merged_df.loc[merged_df['lc_prop3'] == 3, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['lc_prop3'] == 255, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['lc_prop3'] == 27, 'predicted_swe'] = 0\n\n    return merged_df\n\ndef predict(target_date: str = test_start_date, output_path: str=None):\n    \"\"\"\n    Main function for predicting snow water equivalent (SWE).\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'{model_dir}/wormhole_ETHole_latest.joblib'\n    print(f\"Using model: {model_path}\")\n  \n    new_data_path = f'{work_dir}/testing_all_ready_{target_date}.csv'\n    #output_path = f'{work_dir}/test_data_predicted_three_days_only.csv'\n    if output_path is None:\n        output_path = f'{output_dir}/test_data_predicted_latest_{target_date}.csv'\n  \n    if os.path.exists(output_path):\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    print(f\"loading {new_data_path}\")\n    new_data = load_data(new_data_path)\n    new_data = new_data.drop([\"date.1\",], axis=1)\n    print(\"new_data.columns: \", new_data.columns)\n\n    preprocessed_data = preprocess_data(new_data, is_model_input=True)\n    if len(new_data) < len(preprocessed_data):\n      raise ValueError(\"Why the preprocessed data increased?\")\n    #print('Data preprocessing completed.', preprocessed_data.head())\n    #print(f'Model used: {model_path}')\n    predicted_data = predict_swe(model, preprocessed_data)\n    print(\"how many predicted? \", len(predicted_data))\n\n    if \"date\" not in preprocessed_data:\n    \tpreprocessed_data[\"date\"] = test_start_date\n\n    full_preprocessed_data = preprocess_data(new_data, is_model_input=False)\n    predicted_data = merge_data(full_preprocessed_data, predicted_data)\n    \n    predicted_data.to_csv(output_path, index=False)\n    print(\"Prediction successfully done \", output_path)\n\n\ndef do_all_days():\n    # Convert string dates to datetime objects\n    start_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    end_date = datetime.strptime(test_end_date, \"%Y-%m-%d\")\n\n    # Loop over each day in the date range and call the function\n    current_date = start_date\n    while current_date <= end_date:\n        try:\n            # Prepare the cumulative history CSVs for the current date\n            print(\">>>>>\\nPredicing SWE for day\", current_date.strftime(\"%Y-%m-%d\"))\n            current_date_str = current_date.strftime(\"%Y-%m-%d\")\n\n            predict(target_date = current_date_str)\n        except Exception as e:\n            # Log the error and continue with the next day\n            print(f\"Error processing {current_date.strftime('%Y-%m-%d')}: {str(e)}. Skipping this day.\")\n            print(\"Detailed traceback:\")\n            traceback.print_exc()  # Prints the full traceback to help debug the issue\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n\n\nif __name__ == \"__main__\":\n\tdo_all_days()\n",
  "history_output" : "/home/chetana\ntoday date = 2025-01-15\nStart day: 2025-01-05, End day: 2025-01-12\ntest start date:  2025-01-05\ntest end date:  2025-01-12\n>>>>>\nPredicing SWE for day 2025-01-05\nUsing model: /home/chetana/models//wormhole_ETHole_latest.joblib\nFile '/home/chetana/data/output/test_data_predicted_latest_2025-01-05.csv' has been removed.\nloading /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-05.csv\nnew_data.columns:  Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin', 'tmmx',\n       'tmmn', 'vpd', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object')\n    lat      lon  precipitation_amount  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000                   0.0  ...   239         3        2025\n1  49.0 -124.964                   0.0  ...   239         3        2025\n2  49.0 -124.928                   0.0  ...   239         3        2025\n3  49.0 -124.892                   0.0  ...   250        10        2025\n4  49.0 -124.856                   0.0  ...   250        10        2025\n[5 rows x 24 columns]\nIndex(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'potential_evapotranspiration', 'wind_speed', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'mean_vapor_pressure_deficit', 'x', 'y', 'Elevation', 'Slope', 'Aspect',\n       'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object')\nreorganized columns:  Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness', 'lat', 'lon'],\n      dtype='object')\nAssign -1 to fsca column..\nStart to predict (462204, 12)\nExplain the prediction: \nStarting the explanation process...\nSelecting 100 random samples for SHAP explanations...\nPick sample:         SWE  fsca  air_temperature_tmmx  ...     Aspect  Eastness  Northness\n56200    0    -1                 260.0  ...  260.31232  0.778217  -0.166716\n[1 rows x 12 columns]\nInitializing SHAP explainer...\nCalculating SHAP values for 1 samples...\nProcessing 462204 rows...\nExplaining row 1/1...\nX does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\nGenerating plot for row 1...\nFailed to generate plot for row 1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.\nSaving explanations to /home/chetana/data/output/explain_ai.csv...\nExplanations successfully saved to /home/chetana/data/output/explain_ai.csv\nProcess completed. Explanations saved to /home/chetana/data/output/explain_ai.csv. Plots saved in /home/chetana/plots/.\nhow many predicted?  462204\n    lat      lon  precipitation_amount  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000                   0.0  ...   239         3        2025\n1  49.0 -124.964                   0.0  ...   239         3        2025\n2  49.0 -124.928                   0.0  ...   239         3        2025\n3  49.0 -124.892                   0.0  ...   250        10        2025\n4  49.0 -124.856                   0.0  ...   250        10        2025\n[5 rows x 24 columns]\nIndex(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'potential_evapotranspiration', 'wind_speed', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'mean_vapor_pressure_deficit', 'x', 'y', 'Elevation', 'Slope', 'Aspect',\n       'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object')\noriginal_data.columns:  Index(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'potential_evapotranspiration', 'wind_speed', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'mean_vapor_pressure_deficit', 'x', 'y', 'Elevation', 'Slope', 'Aspect',\n       'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object')\nnew_data_extracted.columns:  Index(['date', 'lat', 'lon', 'predicted_swe'], dtype='object')\nnew prediction statistics:  count    462204.000000\nmean          0.664095\nstd           1.402245\nmin           0.000000\n25%           0.050000\n50%           0.351000\n75%           0.507000\nmax          31.287000\nName: predicted_swe, dtype: float64\nfirst merged df:  Index(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'potential_evapotranspiration', 'wind_speed', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'mean_vapor_pressure_deficit', 'x', 'y', 'Elevation', 'Slope', 'Aspect',\n       'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date_x', 'fsca',\n       'lc_prop3', 'water_year', 'date_y', 'predicted_swe'],\n      dtype='object')\ninvalid value encountered in cast\nPrediction successfully done  /home/chetana/data/output/test_data_predicted_latest_2025-01-05.csv\n>>>>>\nPredicing SWE for day 2025-01-06\nUsing model: /home/chetana/models//wormhole_ETHole_latest.joblib\nFile '/home/chetana/data/output/test_data_predicted_latest_2025-01-06.csv' has been removed.\nloading /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-06.csv\nnew_data.columns:  Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin', 'tmmx',\n       'vpd', 'etr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object')\n    lat      lon  precipitation_amount  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000                   0.0  ...   239         3        2025\n1  49.0 -124.964                   0.0  ...   239         3        2025\n2  49.0 -124.928                   0.0  ...   239         3        2025\n3  49.0 -124.892                   0.0  ...   250        10        2025\n4  49.0 -124.856                   0.0  ...   250        10        2025\n[5 rows x 24 columns]\nIndex(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'wind_speed', 'air_temperature_tmmn', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       'potential_evapotranspiration', 'x', 'y', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nreorganized columns:  Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness', 'lat', 'lon'],\n      dtype='object')\nAssign -1 to fsca column..\nStart to predict (462204, 12)\nExplain the prediction: \nStarting the explanation process...\nSelecting 100 random samples for SHAP explanations...\nPick sample:          SWE  fsca  air_temperature_tmmx  ...    Aspect  Eastness  Northness\n128101    0    -1                 268.6  ...  66.27427 -0.741303   0.382538\n[1 rows x 12 columns]\nInitializing SHAP explainer...\nCalculating SHAP values for 1 samples...\nProcessing 462204 rows...\nExplaining row 1/1...\nX does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\nGenerating plot for row 1...\nFailed to generate plot for row 1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (7,) + inhomogeneous part.\nSaving explanations to /home/chetana/data/output/explain_ai.csv...\nExplanations successfully saved to /home/chetana/data/output/explain_ai.csv\nProcess completed. Explanations saved to /home/chetana/data/output/explain_ai.csv. Plots saved in /home/chetana/plots/.\nhow many predicted?  462204\n    lat      lon  precipitation_amount  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000                   0.0  ...   239         3        2025\n1  49.0 -124.964                   0.0  ...   239         3        2025\n2  49.0 -124.928                   0.0  ...   239         3        2025\n3  49.0 -124.892                   0.0  ...   250        10        2025\n4  49.0 -124.856                   0.0  ...   250        10        2025\n[5 rows x 24 columns]\nIndex(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'wind_speed', 'air_temperature_tmmn', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       'potential_evapotranspiration', 'x', 'y', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\noriginal_data.columns:  Index(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'wind_speed', 'air_temperature_tmmn', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       'potential_evapotranspiration', 'x', 'y', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nnew_data_extracted.columns:  Index(['date', 'lat', 'lon', 'predicted_swe'], dtype='object')\nnew prediction statistics:  count    462204.000000\nmean          0.835338\nstd           1.820280\nmin           0.000000\n25%           0.049000\n50%           0.400000\n75%           0.566000\nmax          38.701000\nName: predicted_swe, dtype: float64\nfirst merged df:  Index(['lat', 'lon', 'precipitation_amount', 'relative_humidity_rmax',\n       'wind_speed', 'air_temperature_tmmn', 'relative_humidity_rmin',\n       'air_temperature_tmmx', 'mean_vapor_pressure_deficit',\n       'potential_evapotranspiration', 'x', 'y', 'Elevation', 'Slope',\n       'Aspect', 'Curvature', 'Northness', 'Eastness', 'SWE', 'Flag', 'date_x',\n       'fsca', 'lc_prop3', 'water_year', 'date_y', 'predicted_swe'],\n      dtype='object')\ninvalid value encountered in cast\nPrediction successfully done  /home/chetana/data/output/test_data_predicted_latest_2025-01-06.csv\n>>>>>\nPredicing SWE for day 2025-01-07\nUsing model: /home/chetana/models//wormhole_ETHole_latest.joblib\nFile '/home/chetana/data/output/test_data_predicted_latest_2025-01-07.csv' has been removed.\nloading /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-07.csv\nnew_data.columns:  Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr', 'rmax',\n       'rmin', 'vpd', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object')\n    lat      lon  wind_speed  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000         0.0  ...   239         3        2025\n1  49.0 -124.964         0.0  ...   239         3        2025\n2  49.0 -124.928         0.0  ...   239         3        2025\n3  49.0 -124.892         0.0  ...   250        10        2025\n4  49.0 -124.856         0.0  ...   250        10        2025\n[5 rows x 24 columns]\nIndex(['lat', 'lon', 'wind_speed', 'precipitation_amount',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'x', 'y',\n       'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n       'SWE', 'Flag', 'date', 'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nreorganized columns:  Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness', 'lat', 'lon'],\n      dtype='object')\nAssign -1 to fsca column..\nStart to predict (462204, 12)\nExplain the prediction: \nStarting the explanation process...\nSelecting 100 random samples for SHAP explanations...\nPick sample:         SWE  fsca  air_temperature_tmmx  ...  Aspect  Eastness  Northness\n72179  254    -1                   0.0  ...    -0.0      -1.0       -1.0\n[1 rows x 12 columns]\nInitializing SHAP explainer...\nCalculating SHAP values for 1 samples...\nProcessing 462204 rows...\nExplaining row 1/1...\nX does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\nGenerating plot for row 1...\nFailed to generate plot for row 1: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (6,) + inhomogeneous part.\nSaving explanations to /home/chetana/data/output/explain_ai.csv...\nExplanations successfully saved to /home/chetana/data/output/explain_ai.csv\nProcess completed. Explanations saved to /home/chetana/data/output/explain_ai.csv. Plots saved in /home/chetana/plots/.\nhow many predicted?  462204\n    lat      lon  wind_speed  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000         0.0  ...   239         3        2025\n1  49.0 -124.964         0.0  ...   239         3        2025\n2  49.0 -124.928         0.0  ...   239         3        2025\n3  49.0 -124.892         0.0  ...   250        10        2025\n4  49.0 -124.856         0.0  ...   250        10        2025\n[5 rows x 24 columns]\nIndex(['lat', 'lon', 'wind_speed', 'precipitation_amount',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'x', 'y',\n       'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n       'SWE', 'Flag', 'date', 'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\noriginal_data.columns:  Index(['lat', 'lon', 'wind_speed', 'precipitation_amount',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'x', 'y',\n       'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n       'SWE', 'Flag', 'date', 'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nnew_data_extracted.columns:  Index(['date', 'lat', 'lon', 'predicted_swe'], dtype='object')\nnew prediction statistics:  count    462204.000000\nmean          1.118665\nstd           2.348772\nmin           0.000000\n25%           0.069000\n50%           0.476000\n75%           0.685000\nmax          38.501000\nName: predicted_swe, dtype: float64\nfirst merged df:  Index(['lat', 'lon', 'wind_speed', 'precipitation_amount',\n       'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'mean_vapor_pressure_deficit', 'x', 'y',\n       'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness',\n       'SWE', 'Flag', 'date_x', 'fsca', 'lc_prop3', 'water_year', 'date_y',\n       'predicted_swe'],\n      dtype='object')\ninvalid value encountered in cast\nPrediction successfully done  /home/chetana/data/output/test_data_predicted_latest_2025-01-07.csv\n>>>>>\nPredicing SWE for day 2025-01-08\nUsing model: /home/chetana/models//wormhole_ETHole_latest.joblib\nFile '/home/chetana/data/output/test_data_predicted_latest_2025-01-08.csv' has been removed.\nloading /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-08.csv\nnew_data.columns:  Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs', 'pr',\n       'etr', 'rmax',\n       ...\n       'AMSR_Flag_2025-01-06', 'AMSR_SWE_2025-01-07', 'AMSR_Flag_2025-01-07',\n       'AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_SWE', 'date', 'fsca',\n       'lc_prop3', 'water_year'],\n      dtype='object', length=223)\n    lat      lon  air_temperature_tmmx  ...  fsca  lc_prop3  water_year\n0  49.0 -125.000                   0.0  ...   239         3        2025\n1  49.0 -124.964                   0.0  ...   239         3        2025\n2  49.0 -124.928                   0.0  ...   239         3        2025\n3  49.0 -124.892                   0.0  ...     0        10        2025\n4  49.0 -124.856                   0.0  ...   250        10        2025\n[5 rows x 223 columns]\nIndex(['lat', 'lon', 'air_temperature_tmmx', 'relative_humidity_rmin',\n       'air_temperature_tmmn', 'mean_vapor_pressure_deficit', 'wind_speed',\n       'precipitation_amount', 'potential_evapotranspiration',\n       'relative_humidity_rmax',\n       ...\n       'AMSR_Flag_2025-01-06', 'AMSR_SWE_2025-01-07', 'AMSR_Flag_2025-01-07',\n       'SWE', 'Flag', 'cumulative_SWE', 'date', 'fsca', 'lc_prop3',\n       'water_year'],\n      dtype='object', length=223)\nreorganized columns:  Index(['SWE', 'fsca', 'air_temperature_tmmx', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'relative_humidity_rmax', 'Elevation',\n       'Slope', 'Curvature', 'Aspect', 'Eastness', 'Northness', 'lat', 'lon'],\n      dtype='object')\nAssign -1 to fsca column..\nStart to predict (462204, 12)\nExplain the prediction: \nStarting the explanation process...\nSelecting 100 random samples for SHAP explanations...\nPick sample:         SWE  fsca  air_temperature_tmmx  ...     Aspect  Eastness  Northness\n99470  0.0    -1                 277.5  ...  136.38142 -0.603884  -0.626618\n[1 rows x 12 columns]\nInitializing SHAP explainer...\n",
  "history_begin_time" : 1736926072507,
  "history_end_time" : 1736926334850,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "f3o26axc3fl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741785,
  "history_end_time" : 1736926334428,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "k4xrybp4qun",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741787,
  "history_end_time" : 1736926334429,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bayow2l1fxb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741788,
  "history_end_time" : 1736926334429,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jlw749lj9ig",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741790,
  "history_end_time" : 1736926334429,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l8r4gqqocpv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741822,
  "history_end_time" : 1736926334430,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ika0rdjrrf9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741823,
  "history_end_time" : 1736926334430,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0lu6ittx27w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741825,
  "history_end_time" : 1736926334430,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4woz4uv72hj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741826,
  "history_end_time" : 1736926334431,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9cicbzxnin2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741828,
  "history_end_time" : 1736926334431,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hhf7foifdug",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741829,
  "history_end_time" : 1736926334431,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "k2uywu6pajo",
  "history_input" : "import os\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom snowcast_utils import homedir, work_dir, data_dir, test_start_date, test_end_date\nimport sys\nimport numpy as np\n\ndef get_water_year(date):\n    if date.month >= 10:  # If the month is October or later\n        return date.year + 1  # Water year starts in the following calendar year\n    else:\n        return date.year\n\ndef merge_all_gridmet_amsr_csv_into_one(target_date, gridmet_csv_folder, dem_all_csv, testing_all_csv, water_mask_csv):\n    \"\"\"\n    Merge all GridMET and AMSR CSV files into one combined CSV file.\n\n    Args:\n        gridmet_csv_folder (str): The folder containing GridMET CSV files.\n        dem_all_csv (str): Path to the DEM (Digital Elevation Model) CSV file.\n        testing_all_csv (str): Path to save the merged CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # List of file paths for the CSV files\n    csv_files = []\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('_cumulative.csv') and target_date in file:\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    all_df = None\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        print(f\"reading {file}\")\n        df = pd.read_csv(file)\n        # print(df.head())\n        print(\"df.shape:\", df.shape)\n        df = df.apply(pd.to_numeric, errors='coerce')\n        if all_df is None:\n          all_df = df\n        else:\n          #all_df = all_df.merge(df, on=['Latitude', 'Longitude']).drop_duplicates()\n          df = df.drop(columns=['Latitude', 'Longitude'])\n          all_df = pd.concat([all_df, df], axis=1)\n          \n        # print(\"all_df.head() :\", all_df.head())\n        print(\"all_df.columns\", all_df.columns)\n        print(\"all_df.shape: \", all_df.shape)\n\n    unique_loc_pairs = all_df[['Latitude', 'Longitude']].drop_duplicates()\n    print(\"unique_loc_pairs.shape: \", unique_loc_pairs.shape)\n        \n    dem_df = pd.read_csv(f\"{data_dir}/srtm/dem_all.csv\", encoding='utf-8', index_col=False)\n    #all_df = pd.merge(all_df, dem_df, on=['Latitude', 'Longitude']).drop_duplicates()\n    dem_df = dem_df.drop(columns=['Latitude', 'Longitude'])\n    print(\"dem_df.shape: \", dem_df.shape)\n    all_df = pd.concat([all_df, dem_df], axis=1)\n\n    date = target_date\n    \n    #date = date.replace(\"-\", \".\")\n    amsr_file = f'{data_dir}/amsr_testing/testing_ready_amsr_{date}_cumulative.csv'\n    print(f\"reading {amsr_file}\")\n    amsr_df = pd.read_csv(amsr_file, index_col=False)\n    amsr_df.rename(columns={'gridmet_lat': 'Latitude', 'gridmet_lon': 'Longitude'}, inplace=True)\n    # print(amsr_df.head())\n    print(\"amsr_df.shape = \", amsr_df.shape)\n    amsr_df = amsr_df.drop(columns=['Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, amsr_df], axis=1)\n    \n    fsca_df = pd.read_csv(f'{data_dir}/fsca/final_output/{target_date}_output.csv')\n    # print(fsca_df.head())\n    print(\"fsca_df.shape: \", fsca_df.shape)\n    fsca_df = fsca_df.drop(columns=['Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, fsca_df], axis=1)\n\n    water_mask_df = pd.read_csv(water_mask_csv)\n    water_mask_df = water_mask_df.drop(columns=[\"date\", 'Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, water_mask_df], axis=1)\n    \n    print(\"all columns: \", all_df.columns)\n    # add water year\n    all_df[\"water_year\"] = get_water_year(selected_date)\n    \n    all_df.rename(columns={'date_x': 'date'}, inplace=True)\n    \n    # log10 all the cumulative columns\n    # Get columns with \"cumulative\" in their names\n    for col in all_df.columns:\n        print(\"Checking \", col)\n        if \"cumulative\" in col:\n\t        # Apply log10 transformation to selected columns\n            all_df[col] = np.log10(all_df[col] + 0.1)  # Adding 1 to avoid log(0)\n            print(f\"converted {col} to log10\")\n    \n    # Save the merged dataframe to a new CSV file\n    all_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input CSV files are merged to {testing_all_csv}\")\n    print(all_df.columns)\n    print(\"all_df.shape = \", all_df.shape)\n    # print(all_df.describe(include='all'))\n    # print(all_df[\"fsca\"].describe())\n    # print(all_df[\"cumulative_fsca\"].describe())\n\ndef do_all_days():\n    # Convert string dates to datetime objects\n    start_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    end_date = datetime.strptime(test_end_date, \"%Y-%m-%d\")\n\n    # Loop over each day in the date range and call the function\n    current_date = start_date\n    while current_date <= end_date:\n        # Prepare the cumulative history CSVs for the current date\n        print(\">>>>>\\nGetting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n        current_date_str = current_date.strftime(\"%Y-%m-%d\")\n        # test_year = int(current_date_str[:4])\n        test_year = \"2024\"\n\n        merge_all_gridmet_amsr_csv_into_one(current_date_str, \n                                        f\"{work_dir}/testing_output/\",\n                                        f\"{data_dir}/srtm/dem_all.csv\",\n                                        f\"{work_dir}/testing_all_ready_{current_date_str}.csv\",\n                                        f\"{data_dir}/water_mask/final_output/{test_year}_output.csv\")\n        \n        # Move to the next day\n        current_date += timedelta(days=1)\n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    do_all_days()\n",
  "history_output" : "/home/chetana\ntoday date = 2025-01-15\nStart day: 2025-01-05, End day: 2025-01-12\ntest start date:  2025-01-05\ntest end date:  2025-01-12\n>>>>>\nGetting gridmet for day 2025-01-05\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin', 'tmmx'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin', 'tmmx',\n       'tmmn'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-05.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin', 'tmmx',\n       'tmmn', 'vpd'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-05_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin', 'tmmx',\n       'tmmn', 'vpd', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  pr\nChecking  rmax\nChecking  etr\nChecking  vs\nChecking  rmin\nChecking  tmmx\nChecking  tmmn\nChecking  vpd\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-05.csv\nIndex(['Latitude', 'Longitude', 'pr', 'rmax', 'etr', 'vs', 'rmin', 'tmmx',\n       'tmmn', 'vpd', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nall_df.shape =  (462204, 25)\n>>>>>\nGetting gridmet for day 2025-01-06\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin', 'tmmx'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin', 'tmmx',\n       'vpd'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-06.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin', 'tmmx',\n       'vpd', 'etr'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-06_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin', 'tmmx',\n       'vpd', 'etr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  pr\nChecking  rmax\nChecking  vs\nChecking  tmmn\nChecking  rmin\nChecking  tmmx\nChecking  vpd\nChecking  etr\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-06.csv\nIndex(['Latitude', 'Longitude', 'pr', 'rmax', 'vs', 'tmmn', 'rmin', 'tmmx',\n       'vpd', 'etr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nall_df.shape =  (462204, 25)\n>>>>>\nGetting gridmet for day 2025-01-07\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr', 'rmax'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr', 'rmax',\n       'rmin'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-07.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr', 'rmax',\n       'rmin', 'vpd'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-07_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr', 'rmax',\n       'rmin', 'vpd', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  vs\nChecking  pr\nChecking  tmmx\nChecking  tmmn\nChecking  etr\nChecking  rmax\nChecking  rmin\nChecking  vpd\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-07.csv\nIndex(['Latitude', 'Longitude', 'vs', 'pr', 'tmmx', 'tmmn', 'etr', 'rmax',\n       'rmin', 'vpd', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nall_df.shape =  (462204, 25)\n>>>>>\nGetting gridmet for day 2025-01-08\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs', 'pr'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs', 'pr',\n       'etr'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-08.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs', 'pr',\n       'etr', 'rmax'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-08_cumulative.csv\namsr_df.shape =  (462204, 204)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs', 'pr',\n       'etr', 'rmax',\n       ...\n       'AMSR_Flag_2025-01-06', 'AMSR_SWE_2025-01-07', 'AMSR_Flag_2025-01-07',\n       'AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_SWE', 'date', 'date', 'fsca',\n       'lc_prop3'],\n      dtype='object', length=223)\nChecking  Latitude\nChecking  Longitude\nChecking  tmmx\nChecking  rmin\nChecking  tmmn\nChecking  vpd\nChecking  vs\nChecking  pr\nChecking  etr\nChecking  rmax\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE_2024-10-01\nChecking  AMSR_Flag_2024-10-01\nChecking  AMSR_SWE_2024-10-02\nChecking  AMSR_Flag_2024-10-02\nChecking  AMSR_SWE_2024-10-03\nChecking  AMSR_Flag_2024-10-03\nChecking  AMSR_SWE_2024-10-04\nChecking  AMSR_Flag_2024-10-04\nChecking  AMSR_SWE_2024-10-05\nChecking  AMSR_Flag_2024-10-05\nChecking  AMSR_SWE_2024-10-06\nChecking  AMSR_Flag_2024-10-06\nChecking  AMSR_SWE_2024-10-07\nChecking  AMSR_Flag_2024-10-07\nChecking  AMSR_SWE_2024-10-08\nChecking  AMSR_Flag_2024-10-08\nChecking  AMSR_SWE_2024-10-09\nChecking  AMSR_Flag_2024-10-09\nChecking  AMSR_SWE_2024-10-10\nChecking  AMSR_Flag_2024-10-10\nChecking  AMSR_SWE_2024-10-11\nChecking  AMSR_Flag_2024-10-11\nChecking  AMSR_SWE_2024-10-12\nChecking  AMSR_Flag_2024-10-12\nChecking  AMSR_SWE_2024-10-13\nChecking  AMSR_Flag_2024-10-13\nChecking  AMSR_SWE_2024-10-14\nChecking  AMSR_Flag_2024-10-14\nChecking  AMSR_SWE_2024-10-15\nChecking  AMSR_Flag_2024-10-15\nChecking  AMSR_SWE_2024-10-16\nChecking  AMSR_Flag_2024-10-16\nChecking  AMSR_SWE_2024-10-17\nChecking  AMSR_Flag_2024-10-17\nChecking  AMSR_SWE_2024-10-18\nChecking  AMSR_Flag_2024-10-18\nChecking  AMSR_SWE_2024-10-19\nChecking  AMSR_Flag_2024-10-19\nChecking  AMSR_SWE_2024-10-20\nChecking  AMSR_Flag_2024-10-20\nChecking  AMSR_SWE_2024-10-21\nChecking  AMSR_Flag_2024-10-21\nChecking  AMSR_SWE_2024-10-22\nChecking  AMSR_Flag_2024-10-22\nChecking  AMSR_SWE_2024-10-23\nChecking  AMSR_Flag_2024-10-23\nChecking  AMSR_SWE_2024-10-24\nChecking  AMSR_Flag_2024-10-24\nChecking  AMSR_SWE_2024-10-25\nChecking  AMSR_Flag_2024-10-25\nChecking  AMSR_SWE_2024-10-26\nChecking  AMSR_Flag_2024-10-26\nChecking  AMSR_SWE_2024-10-27\nChecking  AMSR_Flag_2024-10-27\nChecking  AMSR_SWE_2024-10-28\nChecking  AMSR_Flag_2024-10-28\nChecking  AMSR_SWE_2024-10-29\nChecking  AMSR_Flag_2024-10-29\nChecking  AMSR_SWE_2024-10-30\nChecking  AMSR_Flag_2024-10-30\nChecking  AMSR_SWE_2024-10-31\nChecking  AMSR_Flag_2024-10-31\nChecking  AMSR_SWE_2024-11-01\nChecking  AMSR_Flag_2024-11-01\nChecking  AMSR_SWE_2024-11-02\nChecking  AMSR_Flag_2024-11-02\nChecking  AMSR_SWE_2024-11-03\nChecking  AMSR_Flag_2024-11-03\nChecking  AMSR_SWE_2024-11-04\nChecking  AMSR_Flag_2024-11-04\nChecking  AMSR_SWE_2024-11-05\nChecking  AMSR_Flag_2024-11-05\nChecking  AMSR_SWE_2024-11-06\nChecking  AMSR_Flag_2024-11-06\nChecking  AMSR_SWE_2024-11-07\nChecking  AMSR_Flag_2024-11-07\nChecking  AMSR_SWE_2024-11-08\nChecking  AMSR_Flag_2024-11-08\nChecking  AMSR_SWE_2024-11-09\nChecking  AMSR_Flag_2024-11-09\nChecking  AMSR_SWE_2024-11-10\nChecking  AMSR_Flag_2024-11-10\nChecking  AMSR_SWE_2024-11-11\nChecking  AMSR_Flag_2024-11-11\nChecking  AMSR_SWE_2024-11-12\nChecking  AMSR_Flag_2024-11-12\nChecking  AMSR_SWE_2024-11-13\nChecking  AMSR_Flag_2024-11-13\nChecking  AMSR_SWE_2024-11-14\nChecking  AMSR_Flag_2024-11-14\nChecking  AMSR_SWE_2024-11-15\nChecking  AMSR_Flag_2024-11-15\nChecking  AMSR_SWE_2024-11-16\nChecking  AMSR_Flag_2024-11-16\nChecking  AMSR_SWE_2024-11-17\nChecking  AMSR_Flag_2024-11-17\nChecking  AMSR_SWE_2024-11-18\nChecking  AMSR_Flag_2024-11-18\nChecking  AMSR_SWE_2024-11-19\nChecking  AMSR_Flag_2024-11-19\nChecking  AMSR_SWE_2024-11-20\nChecking  AMSR_Flag_2024-11-20\nChecking  AMSR_SWE_2024-11-21\nChecking  AMSR_Flag_2024-11-21\nChecking  AMSR_SWE_2024-11-22\nChecking  AMSR_Flag_2024-11-22\nChecking  AMSR_SWE_2024-11-23\nChecking  AMSR_Flag_2024-11-23\nChecking  AMSR_SWE_2024-11-24\nChecking  AMSR_Flag_2024-11-24\nChecking  AMSR_SWE_2024-11-25\nChecking  AMSR_Flag_2024-11-25\nChecking  AMSR_SWE_2024-11-26\nChecking  AMSR_Flag_2024-11-26\nChecking  AMSR_SWE_2024-11-27\nChecking  AMSR_Flag_2024-11-27\nChecking  AMSR_SWE_2024-11-28\nChecking  AMSR_Flag_2024-11-28\nChecking  AMSR_SWE_2024-11-29\nChecking  AMSR_Flag_2024-11-29\nChecking  AMSR_SWE_2024-11-30\nChecking  AMSR_Flag_2024-11-30\nChecking  AMSR_SWE_2024-12-01\nChecking  AMSR_Flag_2024-12-01\nChecking  AMSR_SWE_2024-12-02\nChecking  AMSR_Flag_2024-12-02\nChecking  AMSR_SWE_2024-12-03\nChecking  AMSR_Flag_2024-12-03\nChecking  AMSR_SWE_2024-12-04\nChecking  AMSR_Flag_2024-12-04\nChecking  AMSR_SWE_2024-12-05\nChecking  AMSR_Flag_2024-12-05\nChecking  AMSR_SWE_2024-12-06\nChecking  AMSR_Flag_2024-12-06\nChecking  AMSR_SWE_2024-12-07\nChecking  AMSR_Flag_2024-12-07\nChecking  AMSR_SWE_2024-12-08\nChecking  AMSR_Flag_2024-12-08\nChecking  AMSR_SWE_2024-12-09\nChecking  AMSR_Flag_2024-12-09\nChecking  AMSR_SWE_2024-12-10\nChecking  AMSR_Flag_2024-12-10\nChecking  AMSR_SWE_2024-12-11\nChecking  AMSR_Flag_2024-12-11\nChecking  AMSR_SWE_2024-12-12\nChecking  AMSR_Flag_2024-12-12\nChecking  AMSR_SWE_2024-12-13\nChecking  AMSR_Flag_2024-12-13\nChecking  AMSR_SWE_2024-12-14\nChecking  AMSR_Flag_2024-12-14\nChecking  AMSR_SWE_2024-12-15\nChecking  AMSR_Flag_2024-12-15\nChecking  AMSR_SWE_2024-12-16\nChecking  AMSR_Flag_2024-12-16\nChecking  AMSR_SWE_2024-12-17\nChecking  AMSR_Flag_2024-12-17\nChecking  AMSR_SWE_2024-12-18\nChecking  AMSR_Flag_2024-12-18\nChecking  AMSR_SWE_2024-12-19\nChecking  AMSR_Flag_2024-12-19\nChecking  AMSR_SWE_2024-12-20\nChecking  AMSR_Flag_2024-12-20\nChecking  AMSR_SWE_2024-12-21\nChecking  AMSR_Flag_2024-12-21\nChecking  AMSR_SWE_2024-12-22\nChecking  AMSR_Flag_2024-12-22\nChecking  AMSR_SWE_2024-12-23\nChecking  AMSR_Flag_2024-12-23\nChecking  AMSR_SWE_2024-12-24\nChecking  AMSR_Flag_2024-12-24\nChecking  AMSR_SWE_2024-12-25\nChecking  AMSR_Flag_2024-12-25\nChecking  AMSR_SWE_2024-12-26\nChecking  AMSR_Flag_2024-12-26\nChecking  AMSR_SWE_2024-12-27\nChecking  AMSR_Flag_2024-12-27\nChecking  AMSR_SWE_2024-12-28\nChecking  AMSR_Flag_2024-12-28\nChecking  AMSR_SWE_2024-12-29\nChecking  AMSR_Flag_2024-12-29\nChecking  AMSR_SWE_2024-12-30\nChecking  AMSR_Flag_2024-12-30\nChecking  AMSR_SWE_2024-12-31\nChecking  AMSR_Flag_2024-12-31\nChecking  AMSR_SWE_2025-01-01\nChecking  AMSR_Flag_2025-01-01\nChecking  AMSR_SWE_2025-01-02\nChecking  AMSR_Flag_2025-01-02\nChecking  AMSR_SWE_2025-01-03\nChecking  AMSR_Flag_2025-01-03\nChecking  AMSR_SWE_2025-01-04\nChecking  AMSR_Flag_2025-01-04\nChecking  AMSR_SWE_2025-01-05\nChecking  AMSR_Flag_2025-01-05\nChecking  AMSR_SWE_2025-01-06\nChecking  AMSR_Flag_2025-01-06\nChecking  AMSR_SWE_2025-01-07\nChecking  AMSR_Flag_2025-01-07\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  cumulative_AMSR_SWE\nconverted cumulative_AMSR_SWE to log10\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-08.csv\nIndex(['Latitude', 'Longitude', 'tmmx', 'rmin', 'tmmn', 'vpd', 'vs', 'pr',\n       'etr', 'rmax',\n       ...\n       'AMSR_SWE_2025-01-07', 'AMSR_Flag_2025-01-07', 'AMSR_SWE', 'AMSR_Flag',\n       'cumulative_AMSR_SWE', 'date', 'date', 'fsca', 'lc_prop3',\n       'water_year'],\n      dtype='object', length=224)\nall_df.shape =  (462204, 224)\n>>>>>\nGetting gridmet for day 2025-01-09\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn', 'vs'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn', 'vs', 'etr'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn', 'vs', 'etr',\n       'rmax'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-09.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn', 'vs', 'etr',\n       'rmax', 'pr'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-09_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn', 'vs', 'etr',\n       'rmax', 'pr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  tmmx\nChecking  rmin\nChecking  vpd\nChecking  tmmn\nChecking  vs\nChecking  etr\nChecking  rmax\nChecking  pr\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-09.csv\nIndex(['Latitude', 'Longitude', 'tmmx', 'rmin', 'vpd', 'tmmn', 'vs', 'etr',\n       'rmax', 'pr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nall_df.shape =  (462204, 25)\n>>>>>\nGetting gridmet for day 2025-01-10\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs', 'vpd'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs', 'vpd', 'rmin'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs', 'vpd', 'rmin',\n       'tmmn'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-10.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs', 'vpd', 'rmin',\n       'tmmn', 'etr'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-10_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs', 'vpd', 'rmin',\n       'tmmn', 'etr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  tmmx\nChecking  rmax\nChecking  pr\nChecking  vs\nChecking  vpd\nChecking  rmin\nChecking  tmmn\nChecking  etr\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-10.csv\nIndex(['Latitude', 'Longitude', 'tmmx', 'rmax', 'pr', 'vs', 'vpd', 'rmin',\n       'tmmn', 'etr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nall_df.shape =  (462204, 25)\n>>>>>\nGetting gridmet for day 2025-01-11\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd', 'etr'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd', 'etr', 'vs'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd', 'etr', 'vs',\n       'tmmn'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-11.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd', 'etr', 'vs',\n       'tmmn', 'tmmx'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-11_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd', 'etr', 'vs',\n       'tmmn', 'tmmx', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  rmax\nChecking  pr\nChecking  rmin\nChecking  vpd\nChecking  etr\nChecking  vs\nChecking  tmmn\nChecking  tmmx\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-11.csv\nIndex(['Latitude', 'Longitude', 'rmax', 'pr', 'rmin', 'vpd', 'etr', 'vs',\n       'tmmn', 'tmmx', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\nall_df.shape =  (462204, 25)\n>>>>>\nGetting gridmet for day 2025-01-12\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs'], dtype='object')\nall_df.shape:  (462204, 3)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn'], dtype='object')\nall_df.shape:  (462204, 4)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr'], dtype='object')\nall_df.shape:  (462204, 5)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax'], dtype='object')\nall_df.shape:  (462204, 6)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax', 'rmin'], dtype='object')\nall_df.shape:  (462204, 7)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax', 'rmin', 'vpd'], dtype='object')\nall_df.shape:  (462204, 8)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax', 'rmin', 'vpd',\n       'tmmx'],\n      dtype='object')\nall_df.shape:  (462204, 9)\nreading /home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-12.csv_cumulative.csv\ndf.shape: (462204, 3)\nall_df.columns Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax', 'rmin', 'vpd',\n       'tmmx', 'pr'],\n      dtype='object')\nall_df.shape:  (462204, 10)\nunique_loc_pairs.shape:  (462204, 2)\ndem_df.shape:  (462204, 8)\nreading /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-12_cumulative.csv\namsr_df.shape =  (462204, 5)\nfsca_df.shape:  (462204, 4)\nall columns:  Index(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax', 'rmin', 'vpd',\n       'tmmx', 'pr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3'],\n      dtype='object')\nChecking  Latitude\nChecking  Longitude\nChecking  vs\nChecking  tmmn\nChecking  etr\nChecking  rmax\nChecking  rmin\nChecking  vpd\nChecking  tmmx\nChecking  pr\nChecking  x\nChecking  y\nChecking  Elevation\nChecking  Slope\nChecking  Aspect\nChecking  Curvature\nChecking  Northness\nChecking  Eastness\nChecking  AMSR_SWE\nChecking  AMSR_Flag\nChecking  date\nChecking  date\nChecking  fsca\nChecking  lc_prop3\nChecking  water_year\n/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:7834: RuntimeWarning: invalid value encountered in cast\n  values = values.astype(str)\nAll input CSV files are merged to /home/chetana/data/gridmet_test_run/testing_all_ready_2025-01-12.csv\nIndex(['Latitude', 'Longitude', 'vs', 'tmmn', 'etr', 'rmax', 'rmin', 'vpd',\n       'tmmx', 'pr', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date', 'date',\n       'fsca', 'lc_prop3', 'water_year'],\n      dtype='object')\n",
  "history_begin_time" : 1736925854234,
  "history_end_time" : 1736926334432,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cmqlag8pe45",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741832,
  "history_end_time" : 1736926334433,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kvms04ygmm3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741834,
  "history_end_time" : 1736926334433,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mvqmgzo1vmg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741835,
  "history_end_time" : 1736926334433,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rzhsd6c6y8b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741837,
  "history_end_time" : 1736926334433,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3yu79q2690d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741838,
  "history_end_time" : 1736926334434,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0uxaj3twz1k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741840,
  "history_end_time" : 1736926334434,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "udlupifewl0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741865,
  "history_end_time" : 1736926334434,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a7l5cwugij9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741866,
  "history_end_time" : 1736926334435,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "60390pb07yu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741868,
  "history_end_time" : 1736926334435,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gojs9eyy0st",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741869,
  "history_end_time" : 1736926334435,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "k8li84jp7fs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741871,
  "history_end_time" : 1736926334435,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0ugrq9aahcm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741872,
  "history_end_time" : 1736926334436,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "76njnukl7i3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741874,
  "history_end_time" : 1736926334436,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "joo6h9ch0da",
  "history_input" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, test_end_date, work_dir, homedir, data_dir, cumulative_mode\nimport matplotlib.pyplot as plt\n\n# Define the folder to store downloaded files\ngridmet_folder_name = f'{work_dir}/gridmet_climatology'\n\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\n# Define your value ranges for color mapping\n#value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n#value_ranges = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.5, 3]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n  if value_ranges == None:\n    max_value = df_col.max()\n    min_value = df_col.min()\n    if min_value < 0:\n      min_value = 0\n    step_size = (max_value - min_value) / 12\n\n    # Create 10 periods\n    new_value_ranges = [min_value + i * step_size for i in range(12)]\n  # Define your custom function to map data values to colors\n  def map_value_to_color(value):\n    # Iterate through the value ranges to find the appropriate color index\n    for i, range_max in enumerate(new_value_ranges):\n      if value <= range_max:\n        return colors[i]\n\n      # If the value is greater than the largest range, return the last color\n      return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n  color_mapping = [map_value_to_color(value) for value in df_col.values]\n  return color_mapping, new_value_ranges\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path, current_year):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                print(f\"Downloading to {target_file_path}\")\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef create_gridmet_to_dem_mapper(nc_file):\n    print(\"creating gridmet to dem mapper\")\n    western_us_dem_df = pd.read_csv(western_us_coords)\n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n    \n      # Use the apply function to apply the custom function to each row\n      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n      \n    # print(western_us_dem_df.head())\n    \n    # Save the new converted AMSR to CSV file\n    western_us_dem_df.to_csv(target_csv_path, index=False)\n    \n    return western_us_dem_df\n  \n  \ndef get_nc_csv_by_coords_and_variable(nc_file,\n                                      var_name,\n                                      target_date=test_start_date):\n    print(\"get_nc_csv_by_coords_and_variable\")\n    create_gridmet_to_dem_mapper(nc_file)\n  \t\n    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      var_col = nc_file.variables[long_var_name][:]\n      #print(\"val_col.shape: \", var_col.shape)\n      \n      # Calculate the day of the year\n      day_of_year = selected_date.timetuple().tm_yday\n      day_index = day_of_year - 1\n      #print('day_index:', day_index)\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        lat_index = int(row[\"gridmet_lat_idx\"])\n        lon_index = int(row[\"gridmet_lon_idx\"])\n        var_value = var_col[day_index, lat_index, lon_index]\n        \n        return var_value\n    \n      # Use the apply function to apply the custom function to each row\n      # print(mapper_df.columns)\n      # print(mapper_df.head())\n      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n      \n      # print(\"mapper_df[var_name]: \", mapper_df[var_name].describe())\n      \n      # drop useless columns\n      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n                               \"dem_lon\": \"Longitude\"}, inplace=True)\n\n      \n    # print(mapper_df.head())\n    return mapper_df\n\n\ndef turn_gridmet_nc_to_csv(target_date=test_start_date):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    generated_csvs = []\n    for root, dirs, files in os.walk(gridmet_folder_name):\n        for file_name in files:\n            \n            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n                print(f\"Checking file: {file_name}\")\n                var_name = get_var_from_file_name(file_name)\n                # print(\"Variable name:\", var_name)\n                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n\n                if os.path.exists(res_csv):\n                    #os.remove(res_csv)\n                    print(f\"{res_csv} already exists. Skipping..\")\n                    generated_csvs.append(res_csv)\n                    continue\n\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n                                                       var_name, target_date)\n                df.replace('--', pd.NA, inplace=True)\n                df.to_csv(res_csv, index=False)\n                print(\"gridmet var saved: \", res_csv)\n                generated_csvs.append(res_csv)\n    return generated_csvs   \n\ndef plot_gridmet(target_date=test_start_date):\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  var_name = \"pr\"\n  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n  gridmet_var_df = pd.read_csv(test_csv)\n  gridmet_var_df.replace('--', pd.NA, inplace=True)\n  gridmet_var_df.dropna(inplace=True)\n  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n  #print(gridmet_var_df.head())\n  #print(gridmet_var_df[\"Latitude\"].describe())\n  #print(gridmet_var_df[\"Longitude\"].describe())\n  #print(gridmet_var_df[\"pr\"].describe())\n  \n  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n  \n  # Create a scatter plot\n  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n              gridmet_var_df[\"Latitude\"].values, \n              label='Pressure', \n              color=colormaplist, \n              marker='o')\n\n  # Add labels and a legend\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Scatter Plot Example')\n  plt.legend()\n  \n  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n  plt.savefig(res_png_path)\n  print(f\"test image is saved at {res_png_path}\")\n                \ndef prepare_folder_and_get_year_list(target_date=test_start_date):\n  # Check if the folder exists, if not, create it\n  if not os.path.exists(gridmet_folder_name):\n      os.makedirs(gridmet_folder_name)\n\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n  year_list = [selected_date.year, past_october_1.year]\n\n  # Remove any existing files in the folder\n  if selected_date.year == datetime.now().year:\n    # check if the current year's netcdf contains the selected date\n    # get etr netcdf and read\n    nc_file = f\"{gridmet_folder_name}/tmmx_{selected_date.year}.nc\"\n    ifremove = False\n    if os.path.exists(nc_file):\n      with nc.Dataset(nc_file) as ncd:\n        day = ncd.variables['day'][:]\n        # Calculate the day of the year\n        day_of_year = selected_date.timetuple().tm_yday\n        day_index = day_of_year - 1\n        if len(day) <= day_index:\n          ifremove = True\n    \n    if ifremove:\n      print(\"The current year netcdf has new data. Redownloading..\")\n      remove_files_in_folder(gridmet_folder_name, selected_date.year)  # only redownload when the year is the current year\n    else:\n      print(\"The existing netcdf already covers the selected date. Avoid downloading..\")\n  return year_list\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].sum()\n  return df\n    \n\ndef prepare_cumulative_history_csvs(target_date=test_start_date, force=False):\n  \"\"\"\n    Prepare cumulative history CSVs for a specified target date.\n\n    Parameters:\n    - target_date (str, optional): The target date in the format 'YYYY-MM-DD'. Default is 'test_start_date'.\n    - force (bool, optional): If True, forcefully regenerate cumulative CSVs even if they already exist. Default is False.\n\n    Returns:\n    None\n\n    This function generates cumulative history CSVs for a specified target date. It traverses the date range from the past\n    October 1 to the target date, downloads gridmet data, converts it to CSV, and merges it into a big DataFrame.\n    The cumulative values are calculated and saved in new CSV files.\n\n    Example:\n    ```python\n    prepare_cumulative_history_csvs(target_date='2023-01-01', force=True)\n    ```\n\n    Note: This function assumes the existence of the following helper functions:\n    - download_gridmet_of_specific_variables\n    - prepare_folder_and_get_year_list\n    - turn_gridmet_nc_to_csv\n    - add_cumulative_column\n    - process_group_value_filling\n    ```\n\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n        past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n        past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Rest of the function logic...\n\n    filled_data = filled_data.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n    print(\"new_df final shape: \", filled_data.head())\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    print(filled_data.describe())\n    ```\nNote: This docstring includes placeholders such as \"download_gridmet_of_specific_variables\" and \"prepare_folder_and_get_year_list\" for the assumed existence of related helper functions. You should replace these placeholders with actual documentation for those functions.\n  \"\"\"\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  \n  date_keyed_objects = {}\n  \n  download_gridmet_of_specific_variables(\n    prepare_folder_and_get_year_list(target_date=target_date)\n  )\n\n  print(\"Downloading gridmet completed. Start to generate csv..\")\n  \n  while current_date <= selected_date:\n    if not cumulative_mode and current_date != selected_date:\n      current_date += timedelta(days=1)\n      continue;\n    print(current_date.strftime('%Y-%m-%d'))\n    current_date_str = current_date.strftime('%Y-%m-%d')\n    generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n    \n    # read the csv into dataframe and merge to the big dataframe\n    date_keyed_objects[current_date_str] = generated_csvs\n    current_date += timedelta(days=1)\n    \n    \n  # print(\"date_keyed_objects: \", date_keyed_objects)\n  target_generated_csvs = date_keyed_objects[target_date]\n  for index, single_csv in enumerate(target_generated_csvs):\n    # traverse the variables of gridmet here\n    # each variable is a loop\n    # print(f\"creating cumulative for {single_csv}\")\n    \n    cumulative_target_path = f\"{single_csv}_cumulative.csv\"\n    # print(\"cumulative_target_path = \", cumulative_target_path)\n    \n    if os.path.exists(cumulative_target_path) and not force:\n      print(f\"{cumulative_target_path} already exists, skipping..\")\n      continue\n    \n    # Extract the file name without extension\n    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n    gap_filled_csv = f\"{cumulative_target_path}_gap_filled.csv\"\n\n\t# Split the file name using underscores\n    var_name = file_name.split('_')[1]\n    # print(f\"Found variable name {var_name}\")\n    current_date = past_october_1\n    new_df = pd.read_csv(single_csv)\n    # print(new_df.head())\n    \n    all_df = pd.read_csv(f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\")\n    all_df[\"date\"] = target_date\n    all_df[var_name] = pd.to_numeric(all_df[var_name], errors='coerce')\n    filled_data = all_df\n    filled_data = filled_data[(filled_data['date'] == target_date)]\n    \n    filled_data.fillna(0, inplace=True)\n    \n    # print(\"Finished correctly \", filled_data.head())\n    #filled_data.to_csv(gap_filled_csv, index=False)\n    #print(f\"New filled values csv is saved to {gap_filled_csv}_gap_filled.csv\")\n    \n    filled_data = filled_data[['Latitude', 'Longitude', \n                               var_name, \n#                                f'cumulative_{var_name}'\n                              ]]\n    # print(filled_data.shape)\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    # print(filled_data.describe())\n\ndef do_all_days():\n  # Convert string dates to datetime objects\n  start_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(test_end_date, \"%Y-%m-%d\")\n\n  # Loop over each day in the date range and call the function\n  current_date = start_date\n  while current_date <= end_date:\n      # Prepare the cumulative history CSVs for the current date\n      print(\"Getting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n      prepare_cumulative_history_csvs(target_date=current_date.strftime(\"%Y-%m-%d\"), force=False)\n      \n      # Move to the next day\n      current_date += timedelta(days=1)\n\nif __name__ == \"__main__\":\n  # Run the download function\n#   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n#   turn_gridmet_nc_to_csv()\n#   plot_gridmet()\n\n  # prepare testing data with cumulative variables\n  # prepare_cumulative_history_csvs(force=True)\n\n  do_all_days()\n\n",
  "history_output" : "/home/chetana\ntoday date = 2025-01-15\nStart day: 2025-01-05, End day: 2025-01-12\ntest start date:  2025-01-05\ntest end date:  2025-01-12\nGetting gridmet for day 2025-01-05\n2025-01-05 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-05\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-05.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-05.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-05.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-05.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-05.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-05.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-05.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-05.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-05.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-05.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-06\n2025-01-06 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-06\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-06.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-06.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-06.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-06.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-06.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-06.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-06.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-06.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-06.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-06.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-07\n2025-01-07 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-07\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-07.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-07.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-07.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-07.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-07.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-07.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-07.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-07.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-07.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-07.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-08\n2025-01-08 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-08\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-08.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-08.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-08.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-08.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-08.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-08.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-08.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-08.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-08.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-08.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-09\n2025-01-09 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-09\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-09.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-09.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-09.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-09.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-09.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-09.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-09.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-09.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-09.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-09.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-10\n2025-01-10 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-10\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-10.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-10.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-10.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-10.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-10.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-10.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-10.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-10.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-10.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-10.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-11\n2025-01-11 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-11\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-11.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-11.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-11.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-11.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-11.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-11.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-11.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-11.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-11.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-11.csv_cumulative.csv already exists, skipping..\nGetting gridmet for day 2025-01-12\n2025-01-12 00:00:00\nThe existing netcdf already covers the selected date. Avoid downloading..\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmn_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/tmmx_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/pr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vpd_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/etr_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmax_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/rmin_2024.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2025.nc exists\nFile /home/chetana/data/gridmet_test_run/gridmet_climatology/vs_2024.nc exists\nDownloading gridmet completed. Start to generate csv..\n2025-01-12\nChecking file: pr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-12.csv already exists. Skipping..\nChecking file: rmax_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-12.csv already exists. Skipping..\nChecking file: vs_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-12.csv already exists. Skipping..\nChecking file: vpd_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-12.csv already exists. Skipping..\nChecking file: tmmx_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-12.csv already exists. Skipping..\nChecking file: tmmn_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-12.csv already exists. Skipping..\nChecking file: rmin_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-12.csv already exists. Skipping..\nChecking file: etr_2025.nc\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-12.csv already exists. Skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_pr_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmax_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vs_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_vpd_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmx_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_tmmn_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_rmin_2025-01-12.csv_cumulative.csv already exists, skipping..\n/home/chetana/data/gridmet_test_run/testing_output/2025_etr_2025-01-12.csv_cumulative.csv already exists, skipping..\n",
  "history_begin_time" : 1736925851594,
  "history_end_time" : 1736926334437,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rj9a64lepf1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741877,
  "history_end_time" : 1736926334437,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dw950tp2szq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741878,
  "history_end_time" : 1736926334437,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "70nzz2tzgo0",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1736926334438,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kz0vuhrvyyt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741888,
  "history_end_time" : 1736926334440,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4pxqpuvzdk5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741889,
  "history_end_time" : 1736926334440,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xrnfcko22kr",
  "history_input" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, data_dir, test_start_date, test_end_date, cumulative_mode\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\nimport sys\nfrom convert_results_to_images import plot_all_variables_in_one_csv\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef is_binary(file_path):\n    try:\n        with open(file_path, 'rb') as file:\n            # Read a chunk of bytes from the file\n            chunk = file.read(1024)\n\n            # Check for null bytes, a common indicator of binary data\n            if b'\\x00' in chunk:\n                return True\n\n            # Check for a high percentage of non-printable ASCII characters\n            text_characters = \"\".join(chr(byte) for byte in chunk if 32 <= byte <= 126)\n            if not text_characters:\n                return True\n\n            # If none of the binary indicators are found, assume it's a text file\n            return False\n\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found.\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n  \ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    print(\"prepare_amsr_grid_mapper\")\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{data_dir}/amsr_testing/amsr_to_gridmet_mapper.csv'\n    # Extract the directory from the target path\n    target_dir = os.path.dirname(target_csv_path)\n\n    # Create all layers of directories if they don't exist\n    os.makedirs(target_dir, exist_ok=True)\n\n    # Now you can safely use the target_csv_path\n    print(f\"Target directory ensured: {target_dir}\")\n    print(f\"Target file path: {target_csv_path}\")\n    \n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{data_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    parent_directory = os.path.dirname(target_amsr_hdf_path)\n    if not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n        print(f\"Parent directory '{parent_directory}' created successfully.\")\n    \n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    print(\"Start to create the grid mapper csv..\")\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    print(\"download_amsr_and_convert_grid\")\n    # the mapper\n    target_mapper_csv_path = f'{data_dir}/amsr_testing/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{data_dir}/amsr_testing/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{data_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n        # Check the exit code\n        if result.returncode != 0:\n            print(f\"Command failed with exit code {result.returncode}.\")\n            if os.path.exists(target_amsr_hdf_path):\n              os.remove(target_amsr_hdf_path)\n              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n    \n    # Read the HDF\n    print(f\"Reading {target_amsr_hdf_path}\")\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\ndef add_cumulative_column(df, column_name):\n    df[f'cumulative_{column_name}'] = df[column_name].sum()\n    return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  # Extract X series (column names)\n  x_all_key = row.index\n  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n  if are_all_values_between_0_and_240:\n    print(\"row[x_subset_key] = \", row[x_subset_key])\n    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n  return row\n    \n    \ndef get_cumulative_amsr_data(target_date = test_start_date, force=False):\n    \n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  target_csv_path = f'{data_dir}/amsr_testing/testing_ready_amsr_{target_date}_cumulative.csv'\n\n  columns_to_be_cumulated = [\"AMSR_SWE\"]\n  \n  gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"AMSR_SWE\"].describe())\n  else:\n    date_keyed_objects = {}\n    data_dict = {}\n    new_df = None\n    while current_date <= selected_date:\n      if not cumulative_mode and current_date != selected_date:\n        current_date += timedelta(days=1)\n        continue;\n      print(current_date.strftime('%Y-%m-%d'))\n      current_date_str = current_date.strftime('%Y-%m-%d')\n\n      data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n      current_df = pd.read_csv(data_dict[current_date_str])\n      current_df.drop(columns=[\"date\"], inplace=True)\n\n      if current_date != selected_date:\n        current_df.rename(columns={\n          \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n          \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n        }, inplace=True)\n      #print(current_df.head())\n\n      if new_df is None:\n        new_df = current_df\n      else:\n        new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n        #new_df = new_df.append(current_df, ignore_index=True)\n\n      current_date += timedelta(days=1)\n\n    print(\"new_df.columns = \", new_df.columns)\n    print(\"new_df.head = \", new_df.head())\n    df = new_df\n\n    #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n    print(\"All current head: \", df.head())\n    print(\"the new_df.shape: \", df.shape)\n\n    if cumulative_mode:\n      print(\"Start to fill in the missing values\")\n      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n      filled_data = pd.DataFrame()\n\n      # Apply the function to each group\n      for column_name in columns_to_be_cumulated:\n        start_time = time.time()\n        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n        #alike_columns = filled_data.filter(like=column_name)\n        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n        print(\"filled_data.columns = \", filled_data.columns)\n        filtered_columns = df.filter(like=column_name)\n        print(filtered_columns.columns)\n        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n        filtered_columns.fillna(0, inplace=True)\n        \n        sum_column = filtered_columns.sum(axis=1)\n        # Define a specific name for the new column\n        df[f'cumulative_{column_name}'] = sum_column\n        df[filtered_columns.columns] = filtered_columns\n        \n        if filtered_columns.isnull().any().any():\n          print(\"filtered_columns :\", filtered_columns)\n          raise ValueError(\"Single group: shouldn't have null values here\")\n      \n        # Concatenate the original DataFrame with the Series containing the sum\n        #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n#         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n#         filled_data[f'cumulative_{column_name}'] = cumulative_column\n        #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n        print(\"filled_data.columns: \", filled_data.columns)\n        end_time = time.time()\n        # Calculate the elapsed time\n        elapsed_time = end_time - start_time\n        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n\n#       if any(filled_data['AMSR_SWE'] > 240):\n#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n    filled_data = df\n    filled_data[\"date\"] = target_date\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {gap_filled_csv}\")\n    df = filled_data\n  \n  result = df\n  print(\"result.head = \", result.head())\n  # fill in the rest NA as 0\n  if result.isnull().any().any():\n    print(\"result :\", result)\n    raise ValueError(\"Single group: shouldn't have null values here\")\n  \n  # only retain the rows of the target date\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n  result.to_csv(target_csv_path, index=False)\n  print(f\"New data is saved to {target_csv_path}\")\n\ndef do_all_days():\n  # Convert string dates to datetime objects\n  start_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(test_end_date, \"%Y-%m-%d\")\n\n  # Loop over each day in the date range and call the function\n  current_date = start_date\n  while current_date <= end_date:\n      # Prepare the cumulative history CSVs for the current date\n      print(\"Getting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n      get_cumulative_amsr_data(target_date=current_date.strftime(\"%Y-%m-%d\"), force=False)\n      \n      # Move to the next day\n      current_date += timedelta(days=1)\n    \nif __name__ == \"__main__\":\n    # Run the download and conversion function\n    #prepare_amsr_grid_mapper()\n    prepare_amsr_grid_mapper()\n    do_all_days()\n#     download_amsr_and_convert_grid()\n    \n    # get_cumulative_amsr_data(force=False)\n    input_time_series_file = f'{data_dir}/amsr_testing/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n\n    #plot_all_variables_in_one_csv(input_time_series_file, f\"{input_time_series_file}.png\")\n",
  "history_output" : "/home/chetana\ntoday date = 2025-01-15\nStart day: 2025-01-05, End day: 2025-01-12\ntest start date:  2025-01-05\ntest end date:  2025-01-12\nprepare_amsr_grid_mapper\nTarget directory ensured: /home/chetana/data/amsr_testing\nTarget file path: /home/chetana/data/amsr_testing/amsr_to_gridmet_mapper.csv\nFile /home/chetana/data/amsr_testing/amsr_to_gridmet_mapper.csv already exists, skipping..\nGetting gridmet for day 2025-01-05\n2025-01-05 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-05_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        114.778295\nstd         123.156757\nmin           0.000000\n25%           0.000000\n50%          28.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000         0        241  2025-01-05\n1         49.0     -124.964         0        241  2025-01-05\n2         49.0     -124.928         0        241  2025-01-05\n3         49.0     -124.892         0        241  2025-01-05\n4         49.0     -124.856         0        241  2025-01-05\n['2025-01-05']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      114.778295     246.763401\nstd       123.156757       6.589377\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        28.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-05_cumulative.csv\nGetting gridmet for day 2025-01-06\n2025-01-06 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-06_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        119.291988\nstd         123.727454\nmin           0.000000\n25%           0.000000\n50%          34.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000         0        241  2025-01-06\n1         49.0     -124.964         0        241  2025-01-06\n2         49.0     -124.928         0        241  2025-01-06\n3         49.0     -124.892         0        241  2025-01-06\n4         49.0     -124.856         0        241  2025-01-06\n['2025-01-06']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      119.291988     246.997867\nstd       123.727454       6.603545\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        34.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-06_cumulative.csv\nGetting gridmet for day 2025-01-07\n2025-01-07 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-07_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        109.995145\nstd         122.076934\nmin           0.000000\n25%           0.000000\n50%          26.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000         0        241  2025-01-07\n1         49.0     -124.964         0        241  2025-01-07\n2         49.0     -124.928         0        241  2025-01-07\n3         49.0     -124.892         0        241  2025-01-07\n4         49.0     -124.856         0        241  2025-01-07\n['2025-01-07']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      109.995145     246.445407\nstd       122.076934       6.509347\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        26.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-07_cumulative.csv\nGetting gridmet for day 2025-01-08\n2025-01-08 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-08_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean          6.087167\nstd          15.295603\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           0.000000\nmax         186.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  ...  cumulative_AMSR_SWE        date\n0         49.0     -125.000  ...                  0.0  2025-01-08\n1         49.0     -124.964  ...                  0.0  2025-01-08\n2         49.0     -124.928  ...                  0.0  2025-01-08\n3         49.0     -124.892  ...                  0.0  2025-01-08\n4         49.0     -124.856  ...                  0.0  2025-01-08\n[5 rows x 204 columns]\n['2025-01-08']\n(462204, 204)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean        6.087167     247.301646\nstd        15.295603       6.620045\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%         0.000000     241.000000\n75%         0.000000     254.000000\nmax       186.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-08_cumulative.csv\nGetting gridmet for day 2025-01-09\n2025-01-09 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-09_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        112.878763\nstd         121.892237\nmin           0.000000\n25%           0.000000\n50%          28.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000         0        241  2025-01-09\n1         49.0     -124.964         0        241  2025-01-09\n2         49.0     -124.928         0        241  2025-01-09\n3         49.0     -124.892         0        241  2025-01-09\n4         49.0     -124.856         0        241  2025-01-09\n['2025-01-09']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      112.878763     246.526118\nstd       121.892237       6.488804\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        28.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-09_cumulative.csv\nGetting gridmet for day 2025-01-10\n2025-01-10 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-10_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        129.589469\nstd         121.710311\nmin           0.000000\n25%           0.000000\n50%          60.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000       255        255  2025-01-10\n1         49.0     -124.964       255        255  2025-01-10\n2         49.0     -124.928       255        255  2025-01-10\n3         49.0     -124.892       255        255  2025-01-10\n4         49.0     -124.856       255        255  2025-01-10\n['2025-01-10']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      129.589469     247.398975\nstd       121.710311       6.626989\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        60.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-10_cumulative.csv\nGetting gridmet for day 2025-01-11\n2025-01-11 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-11_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        113.157452\nstd         122.920253\nmin           0.000000\n25%           0.000000\n50%          24.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000         0        241  2025-01-11\n1         49.0     -124.964         0        241  2025-01-11\n2         49.0     -124.928         0        241  2025-01-11\n3         49.0     -124.892         0        241  2025-01-11\n4         49.0     -124.856         0        241  2025-01-11\n['2025-01-11']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      113.157452     246.637152\nstd       122.920253       6.537375\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        24.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-11_cumulative.csv\nGetting gridmet for day 2025-01-12\n2025-01-12 00:00:00\n/home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-12_cumulative.csv_gap_filled.csv already exists, skipping..\ncount    462204.000000\nmean        123.285039\nstd         120.578394\nmin           0.000000\n25%           0.000000\n50%          45.000000\n75%         254.000000\nmax         255.000000\nName: AMSR_SWE, dtype: float64\nresult.head =     gridmet_lat  gridmet_lon  AMSR_SWE  AMSR_Flag        date\n0         49.0     -125.000         0        241  2025-01-12\n1         49.0     -124.964         0        241  2025-01-12\n2         49.0     -124.928         0        241  2025-01-12\n3         49.0     -124.892         0        241  2025-01-12\n4         49.0     -124.856         0        241  2025-01-12\n['2025-01-12']\n(462204, 5)\n            AMSR_SWE      AMSR_Flag\ncount  462204.000000  462204.000000\nmean      123.285039     247.024615\nstd       120.578394       6.627528\nmin         0.000000     241.000000\n25%         0.000000     241.000000\n50%        45.000000     241.000000\n75%       254.000000     254.000000\nmax       255.000000     255.000000\nNew data is saved to /home/chetana/data/amsr_testing/testing_ready_amsr_2025-01-12_cumulative.csv\n",
  "history_begin_time" : 1736925747457,
  "history_end_time" : 1736926334441,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hur0ow3zyfy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741915,
  "history_end_time" : 1736926334441,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bwd8cpxjavx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741922,
  "history_end_time" : 1736926334442,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "alviuiw94p8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741924,
  "history_end_time" : 1736926334442,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0em3wv8o3ow",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741927,
  "history_end_time" : 1736926334442,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4k1iyj90iwp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741929,
  "history_end_time" : 1736926334443,
  "history_notes" : null,
  "history_process" : "d4zcq6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vgwgnuo95y8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741931,
  "history_end_time" : 1736926334443,
  "history_notes" : null,
  "history_process" : "6x6myw",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yuh7nbtrnua",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741932,
  "history_end_time" : 1736926334443,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w4tea8mgww3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741934,
  "history_end_time" : 1736926334443,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gebjcgwegwh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741935,
  "history_end_time" : 1736926334444,
  "history_notes" : null,
  "history_process" : "ee5ur4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pj3pol3bobd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741936,
  "history_end_time" : 1736926334444,
  "history_notes" : null,
  "history_process" : "f03i7p",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9x0sks6bzg3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741938,
  "history_end_time" : 1736926334444,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yi83jm4dx6i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741939,
  "history_end_time" : 1736926334444,
  "history_notes" : null,
  "history_process" : "j8swco",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e6esctu7v7t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741940,
  "history_end_time" : 1736926334445,
  "history_notes" : null,
  "history_process" : "pnr64x",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jj55ibvokrk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741942,
  "history_end_time" : 1736926334445,
  "history_notes" : null,
  "history_process" : "qg80lj",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1ag4qx37o0u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741943,
  "history_end_time" : 1736926334445,
  "history_notes" : null,
  "history_process" : "ggy7gf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rg127lcx3tt",
  "history_input" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, data_dir, test_start_date, test_end_date, date_to_julian, cumulative_mode\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nos.chdir(f\"{data_dir}/fsca/\")\n\n# Set the PROJ_LIB environment variable\nenv = os.environ.copy()  # Get the current environment variables\nenv['PROJ_LIB'] = '/home/geo2021/anaconda3/share/proj'  # Set the path to PROJ_LIB\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    # if \"PROJ_LIB\" in os.environ:\n    #     os.environ.pop(\"PROJ_LIB\")\n    #     print(f\"Environment variable PROJ_LIB removed.\")\n    os.environ[\"PROJ_LIB\"] = \"/home/geo2021/anaconda3/share/proj/\"\n\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  count = 0\n  for i in date_list:\n    count += 1\n    if not cumulative_mode and count != len(date_list):\n        continue;\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    print(f\"Creating {target_output_tif}\")\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing(target_date = test_start_date):\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get target_date = \", target_date)\n  end_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  count = 0\n  for i in date_list:\n    count += 1\n    if not cumulative_mode and count != len(date_list):\n      continue;\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  if cumulative_mode:\n    add_time_series_columns(start_date, end_date, force=True)\n  else:\n    cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n    shutil.copy(outfile, cumulative_file_path)\n    print(f\"File is backed up to {cumulative_file_path}\")\n\ndef do_all_days():\n  # Convert string dates to datetime objects\n  start_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  end_date = datetime.strptime(test_end_date, \"%Y-%m-%d\")\n\n  # Loop over each day in the date range and call the function\n  current_date = start_date\n  while current_date <= end_date:\n      # Prepare the cumulative history CSVs for the current date\n      print(\"Getting gridmet for day\", current_date.strftime(\"%Y-%m-%d\"))\n      extract_data_for_testing(target_date=current_date.strftime(\"%Y-%m-%d\"))\n      \n      # Move to the next day\n      current_date += timedelta(days=1)\n  \n\nif __name__ == \"__main__\":\n  # SnowCover is missing from 10-12 to 10-23\n  # download_tiles_and_merge(datetime.strptime(\"2022-10-01\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-01\", \"%Y-%m-%d\"))\n  \n  do_all_days()\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  \n\n",
  "history_output" : "/home/chetana\ntoday date = 2025-01-15\nStart day: 2025-01-05, End day: 2025-01-12\ntest start date:  2025-01-05\ntest end date:  2025-01-12\nGetting gridmet for day 2025-01-05\nget target_date =  2025-01-05\n2025-01-05 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-05__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-05__snow_cover.tif exists. skip.\nextracting data for 2025-01-05\nThe file /home/chetana/data/fsca/final_output/2025-01-05_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-06\nget target_date =  2025-01-06\n2025-01-06 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-06__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-06__snow_cover.tif exists. skip.\nextracting data for 2025-01-06\nThe file /home/chetana/data/fsca/final_output/2025-01-06_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-07\nget target_date =  2025-01-07\n2025-01-07 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-07__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-07__snow_cover.tif exists. skip.\nextracting data for 2025-01-07\nThe file /home/chetana/data/fsca/final_output/2025-01-07_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-08\nget target_date =  2025-01-08\n2025-01-08 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-08__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-08__snow_cover.tif exists. skip.\nextracting data for 2025-01-08\nThe file /home/chetana/data/fsca/final_output/2025-01-08_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-09\nget target_date =  2025-01-09\n2025-01-09 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-09__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-09__snow_cover.tif exists. skip.\nextracting data for 2025-01-09\nThe file /home/chetana/data/fsca/final_output/2025-01-09_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-10\nget target_date =  2025-01-10\n2025-01-10 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-10__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-10__snow_cover.tif exists. skip.\nextracting data for 2025-01-10\nThe file /home/chetana/data/fsca/final_output/2025-01-10_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-11\nget target_date =  2025-01-11\n2025-01-11 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-11__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-11__snow_cover.tif exists. skip.\nextracting data for 2025-01-11\nThe file /home/chetana/data/fsca/final_output/2025-01-11_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\nGetting gridmet for day 2025-01-12\nget target_date =  2025-01-12\n2025-01-12 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nCreating /home/chetana/data/fsca/final_output//2025-01-12__snow_cover.tif\nfile_size_bytes: 509308\nThe file /home/chetana/data/fsca/final_output//2025-01-12__snow_cover.tif exists. skip.\nextracting data for 2025-01-12\nThe file /home/chetana/data/fsca/final_output/2025-01-12_output.csv exists. skip.\nFile is backed up to /home/chetana/data/fsca/final_output//2025-01-05_output.csv_cumulative.csv\n",
  "history_begin_time" : 1736925743263,
  "history_end_time" : 1736926334446,
  "history_notes" : null,
  "history_process" : "c2qa9u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xleq8gsb2on",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741946,
  "history_end_time" : 1736926334446,
  "history_notes" : null,
  "history_process" : "lnrsop",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qyrno45s2sh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741947,
  "history_end_time" : 1736926334446,
  "history_notes" : null,
  "history_process" : "c8isgf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4q1pfqcpm14",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741949,
  "history_end_time" : 1736926334447,
  "history_notes" : null,
  "history_process" : "16qpco",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "apzs8hcxuna",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, data_dir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nwater_mask_dir = f\"{data_dir}/water_mask/\"\nos.makedirs(water_mask_dir, exist_ok=True)\nos.chdir(water_mask_dir)\n\nenv = os.environ.copy()  # Get the current environment variables\nenv['PROJ_LIB'] = '/home/geo2021/anaconda3/share/proj'  # Set the path to PROJ_LIB\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\n\nwestern_us_coords = f'{data_dir}/srtm/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\ntarget_dir = os.path.dirname(mapper_file)\nos.makedirs(target_dir, exist_ok=True)\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    # if \"PROJ_LIB\" in os.environ:\n    #     os.environ.pop(\"PROJ_LIB\")\n    #     print(f\"Environment variable PROJ_LIB removed.\")\n    os.environ[\"PROJ_LIB\"] = \"\"\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=env)\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=env)\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=env)\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2024__water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "/home/chetana\ntoday date = 2025-01-15\nStart day: 2025-01-05, End day: 2025-01-12\ntest start date:  2025-01-05\ntest end date:  2025-01-12\nget test_start_date =  2025-01-05\n2025-01-05 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/data/water_mask/final_output/modis_to_dem_mapper.csv exists. skip.\nfile_size_bytes: 507830\nThe file /home/chetana/data/water_mask/final_output//2024__water_mask.tif exists. skip.\nextracting data for 2024-10-01\nThe file /home/chetana/data/water_mask/final_output/2024_output.csv exists. skip.\n",
  "history_begin_time" : 1736925743281,
  "history_end_time" : 1736926334447,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qlojg4gkmz3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741951,
  "history_end_time" : 1736926334448,
  "history_notes" : null,
  "history_process" : "uw1w1u",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ot65l4ortjq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741952,
  "history_end_time" : 1736926334448,
  "history_notes" : null,
  "history_process" : "14bhpn",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "crnq315dvgj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741954,
  "history_end_time" : 1736926334448,
  "history_notes" : null,
  "history_process" : "pyn9xn",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q0prljzzze8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741955,
  "history_end_time" : 1736926334449,
  "history_notes" : null,
  "history_process" : "h1952i",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ejcgduyfomw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741956,
  "history_end_time" : 1736926334449,
  "history_notes" : null,
  "history_process" : "k1aoz3",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "okifw46obh2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741958,
  "history_end_time" : 1736926334449,
  "history_notes" : null,
  "history_process" : "i66nk8",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1xycrzqtn29",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741959,
  "history_end_time" : 1736926334449,
  "history_notes" : null,
  "history_process" : "sacl4k",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "y6wodf27br2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741961,
  "history_end_time" : 1736926334449,
  "history_notes" : null,
  "history_process" : "f86ae7",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "b4gmgc4divc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741962,
  "history_end_time" : 1736926334450,
  "history_notes" : null,
  "history_process" : "7ktwm9",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "81d9f5nocvt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741963,
  "history_end_time" : 1736926334450,
  "history_notes" : null,
  "history_process" : "04fgyq",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0j7dwb8e7ga",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741965,
  "history_end_time" : 1736926334450,
  "history_notes" : null,
  "history_process" : "dwa3fy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "90amwzdqzms",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1736925741966,
  "history_end_time" : 1736926334450,
  "history_notes" : null,
  "history_process" : "gz5syq",
  "host_id" : "100001",
  "indicator" : "Stopped"
}]
