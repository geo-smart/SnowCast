[{
  "history_id" : "pgyvut80mk1",
  "history_input" : "# Data Preparation for Sentinel 2\n\nprint(\"Not ready yet..Prepare sentinel 2 into .csv\")\n\n",
  "history_output" : "Not ready yet..Prepare sentinel 2 into .csv\n",
  "history_begin_time" : 1656360652758,
  "history_end_time" : 1656360652885,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "h8qcvvud7gd",
  "history_input" : "# Create LSTM model\n\nprint(\"Create LSTM\")\n\n",
  "history_output" : "",
  "history_begin_time" : 1656360650399,
  "history_end_time" : 1656360651206,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "qx7hkuhk862",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass RandomForestHole(BaseHole):\n  \n  all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n  \n\n  def preprocessing(self):\n    all_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n    all_ready_pd = all_ready_pd.fillna(10000) # replace all nan with 10000\n    train, test = train_test_split(all_ready_pd, test_size=0.2)\n    self.train_x, self.train_y = train[['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']].to_numpy().astype('float'), train['swe'].to_numpy().astype('float')\n    self.test_x, self.test_y = test[['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']].to_numpy().astype('float'), test['swe'].to_numpy().astype('float')\n  \n  def get_model(self):\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    return rfc_pipeline\n\n  def evaluate(self):\n    mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n    mse = metrics.mean_squared_error(self.test_y, self.test_y_results)\n    r2 = metrics.r2_score(self.test_y, self.test_y_results)\n    rmse = math.sqrt(mse)\n\n    print(\"The random forest model performance for testing set\")\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    return {\"mae\":mae, \"mse\": mse, \"r2\": r2, \"rmse\": rmse}\n  \n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\qx7hkuhk862\\model_creation_rf.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360653179,
  "history_end_time" : 1656360655284,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "fj4f2uezx0h",
  "history_input" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "history_output" : "Create GhostNet\n",
  "history_begin_time" : 1656360650397,
  "history_end_time" : 1656360651201,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ao7a70gzzzw",
  "history_input" : "# Find the best model\nprint(\"model comparison script\")\nprint(\"hello world\")",
  "history_output" : "model comparison script\nhello world\n",
  "history_begin_time" : 1656360661315,
  "history_end_time" : 1656360661428,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "uhjz2r5deu4",
  "history_input" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom datetime import datetime\n\n\nprint(\"integrating datasets into one dataset\")\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\n#example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\n#print(training_feature_pd.head())\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n#print(station_cell_mapper_pd.head())\n\n#example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n#print(example_mod_pd.shape)\ndef getDateStr(x):\n  return x.split(\" \")[0]\n\ndef integrate_modis():\n  \"\"\"\n  Integrate all MODIS data into mod_all.csv\n  \"\"\"\n  all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n  if os.path.isfile(all_mod_file):\n    return\n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  mod_all_df = pd.DataFrame(columns=[\"date\"])\n  mod_all_df['date'] = dates\n  \n  #print(mod_all_df.head())\n  for ind in station_cell_mapper_pd.index:\n    current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n    print(current_cell_id)\n    mod_single_file = f\"{github_dir}/data/sat_training/modis/mod10a1_ndsi_{current_cell_id}.csv\"\n    if os.path.isfile(mod_single_file):\n      mod_single_pd = pd.read_csv(mod_single_file, header=0)\n      mod_single_pd = mod_single_pd[[\"date\", \"mod10a1_ndsi\"]]\n      mod_single_pd = mod_single_pd.rename(columns={\"mod10a1_ndsi\": current_cell_id})\n      mod_single_pd['date'] = pd.to_datetime(mod_single_pd['date']).astype(str)\n      print(mod_all_df.shape)\n      mod_all_df = pd.merge(mod_all_df, mod_single_pd, how='left', on=\"date\")\n  mod_all_df.to_csv(all_mod_file)\n\n  \ndef integrate_sentinel1():\n  \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  \"\"\"\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n  if os.path.isfile(all_sentinel1_file):\n    return\n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  sentinel1_all_df = pd.DataFrame(columns=[\"date\"])\n  sentinel1_all_df['date'] = dates\n  #print(mod_all_df.head())\n  \n  for ind in station_cell_mapper_pd.index:\n    current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n    print(current_cell_id)\n    sentinel1_single_file = f\"{github_dir}/data/sat_training/sentinel1/s1_grd_vv_{current_cell_id}.csv\"\n    if os.path.isfile(sentinel1_single_file) and current_cell_id not in sentinel1_all_df :\n      sentinel1_single_pd = pd.read_csv(sentinel1_single_file, header=0)\n      sentinel1_single_pd = sentinel1_single_pd[[\"date\", \"s1_grd_vv\"]]\n      sentinel1_single_pd = sentinel1_single_pd.rename(columns={\"s1_grd_vv\": current_cell_id})\n      #sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n      sentinel1_single_pd['date'] = pd.to_datetime(sentinel1_single_pd['date']).dt.round(\"D\").astype(str)\n      print(\"sentinel1_single_pd: \", sentinel1_single_pd.head())\n      print(\"sentinel1_single_pd check value: \", sentinel1_single_pd[sentinel1_single_pd[\"date\"]==\"2015-04-01\"])\n      sentinel1_single_pd = sentinel1_single_pd.drop_duplicates(subset=['date'], keep='first') # this will remove all the other values of the same day\n      \n      sentinel1_all_df = pd.merge(sentinel1_all_df, sentinel1_single_pd, how='left', on=\"date\")\n      print(\"sentinel1_all_df check value: \", sentinel1_all_df[sentinel1_all_df[\"date\"]==\"2015-04-01\"])\n      print(\"sentinel1_all_df: \", sentinel1_all_df.shape)\n      \n\n  print(sentinel1_all_df.shape)\n  sentinel1_all_df.to_csv(all_sentinel1_file)\n\ndef integrate_gridmet():\n  \"\"\"\n  Integrate all gridMET data into gridmet_all.csv\n  \"\"\"\n  \n  \n  dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n  \n  #print(mod_all_df.head())\n  var_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n  \n  for var in var_list:\n    gridmet_all_df = pd.DataFrame(columns=[\"date\"])\n    gridmet_all_df['date'] = dates\n    all_gridmet_file = f\"{github_dir}/data/ready_for_training/gridmet_{var}_all.csv\"\n    if os.path.isfile(all_gridmet_file):\n      return\n    for ind in station_cell_mapper_pd.index:\n      current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n      print(current_cell_id)\n      gridmet_single_file = f\"{github_dir}/data/sim_training/gridmet/{var}_{current_cell_id}.csv\"\n      if os.path.isfile(gridmet_single_file) and current_cell_id not in gridmet_all_df :\n        gridmet_single_pd = pd.read_csv(gridmet_single_file, header=0)\n        gridmet_single_pd = gridmet_single_pd[[\"date\", var]]\n        gridmet_single_pd = gridmet_single_pd.rename(columns={var: current_cell_id})\n        #sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n        gridmet_single_pd['date'] = pd.to_datetime(gridmet_single_pd['date']).dt.round(\"D\").astype(str)\n        print(\"gridmet_single_pd: \", gridmet_single_pd.head())\n        print(\"gridmet_single_pd check value: \", gridmet_single_pd[gridmet_single_pd[\"date\"]==\"2015-04-01\"])\n        gridmet_single_pd = gridmet_single_pd.drop_duplicates(subset=['date'], keep='first') # this will remove all the other values of the same day\n\n        gridmet_all_df = pd.merge(gridmet_all_df, gridmet_single_pd, how='left', on=\"date\")\n        print(\"gridmet_all_df check value: \", gridmet_all_df[gridmet_all_df[\"date\"]==\"2015-04-01\"])\n        print(\"gridmet_all_df: \", gridmet_all_df.shape)\n      \n    print(gridmet_all_df.shape)\n    gridmet_all_df.to_csv(all_gridmet_file)\n  \n  \ndef prepare_training_csv():\n  \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n  all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n  if os.path.isfile(all_ready_file):\n      return\n  \n  all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n  modis_all_pd = pd.read_csv(all_mod_file, header=0)\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n  sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0)\n  all_gridmet_eto_file = f\"{github_dir}/data/ready_for_training/gridmet_eto_all.csv\"\n  gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col = 0)\n  all_gridmet_pr_file = f\"{github_dir}/data/ready_for_training/gridmet_pr_all.csv\"\n  gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col = 0)\n  all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_training/gridmet_rmax_all.csv\"\n  gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col = 0)\n  all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_training/gridmet_rmin_all.csv\"\n  gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col = 0)\n  all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmn_all.csv\"\n  gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col = 0)\n  all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmx_all.csv\"\n  gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col = 0)\n  all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_training/gridmet_vpd_all.csv\"\n  gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col = 0)\n  all_gridmet_vs_file = f\"{github_dir}/data/ready_for_training/gridmet_vs_all.csv\"\n  gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col = 0)\n  \n  grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n  grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col = 1)\n  \n  print(\"modis_all_size: \", modis_all_pd.shape)\n  print(\"station size: \", station_cell_mapper_pd.shape)\n  print(\"training_feature_pd size: \", training_feature_pd.shape)\n  print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n  \n  all_training_pd = pd.DataFrame(columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe\"])\n  all_training_pd = all_training_pd.reset_index()\n  for index, row in modis_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    month = dt.month\n    year = dt.year\n    doy = dt.timetuple().tm_yday\n    print(f\"Dealing {year} {doy}\")\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i][:-2]\n      if cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        ndsi = row.values[i]\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        grd = sentinel1_all_pd.loc[index, cell_id]\n        eto = gridmet_eto_all_pd.loc[index, cell_id]\n        pr = gridmet_pr_all_pd.loc[index, cell_id]\n        rmax = gridmet_rmax_all_pd.loc[index, cell_id]\n        rmin = gridmet_rmin_all_pd.loc[index, cell_id]\n        tmmn = gridmet_tmmn_all_pd.loc[index, cell_id]\n        tmmx = gridmet_tmmx_all_pd.loc[index, cell_id]\n        vpd = gridmet_vpd_all_pd.loc[index, cell_id]\n        vs = gridmet_vs_all_pd.loc[index, cell_id]\n        lat = grid_terrain_pd.loc[cell_id, \"Longitude [deg]\"]\n        lon = grid_terrain_pd.loc[cell_id, \"Latitude [deg]\"]\n        elevation = grid_terrain_pd.loc[cell_id, \"Elevation [m]\"]\n        aspect = grid_terrain_pd.loc[cell_id, \"Aspect [deg]\"]\n        curvature = grid_terrain_pd.loc[cell_id, \"Curvature [ratio]\"]\n        slope = grid_terrain_pd.loc[cell_id, \"Slope [deg]\"]\n        eastness = grid_terrain_pd.loc[cell_id, \"Eastness [unitCirc.]\"]\n        northness = grid_terrain_pd.loc[cell_id, \"Northness [unitCirc.]\"]\n        \n        if not np.isnan(swe):\n          json_kv = {\"cell_id\": cell_id, \"year\":year, \"m\":month, \"doy\": doy, \"ndsi\":ndsi, \"grd\":grd, \"eto\":eto, \"pr\":pr, \"rmax\":rmax, \"rmin\":rmin, \"tmmn\":tmmn, \"tmmx\":tmmx, \"vpd\":vpd, \"vs\":vs, \"lat\":lat, \"lon\":lon, \"elevation\":elevation, \"aspect\":aspect, \"curvature\":curvature, \"slope\":slope, \"eastness\":eastness, \"northness\":northness, \"swe\":swe}\n          # print(json_kv)\n          all_training_pd = all_training_pd.append(json_kv, ignore_index = True)\n  \n  print(all_training_pd.shape)\n  all_training_pd.to_csv(all_ready_file)\n  \n  \"\"\"\n  grd_all_pd = pd.DataFrame(columns=[\"year\", \"m\", \"doy\", \"grd\", \"swe\"])\n  grd_all_pd = grd_all_pd.reset_index()\n  for index, row in sentinel1_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    year = dt.year\n    month = dt.month\n    doy = dt.timetuple().tm_yday\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i]\n      grd = row.values[i]\n      if not np.isnan(grd) and cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        if not np.isnan(swe):\n          print([month, doy, grd, swe])\n          grd_all_pd = grd_all_pd.append({\"year\": year, \"m\":month, \"doy\": doy, \"grd\": grd, \"swe\": swe}, ignore_index = True)\n  \n  print(grd_all_pd.shape)\n  grd_all_pd.to_csv(f\"{github_dir}/data/ready_for_training/sentinel1_ready.csv\")\n  \"\"\"\n  \n#exit() # done already\n\n#integrate_modis()\n#integrate_sentinel1()\n#integrate_gridmet()\n#prepare_training_csv()\n\n\n  \n  \n  \n",
  "history_output" : "",
  "history_begin_time" : 1656360657630,
  "history_end_time" : 1656360658276,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "l3r1b7sll35",
  "history_input" : "# Deploy model to service\n\nprint(\"deploy model to service\")\n",
  "history_output" : "deploy model to service\n",
  "history_begin_time" : 1656360663257,
  "history_end_time" : 1656360663365,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "wpq6wsigduc",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit()  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\wpq6wsigduc\\service_prediction.py\", line 7, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360664476,
  "history_end_time" : 1656360666291,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "l4vxsij8zse",
  "history_input" : "from model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\n\nprint(\"Train Models\")\n\nworm_holes = [RandomForestHole(), XGBoostHole()]\n\nfor hole in worm_holes:\n  hole.preprocessing()\n  print(hole.train_x.dtype)\n  print(hole.train_y.dtype)\n  print(hole.train_x.shape)\n  print(hole.train_y.shape)\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  \nprint(\"Finished training and validating all the models.\")\n",
  "history_output" : "",
  "history_begin_time" : 1656360658292,
  "history_end_time" : 1656360659235,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "prjszgr1wma",
  "history_input" : "# Test models\n\n# Random Forest model creation and save to file\n\nfrom sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom datetime import datetime\n\ndef turn_doy_to_date(year, doy):\n  doy = int(doy)\n  doy = str(doy)\n  doy.rjust(3 + len(doy), '0')\n  #res = datetime.strptime(str(year) + \"-\" + doy, \"%Y-%j\").strftime(\"%m/%d/%Y\")\n  res = datetime.strptime(str(year) + \"-\" + doy, \"%Y-%j\").strftime(\"%Y-%m-%d\")\n  return res\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\ntest_ready_file = f\"{github_dir}/data/ready_for_testing/all_ready_3.csv\"\ntest_ready_pd = pd.read_csv(test_ready_file, header=0, index_col=0)\nsubmission_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_pd = pd.read_csv(submission_file, header=0, index_col=0)\npredicted_file = f\"{homedir}/Documents/GitHub/SnowCast/data/results/wormhole_output_4.csv\"\n\ntrain_cols = ['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']\n\nprint(test_ready_pd.shape)\npd_to_clean = test_ready_pd[train_cols]\nprint(\"PD shape: \", pd_to_clean.shape)\n\ndoy_list = test_ready_pd[\"doy\"].unique()\nprint(doy_list)\n\ndate_list = [turn_doy_to_date(2022, doy_list[i]) for i in range(len(doy_list)) ]\nprint(date_list)\n\nall_features = pd_to_clean.to_numpy()\nall_features = np.nan_to_num(all_features)\nprint(\"train feature shape: \", all_features.shape)\n#all_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\n#all_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/SnowCast/model/wormhole_20221305163806.joblib\")\ny_predicted = best_random.predict(all_features)\nprint(y_predicted) #first got daily prediction\n\ntarget_dates = [\"2022-01-13\",\"2022-01-20\",\"2022-01-27\",\"2022-02-03\",\"2022-02-10\",\"2022-02-17\",\"2022-02-24\",\"2022-03-03\",\"2022-03-10\",\"2022-03-17\",\"2022-03-24\",\"2022-03-31\",\"2022-04-07\",\"2022-04-14\",\"2022-04-21\",\"2022-04-28\",\"2022-05-05\",\"2022-05-12\",\"2022-05-19\",\"2022-05-26\",\"2022-06-02\",\"2022-06-09\",\"2022-06-16\",\"2022-06-23\",\"2022-06-30\"]\nprint(\"taregt date list: \", len(target_dates))\n\ndaily_predictions = pd.DataFrame(columns = target_dates, index = submission_pd.index)\nfor i in range(len(y_predicted)):\n  doy = all_features[i][2]\n  #print(doy)\n  ndate = turn_doy_to_date(2022, doy)\n  if ndate in target_dates:\n    cell_id = test_ready_pd[\"cell_id\"].iloc[i]\n    daily_predictions.at[cell_id, ndate] = y_predicted[i]\n  #print(ndate, cell_id)\n  #print(y_predicted[i])\n  \nprint(daily_predictions.shape)\n#daily_predictions = daily_predictions[[\"2022-01-13\"]]\n\nif os.path.exists(predicted_file):\n  os.remove(predicted_file)\n  \ndaily_predictions.fillna(0.0, inplace=True)\ndaily_predictions.to_csv(predicted_file, date_format=\"%Y-%d-%m\")\n\n\n# turn daily into weekly using mean values\n\n\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\prjszgr1wma\\model_test.py\", line 9, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360659297,
  "history_end_time" : 1656360661244,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "knwtwdq4fjd",
  "history_input" : "# Load dependencies\nimport geopandas as gpd\nimport json\nimport geojson\nfrom pystac_client import Client\nimport planetary_computer\nimport xarray\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyproj import Proj, transform\nimport os\nimport sys, traceback\nimport requests\n\nhome_dir = os.path.expanduser('~')\nsnowcast_github_dir = f\"{home_dir}/Documents/GitHub/SnowCast/\"\n\n#exit() # this process no longer need to execute, we need to make Geoweaver to specify which process doesn't need to run\n\n# user-defined paths for data-access\ndata_dir = f'{snowcast_github_dir}data/'\ngridcells_file = data_dir+'snowcast_provided/grid_cells_eval.geojson'\nstations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\ngridcells_outfile = data_dir+'terrain/gridcells_terrainData_eval.csv'\nstations_outfile = data_dir+'terrain/station_terrainData_eval.csv'\n\nrequests.get('https://planetarycomputer.microsoft.com/api/stac/v1')\n\n# setup client for handshaking and data-access\nprint(\"setup planetary computer client\")\nclient = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",ignore_conformance=True)\n\n# Load metadata\ngridcellsGPD = gpd.read_file(gridcells_file)\ngridcells = geojson.load(open(gridcells_file))\nstations = pd.read_csv(stations_file)\n\n# instantiate output panda dataframes\ndf_gridcells = df = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                            \"Elevation [m]\",\"Aspect [deg]\",\n                                            \"Curvature [ratio]\",\"Slope [deg]\",\n                                            \"Eastness [unitCirc.]\",\"Northness [unitCirc.]\"))\ndf_station = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                   \"Elevation [m]\",\"Elevation_30 [m]\",\"Elevation_1000 [m]\",\n                                   \"Aspect_30 [deg]\",\"Aspect_1000 [deg]\",\n                                   \"Curvature_30 [ratio]\",\"Curvature_1000 [ratio]\",\n                                   \"Slope_30 [deg]\",\"Slope_1000 [deg]\",\n                                   \"Eastness_30 [unitCirc.]\",\"Northness_30 [unitCirc.]\",\n                                   \"Eastness_1000 [unitCirc.]\",\"Northness_1000 [unitCirc.]\"))\n\ndef prepareGridCellTerrain():\n  # instantiate output panda dataframes\n  # Calculate gridcell characteristics using Copernicus DEM data\n  print(\"Prepare GridCell Terrain data\")\n  for idx,cell in enumerate(gridcells['features']):\n      print(\"Processing grid \", idx)\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Polygon\", \"coordinates\":cell['geometry']['coordinates']},\n      )\n      items = list(search.get_items())\n      print(\"==> Searched items: \", len(items))\n\n      cropped_data = None\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              #xarray.open_rasterio(signed_asset.href)\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n      except:\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n\n      # calculate lat/long of center of gridcell\n      longitude = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      latitude = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      print(\"reproject data to EPSG:32612\")\n      # reproject the cropped dem data\n      cropped_data = cropped_data.rio.reproject(\"EPSG:32612\")\n\n      # Mean elevation of gridcell\n      mean_elev = cropped_data.mean().values\n      print(\"Elevation: \", mean_elev)\n\n      # Calculate directional components\n      aspect = xrspatial.aspect(cropped_data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(\"Aspect: \", mean_aspect)\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n      print(\"Eastness: \", mean_eastness)\n      print(\"Northness: \", mean_northness)\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(cropped_data)\n      mean_curvature = curvature.mean().values\n      print(\"Curvature: \", mean_curvature)\n\n      # Calculate mean slope\n      slope = xrspatial.slope(cropped_data)\n      mean_slope = slope.mean().values\n      print(\"Slope: \", mean_slope)\n\n      # Fill pandas dataframe\n      df_gridcells.loc[idx] = [longitude,latitude,\n                               mean_elev,mean_aspect,\n                               mean_curvature,mean_slope,\n                               mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n      #     df_gridcells.to_csv(gridcells_outfile)\n\n  # Save output data into csv format\n  df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n  df_gridcells.to_csv(gridcells_outfile)\n\ndef prepareStationTerrain():\n  # Calculate terrain characteristics of stations, and surrounding regions using COP 30\n  for idx,station in stations.iterrows():\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Point\", \"coordinates\":[station['longitude'],station['latitude']]},\n      )\n      items = list(search.get_items())\n      print(f\"Returned {len(items)} items\")\n\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-station['longitude'])\n          ydiff = np.abs(data.y-station['latitude'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n      except:\n          traceback.print_exc(file=sys.stdout)\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-station['longitude'])\n          ydiff = np.abs(data.y-station['latitude'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n\n      # Reproject the station data to better include only 1000m surrounding area\n      inProj = Proj(init='epsg:4326')\n      outProj = Proj(init='epsg:32612')\n      new_x,new_y = transform(inProj,outProj,station['longitude'],station['latitude'])\n\n      # Calculate elevation of station and surroundings\n      mean_elevation = data.mean().values\n      elevation = data.sel(x=new_x,y=new_y,method='nearest')\n      print(elevation.values)\n\n      # Calcuate directional components\n      aspect = xrspatial.aspect(data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(mean_aspect)\n      aspect = aspect.sel(x=new_x,y=new_y,method='nearest')\n      print(aspect.values)\n      eastness = np.cos(aspect*(np.pi/180))\n      northness = np.sin(aspect*(np.pi/180))\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(data)\n      mean_curvature = curvature.mean().values\n      curvature = curvature.sel(x=new_x,y=new_y,method='nearest')\n      print(curvature.values)\n\n      # Calculate slope\n      slope = xrspatial.slope(data)\n      mean_slope = slope.mean().values\n      slope = slope.sel(x=new_x,y=new_y,method='nearest')\n      print(slope.values)\n\n      # Fill pandas dataframe\n      df_station.loc[idx] = [station['longitude'],station['latitude'],\n                             station['elevation_m'],elevation.values,mean_elevation,\n                             aspect.values,mean_aspect,\n                             curvature.values,mean_curvature,\n                             slope.values,mean_slope,\n                             eastness.values,northness.values,\n                             mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n      #     df_station.to_csv(stations_outfile)\n\n  # Save output data into CSV format\n  df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n  df_station.to_csv(stations_outfile)\n\ntry:\n  prepareGridCellTerrain()\n  #prepareStationTerrain()\nexcept:\n  traceback.print_exc(file=sys.stdout)\n",
  "history_output" : "",
  "history_begin_time" : 1656360653227,
  "history_end_time" : 1656360653339,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "on86ofmrftj",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n\n#org_name = 'sentinel1'\n#product_name = 'COPERNICUS/S1_GRD'\n#var_name = 'VV'\n#column_name = 's1_grd_vv'\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n  \t  single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/modis/{column_name}_{current_cell_id}.csv\"\n\n  \t  if os.path.exists(single_csv_file):\n  \t    print(\"exists skipping..\")\n  \t    continue\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate('2013-01-01','2017-12-31')\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df1 = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  viirs2 = ee.ImageCollection(product_name).filterDate('2018-01-01','2021-12-31')\n  \t  poi_reduced_imgs2 = viirs2.map(poi_mean)\n  \t  nested_list2 = poi_reduced_imgs2.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df2 = pd.DataFrame(nested_list2.getInfo(), columns=['date',column_name])\n      \n\n  \t  df = pd.concat([df1, df2])\n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\on86ofmrftj\\data_gee_modis_station_only.py\", line 8, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360656299,
  "history_end_time" : 1656360657270,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "xdufnkp4ktw",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}_{current_cell_id}.csv\"\n\n      if os.path.exists(single_csv_file):\n          print(\"exists skipping..\")\n          continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      viirs = ee.ImageCollection(product_name).filterDate('2013-01-01','2021-12-31').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\xdufnkp4ktw\\data_gee_sentinel1_station_only.py\", line 8, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360655365,
  "history_end_time" : 1656360656320,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "s2mgzb2bp9d",
  "history_input" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\n\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n\nready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\nresult_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\n\nif os.path.exists(result_mapping_file):\n    exit()\n\n\ngridcells = geojson.load(open(gridcells_file))\ntraining_df = pd.read_csv(training_feature_file, header=0)\ntesting_df = pd.read_csv(testing_feature_file, header=0)\nground_measure_metadata_df = pd.read_csv(ground_measure_metadata_file, header=0)\ntrain_labels_df = pd.read_csv(train_labels_file, header=0)\n\nprint(\"training: \", training_df.head())\nprint(\"testing: \", testing_df.head())\nprint(\"ground measure metadata: \", ground_measure_metadata_df.head())\nprint(\"training labels: \", train_labels_df.head())\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1-lat2)**2 + (lon1-lon2)**2)\n  \n# prepare the training data\n\nstation_cell_mapper_df = pd.DataFrame(columns = [\"station_id\", \"cell_id\", \"lat\", \"lon\"])\n\nground_measure_metadata_df = ground_measure_metadata_df.reset_index()  # make sure indexes pair with number of rows\nfor index, row in ground_measure_metadata_df.iterrows():\n  \t\n    print(row['station_id'], row['name'], row['latitude'], row['longitude'])\n    station_lat = row['latitude']\n    station_lon = row['longitude']\n    \n    shortest_dis = 999\n    associated_cell_id = None\n    associated_lat = None\n    associated_lon = None\n    \n    for idx,cell in enumerate(gridcells['features']):\n    \n      current_cell_id = cell['properties']['cell_id']\n\n      #print(\"collecting \", current_cell_id)\n      cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      dist = calculateDistance(station_lat, station_lon, cell_lat, cell_lon)\n\n      if dist < shortest_dis:\n        associated_cell_id = current_cell_id\n        shortest_dis = dist\n        associated_lat = cell_lat\n        associated_lon = cell_lon\n    \n    station_cell_mapper_df.loc[len(station_cell_mapper_df.index)] = [row['station_id'], associated_cell_id, associated_lat, associated_lon]\n    \nprint(station_cell_mapper_df.head())\nstation_cell_mapper_df.to_csv(f\"{ready_for_training_folder}station_cell_mapping.csv\")\n    \n\n\n      \n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\s2mgzb2bp9d\\data_associate_station_grid_cell.py\", line 4, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360650397,
  "history_end_time" : 1656360652160,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "r2w392a61g6",
  "history_input" : "# This script will download modis data for all the testing sites from Google Earth Engine.\n# The start date is the last stop date of the last run.\n\nfrom all_dependencies import *\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\n\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_df = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n#start_date = \"2022-04-20\"#test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sat_testing/modis\", \"%Y-%m-%d\")\nend_date = test_end_date\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\nif os.path.exists(final_csv_file):\n    #print(\"exists exiting..\")\n    #exit()\n    os.remove(final_csv_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\nprint(\"start to traverse the cells in submission_format_eval.csv..\")\n\nfor current_cell_id in submission_format_df.index:\n    \n    try:\n      \n  \t  longitude = all_cell_coords_df['lon'][current_cell_id]\n  \t  latitude = all_cell_coords_df['lat'][current_cell_id]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate(start_date, end_date)\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  #df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      print(traceback.format_exc())\n      print(\"failed\", e)\n      pass\n    \n    \nall_cell_df.to_csv(final_csv_file)  \n\nprint(f\"All points have been saved to {final_csv_file}\")\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\r2w392a61g6\\data_gee_modis_real_time.py\", line 4, in <module>\n    from all_dependencies import *\nModuleNotFoundError: No module named 'all_dependencies'\n",
  "history_begin_time" : 1656360653194,
  "history_end_time" : 1656360653334,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "k6a5wsg79ts",
  "history_input" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nfrom all_dependencies import *\nfrom snowcast_utils import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\n\nprint(\"submission_format_df shape: \", submission_format_df.shape)\n\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_df = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n#start_date = \"2022-04-20\"#test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1\",\"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\n\nif os.path.exists(final_csv_file):\n    #print(\"exists skipping..\")\n    #exit()\n    os.remove(final_csv_file)\n\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor current_cell_id in submission_format_df.index:\n  \n    try:\n  \t\n      #print(\"collecting \", current_cell_id)\n      \n      longitude = all_cell_coords_df['lon'][current_cell_id]\n      latitude = all_cell_coords_df['lat'][current_cell_id]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(10)\n\n      viirs = ee.ImageCollection(product_name) \\\n          \t.filterDate(start_date, end_date) \\\n            .filterBounds(poi) \\\n          \t.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n      \t\t.select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      #print(e)\n      pass\n    \nall_cell_df.to_csv(final_csv_file)\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\k6a5wsg79ts\\data_gee_sentinel1_real_time.py\", line 5, in <module>\n    from all_dependencies import *\nModuleNotFoundError: No module named 'all_dependencies'\n",
  "history_begin_time" : 1656360657347,
  "history_end_time" : 1656360657464,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "58vahkgnmrs",
  "history_input" : "'''\nThe wrapper for all the snowcast_wormhole predictors\n'''\nimport os\nimport joblib\nfrom datetime import datetime\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass BaseHole:\n  \n  def __init__(self):\n    self.classifier = self.get_model()\n    self.train_x = None\n    self.train_y = None\n    self.test_x = None\n    self.test_y = None\n    self.test_y_results = None\n    self.save_file = None\n    \n  def save(self):\n    now = datetime.now()\n    date_time = now.strftime(\"%Y%d%m%H%M%S\")\n    self.save_file = f\"{github_dir}/model/wormhole_{date_time}.joblib\"\n    print(f\"Saving model to {self.save_file}\")\n    joblib.dump(self.classifier, self.save_file)\n  \n  def preprocessing(self):\n    pass\n  \n  def train(self):\n    self.classifier.fit(self.train_x, self.train_y)\n  \n  def test(self):\n    self.test_y_results = self.classifier.predict(self.test_x)\n    return self.test_y_results\n  \n  def predict(self, input_x):\n    return self.classifier.predict(input_x)\n  \n  def evaluate(self):\n    pass\n  \n  def get_model(self):\n    pass\n  \n  def post_processing(self):\n    pass",
  "history_output" : "",
  "history_begin_time" : 1656360650399,
  "history_end_time" : 1656360651737,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "tat81la5ni8",
  "history_input" : "\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = '2013-01-01'\nend_date = '2021-12-31'\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\nfor var in var_list:\n\n    var_name = var\n    column_name = var\n\n    dfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n    if not os.path.exists(dfolder):\n        os.makedirs(dfolder)\n\n    all_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\n    for ind in station_cell_mapper_df.index:\n\n        try:\n\n          current_cell_id = station_cell_mapper_df['cell_id'][ind]\n          print(\"collecting \", current_cell_id)\n          single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n          if os.path.exists(single_csv_file):\n              print(\"exists skipping..\")\n              continue\n\n          longitude = station_cell_mapper_df['lon'][ind]\n          latitude = station_cell_mapper_df['lat'][ind]\n\n          # identify a 500 meter buffer around our Point Of Interest (POI)\n          poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n          viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_name)\n\n          def poi_mean(img):\n              reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n              mean = reducer.get(var_name)\n              return img.set('date', img.date().format()).set(column_name,mean)\n\n\n          poi_reduced_imgs = viirs.map(poi_mean)\n\n          nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n          # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n          df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n          df['date'] = pd.to_datetime(df['date'])\n          df = df.set_index('date')\n\n          df['cell_id'] = current_cell_id\n          df['latitude'] = latitude\n          df['longitude'] = longitude\n          df.to_csv(single_csv_file)\n\n          df_list = [all_cell_df, df]\n          all_cell_df = pd.concat(df_list) # merge into big dataframe\n\n        except Exception as e:\n\n          print(e)\n          pass\n    \n    all_cell_df.to_csv(f\"{dfolder}/{column_name}.csv\")  \n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\tat81la5ni8\\data_gee_gridmet_station_only.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360654256,
  "history_end_time" : 1656360655237,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "5a70kqpq40k",
  "history_input" : "\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import date\nfrom snowcast_utils import *\nimport traceback\n\n\n# exit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_pd = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\nprint(submission_format_df.shape)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\n#start_date = \"2022-04-20\"#test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sim_testing/{org_name}/\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n#start_date = \"2022-04-06\"\n#end_date = \"2022-04-18\"\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_testing/{org_name}/\"\nif not os.path.exists(dfolder):\n  os.makedirs(dfolder)\n  \ncolumn_list = ['date', 'cell_id', 'latitude', 'longitude']\ncolumn_list.extend(var_list)\nreduced_column_list = ['date']\nreduced_column_list.extend(var_list)\n\nall_cell_df = pd.DataFrame(columns = column_list)\n\ncount = 0\n\nfor current_cell_id in submission_format_df.index:\n\n  try:\n    count+=1\n    print(f\"=> Collected GridMet data for {count} cells\")\n    print(\"collecting \", current_cell_id)\n    #single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n    #if os.path.exists(single_csv_file):\n    #  os.remove(single_csv_file)\n    #  print(\"exists skipping..\")\n    #  continue\n\n    longitude = all_cell_coords_pd['lon'][current_cell_id]\n    latitude = all_cell_coords_pd['lat'][current_cell_id]\n\n    # identify a 500 meter buffer around our Point Of Interest (POI)\n    poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n    viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_list)\n\n    def poi_mean(img):\n      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n      img = img.set('date', img.date().format());\n      for var in var_list:\n        column_name = var\n        mean = reducer.get(column_name)\n        img = img.set(column_name,mean)\n      return img\n\n\n    poi_reduced_imgs = viirs.map(poi_mean)\n\n    nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(9), reduced_column_list).values().get(0)\n\n    # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n    df = pd.DataFrame(nested_list.getInfo(), columns=reduced_column_list)\n\n    df['date'] = pd.to_datetime(df['date'])\n    df = df.set_index('date')\n\n    df['cell_id'] = current_cell_id\n    df['latitude'] = latitude\n    df['longitude'] = longitude\n    #df.to_csv(single_csv_file)\n\n    #print(df.head())\n    \n    df_list = [all_cell_df, df]\n    all_cell_df = pd.concat(df_list) # merge into big dataframe\n    \n    #if count % 4 == 0:\n\n  except Exception as e:\n    print(traceback.format_exc())\n    print(\"Failed: \", e)\n    pass\n\nall_cell_df.to_csv(f\"{dfolder}/all_vars_{start_date}_{end_date}.csv\")  \n\n\n",
  "history_output" : "",
  "history_begin_time" : 1656360657082,
  "history_end_time" : 1656360657270,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ptfn1ne747t",
  "history_input" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass XGBoostHole(RandomForestHole):\n\n  def get_model(self):\n    \"\"\"\n    rfc_pipeline = Pipeline(steps = [\n      ('data_scaling', StandardScaler()),\n      ('model', RandomForestRegressor(max_depth = 15,\n                                       min_samples_leaf = 0.004,\n                                       min_samples_split = 0.008,\n                                       n_estimators = 25))])\n    #return rfc_pipeline\n  \t\"\"\"\n    etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    #min_impurity_split=None, \n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n    return etmodel\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\ptfn1ne747t\\model_creation_xgboost.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360656252,
  "history_end_time" : 1656360658277,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "y8oodnud7yk",
  "history_input" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom datetime import datetime as dt\n\nfrom datetime import date\nfrom snowcast_utils import *\n\npd.set_option('display.max_columns', None)\n\ntoday = date.today()\n\n# dd/mm/YY\nstart_date = \"2022-01-01\"\n#end_date = today.strftime(\"%Y-%m-%d\")\nend_date = findLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\nprint(\"d1 =\", end_date)\n\nprint(\"integrating datasets into one dataset\")\n#pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\n\n#example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\nsubmission_format_pd = pd.read_csv(submission_format_file, header=0, index_col=0)\n#print(training_feature_pd.head())\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n#print(station_cell_mapper_pd.head())\n\n#example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n#print(example_mod_pd.shape)\ndef getDateStr(x):\n  return x.split(\" \")[0]\n\ndef integrate_modis():\n  \"\"\"\n  Integrate all MODIS data into mod_all.csv. Traverse all the csv files in the sat_testing/modis folder\n  and aggregate them into one file with good headers.\n  \"\"\"\n  all_mod_file = f\"{github_dir}/data/ready_for_testing/modis_all.csv\"\n  ready_mod_file = f\"{github_dir}/data/sat_testing/modis/mod10a1_ndsi_{start_date}_{end_date}.csv\"\n  mod_testing_folder = f\"{github_dir}/data/sat_testing/modis/\"\n  if os.path.exists(all_mod_file):\n    os.remove(all_mod_file)\n    \n  new_modis_pd = None\n  \n  for filename in os.listdir(mod_testing_folder):\n    f = os.path.join(mod_testing_folder, filename)\n    if os.path.isfile(f) and \".csv\" in f:\n      print(f)\n      old_modis_pd = pd.read_csv(f, header = 0)\n      old_modis_pd = old_modis_pd.drop(columns=['date'])\n      old_modis_pd.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n      #cell_id_list = old_modis_pd[\"cell_id\"].unique()\n      #cell_id_list = np.insert(cell_id_list, 0, \"data\")\n      cell_id_list = submission_format_pd.index\n      date_list = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n\n      rows = date_list\n      cols = cell_id_list\n      \n      if new_modis_pd is None:\n        new_modis_pd = pd.DataFrame(([0.0 for col in cols] for row in rows), index=rows, columns=cols)\n      \n      for i, row in old_modis_pd.iterrows():\n        cdate = row['date']\n        ndsi = row['mod10a1_ndsi']\n        cellid = row['cell_id']\n        #print(f\"{cdate} - {ndsi} - {cellid}\")\n        if ndsi != 0:\n           new_modis_pd.at[cdate, cellid] = ndsi\n  \n  #modis_np = numpy.zeros((len(date_list), len(cell_id_list)+1))\n  #modis_np[0] = cell_id_list\n  \n  #s1_pd.loc[:, ~s1_pd.columns.str.match('Unnamed')]\n  #print(new_modis_pd.head())\n  new_modis_pd.to_csv(all_mod_file)\n\n  \ndef integrate_sentinel1():\n  \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  Turn the rows into \"daily\", right now it has datetime stamps.\n  \"\"\"\n  all_sentinel1_file = f\"{github_dir}/data/ready_for_testing/sentinel1_all.csv\"\n  ready_sentinel1_file = f\"{github_dir}/data/sat_testing/sentinel1/\"\n  if os.path.exists(all_sentinel1_file):\n    os.remove(all_sentinel1_file)\n  new_s1_pd = None\n  for filename in os.listdir(ready_sentinel1_file):\n    f = os.path.join(ready_sentinel1_file, filename)\n    if os.path.isfile(f) and \".csv\" in f:\n      print(f)\n      old_s1_pd = pd.read_csv(f, header = 0)\n      old_s1_pd = old_s1_pd.drop(columns=['date'])\n      old_s1_pd.rename(columns = {'Unnamed: 0':'date'}, inplace = True)\n      #s1_pd.loc[:, ~s1_pd.columns.str.match('Unnamed')]\n\n      #cell_id_list = old_s1_pd[\"cell_id\"].unique()\n      cell_id_list = submission_format_pd.index\n      #date_list = old_s1_pd[\"date\"].unique()\n      date_list = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n      rows = date_list\n      cols = cell_id_list\n      \n      if new_s1_pd is None:\n        new_s1_pd = pd.DataFrame(([0.0 for col in cols] for row in rows), index=rows, columns=cols)\n\n      for i, row in old_s1_pd.iterrows():\n        cdate = row['date']\n        xdate = dt.strptime(cdate, \"%Y-%m-%d %H:%M:%S\") #3/7/2022  2:00:48 AM\n        sdate = xdate.strftime(\"%Y-%m-%d\")\n        grd = row['s1_grd_vv']\n        cellid = row['cell_id']\n        if grd == 0:\n          continue\n        new_s1_pd.at[sdate, cellid] = float(grd)\n  \n  new_s1_pd.to_csv(all_sentinel1_file)\n\ndef integrate_gridmet():\n  \"\"\"\n  Integrate all gridMET data into gridmet_all.csv\n  \"\"\"\n  \n  dates = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n  \n  #print(mod_all_df.head())\n  var_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n  \n  for var in var_list:\n    print(\"Processing \", var)\n    all_single_var_file = f\"{github_dir}/data/ready_for_testing/gridmet_{var}_all.csv\"\n    \n    all_gridmet_var_folder = f\"{github_dir}/data/sim_testing/gridmet/\"\n    new_var_pd = None\n    \n    for filename in os.listdir(all_gridmet_var_folder):\n      f = os.path.join(all_gridmet_var_folder, filename)\n      if os.path.isfile(f) and \".csv\" in f:\n        print(f)\n        all_gridmet_var_pd = pd.read_csv(f, header=0)\n        #cell_id_list = old_s1_pd[\"cell_id\"].unique()\n        cell_id_list = submission_format_pd.index\n        #date_list = old_s1_pd[\"date\"].unique()\n        date_list = pd.date_range(start=start_date, end=end_date, freq='D').astype(str)\n        rows = date_list\n        cols = cell_id_list\n        if new_var_pd is None:\n          new_var_pd = pd.DataFrame(([0.0 for col in cols] for row in rows), index=rows, columns=cols)\n\n        for i, row in all_gridmet_var_pd.iterrows():\n          cdate = row[\"Unnamed: 0\"]\n          xdate = dt.strptime(cdate, \"%Y-%m-%d %H:%M:%S\") #3/7/2022  2:00:48 AM\n          sdate = xdate.strftime(\"%Y-%m-%d\")\n          newval = row[var]\n          cellid = row['cell_id']\n          if newval != 0:\n            new_var_pd.at[sdate, cellid] = float(newval)\n  \n    new_var_pd.to_csv(all_single_var_file)\n  \n  \ndef prepare_testing_csv():\n  \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n  all_ready_file = f\"{github_dir}/data/ready_for_testing/all_ready_3.csv\"\n  if os.path.exists(all_ready_file):\n    os.remove(all_ready_file)\n  \n  all_mod_file = f\"{github_dir}/data/ready_for_testing/modis_all.csv\"\n  modis_all_pd = pd.read_csv(all_mod_file, header=0, index_col = 0)\n  modis_all_np = modis_all_pd.to_numpy()\n  \n  all_sentinel1_file = f\"{github_dir}/data/ready_for_testing/sentinel1_all.csv\"\n  sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0, index_col = 0)\n  sentinel1_all_np = sentinel1_all_pd.to_numpy()\n  \n  all_gridmet_eto_file = f\"{github_dir}/data/ready_for_testing/gridmet_eto_all.csv\"\n  gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col = 0)\n  gridmet_eto_all_np = gridmet_eto_all_pd.to_numpy()\n  \n  all_gridmet_pr_file = f\"{github_dir}/data/ready_for_testing/gridmet_pr_all.csv\"\n  gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col = 0)\n  gridmet_pr_all_np = gridmet_pr_all_pd.to_numpy()\n  \n  all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_testing/gridmet_rmax_all.csv\"\n  gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col = 0)\n  gridmet_rmax_all_np = gridmet_rmax_all_pd.to_numpy()\n  \n  all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_testing/gridmet_rmin_all.csv\"\n  gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col = 0)\n  gridmet_rmin_all_np = gridmet_rmin_all_pd.to_numpy()\n  \n  all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_testing/gridmet_tmmn_all.csv\"\n  gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col = 0)\n  gridmet_tmmn_all_np = gridmet_tmmn_all_pd.to_numpy()\n  \n  all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_testing/gridmet_tmmx_all.csv\"\n  gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col = 0)\n  gridmet_tmmx_all_np = gridmet_tmmx_all_pd.to_numpy()\n  \n  all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_testing/gridmet_vpd_all.csv\"\n  gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col = 0)\n  gridmet_vpd_all_np = gridmet_vpd_all_pd.to_numpy()\n  \n  all_gridmet_vs_file = f\"{github_dir}/data/ready_for_testing/gridmet_vs_all.csv\"\n  gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col = 0)\n  gridmet_vs_all_np = gridmet_vs_all_pd.to_numpy()\n  \n  grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_eval_terrainData.csv\"\n  grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col = 0)\n  grid_terrain_np = grid_terrain_pd.to_numpy()\n  \n  sentinel1_all_pd = sentinel1_all_pd[:modis_all_pd.shape[0]]\n  \n  \n  \n  print(\"modis_all_size: \", modis_all_pd.shape)\n  print(\"sentinel1_all_size: \", sentinel1_all_pd.shape)\n  print(\"gridmet rmax size: \", gridmet_rmax_all_pd.shape)\n  print(\"gridmet eto size: \", gridmet_eto_all_pd.shape)\n  print(\"gridmet vpd size: \", gridmet_vpd_all_pd.shape)\n  print(\"gridmet pr size: \", gridmet_pr_all_pd.shape)\n  print(\"gridmet rmin size: \", gridmet_rmin_all_pd.shape)\n  print(\"gridmet tmmn size: \", gridmet_tmmn_all_pd.shape)\n  print(\"gridmet tmmx size: \", gridmet_tmmx_all_pd.shape)\n  print(\"gridmet vs size: \", gridmet_vs_all_pd.shape)\n  print(\"grid terrain size: \", grid_terrain_pd.shape)\n  print(\"cell_size: \", len(submission_format_pd.index))\n  print(\"station size: \", station_cell_mapper_pd.shape)\n  print(\"training_feature_pd size: \", training_feature_pd.shape)\n  print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n  print(\"grid_terrain_np shape: \", grid_terrain_np.shape)\n  \n  min_len = min( modis_all_pd.shape[0], sentinel1_all_pd.shape[0], gridmet_rmax_all_pd.shape[0], gridmet_eto_all_pd.shape[0], gridmet_vpd_all_pd.shape[0], gridmet_pr_all_pd.shape[0], gridmet_rmin_all_pd.shape[0], gridmet_tmmn_all_pd.shape[0], gridmet_tmmx_all_pd.shape[0], gridmet_vs_all_pd.shape[0], grid_terrain_pd.shape[0] )\n  \n  cell_id_list = modis_all_pd.columns.values\n  \n  \n  \n  # create a multiple numpy array, the dimension is (cell_id, date, variable)\n  #all_testing_np = np.empty((len(modis_all_pd.index.values), len(modis_all_pd.columns.values),  23))\n  all_testing_np = np.empty((min_len, len(modis_all_pd.columns.values),  23))\n  print(\"final all numpy shape: \", all_testing_np.shape)\n  \n  modis_all_np = np.expand_dims(modis_all_np[:min_len, :], axis=2)\n  sentinel1_all_np = np.expand_dims(sentinel1_all_np[:min_len, :], axis=2)\n  gridmet_eto_all_np = np.expand_dims(gridmet_eto_all_np[:min_len, :], axis=2)\n  gridmet_pr_all_np = np.expand_dims(gridmet_pr_all_np[:min_len, :], axis=2)\n  gridmet_rmax_all_np = np.expand_dims(gridmet_rmax_all_np[:min_len, :], axis=2)\n  gridmet_rmin_all_np = np.expand_dims(gridmet_rmin_all_np[:min_len, :], axis=2)\n  gridmet_tmmn_all_np = np.expand_dims(gridmet_tmmn_all_np[:min_len, :], axis=2)\n  gridmet_tmmx_all_np = np.expand_dims(gridmet_tmmx_all_np[:min_len, :], axis=2)\n  gridmet_vpd_all_np = np.expand_dims(gridmet_vpd_all_np[:min_len, :], axis=2)\n  gridmet_vs_all_np = np.expand_dims(gridmet_vs_all_np[:min_len, :], axis=2)\n  \n  cell_id_np = np.expand_dims(cell_id_list, axis=0)\n  cell_id_np = np.repeat(cell_id_np, min_len, axis=0)\n  cell_id_np = np.expand_dims(cell_id_np, axis=2)\n  print(\"cell_id_np shape: \", cell_id_np.shape)\n  \n  grid_terrain_np = np.expand_dims(grid_terrain_np, axis=0)\n  grid_terrain_np = np.repeat(grid_terrain_np, min_len, axis=0)\n  \n  date_np = np.empty((min_len, len(modis_all_pd.columns.values),  3))\n  for i in range(min_len):\n    #print(i, \" - \", modis_all_pd.index.values[i])\n    date_time_obj = dt.strptime(modis_all_pd.index.values[i], '%Y-%m-%d')\n    date_np[i, :, 0] = date_time_obj.year\n    date_np[i, :, 1] = date_time_obj.month\n    date_np[i, :, 2] = date_time_obj.timetuple().tm_yday\n  \n  new_np = np.concatenate((cell_id_np, date_np, modis_all_np, sentinel1_all_np, gridmet_eto_all_np, gridmet_pr_all_np, gridmet_rmax_all_np, gridmet_rmin_all_np, gridmet_tmmn_all_np, gridmet_tmmx_all_np, gridmet_vpd_all_np, gridmet_vs_all_np, grid_terrain_np), axis=2)\n  print(\"new numpy shape: \", new_np.shape)\n  \n  new_np = new_np.reshape(-1,new_np.shape[-1])\n  print(\"reshaped: \", new_np.shape)\n  \n  #all_training_pd = pd.DataFrame(columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe\"])\n  all_testing_pd = pd.DataFrame(new_np, columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\"])\n  \n  #print(\"MODIS all np shape: \", modis_all_np.shape)\n  #print(\"Terrain numpy shape: \", grid_terrain_np.shape)\n  \n  #print(\"Head\", all_testing_pd.head())\n  all_testing_pd.to_csv(all_ready_file)\n  \n  \n  \n#exit() # done already\n\nintegrate_modis()\nintegrate_sentinel1()\nintegrate_gridmet()\nprepare_testing_csv()\n\n\n  \n  \n  \n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\y8oodnud7yk\\testing_data_integration.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360658292,
  "history_end_time" : 1656360659228,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "vslozl9om48",
  "history_input" : "from datetime import date\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nimport datetime\n\ntoday = date.today()\n\n# dd/mm/YY\nd1 = today.strftime(\"%Y-%m-%d\")\nprint(\"today date =\", d1)\n\ntrain_start_date = \"\"\ntrain_end_date = \"\"\n\ntest_start_date = \"2022-01-01\"\ntest_end_date = d1\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1-lat2)**2 + (lon1-lon2)**2)\n\ndef create_cell_location_csv():\n  # read grid cell\n  gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n  all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n  if os.path.exists(all_cell_coords_file):\n    os.remove(all_cell_coords_file)\n\n  grid_coords_df = pd.DataFrame(columns=[\"cell_id\", \"lat\", \"lon\"])\n  print(grid_coords_df.head())\n  gridcells = geojson.load(open(gridcells_file))\n  for idx,cell in enumerate(gridcells['features']):\n    \n    current_cell_id = cell['properties']['cell_id']\n    cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n    cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n    grid_coords_df.loc[len(grid_coords_df.index)] = [current_cell_id, cell_lat, cell_lon]\n    \n  #grid_coords_np = grid_coords_df.to_numpy()\n  #print(grid_coords_np.shape)\n  grid_coords_df.to_csv(all_cell_coords_file, index=False)\n  #np.savetxt(all_cell_coords_file, grid_coords_np[:, 1:], delimiter=\",\")\n  #print(grid_coords_np.shape)\n  \ndef get_latest_date_from_an_array(arr, date_format):\n  return max(arr, key=lambda x: datetime.datetime.strptime(x, date_format))\n  \n  \ndef findLastStopDate(target_testing_dir, data_format):\n  date_list = []\n  for filename in os.listdir(target_testing_dir):\n    f = os.path.join(target_testing_dir, filename)\n    # checking if it is a file\n    if os.path.isfile(f) and \".csv\" in f:\n        pdf = pd.read_csv(f,header=0, index_col=0)\n        date_list = np.concatenate((date_list, pdf.index.unique()))\n  latest_date = get_latest_date_from_an_array(date_list, data_format)\n  print(latest_date)\n  date_time_obj = datetime.datetime.strptime(latest_date, data_format)\n  return date_time_obj.strftime(\"%Y-%m-%d\")\n\n#create_cell_location_csv()\nfindLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\n#findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1/\", \"%Y-%m-%d %H:%M:%S\")\n#findLastStopDate(f\"{github_dir}/data/sat_testing/modis/\", \"%Y-%m-%d\")\n\n\n\n      \n",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\vslozl9om48\\snowcast_utils.py\", line 5, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1656360650399,
  "history_end_time" : 1656360652158,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "kzs0v5nuycd",
  "history_input" : "\nfrom BaseHole import *\n\nclass KehanModel(BaseHole):\n\t\n  def preprocessing():\n    pass  \n  \n  def train():\n    pass\n  \n  def test():\n    pass",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\kzs0v5nuycd\\model_create_kehan.py\", line 2, in <module>\n    from BaseHole import *\nModuleNotFoundError: No module named 'BaseHole'\n",
  "history_begin_time" : 1656360655380,
  "history_end_time" : 1656360655604,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "z48fzbynm0g",
  "history_input" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n\n# Write first python in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n\n\n#snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n#df = snotel_point.get_daily_data(\n#    datetime(2020, 1, 2), datetime(2020, 1, 20),\n#    [snotel_point.ALLOWED_VARIABLES.SWE]\n#)\n#print(df)",
  "history_output" : "Traceback (most recent call last):\n  File \"C:\\Users\\BLi\\gw-workspace\\z48fzbynm0g\\data_snotel_real_time.py\", line 2, in <module>\n    from metloom.pointdata import SnotelPointData\nModuleNotFoundError: No module named 'metloom'\n",
  "history_begin_time" : 1656360654905,
  "history_end_time" : 1656360655025,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Failed"
}]
