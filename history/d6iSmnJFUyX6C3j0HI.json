[{
  "history_id" : "jeu1tcz0wd7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594676,
  "history_end_time" : 1694970594676,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i3yweomulcc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594678,
  "history_end_time" : 1694970594678,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gu6sg4055y4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594688,
  "history_end_time" : 1694970594688,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yhrayzgjq12",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594689,
  "history_end_time" : 1694970594689,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xuj11x5kvbh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594702,
  "history_end_time" : 1694970594702,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ydq69k5dhvh",
  "history_input" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\n\ndef load_model(model_path):\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.rename(columns={'Latitude': 'lat', 'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', 'pr': 'precipitation_amount',\n                         'rmax': 'relative_humidity', 'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                        'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                        'AMSR_SWE': 'SWE',\n                        'AMSR_Flag': 'Flag',\n                        'Elevation': 'elevation',\n                        'Slope': 'slope',\n                        'Aspect': 'aspect',\n                        'Curvature': 'curvature',\n                        'Northness': 'northness',\n                        'Eastness': 'eastness'\n                        }, inplace=True)\n\n    numerical_columns = ['lat', 'lon', 'mean_vapor_pressure_deficit', 'wind_speed', 'precipitation_amount', 'relative_humidity', 'potential_evapotranspiration', 'air_temperature_tmmn', 'air_temperature_tmmx', 'relative_humidity_rmin', 'relative_humidity_rmax', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    #data[numerical_columns] = data[numerical_columns].apply(pd.to_numeric, errors='coerce')\n\n    columns_to_delete = [0, 4, 6, 8, 10, 12, 14, 16, 18, 19]\n    data.drop(data.columns[columns_to_delete], axis=1, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    data = data.reindex(columns=desired_order)\n    print('features:', data.columns)\n    data.dropna(inplace=True)\n    return data\n\ndef predict_swe(model, data):\n    predictions = model.predict(data)\n    print(predictions[:10])\n    data['predicted_swe'] = predictions\n    return data\n\ndef merge_data(original_data, predicted_data):\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    return merged_df\n\ndef save_data(data, file_path):\n    data.to_csv(file_path, index=False)\n\ndef predict():\n  height = 666\n  width = 694\n  #model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231109162144.joblib'\n  model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231709164339.joblib'\n  new_data_path = f'{work_dir}/testing_all_ready.csv'\n  #new_data_path = f\"{work_dir}/test_predict_sample.csv\"\n  output_path = f'{work_dir}/test_data_predicted.csv'\n\n  model = load_model(model_path)\n  new_data = load_data(new_data_path)\n\n  preprocessed_data = preprocess_data(new_data)\n  print('data preprocessing completed.')\n  print(f'model used: {model_path}')\n  predicted_data = predict_swe(model, preprocessed_data)\n  print('data prediction completed.')\n  final_data = merge_data(new_data, predicted_data)\n\n  save_data(final_data, output_path)\n\n  print(\"Prediction successfully done\")\n\n  if len(final_data) == height * width:\n    print(f\"The image width, height match with the number of rows in the csv. {len(final_data)} rows\")\n  else:\n    raise Exception(\"The total number of rows do not match\")\n\npredict()\n",
  "history_output" : "/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py:457: UserWarning: X has feature names, but ExtraTreesRegressor was fitted without feature names\n  warnings.warn(\ntoday date = 2023-09-17\ntest start date:  2022-11-05\ntest end date:  2023-09-17\n/home/chetana\nfeatures: Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\ndata preprocessing completed.\nmodel used: /home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_20231709164339.joblib\n[0.38  0.379 0.393 0.419 0.404 0.423 0.396 0.423 0.404 0.404]\ndata prediction completed.\nColumns after merge: Index(['lat', 'lon', 'mean_vapor_pressure_deficit', 'wind_speed',\n       'precipitation_amount', 'relative_humidity_rmax',\n       'potential_evapotranspiration', 'air_temperature_tmmn',\n       'air_temperature_tmmx', 'relative_humidity_rmin', 'elevation', 'slope',\n       'aspect', 'curvature', 'northness', 'eastness', 'SWE', 'Flag', 'date',\n       'predicted_swe'],\n      dtype='object')\nPrediction successfully done\nThe image width, height match with the number of rows in the csv. 462204 rows\n",
  "history_begin_time" : 1694970627635,
  "history_end_time" : 1694970638967,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5karlaauy8b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594709,
  "history_end_time" : 1694970594709,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "04xims4tfkf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594719,
  "history_end_time" : 1694970594719,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5jv8dk7rbx7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594720,
  "history_end_time" : 1694970594720,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2sa2l8xijkr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594722,
  "history_end_time" : 1694970594722,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q5b6uuhsw8v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594738,
  "history_end_time" : 1694970594738,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k0s01w5nsfy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594739,
  "history_end_time" : 1694970594739,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3doqw89udn3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594741,
  "history_end_time" : 1694970594741,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "luow5n37hnn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594743,
  "history_end_time" : 1694970594743,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bcnfavfmyrb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594745,
  "history_end_time" : 1694970594745,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qk0or3al5me",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594746,
  "history_end_time" : 1694970594746,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9v2w22osx7s",
  "history_input" : "import os\nimport pandas as pd\nimport netCDF4 as nc\nimport csv\nfrom datetime import datetime\nfrom snowcast_utils import day_index, work_dir, test_start_date\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n\n\ndem_csv = f\"{work_dir}/dem_all.csv\"\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef get_nc_csv_by_coords_and_variable(nc_file, coordinates, var_name):\n    coordinates = get_coordinates_of_template_tif()\n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    new_lat_data = []\n    new_lon_data = []\n    new_var_data = []\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n\n      # Print the variables and their shapes\n      for variable in variables:\n        shape = nc_file.variables[variable].shape\n        print(f\"Variable: {variable}, Shape: {shape}\")\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      print(\"long var name: \", long_var_name)\n      var_col = nc_file.variables[long_var_name][:]\n      \n      print(f\"latitudes shape: {latitudes.shape}\")\n      print(f\"longitudes shape: {longitudes.shape}\")\n      print(f\"day shape: {day.shape}\")\n      print(f\"val col shape: {var_col.shape}\")\n      \n      #day_index = convert_date_to_1900(test_start_date)\n      #day_index = day[day.shape[0]-1]\n      #day_index = 44998\n      print('day_index:', day_index)\n      \n      for coord in coordinates:\n        lon, lat = coord\n        new_lat_data.append(lat)\n        new_lon_data.append(lon)\n        # Access the variables in the NetCDF file\n        # Find the nearest indices for the given coordinates\n        lon_index = find_nearest_index(longitudes, lon)\n        lat_index = find_nearest_index(latitudes, lat)\n        #day_index = find_nearest_index(day, day[day.shape[0]-1])\n        #print(f\"last day: {day_index}\")\n\n        # Get the value at the specified coordinates\n        the_value = var_col[day.shape[0]-1, lat_index, lon_index]  # Assuming data_variable is a 3D variable (time, lat, lon)\n        if the_value == \"--\":\n          the_value = -9999\n        new_var_data.append(the_value)\n        #print(f\"lon - {lon} lat - {lat} lon-index {lon_index} lat-index {lat_index} day-index {day_index} value - {the_value}\")\n    # Create the DataFrame\n    data = { 'Latitude': new_lat_data, 'Longitude': new_lon_data, var_name: new_var_data}\n    df = pd.DataFrame(data)\n    return df\n\ndef turn_gridmet_nc_to_csv(folder_path, dem_all_csv, testing_all_csv):\n    coordinates = get_coordinates_of_template_tif()\n    current_year = get_current_year()\n    for root, dirs, files in os.walk(folder_path):\n        for file_name in files:\n            var_name = get_var_from_file_name(file_name)\n            print(\"Variable name:\", var_name)\n            res_csv = f\"{work_dir}/testing_output/{str(current_year)}_{var_name}.csv\"\n            if os.path.exists(res_csv):\n                os.remove(res_csv)\n                print(f\"remove old {res_csv}\")\n            \n            if str(current_year) in file_name :\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n                print(\"File Name:\", file_name)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, coordinates, var_name)\n                df.to_csv(res_csv)\n            \ndef merge_all_gridmet_amsr_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv):\n    # List of file paths for the CSV files\n    csv_files = []\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('.csv'):\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8', index_col=False)\n        dfs.append(df)\n\n    dem_df = pd.read_csv(dem_all_csv, encoding='utf-8', index_col=False)\n    dfs.append(dem_df)\n    \n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    amsr_df = pd.read_csv(f'{work_dir}/testing_ready_amsr_{date}.csv', index_col=False)\n    amsr_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n    merged_df = pd.merge(merged_df, amsr_df, on=['Latitude', 'Longitude'])\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input csv files are merged to {testing_all_csv}\")\n    print(merged_df.head())\n\n    \n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = f\"{work_dir}/gridmet_climatology/\"\n    #turn_gridmet_nc_to_csv(gridmet_csv_folder)\n    merge_all_gridmet_amsr_csv_into_one(f\"{work_dir}/testing_output/\",\n                                        f\"{work_dir}/dem_all.csv\",\n                                        f\"{work_dir}/testing_all_ready.csv\")\n\n",
  "history_output" : "/home/chetana/gw-workspace/9v2w22osx7s/testing_data_integration.py:153: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/9v2w22osx7s/testing_data_integration.py:153: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/9v2w22osx7s/testing_data_integration.py:153: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\ntoday date = 2023-09-17\ntest start date:  2022-11-05\ntest end date:  2023-09-17\n/home/chetana\nAll input csv files are merged to /home/chetana/gridmet_test_run/testing_all_ready.csv\n   Unnamed: 0_x  Latitude  Longitude  ... AMSR_SWE  AMSR_Flag        date\n0             0      49.0   -125.000  ...        0        241  2022-11-05\n1             1      49.0   -124.964  ...        0        241  2022-11-05\n2             2      49.0   -124.928  ...        0        241  2022-11-05\n3             3      49.0   -124.892  ...        0        241  2022-11-05\n4             4      49.0   -124.856  ...        0        241  2022-11-05\n[5 rows x 29 columns]\n",
  "history_begin_time" : 1694970614585,
  "history_end_time" : 1694970626149,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "marq9rrwaqf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594750,
  "history_end_time" : 1694970594750,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4m54zsuhj9q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594751,
  "history_end_time" : 1694970594751,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ri8s47bkhex",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594752,
  "history_end_time" : 1694970594752,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xmdmnb9dqjn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594752,
  "history_end_time" : 1694970594752,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t29k8gzmzlc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594753,
  "history_end_time" : 1694970594753,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "999n59qskqf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594754,
  "history_end_time" : 1694970594754,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rgafsfd0xef",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594755,
  "history_end_time" : 1694970594755,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fk6a9ot3srh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594756,
  "history_end_time" : 1694970594756,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jhiiy8t4mqv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594756,
  "history_end_time" : 1694970594756,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "stguu7xpn9n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594757,
  "history_end_time" : 1694970594757,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0i6729u06bv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594758,
  "history_end_time" : 1694970594758,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qm1db4jznn5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594759,
  "history_end_time" : 1694970594759,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9wobw7m87a8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594759,
  "history_end_time" : 1694970594759,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yth1u8sflu3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594760,
  "history_end_time" : 1694970594760,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ps8xbjp3r63",
  "history_input" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, work_dir\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\n# Define the folder to store downloaded files\nfolder_name = f'{work_dir}/gridmet_climatology'\n\n# Check if the folder exists, if not, create it\nif not os.path.exists(folder_name):\n    os.makedirs(folder_name)\n\nselected_yr = datetime.strptime(test_start_date, \"%Y-%m-%d\")\nyear_list = [selected_yr.year]\n    \n# Remove any existing files in the folder\nif selected_yr.year == datetime.now().year:\n    remove_files_in_folder(folder_name)  # only redownload when the year is the current year\n\n# Run the download function\ndownload_gridmet_of_specific_variables(year_list)\n",
  "history_output" : "",
  "history_begin_time" : 1694970595170,
  "history_end_time" : 1694970612621,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ys9j44e4d4q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594763,
  "history_end_time" : 1694970594763,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nf3bcx685uc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594764,
  "history_end_time" : 1694970594764,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g473ur70nqp",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport uuid\nfrom snowcast_utils import day_index\nimport matplotlib.colors as mcolors\nfrom matplotlib.patches import Patch\n\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\ncmap_name = 'custom_snow_colormap'\ncustom_cmap = mcolors.ListedColormap(colors)\n\n# Define the lat_lon_to_map_coordinates function\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    x, y = m(lon, lat)\n    return x, y\n\n# Define your value ranges for color mapping\nvalue_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n\ndef convert_csvs_to_images():\n    data = pd.read_csv(\"/home/chetana/gridmet_test_run/test_data_predicted.csv\")\n    data['predicted_swe'].fillna(0, inplace=True)\n    data['predicted_swe'] = data['predicted_swe'].apply(lambda x: 0 if x < 2 else x)\n    print(data[:5])\n\n    lon_min, lon_max = -125, -100\n    lat_min, lat_max = 25, 49.5\n\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    x, y = m(data['lon'].values, data['lat'].values)\n\n    # Create a figure with a white background\n    fig = plt.figure(facecolor='white')\n\n    # Define your custom function to map data values to colors\n    def map_value_to_color(value):\n        # Iterate through the value ranges to find the appropriate color index\n        for i, range_max in enumerate(value_ranges):\n            if value <= range_max:\n                return colors[i]\n        \n        # If the value is greater than the largest range, return the last color\n        return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n    color_mapping = [map_value_to_color(value) for value in data['predicted_swe']]\n    \n    # Plot the data using the custom colormap\n    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n\n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_date = datetime(1900, 1, 1)\n    day_value = day_index\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n    \n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}??W\" if lon < 0 else f\"{lon:.1f}??E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}??N\" if lat >= 0 else f\"{abs(lat):.1f}??S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}??N\" if lat >= 0 else f\"{abs(lat):.1f}??S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    # Create custom legend elements using the same colormap\n    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n\n    # Create the legend outside the map\n    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n    legend.set_bbox_to_anchor((1.01, 1)) \n\n    # Remove the color bar\n    plt.colorbar().remove()\n\n    plt.text(0.98, 0.02, 'Copyright ?? SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Set the aspect ratio to 'equal' to keep the plot at the center\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n    # Show the plot or save it to a file\n    new_plot_path = f'/home/chetana/gridmet_test_run/predicted_swe-{timestamp_string}.png'\n    print(f\"The new plot is saved to {new_plot_path}\")\n    plt.savefig(new_plot_path)\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\nconvert_csvs_to_images()\n",
  "history_output" : "/home/chetana/gw-workspace/g473ur70nqp/convert_results_to_images.py:70: UserWarning: No data for colormapping provided via 'c'. Parameters 'cmap' will be ignored\n  plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\ntoday date = 2023-09-17\ntest start date:  2022-11-05\ntest end date:  2023-09-17\n/home/chetana\n    lat      lon  mean_vapor_pressure_deficit  ...  Flag   date  predicted_swe\n0  49.0 -125.000                          NaN  ...   241  44868              0\n1  49.0 -124.964                          NaN  ...   241  44868              0\n2  49.0 -124.928                          NaN  ...   241  44868              0\n3  49.0 -124.892                          NaN  ...   241  44868              0\n4  49.0 -124.856                          NaN  ...   241  44868              0\n[5 rows x 20 columns]\nThe new plot is saved to /home/chetana/gridmet_test_run/predicted_swe-2022-11-05.png\n",
  "history_begin_time" : 1694970640619,
  "history_end_time" : 1694970651037,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "03046ugepr5",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit(1)  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "",
  "history_begin_time" : 1694970652987,
  "history_end_time" : 1694970656409,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "kmqtbzvir2v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594770,
  "history_end_time" : 1694970594770,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rlecemqeslg",
  "history_input" : "\"\"\"\nScript for downloading AMSR snow data, converting it to gridMET format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with gridMET, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\nimport time\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = '/home/chetana/gridmet_test_run/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid():\n    \"\"\"\n    Download AMSR snow data, convert it to gridMET format, and save as a CSV file.\n    \"\"\"\n    \n    prepare_amsr_grid_mapper()\n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    print(swe)\n  \n    # Convert the AMSR grid into our gridMET 1km grid\n    #western_us_df = pd.read_csv(western_us_coords)\n#     for idx, row in mapper_df.iterrows():\n#         target_lat = row['gridmet_lat']\n#         target_lon = row['gridmet_lon']\n#         closest_lat_idx = int(row['amsr_lat_idx'])\n#         closest_lon_idx = int(row['amsr_lon_idx'])\n#         closest_swe = swe[closest_lat_idx, closest_lon_idx]\n#         closest_flag = flag[closest_lat_idx, closest_lon_idx]\n#         new_rows = [{'date': date, \n#                      'lat': target_lat,\n#                      'lon': target_lon,\n#                      'AMSR_SWE': closest_swe,\n#                      'AMSR_Flag': closest_flag,\n#                     }]\n#         #df = df.append(new_rows, ignore_index=True)\n#         df = pd.concat([df, pd.DataFrame(new_rows)], ignore_index=True)\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'gridmet_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'gridmet_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n\n# Run the download and conversion function\n#prepare_amsr_grid_mapper()\ndownload_amsr_and_convert_grid()\n",
  "history_output" : "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  9.9M  100  9.9M    0     0  15.9M      0 --:--:-- --:--:-- --:--:-- 15.9M\nWarning: Got more output options than URLs\ntoday date = 2023-09-17\ntest start date:  2022-11-05\ntest end date:  2023-09-17\n/home/chetana\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\n    amsr_lat    amsr_lon  amsr_lat_idx  amsr_lon_idx  gridmet_lat  gridmet_lon\n0  48.978748 -124.939308         258.0         214.0         49.0     -125.000\n1  48.978748 -124.939308         258.0         214.0         49.0     -124.964\n2  48.978748 -124.939308         258.0         214.0         49.0     -124.928\n3  48.978748 -124.939308         258.0         214.0         49.0     -124.892\n4  48.978748 -124.939308         258.0         214.0         49.0     -124.856\nRunning command: curl --output /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_2022.11.05.he5 -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2022.11.05/AMSR_U2_L3_DailySnow_B02_20221105.he5\n[[248 248 248 ... 248 248 248]\n [248 248 248 ... 248 248 248]\n [248 248 248 ... 248 248 248]\n ...\n [248 248 248 ... 248 248 248]\n [248 248 248 ... 248 248 248]\n [248 248 248 ... 248 248 248]]\nresult df:      lat      lon  AMSR_SWE  AMSR_Flag       date\n0  49.0 -125.000         0        241 2022-11-05\n1  49.0 -124.964         0        241 2022-11-05\n2  49.0 -124.928         0        241 2022-11-05\n3  49.0 -124.892         0        241 2022-11-05\n4  49.0 -124.856         0        241 2022-11-05\nsaving the new AMSR SWE to csv: /home/chetana/gridmet_test_run/testing_ready_amsr_2022.11.05.csv\nCompleted AMSR testing data collection.\n",
  "history_begin_time" : 1694970594971,
  "history_end_time" : 1694970612598,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "urhcnbiul8k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594773,
  "history_end_time" : 1694970594773,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "srraf32mp6c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594774,
  "history_end_time" : 1694970594774,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "884p4ag7xz2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594775,
  "history_end_time" : 1694970594775,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ke5pgpmi9i5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594775,
  "history_end_time" : 1694970594775,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ug1qgkk794r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594776,
  "history_end_time" : 1694970594776,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gie8kxdn00w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594777,
  "history_end_time" : 1694970594777,
  "history_notes" : null,
  "history_process" : "d4zcq6",
  "host_id" : "100001",
  "indicator" : "Skipped"
}]
