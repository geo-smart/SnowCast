[{
  "history_id" : "c76s14tfsrf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616205,
  "history_end_time" : 1691530616205,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "1meaasumlu3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616214,
  "history_end_time" : 1691530616214,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "uumljqf4qwt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616233,
  "history_end_time" : 1691530616233,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "m3wrgpiv604",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616250,
  "history_end_time" : 1691530616250,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "c30w3u8dchp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616267,
  "history_end_time" : 1691530616267,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "mdncol13j7u",
  "history_input" : "import joblib\nimport pandas as pd\nimport netCDF4 as nc\nfrom datetime import timedelta, datetime\nfrom sklearn.preprocessing import MinMaxScaler\nfrom snowcast_utils import convert_date_from_1900, test_start_date\n\nmodel = joblib.load('/home/chetana/gridmet_test_run/model_creation_et_3_yrs.pkl')\nnew_data = pd.read_csv(\"/home/chetana/gridmet_test_run/testing_all_ready.csv\")\nreference_nc_file = nc.Dataset('/home/chetana/gridmet_test_run/gridmet_climatology/etr_2023.nc')\n\nreference_date = datetime(1900, 1, 1)\nday = reference_nc_file.variables['day'][:]\n\n# day_value = day[-1]\nday_value = convert_date_from_1900(test_start_date)\nprint('current day count:', day_value)\n\nresult_date = reference_date + timedelta(days=day_value)\nnew_data['date'] = result_date.strftime(\"%Y-%m-%d\")\n\nnew_data['date'] = pd.to_datetime(new_data['date'])\n# Calculate the number of days since the year 1900\nreference_date = pd.to_datetime('1900-01-01')\nnew_data['days_since_reference'] = (new_data['date'] - reference_date).dt.days\n\nnew_data = new_data.dropna(subset=['SWE_x'])\ncolumn_means = new_data.mean()\nnew_data = new_data.fillna(column_means)\n\n# Create a new feature 'scaled_date' by applying Min-Max scaling to 'days_since_reference'\nscaler = MinMaxScaler()\nnew_datadata['scaled_date'] = scaler.fit_transform(new_data['days_since_reference'].values.reshape(-1, 1))\n\n\nnew_data.drop(['swe_change', 'snow_depth_change'], axis=1, inplace=True, errors='ignore')\nnew_data.drop('date', axis=1, inplace=True)\nnew_data.replace('--', pd.NA, inplace=True)\n\nnew_data.rename(columns={'Elevation': 'elevation', 'Slope': 'slope',\n                         'Aspect': 'aspect', 'Curvature': 'curvature',\n                         'Northness': 'northness', 'Eastness': 'eastness',\n                         'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Handle missing values by replacing with the mean of each column\nnumerical_columns = ['lat', 'lon', 'vpd', 'vs', 'pr', 'rmax', 'etr', 'tmmn', 'tmmx', 'rmin', 'elevation', 'slope',\n                     'aspect', 'curvature', 'northness', 'eastness']\nnew_data[numerical_columns] = new_data[numerical_columns].apply(pd.to_numeric, errors='coerce')\n\n# Calculate the mean of each column\nmean_values = new_data[numerical_columns].mean()\n\ncolumns_to_delete = [0, 4, 6, 8, 10, 12, 14, 16, 18, 19]\nnew_data.drop(new_data.columns[columns_to_delete], axis=1, inplace=True)\n\n# Fill missing data with mean values\n#new_data[numerical_columns] = new_data[numerical_columns].fillna(mean_values)\nnew_data.dropna(inplace=True)\n\n# ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n#                     'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n#                     'elevation',\n#                     'slope', 'curvature', 'aspect', 'eastness',\n#                     'northness', 'Snow Water Equivalent (in) Start of Day Values']\n\ndesired_order = ['lat', 'lon', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'AMSR_SWE', 'AMSR_Flag', 'year', 'month', 'day', 'day_of_week']\n\n# Reindex the DataFrame with the desired order of columns\nnew_data = new_data.reindex(columns=desired_order)\n\nnew_predictions = model.predict(new_data)\n\nnew_data['predicted_swe'] = new_predictions\n\nnew_data.to_csv('/home/chetana/gridmet_test_run/test_data_prediected.csv', index=False)\n\nprint(\"prediction successfully done\")",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/mdncol13j7u/model_predict.py\", line 6, in <module>\n    from snowcast_utils import convert_date_from_1900, test_start_date\n  File \"/home/chetana/gw-workspace/mdncol13j7u/snowcast_utils.py\", line 4, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1691530636186,
  "history_end_time" : 1691530641084,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "tq3z35",
  "indicator" : "Failed"
},{
  "history_id" : "3xwj10jcd4h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616292,
  "history_end_time" : 1691530616292,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "kfovl8klvbo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616293,
  "history_end_time" : 1691530616293,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "7ytbrvkd6yu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616300,
  "history_end_time" : 1691530616300,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "3lpl84pgh9l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616323,
  "history_end_time" : 1691530616323,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "3wro6ao6pcy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616326,
  "history_end_time" : 1691530616326,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "czu7a6uluae",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616328,
  "history_end_time" : 1691530616328,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "4hr6w6v7e2o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530616934,
  "history_end_time" : 1691530616934,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q7snejrxp9h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617007,
  "history_end_time" : 1691530617007,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "1nrhyapsui0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617009,
  "history_end_time" : 1691530617009,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "dr7w67jxc1j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617011,
  "history_end_time" : 1691530617011,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "adi9ich386m",
  "history_input" : "import os\nimport pandas as pd\nimport netCDF4 as nc\nimport csv\nfrom datetime import datetime\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n\n\ndem_csv = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Longitude\"]), float(row[\"Latitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef get_nc_csv_by_coords_and_variable(nc_file, coordinates, var_name):\n    coordinates = get_coordinates_of_template_tif()\n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    new_lat_data = []\n    new_lon_data = []\n    new_var_data = []\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n\n      # Print the variables and their shapes\n      for variable in variables:\n        shape = nc_file.variables[variable].shape\n        print(f\"Variable: {variable}, Shape: {shape}\")\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      print(\"long var name: \", long_var_name)\n      var_col = nc_file.variables[long_var_name][:]\n      \n      print(f\"latitudes shape: {latitudes.shape}\")\n      print(f\"longitudes shape: {longitudes.shape}\")\n      print(f\"day shape: {day.shape}\")\n      print(f\"val col shape: {var_col.shape}\")\n      \n      day_index = day[day.shape[0]-1]\n      day_index = 44998\n      print('day_index:', day_index)\n      \n      for coord in coordinates:\n        lon, lat = coord\n        new_lat_data.append(lat)\n        new_lon_data.append(lon)\n        # Access the variables in the NetCDF file\n        # Find the nearest indices for the given coordinates\n        lon_index = find_nearest_index(longitudes, lon)\n        lat_index = find_nearest_index(latitudes, lat)\n        #day_index = find_nearest_index(day, day[day.shape[0]-1])\n        #print(f\"last day: {day_index}\")\n\n        # Get the value at the specified coordinates\n        the_value = var_col[day.shape[0]-1, lat_index, lon_index]  # Assuming data_variable is a 3D variable (time, lat, lon)\n        if the_value == \"--\":\n          the_value = -9999\n        new_var_data.append(the_value)\n        #print(f\"lon - {lon} lat - {lat} lon-index {lon_index} lat-index {lat_index} day-index {day_index} value - {the_value}\")\n    # Create the DataFrame\n    data = { 'Latitude': new_lat_data, 'Longitude': new_lon_data, var_name: new_var_data}\n    df = pd.DataFrame(data)\n    return df\n\ndef turn_gridmet_nc_to_csv(folder_path, dem_all_csv, testing_all_csv):\n    coordinates = get_coordinates_of_template_tif()\n    current_year = get_current_year()\n    for root, dirs, files in os.walk(folder_path):\n        for file_name in files:\n            var_name = get_var_from_file_name(file_name)\n            print(\"Variable name:\", var_name)\n            res_csv = f\"/home/chetana/gridmet_test_run/testing_output/{str(current_year)}_{var_name}.csv\"\n            if os.path.exists(res_csv):\n                os.remove(res_csv)\n                print(f\"remove old {res_csv}\")\n            \n            if str(current_year) in file_name :\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n                print(\"File Name:\", file_name)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, coordinates, var_name)\n                df.to_csv(res_csv)\n            \ndef merge_all_gridmet_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv):\n    # List of file paths for the CSV files\n    csv_files = []\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('.csv'):\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8', index_col=False)\n        dfs.append(df)\n\n    dem_df = pd.read_csv(dem_all_csv, encoding='utf-8', index_col=False)\n    dfs.append(dem_df)\n    \n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n    amsr_df = pd.read_csv('/home/chetana/gridmet_test_run/testing_ready_amsr.csv', index_col=False)\n    amsr_df.rename(columns={'lat': 'Latitude', 'lon': 'Longitude'}, inplace=True)\n    merged_df = pd.merge(merged_df, amsr_df, on=['Latitude', 'Longitude'])\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input csv files are merged to {testing_all_csv}\")\n    print(merged_df.head())\n\n    \n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = \"/home/chetana/gridmet_test_run/gridmet_climatology/\"\n    #turn_gridmet_nc_to_csv(gridmet_csv_folder)\n    merge_all_gridmet_csv_into_one(\"/home/chetana/gridmet_test_run/testing_output/\",\n                                  \"/home/chetana/gridmet_test_run/dem_all.csv\",\n                                  \"/home/chetana/gridmet_test_run/testing_all_ready.csv\")\n\n",
  "history_output" : "/home/chetana/gw-workspace/adi9ich386m/testing_data_integration.py:151: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/adi9ich386m/testing_data_integration.py:151: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n/home/chetana/gw-workspace/adi9ich386m/testing_data_integration.py:151: FutureWarning: Passing 'suffixes' which cause duplicate columns {'Unnamed: 0_x'} in the result is deprecated and will raise a MergeError in a future version.\n  merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\nAll input csv files are merged to /home/chetana/gridmet_test_run/testing_all_ready.csv\n   Unnamed: 0_x  Latitude  Longitude  ...        date  AMSR_SWE AMSR_Flag\n0             0      49.0   -125.000  ...  2022-01-01         0       241\n1             1      49.0   -124.964  ...  2022-01-01         0       241\n2             2      49.0   -124.928  ...  2022-01-01         0       241\n3             3      49.0   -124.892  ...  2022-01-01         0       241\n4             4      49.0   -124.856  ...  2022-01-01         0       241\n\n[5 rows x 29 columns]\n",
  "history_begin_time" : 1691530621443,
  "history_end_time" : 1691530635300,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "tq3z35",
  "indicator" : "Done"
},{
  "history_id" : "0gbam3guqga",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617058,
  "history_end_time" : 1691530617058,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "94y1yf5ixu7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617100,
  "history_end_time" : 1691530617100,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "j8xfq7trv74",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617109,
  "history_end_time" : 1691530617109,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "rpcv4gql33w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617111,
  "history_end_time" : 1691530617111,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "p8566nv4jaj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617135,
  "history_end_time" : 1691530617135,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "xzhx1anwsri",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617155,
  "history_end_time" : 1691530617155,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "yaxbmjcown7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617158,
  "history_end_time" : 1691530617158,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "mavp4ta2zsy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617186,
  "history_end_time" : 1691530617186,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "lc2u16bk3jn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617188,
  "history_end_time" : 1691530617188,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "1epbbtfgtxr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617201,
  "history_end_time" : 1691530617201,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "q2uwk2l9z7l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617237,
  "history_end_time" : 1691530617237,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "nhq5d9c9tua",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617256,
  "history_end_time" : 1691530617256,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "sq98l75csxi",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport warnings\nimport rasterio\nimport csv\nfrom rasterio.transform import Affine\nfrom scipy.ndimage import sobel, gaussian_filter\n\n# Set the warning filter globally to ignore the FutureWarning\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ndef lat_lon_to_pixel(lat, lon, geotransform):\n    x = int((lon - geotransform[0]) / geotransform[1])\n    y = int((lat - geotransform[3]) / geotransform[5])\n    return x, y\n\n\ndef calculate_slope_aspect_for_single(elevation_data, pixel_size_x, pixel_size_y):\n    # Calculate slope using the Sobel operator\n    slope_x = np.gradient(elevation_data, pixel_size_x, axis=1)\n    slope_y = np.gradient(elevation_data, pixel_size_y, axis=0)\n    slope_rad = np.arctan(np.sqrt(slope_x ** 2 + slope_y ** 2))\n    slope_deg = np.degrees(slope_rad)\n\n    # Calculate aspect (direction of the steepest descent)\n    aspect_rad = np.arctan2(slope_y, -slope_x)\n    aspect_deg = (np.degrees(aspect_rad) + 360) % 360\n\n    return slope_deg, aspect_deg\n\n\ndef save_as_geotiff(data, output_file, src_file):\n    with rasterio.open(src_file) as src_dataset:\n        profile = src_dataset.profile\n        transform = src_dataset.transform\n\n        # Update the data type, count, and set the transform for the new dataset\n        profile.update(dtype=rasterio.float32, count=1, transform=transform)\n\n        # Create the new GeoTIFF file\n        with rasterio.open(output_file, 'w', **profile) as dst_dataset:\n            # Write the data to the new GeoTIFF\n            dst_dataset.write(data, 1)\n  \n\ndef calculate_slope_aspect(dem_file):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Calculate the slope and aspect using numpy\n        dx, dy = np.gradient(dem_data, transform[0], transform[4])\n        slope = np.arctan(np.sqrt(dx ** 2 + dy ** 2)) * (180.0 / np.pi)\n        aspect = np.arctan2(-dy, dx) * (180.0 / np.pi)\n\n        # Adjust aspect values to range from 0 to 360 degrees\n        aspect[aspect < 0] += 360\n        print(f\"slope shape: {slope.shape}\")\n        print(f\"aspect shape: {aspect.shape}\")\n        \n        \n    return slope, aspect\n  \ndef calculate_curvature(elevation_data, pixel_size_x, pixel_size_y):\n    # Calculate curvature using the Laplacian operator\n    curvature_x = np.gradient(np.gradient(elevation_data, pixel_size_x, axis=1), pixel_size_x, axis=1)\n    curvature_y = np.gradient(np.gradient(elevation_data, pixel_size_y, axis=0), pixel_size_y, axis=0)\n    curvature = curvature_x + curvature_y\n\n    return curvature\n\n  \ndef calculate_curvature(dem_file, sigma=1):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradient using the Sobel filter\n        dx = sobel(dem_data, axis=1, mode='constant')\n        dy = sobel(dem_data, axis=0, mode='constant')\n\n        # Calculate the second derivatives using the Sobel filter\n        dxx = sobel(dx, axis=1, mode='constant')\n        dyy = sobel(dy, axis=0, mode='constant')\n\n        # Calculate the curvature using the second derivatives\n        curvature = dxx + dyy\n\n        # Smooth the curvature using Gaussian filtering (optional)\n        curvature = gaussian_filter(curvature, sigma)\n\n    return curvature\n  \ndef calculate_gradients(dem_file):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradients along the North and East directions\n        dy, dx = np.gradient(dem_data, dataset.res[0], dataset.res[1])\n\n        # Calculate the Northness and Eastness\n        northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n        eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\n\n    return northness, eastness\n  \n  \ndef geotiff_to_csv(geotiff_file, csv_file, column_name):\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_file) as dataset:\n        # Get the pixel values as a 2D array\n        data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Get the width and height of the GeoTIFF\n        height, width = data.shape\n\n        # Open the CSV file for writing\n        with open(csv_file, 'w', newline='') as csvfile:\n            csvwriter = csv.writer(csvfile)\n\n            # Write the CSV header\n            csvwriter.writerow(['Latitude', 'Longitude', 'x', 'y', column_name])\n\n            # Loop through each pixel and extract latitude, longitude, and image value\n            for y in range(height):\n                for x in range(width):\n                    # Get the pixel value\n                    image_value = data[y, x]\n\n                    # Convert pixel coordinates to geographic coordinates\n                    lon, lat = transform * (x, y)\n\n                    # Write the data to the CSV file\n                    csvwriter.writerow([lat, lon, x, y, image_value])\n\n  \ndef read_elevation_data(file_path, result_dem_csv_path, result_dem_feature_csv_path):\n    neighborhood_size=4\n    df = pd.read_csv(file_path)\n    \n    dataset = rasterio.open(geotiff_file)\n    data = dataset.read(1)\n\n    # Get the width and height of the GeoTIFF\n    height, width = data.shape\n    \n    # Create an empty DataFrame with column names\n    columns = ['lat', 'lon', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    all_df = pd.DataFrame(columns=columns)\n    \n    all_df.to_csv(result_dem_feature_csv_path)\n    print(f\"DEM and other columns are saved to file {result_dem_feature_csv_path}\")\n    return all_df\n\n\n  \n# Usage example:\nresult_dem_csv_path = \"/home/chetana/gridmet_test_run/dem_template.csv\"\nresult_dem_feature_csv_path = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\n\ndem_file = \"/home/chetana/gridmet_test_run/dem_file.tif\"\nslope_file = '/home/chetana/gridmet_test_run/slope_file.tif'\naspect_file = '/home/chetana/gridmet_test_run/aspect_file.tif'\ncurvature_file = '/home/chetana/gridmet_test_run/curvature_file.tif'\nnorthness_file = '/home/chetana/gridmet_test_run/northness_file.tif'\neastness_file = '/home/chetana/gridmet_test_run/eastness_file.tif'\n\n\nslope, aspect = calculate_slope_aspect(dem_file)\ncurvature = calculate_curvature(dem_file)\nnorthness, eastness = calculate_gradients(dem_file)\n\n# Save the slope and aspect as new GeoTIFF files\nsave_as_geotiff(slope, slope_file, dem_file)\nsave_as_geotiff(aspect, aspect_file, dem_file)\nsave_as_geotiff(curvature, curvature_file, dem_file)\nsave_as_geotiff(northness, northness_file, dem_file)\nsave_as_geotiff(eastness, eastness_file, dem_file)\n\ngeotiff_to_csv(dem_file, dem_file+\".csv\", \"Elevation\")\ngeotiff_to_csv(slope_file, slope_file+\".csv\", \"Slope\")\ngeotiff_to_csv(aspect_file, aspect_file+\".csv\", \"Aspect\")\ngeotiff_to_csv(curvature_file, curvature_file+\".csv\", \"Curvature\")\ngeotiff_to_csv(northness_file, northness_file+\".csv\", \"Northness\")\ngeotiff_to_csv(eastness_file, eastness_file+\".csv\", \"Eastness\")\n\n# List of file paths for the CSV files\ncsv_files = [dem_file+\".csv\", slope_file+\".csv\", aspect_file+\".csv\", \n             curvature_file+\".csv\", northness_file+\".csv\", eastness_file+\".csv\"]\n\n# Initialize an empty list to store all dataframes\ndfs = []\n\n# Read each CSV file into separate dataframes\nfor file in csv_files:\n    df = pd.read_csv(file, encoding='utf-8')\n    dfs.append(df)\n\n# Merge the dataframes based on the latitude and longitude columns\nmerged_df = dfs[0]  # Start with the first dataframe\nfor i in range(1, len(dfs)):\n    merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude', 'x', 'y'])\n\n# Save the merged dataframe to a new CSV file\nmerged_df.to_csv(result_dem_feature_csv_path, index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691530618001,
  "history_end_time" : 1691530620308,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "tq3z35",
  "indicator" : "Failed"
},{
  "history_id" : "x6tc6ltplvh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617266,
  "history_end_time" : 1691530617266,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "b4x1gi74qxh",
  "history_input" : "#############################################\n# Process Name: gridmet_station_only\n# Person Assigned: Gokul Prathin A\n# Last Changes On: 1st July 2023\n#############################################\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n#year_list = [get_current_year()]\nselected_yr = datetime.strptime(test_start_date, \"%y-%m-%d\")\n\n\nyear_list = [selected_yr.year]\n\ndef remove_files_in_folder(folder_path):\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\n\ndef download_gridmet_of_specific_variables():\n    # make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\nfolder_name = '/home/chetana/gridmet_test_run/gridmet_climatology'\nif not os.path.exists(folder_name):\n    os.makedirs(folder_name)\nremove_files_in_folder(folder_name)\ndownload_gridmet_of_specific_variables()\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/b4x1gi74qxh/gridmet_testing.py\", line 13, in <module>\n    from snowcast_utils import test_start_date\n  File \"/home/chetana/gw-workspace/b4x1gi74qxh/snowcast_utils.py\", line 4, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1691530617560,
  "history_end_time" : 1691530620430,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "tq3z35",
  "indicator" : "Failed"
},{
  "history_id" : "y7mw526j5xz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617288,
  "history_end_time" : 1691530617288,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "wika90wt1tu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617289,
  "history_end_time" : 1691530617289,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "lq9r1uao1un",
  "history_input" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nimport netCDF4 as nc\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport pyproj\nimport uuid\nfrom snowcast_utils import convert_date_from_1900, test_start_date\n\nreference_date = datetime(1900, 1, 1)\nday_value = convert_date_from_1900(test_start_date)\nresult_date = reference_date + timedelta(days=day_value)\ncurrent_datetime = result_date.strftime(\"%Y-%m-%d\")\n# current_datetime = datetime.now()\ntimestamp_string = current_datetime\n\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    x, y = m(lon, lat)\n    return x, y\n\ndef convert_csvs_to_images():\n    # Load the CSV data into a DataFrame\n    data = pd.read_csv('/home/chetana/gridmet_test_run/test_data_prediected.csv')\n\n    # Define the map boundaries for the Western US\n    lon_min, lon_max = -125, -100\n    lat_min, lat_max = 25, 49.5\n\n    # Create the Basemap instance\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    # Convert lon/lat to map coordinates\n    x, y = m(data['lon'].values, data['lat'].values)\n\n    # Plot the data using vibrant colors based on predicted_swe\n    plt.scatter(x, y, c=data['predicted_swe'], cmap='coolwarm', s=30, edgecolors='none', alpha=0.7)\n\n    # Add colorbar for reference\n    cbar = plt.colorbar()\n    cbar.set_label('Predicted SWE')\n\n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_nc_file = nc.Dataset('/home/chetana/gridmet_test_run/gridmet_climatology/etr_2023.nc')\n\n    reference_date = datetime(1900, 1, 1)\n    day = reference_nc_file.variables['day'][:]\n    day_value = day[-1]\n    \n\tday_value = convert_date_from_1900(test_start_date)\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    \n\n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}°W\" if lon < 0 else f\"{lon:.1f}°E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    plt.text(0.98, 0.02, 'Copyright © SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15)\n    # Show the plot or save it to a file\n    plt.savefig(f'/home/chetana/gridmet_test_run/predicted_swe-{timestamp_string}-{uuid.uuid4().hex}.png')\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\nconvert_csvs_to_images()\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/lq9r1uao1un/convert_results_to_images.py\", line 55\n    day_value = convert_date_from_1900(test_start_date)\nTabError: inconsistent use of tabs and spaces in indentation\n",
  "history_begin_time" : 1691530641313,
  "history_end_time" : 1691530643642,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "tq3z35",
  "indicator" : "Failed"
},{
  "history_id" : "xsg02cmqm0l",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit(1)  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/xsg02cmqm0l/service_prediction.py\", line 7, in <module>\n    import seaborn as sns\nModuleNotFoundError: No module named 'seaborn'\n",
  "history_begin_time" : 1691530644097,
  "history_end_time" : 1691530649812,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "tq3z35",
  "indicator" : "Failed"
},{
  "history_id" : "njp9llmf8or",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617303,
  "history_end_time" : 1691530617303,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "kqyx1k0vg1b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617304,
  "history_end_time" : 1691530617304,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "pu6hqsr4kib",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617305,
  "history_end_time" : 1691530617305,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "bni82l003o9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617328,
  "history_end_time" : 1691530617328,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "li45n4zcf64",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617330,
  "history_end_time" : 1691530617330,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "43bfxjtccqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617331,
  "history_end_time" : 1691530617331,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "9al7pyqh403",
  "history_input" : "import h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import test_start_date\n\nwestern_us_coords = '/home/chetana/gridmet_test_run/dem_file.tif.csv'\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    lat_diff = np.abs(lat_grid - target_latitude)\n    lon_diff = np.abs(lon_grid - target_longitude)\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\nif __name__ == \"__main__\":\n  df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE', 'AMSR_Flag'])\n  date = test_start_date\n  he5_date = date.replace(\".\", \"\")\n  cmd = f\"curl --output /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_file.he5 -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n  print(f'running command {cmd}')\n  subprocess.run(cmd, shell=True)\n  file = h5py.File('/home/chetana/gridmet_test_run/amsr_testing/testing_amsr_file.he5', 'r')\n  hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n  lat = hem_group['lat'][:]\n  lon = hem_group['lon'][:]\n  swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n  flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n  date = datetime.strptime(date, '%Y.%m.%d')\n  \n  western_us_df = pd.read_csv(western_us_coords)\n  for idx, row in western_us_df.iterrows():\n    target_lat = row['Latitude']\n    target_lon = row['Longitude']\n    closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n    closest_swe = swe[closest_lat_idx, closest_lon_idx]\n    closest_flag = flag[closest_lat_idx, closest_lon_idx]\n    df.loc[len(df.index)] = [date,\n                             target_lat, target_lon,\n                            closest_swe, closest_flag]\n  df.to_csv('/home/chetana/gridmet_test_run/testing_ready_amsr.csv', index=False)\n  \n  print('completed amsr testing data collection.')\n\n    ",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/9al7pyqh403/amsr_testing_realtime.py\", line 6, in <module>\n    from snowcast_utils import test_start_date\n  File \"/home/chetana/gw-workspace/9al7pyqh403/snowcast_utils.py\", line 4, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1691530617757,
  "history_end_time" : 1691530620522,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "tq3z35",
  "indicator" : "Failed"
}]
