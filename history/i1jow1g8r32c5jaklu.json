[{
  "history_id" : "jmoqpd9lppb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235212747,
  "history_end_time" : 1734235212747,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w8eiy96m4ak",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235212807,
  "history_end_time" : 1734235212807,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xuhfu6zrpcj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235212842,
  "history_end_time" : 1734235212842,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "12n98g6jnp1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235212865,
  "history_end_time" : 1734235212865,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lsnv7069ajj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235212919,
  "history_end_time" : 1734235212919,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j9komk8fbkm",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1734235640641,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7qr7old5r0p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235212999,
  "history_end_time" : 1734235212999,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zrk4m6sxg80",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213051,
  "history_end_time" : 1734235213051,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aghlwsd88hw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213069,
  "history_end_time" : 1734235213069,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g2p2a900pj1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213080,
  "history_end_time" : 1734235213080,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "68j31nfc73h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213092,
  "history_end_time" : 1734235213092,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8mitdds08e4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213110,
  "history_end_time" : 1734235213110,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gjeqe3orkfx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213136,
  "history_end_time" : 1734235213136,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "whixozmwscn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213154,
  "history_end_time" : 1734235213154,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r73hyem3gt6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213176,
  "history_end_time" : 1734235213176,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n4ga3okdo0e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213200,
  "history_end_time" : 1734235213200,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "slxj3fwkcxn",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1734235640598,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1auyci3vql7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213249,
  "history_end_time" : 1734235213249,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dbqqrbtzio5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213281,
  "history_end_time" : 1734235213281,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wt9myks4j29",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213324,
  "history_end_time" : 1734235213324,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "08jg9vwo1q5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213348,
  "history_end_time" : 1734235213348,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "66r4l10fehu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213380,
  "history_end_time" : 1734235213380,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qcav6lexr4x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213391,
  "history_end_time" : 1734235213391,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "951j8d5e5z5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213410,
  "history_end_time" : 1734235213410,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j7xk9xgy9dm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213448,
  "history_end_time" : 1734235213448,
  "history_notes" : null,
  "history_process" : "br9etb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5br4dc2ilaa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213470,
  "history_end_time" : 1734235213470,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u0vsvc7zvyy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213480,
  "history_end_time" : 1734235213480,
  "history_notes" : null,
  "history_process" : "doinnd",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "48240z10330",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213512,
  "history_end_time" : 1734235213512,
  "history_notes" : null,
  "history_process" : "b7a4fu",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4mesjbjvlov",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213531,
  "history_end_time" : 1734235213531,
  "history_notes" : null,
  "history_process" : "gnpbdq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gz7io81rjy7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213545,
  "history_end_time" : 1734235213545,
  "history_notes" : null,
  "history_process" : "oon4sb",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bqy8u1o7w8r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213565,
  "history_end_time" : 1734235213565,
  "history_notes" : null,
  "history_process" : "fa7e4u",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ulqb0egyf31",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1734235640514,
  "history_notes" : null,
  "history_process" : "drwmbo",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a9rwu5vwyin",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213622,
  "history_end_time" : 1734235213622,
  "history_notes" : null,
  "history_process" : "2n7b06",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lydm90yklep",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213665,
  "history_end_time" : 1734235213665,
  "history_notes" : null,
  "history_process" : "bwdy3s",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d88looquk03",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1734235640678,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ydrqoqcs7d0",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1734235640714,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "eyadr3gtu2y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213724,
  "history_end_time" : 1734235213724,
  "history_notes" : null,
  "history_process" : "2o6cp8",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4cjkee4x50k",
  "history_input" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, cumulative_mode\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\nimport sys\nfrom convert_results_to_images import plot_all_variables_in_one_csv\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef is_binary(file_path):\n    try:\n        with open(file_path, 'rb') as file:\n            # Read a chunk of bytes from the file\n            chunk = file.read(1024)\n\n            # Check for null bytes, a common indicator of binary data\n            if b'\\x00' in chunk:\n                return True\n\n            # Check for a high percentage of non-printable ASCII characters\n            text_characters = \"\".join(chr(byte) for byte in chunk if 32 <= byte <= 126)\n            if not text_characters:\n                return True\n\n            # If none of the binary indicators are found, assume it's a text file\n            return False\n\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found.\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n  \ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    parent_directory = os.path.dirname(target_amsr_hdf_path)\n    if not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n        print(f\"Parent directory '{parent_directory}' created successfully.\")\n    \n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n        # Check the exit code\n        if result.returncode != 0:\n            print(f\"Command failed with exit code {result.returncode}.\")\n            if os.path.exists(target_amsr_hdf_path):\n              os.remove(target_amsr_hdf_path)\n              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n    \n    # Read the HDF\n    print(f\"Reading {target_amsr_hdf_path}\")\n    try:\n      file = h5py.File(target_amsr_hdf_path, 'r')\n      hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n      lat = hem_group['lat'][:]\n      lon = hem_group['lon'][:]\n    except Exception as e:\n      print(f\"Error reading {target_amsr_hdf_path}: {e}\")\n      os.remove(target_amsr_hdf_path)\n      raise Exception(f\"Invalid HDF5 file {target_amsr_hdf_path} removed. Redownload required.\")\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\ndef add_cumulative_column(df, column_name):\n    df[f'cumulative_{column_name}'] = df[column_name].sum()\n    return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  # Extract X series (column names)\n  x_all_key = row.index\n  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n  if are_all_values_between_0_and_240:\n    print(\"row[x_subset_key] = \", row[x_subset_key])\n    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n  return row\n    \n    \ndef get_cumulative_amsr_data(target_date = test_start_date, force=False):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n      past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Traverse and print every day from past October 1 to the specific date\n    current_date = past_october_1\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n\n    columns_to_be_cumulated = [\"AMSR_SWE\"]\n    \n    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n    if os.path.exists(gap_filled_csv) and not force:\n      print(f\"{gap_filled_csv} already exists, skipping..\")\n      df = pd.read_csv(gap_filled_csv)\n      print(df[\"AMSR_SWE\"].describe())\n    else:\n      date_keyed_objects = {}\n      data_dict = {}\n      new_df = None\n      while current_date <= selected_date:\n        if not cumulative_mode and current_date != selected_date:\n          current_date += timedelta(days=1)\n          continue;\n        print(current_date.strftime('%Y-%m-%d'))\n        current_date_str = current_date.strftime('%Y-%m-%d')\n\n        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n        current_df = pd.read_csv(data_dict[current_date_str])\n        current_df.drop(columns=[\"date\"], inplace=True)\n\n        if current_date != selected_date:\n          current_df.rename(columns={\n            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n          }, inplace=True)\n        #print(current_df.head())\n\n        if new_df is None:\n          new_df = current_df\n        else:\n          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n          #new_df = new_df.append(current_df, ignore_index=True)\n\n        current_date += timedelta(days=1)\n\n      print(\"new_df.columns = \", new_df.columns)\n      print(\"new_df.head = \", new_df.head())\n      df = new_df\n\n      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n      print(\"All current head: \", df.head())\n      print(\"the new_df.shape: \", df.shape)\n\n      if cumulative_mode:\n        print(\"Start to fill in the missing values\")\n        #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n        filled_data = pd.DataFrame()\n\n        # Apply the function to each group\n        for column_name in columns_to_be_cumulated:\n          start_time = time.time()\n          #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n          #alike_columns = filled_data.filter(like=column_name)\n          #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n          print(\"filled_data.columns = \", filled_data.columns)\n          filtered_columns = df.filter(like=column_name)\n          print(filtered_columns.columns)\n          filtered_columns = filtered_columns.mask(filtered_columns > 240)\n          filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n          filtered_columns.fillna(0, inplace=True)\n          \n          sum_column = filtered_columns.sum(axis=1)\n          # Define a specific name for the new column\n          df[f'cumulative_{column_name}'] = sum_column\n          df[filtered_columns.columns] = filtered_columns\n          \n          if filtered_columns.isnull().any().any():\n            print(\"filtered_columns :\", filtered_columns)\n            raise ValueError(\"Single group: shouldn't have null values here\")\n        \n          # Concatenate the original DataFrame with the Series containing the sum\n          #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n  #         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n  #         filled_data[f'cumulative_{column_name}'] = cumulative_column\n          #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n          print(\"filled_data.columns: \", filled_data.columns)\n          end_time = time.time()\n          # Calculate the elapsed time\n          elapsed_time = end_time - start_time\n          print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n\n#       if any(filled_data['AMSR_SWE'] > 240):\n#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n      filled_data = df\n      filled_data[\"date\"] = target_date\n      print(\"Finished correctly \", filled_data.head())\n      filled_data.to_csv(gap_filled_csv, index=False)\n      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n      df = filled_data\n    \n    result = df\n    print(\"result.head = \", result.head())\n    # fill in the rest NA as 0\n    if result.isnull().any().any():\n      print(\"result :\", result)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    # only retain the rows of the target date\n    print(result['date'].unique())\n    print(result.shape)\n    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n    result.to_csv(target_csv_path, index=False)\n    print(f\"New data is saved to {target_csv_path}\")\n    \n      \n    \nif __name__ == \"__main__\":\n    # Run the download and conversion function\n    #prepare_amsr_grid_mapper()\n    prepare_amsr_grid_mapper()\n#     download_amsr_and_convert_grid()\n    \n    get_cumulative_amsr_data(force=False)\n    input_time_series_file = f'{work_dir}/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n\n    #plot_all_variables_in_one_csv(input_time_series_file, f\"{input_time_series_file}.png\")\n",
  "history_output" : "/home/chetana\ntoday date = 2024-12-15\n2024-12-12\ntest start date:  2024-12-12\ntest end date:  2024-5-19\nFile /home/chetana/gridmet_test_run/amsr_to_gridmet_mapper.csv already exists, skipping..\n2024-12-12 00:00:00\n2024-10-01\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.01.csv already exists, skipping..\n2024-10-02\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.02.csv already exists, skipping..\n2024-10-03\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.03.csv already exists, skipping..\n2024-10-04\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.04.csv already exists, skipping..\n2024-10-05\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.05.csv already exists, skipping..\n2024-10-06\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.06.csv already exists, skipping..\n2024-10-07\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.07.csv already exists, skipping..\n2024-10-08\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.08.csv already exists, skipping..\n2024-10-09\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.09.csv already exists, skipping..\n2024-10-10\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.10.csv already exists, skipping..\n2024-10-11\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.11.csv already exists, skipping..\n2024-10-12\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.12.csv already exists, skipping..\n2024-10-13\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.13.csv already exists, skipping..\n2024-10-14\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.14.csv already exists, skipping..\n2024-10-15\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.15.csv already exists, skipping..\n2024-10-16\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.16.csv already exists, skipping..\n2024-10-17\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.17.csv already exists, skipping..\n2024-10-18\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.18.csv already exists, skipping..\n2024-10-19\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.19.csv already exists, skipping..\n2024-10-20\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.20.csv already exists, skipping..\n2024-10-21\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.21.csv already exists, skipping..\n2024-10-22\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.22.csv already exists, skipping..\n2024-10-23\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.23.csv already exists, skipping..\n2024-10-24\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.24.csv already exists, skipping..\n2024-10-25\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.25.csv already exists, skipping..\n2024-10-26\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.26.csv already exists, skipping..\n2024-10-27\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.27.csv already exists, skipping..\n2024-10-28\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.28.csv already exists, skipping..\n2024-10-29\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.29.csv already exists, skipping..\n2024-10-30\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.30.csv already exists, skipping..\n2024-10-31\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.10.31.csv already exists, skipping..\n2024-11-01\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.01.csv already exists, skipping..\n2024-11-02\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.02.csv already exists, skipping..\n2024-11-03\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.03.csv already exists, skipping..\n2024-11-04\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.04.csv already exists, skipping..\n2024-11-05\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.05.csv already exists, skipping..\n2024-11-06\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.06.csv already exists, skipping..\n2024-11-07\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.07.csv already exists, skipping..\n2024-11-08\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.08.csv already exists, skipping..\n2024-11-09\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.09.csv already exists, skipping..\n2024-11-10\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.10.csv already exists, skipping..\n2024-11-11\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.11.csv already exists, skipping..\n2024-11-12\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.12.csv already exists, skipping..\n2024-11-13\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.13.csv already exists, skipping..\n2024-11-14\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.14.csv already exists, skipping..\n2024-11-15\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.15.csv already exists, skipping..\n2024-11-16\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.16.csv already exists, skipping..\n2024-11-17\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.17.csv already exists, skipping..\n2024-11-18\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.18.csv already exists, skipping..\n2024-11-19\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.19.csv already exists, skipping..\n2024-11-20\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.20.csv already exists, skipping..\n2024-11-21\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.21.csv already exists, skipping..\n2024-11-22\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.22.csv already exists, skipping..\n2024-11-23\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.23.csv already exists, skipping..\n2024-11-24\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.24.csv already exists, skipping..\n2024-11-25\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.25.csv already exists, skipping..\n2024-11-26\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.26.csv already exists, skipping..\n2024-11-27\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.27.csv already exists, skipping..\n2024-11-28\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.28.csv already exists, skipping..\n2024-11-29\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.29.csv already exists, skipping..\n2024-11-30\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.11.30.csv already exists, skipping..\n2024-12-01\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.01.csv already exists, skipping..\n2024-12-02\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.02.csv already exists, skipping..\n2024-12-03\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.03.csv already exists, skipping..\n2024-12-04\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.04.csv already exists, skipping..\n2024-12-05\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.05.csv already exists, skipping..\n2024-12-06\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.06.csv already exists, skipping..\n2024-12-07\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.07.csv already exists, skipping..\n2024-12-08\nFile /home/chetana/gridmet_test_run/testing_ready_amsr_2024.12.08.csv already exists, skipping..\n2024-12-09\nRunning command: curl --output /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_2024.12.09.he5 -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/2024.12.09/AMSR_U2_L3_DailySnow_B02_20241209.he5\nReading /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_2024.12.09.he5\nError reading /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_2024.12.09.he5: Unable to open file (file signature not found)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/4cjkee4x50k/amsr_testing_realtime.py\", line 222, in download_amsr_and_convert_grid\n    file = h5py.File(target_amsr_hdf_path, 'r')\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py\", line 567, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/h5py/_hl/files.py\", line 231, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5f.pyx\", line 106, in h5py.h5f.open\nOSError: Unable to open file (file signature not found)\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/4cjkee4x50k/amsr_testing_realtime.py\", line 445, in <module>\n    get_cumulative_amsr_data(force=False)\n  File \"/home/chetana/gw-workspace/4cjkee4x50k/amsr_testing_realtime.py\", line 349, in get_cumulative_amsr_data\n    data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n  File \"/home/chetana/gw-workspace/4cjkee4x50k/amsr_testing_realtime.py\", line 229, in download_amsr_and_convert_grid\n    raise Exception(f\"Invalid HDF5 file {target_amsr_hdf_path} removed. Redownload required.\")\nException: Invalid HDF5 file /home/chetana/gridmet_test_run/amsr_testing/testing_amsr_2024.12.09.he5 removed. Redownload required.\n",
  "history_begin_time" : 1734235516307,
  "history_end_time" : 1734235640750,
  "history_notes" : null,
  "history_process" : "0n26v2",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nibpnpviv37",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213769,
  "history_end_time" : 1734235213769,
  "history_notes" : null,
  "history_process" : "rvqv35",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "03dwn24mre4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213795,
  "history_end_time" : 1734235213795,
  "history_notes" : null,
  "history_process" : "vo8bc9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qhi7zempus6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213818,
  "history_end_time" : 1734235213818,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8bur3fj8oj0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213831,
  "history_end_time" : 1734235213831,
  "history_notes" : null,
  "history_process" : "76ewp5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wixrijilzvd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213854,
  "history_end_time" : 1734235213854,
  "history_notes" : null,
  "history_process" : "5wzgx5",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n526kihycfs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213867,
  "history_end_time" : 1734235213867,
  "history_notes" : null,
  "history_process" : "d4zcq6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "koxruhn6hpv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213883,
  "history_end_time" : 1734235213883,
  "history_notes" : null,
  "history_process" : "6x6myw",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dju06b3fyfs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213895,
  "history_end_time" : 1734235213895,
  "history_notes" : null,
  "history_process" : "r4knm9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v6zqlqsqlmp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213907,
  "history_end_time" : 1734235213907,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "frudcpx9bws",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213948,
  "history_end_time" : 1734235213948,
  "history_notes" : null,
  "history_process" : "ee5ur4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wq39svf2tsv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235213973,
  "history_end_time" : 1734235213973,
  "history_notes" : null,
  "history_process" : "f03i7p",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "50mlfkrjboj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214024,
  "history_end_time" : 1734235214024,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "32kgvn7e83e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214068,
  "history_end_time" : 1734235214068,
  "history_notes" : null,
  "history_process" : "j8swco",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q9535tsk3sp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214085,
  "history_end_time" : 1734235214085,
  "history_notes" : null,
  "history_process" : "pnr64x",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e8rokncjjc8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214134,
  "history_end_time" : 1734235214134,
  "history_notes" : null,
  "history_process" : "qg80lj",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sq2yiyscw00",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214178,
  "history_end_time" : 1734235214178,
  "history_notes" : null,
  "history_process" : "ggy7gf",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ntynw7pjcat",
  "history_input" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian, cumulative_mode\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nos.chdir(f\"{homedir}/fsca/\")\n\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  count = 0\n  for i in date_list:\n    count += 1\n    if not cumulative_mode and count != len(date_list):\n        continue;\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  count = 0\n  for i in date_list:\n    count += 1\n    if not cumulative_mode and count != len(date_list):\n      continue;\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  if cumulative_mode:\n    add_time_series_columns(start_date, end_date, force=True)\n  else:\n    cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n    shutil.copy(outfile, cumulative_file_path)\n    print(f\"File is backed up to {cumulative_file_path}\")\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n",
  "history_output" : "/home/chetana\ntoday date = 2024-12-15\n2024-12-12\ntest start date:  2024-12-12\ntest end date:  2024-5-19\nget test_start_date =  2024-12-12\n2024-12-12 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nThe file /home/chetana/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-01__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-02__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-03__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-04__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-05__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-06__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-07__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-08__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-09__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-10__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-11__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-12__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-13__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-14__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-15__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-16__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-10-17__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-10-18__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-10-19__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-10-20__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-21__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-10-22__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-23__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-24__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-25__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-26__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-27__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-10-28__snow_cover.tif exists. skip.\nfile_size_bytes: 1018600\nThe file /home/chetana/fsca/final_output//2024-10-29__snow_cover.tif exists. skip.\nfile_size_bytes: 1018608\nThe file /home/chetana/fsca/final_output//2024-10-30__snow_cover.tif exists. skip.\nfile_size_bytes: 1018600\nThe file /home/chetana/fsca/final_output//2024-10-31__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-01__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-02__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-11-03__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-04__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-05__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-06__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-07__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-08__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-09__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-10__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-11__snow_cover.tif exists. skip.\nfile_size_bytes: 509282\nThe file /home/chetana/fsca/final_output//2024-11-12__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-13__snow_cover.tif exists. skip.\nfile_size_bytes: 511156\nThe file /home/chetana/fsca/final_output//2024-11-14__snow_cover.tif exists. skip.\nfile_size_bytes: 509282\nThe file /home/chetana/fsca/final_output//2024-11-15__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-16__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-11-17__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-18__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-11-19__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-20__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-11-21__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-22__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-23__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-24__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-25__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-26__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-27__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2024-11-28__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-11-29__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-11-30__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-01__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-02__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-03__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-04__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-05__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-12-06__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-07__snow_cover.tif exists. skip.\nfile_size_bytes: 509354\nThe file /home/chetana/fsca/final_output//2024-12-08__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-09__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-10__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2024-12-11__snow_cover.tif exists. skip.\nThe file /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.08 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nQUEUEING TASKS | : 100%|| 17/17 [00:00<00:00, 2138.99it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | :   6%|         | 1/17 [00:00<00:14,  1.14it/s]\nPROCESSING TASKS | :  29%|       | 5/17 [00:01<00:02,  5.48it/s]\nPROCESSING TASKS | :  47%|     | 8/17 [00:01<00:01,  5.37it/s]\nPROCESSING TASKS | :  53%|    | 9/17 [00:01<00:01,  5.12it/s]\nPROCESSING TASKS | :  59%|    | 10/17 [00:02<00:01,  5.66it/s]\nPROCESSING TASKS | :  65%|   | 11/17 [00:02<00:01,  5.59it/s]\nPROCESSING TASKS | :  88%| | 15/17 [00:02<00:00,  6.28it/s]\nPROCESSING TASKS | :  94%|| 16/17 [00:03<00:00,  5.66it/s]\nPROCESSING TASKS | : 100%|| 17/17 [00:07<00:00,  1.07s/it]\nPROCESSING TASKS | : 100%|| 17/17 [00:07<00:00,  2.17it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|| 17/17 [00:00<00:00, 49106.87it/s]\ndone with downloading, start to convert HDF to geotiff..\n/home/chetana/anaconda3/lib/python3.9/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n  warnings.warn(\n/home/chetana/anaconda3/lib/python3.9/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n  warnings.warn(\n/home/chetana/anaconda3/lib/python3.9/site-packages/osgeo/gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n  warnings.warn(\n/home/chetana\ntoday date = 2024-12-15\n2024-12-12\ntest start date:  2024-12-12\ntest end date:  2024-5-19\n/home/chetana\ntoday date = 2024-12-15\n2024-12-12\ntest start date:  2024-12-12\ntest end date:  2024-5-19\n/home/chetana\ntoday date = 2024-12-15\n2024-12-12\ntest start date:  2024-12-12\ntest end date:  2024-5-19\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2024347\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v04.061.2024349044433.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v05.061.2024349043927.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v05.061.2024347030413.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v06.061.2024347030120.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v04.061.2024347031817.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v06.061.2024349043926.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v06.061.2024349043102.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h07v06.061.2024347030049.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v04.061.2024349044434.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v04.061.2024349044158.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h07v05.061.2024349043043.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v04.061.2024349044152.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v06.061.2024347030050.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v05.061.2024347030450.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v06.061.2024347030346.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v05.061.2024349044430.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v05.061.2024349043727.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v06.061.2024347031358.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v05.061.2024349044157.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v04.061.2024347030448.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v06.061.2024349043928.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h07v05.061.2024347030133.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h12v05.061.2024347030407.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v05.061.2024347030412.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v04.061.2024347030928.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v04.061.2024349043928.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v06.061.2024349044159.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h07v06.061.2024349044154.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h13v04.061.2024349043923.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h12v04.061.2024347031629.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v04.061.2024347030436.tif /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v05.061.2024349043927.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h13v04.061.2024347030406.tif /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v05.061.2024347031535.tif /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif_500m.tif\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v04.061.2024349044433.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v04.061.2024349044433.tif.\nCopying nodata values from source /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v04.061.2024349044433.tif to destination /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v05.061.2024349043927.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v05.061.2024349043927.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v05.061.2024347030413.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v05.061.2024347030413.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v06.061.2024347030120.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v06.061.2024347030120.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v04.061.2024347031817.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v04.061.2024347031817.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v06.061.2024349043926.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v06.061.2024349043926.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v06.061.2024349043102.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v06.061.2024349043102.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h07v06.061.2024347030049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h07v06.061.2024347030049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v04.061.2024349044434.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v04.061.2024349044434.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v04.061.2024349044158.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v04.061.2024349044158.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h07v05.061.2024349043043.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h07v05.061.2024349043043.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v04.061.2024349044152.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v04.061.2024349044152.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v06.061.2024347030050.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v06.061.2024347030050.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v05.061.2024347030450.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v05.061.2024347030450.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v06.061.2024347030346.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v06.061.2024347030346.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v05.061.2024349044430.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v05.061.2024349044430.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v05.061.2024349043727.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v05.061.2024349043727.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v06.061.2024347031358.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v06.061.2024347031358.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v05.061.2024349044157.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h12v05.061.2024349044157.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v04.061.2024347030448.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h11v04.061.2024347030448.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v06.061.2024349043928.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h11v06.061.2024349043928.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h07v05.061.2024347030133.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h07v05.061.2024347030133.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h12v05.061.2024347030407.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h12v05.061.2024347030407.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v05.061.2024347030412.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v05.061.2024347030412.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v04.061.2024347030928.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h09v04.061.2024347030928.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v04.061.2024349043928.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h08v04.061.2024349043928.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v06.061.2024349044159.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h10v06.061.2024349044159.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h07v06.061.2024349044154.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h07v06.061.2024349044154.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h13v04.061.2024349043923.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h13v04.061.2024349043923.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h12v04.061.2024347031629.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h12v04.061.2024347031629.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v04.061.2024347030436.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h10v04.061.2024347030436.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v05.061.2024349043927.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024347.h09v05.061.2024349043927.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h13v04.061.2024347030406.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h13v04.061.2024347030406.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v05.061.2024347031535.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/output_folder/MOD10A1.A2024345.h08v05.061.2024347031535.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif_500m.tif /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif_500m.tif.\nCopying nodata values from source /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif_500m.tif to destination /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif\nextracting data for 2024-10-01\nThe file /home/chetana/fsca/final_output/2024-10-01_output.csv exists. skip.\nextracting data for 2024-10-02\nThe file /home/chetana/fsca/final_output/2024-10-02_output.csv exists. skip.\nextracting data for 2024-10-03\nThe file /home/chetana/fsca/final_output/2024-10-03_output.csv exists. skip.\nextracting data for 2024-10-04\nThe file /home/chetana/fsca/final_output/2024-10-04_output.csv exists. skip.\nextracting data for 2024-10-05\nThe file /home/chetana/fsca/final_output/2024-10-05_output.csv exists. skip.\nextracting data for 2024-10-06\nThe file /home/chetana/fsca/final_output/2024-10-06_output.csv exists. skip.\nextracting data for 2024-10-07\nThe file /home/chetana/fsca/final_output/2024-10-07_output.csv exists. skip.\nextracting data for 2024-10-08\nThe file /home/chetana/fsca/final_output/2024-10-08_output.csv exists. skip.\nextracting data for 2024-10-09\nThe file /home/chetana/fsca/final_output/2024-10-09_output.csv exists. skip.\nextracting data for 2024-10-10\nThe file /home/chetana/fsca/final_output/2024-10-10_output.csv exists. skip.\nextracting data for 2024-10-11\nThe file /home/chetana/fsca/final_output/2024-10-11_output.csv exists. skip.\nextracting data for 2024-10-12\nThe file /home/chetana/fsca/final_output/2024-10-12_output.csv exists. skip.\nextracting data for 2024-10-13\nThe file /home/chetana/fsca/final_output/2024-10-13_output.csv exists. skip.\nextracting data for 2024-10-14\nThe file /home/chetana/fsca/final_output/2024-10-14_output.csv exists. skip.\nextracting data for 2024-10-15\nThe file /home/chetana/fsca/final_output/2024-10-15_output.csv exists. skip.\nextracting data for 2024-10-16\nThe file /home/chetana/fsca/final_output/2024-10-16_output.csv exists. skip.\nextracting data for 2024-10-17\nThe file /home/chetana/fsca/final_output/2024-10-17_output.csv exists. skip.\nextracting data for 2024-10-18\nThe file /home/chetana/fsca/final_output/2024-10-18_output.csv exists. skip.\nextracting data for 2024-10-19\nThe file /home/chetana/fsca/final_output/2024-10-19_output.csv exists. skip.\nextracting data for 2024-10-20\nThe file /home/chetana/fsca/final_output/2024-10-20_output.csv exists. skip.\nextracting data for 2024-10-21\nThe file /home/chetana/fsca/final_output/2024-10-21_output.csv exists. skip.\nextracting data for 2024-10-22\nThe file /home/chetana/fsca/final_output/2024-10-22_output.csv exists. skip.\nextracting data for 2024-10-23\nThe file /home/chetana/fsca/final_output/2024-10-23_output.csv exists. skip.\nextracting data for 2024-10-24\nThe file /home/chetana/fsca/final_output/2024-10-24_output.csv exists. skip.\nextracting data for 2024-10-25\nThe file /home/chetana/fsca/final_output/2024-10-25_output.csv exists. skip.\nextracting data for 2024-10-26\nThe file /home/chetana/fsca/final_output/2024-10-26_output.csv exists. skip.\nextracting data for 2024-10-27\nThe file /home/chetana/fsca/final_output/2024-10-27_output.csv exists. skip.\nextracting data for 2024-10-28\nThe file /home/chetana/fsca/final_output/2024-10-28_output.csv exists. skip.\nextracting data for 2024-10-29\nThe file /home/chetana/fsca/final_output/2024-10-29_output.csv exists. skip.\nextracting data for 2024-10-30\nThe file /home/chetana/fsca/final_output/2024-10-30_output.csv exists. skip.\nextracting data for 2024-10-31\nThe file /home/chetana/fsca/final_output/2024-10-31_output.csv exists. skip.\nextracting data for 2024-11-01\nThe file /home/chetana/fsca/final_output/2024-11-01_output.csv exists. skip.\nextracting data for 2024-11-02\nThe file /home/chetana/fsca/final_output/2024-11-02_output.csv exists. skip.\nextracting data for 2024-11-03\nThe file /home/chetana/fsca/final_output/2024-11-03_output.csv exists. skip.\nextracting data for 2024-11-04\nThe file /home/chetana/fsca/final_output/2024-11-04_output.csv exists. skip.\nextracting data for 2024-11-05\nThe file /home/chetana/fsca/final_output/2024-11-05_output.csv exists. skip.\nextracting data for 2024-11-06\nThe file /home/chetana/fsca/final_output/2024-11-06_output.csv exists. skip.\nextracting data for 2024-11-07\nThe file /home/chetana/fsca/final_output/2024-11-07_output.csv exists. skip.\nextracting data for 2024-11-08\nThe file /home/chetana/fsca/final_output/2024-11-08_output.csv exists. skip.\nextracting data for 2024-11-09\nThe file /home/chetana/fsca/final_output/2024-11-09_output.csv exists. skip.\nextracting data for 2024-11-10\nThe file /home/chetana/fsca/final_output/2024-11-10_output.csv exists. skip.\nextracting data for 2024-11-11\nThe file /home/chetana/fsca/final_output/2024-11-11_output.csv exists. skip.\nextracting data for 2024-11-12\nThe file /home/chetana/fsca/final_output/2024-11-12_output.csv exists. skip.\nextracting data for 2024-11-13\nThe file /home/chetana/fsca/final_output/2024-11-13_output.csv exists. skip.\nextracting data for 2024-11-14\nThe file /home/chetana/fsca/final_output/2024-11-14_output.csv exists. skip.\nextracting data for 2024-11-15\nThe file /home/chetana/fsca/final_output/2024-11-15_output.csv exists. skip.\nextracting data for 2024-11-16\nThe file /home/chetana/fsca/final_output/2024-11-16_output.csv exists. skip.\nextracting data for 2024-11-17\nThe file /home/chetana/fsca/final_output/2024-11-17_output.csv exists. skip.\nextracting data for 2024-11-18\nThe file /home/chetana/fsca/final_output/2024-11-18_output.csv exists. skip.\nextracting data for 2024-11-19\nThe file /home/chetana/fsca/final_output/2024-11-19_output.csv exists. skip.\nextracting data for 2024-11-20\nThe file /home/chetana/fsca/final_output/2024-11-20_output.csv exists. skip.\nextracting data for 2024-11-21\nThe file /home/chetana/fsca/final_output/2024-11-21_output.csv exists. skip.\nextracting data for 2024-11-22\nThe file /home/chetana/fsca/final_output/2024-11-22_output.csv exists. skip.\nextracting data for 2024-11-23\nThe file /home/chetana/fsca/final_output/2024-11-23_output.csv exists. skip.\nextracting data for 2024-11-24\nThe file /home/chetana/fsca/final_output/2024-11-24_output.csv exists. skip.\nextracting data for 2024-11-25\nThe file /home/chetana/fsca/final_output/2024-11-25_output.csv exists. skip.\nextracting data for 2024-11-26\nThe file /home/chetana/fsca/final_output/2024-11-26_output.csv exists. skip.\nextracting data for 2024-11-27\nThe file /home/chetana/fsca/final_output/2024-11-27_output.csv exists. skip.\nextracting data for 2024-11-28\nThe file /home/chetana/fsca/final_output/2024-11-28_output.csv exists. skip.\nextracting data for 2024-11-29\nThe file /home/chetana/fsca/final_output/2024-11-29_output.csv exists. skip.\nextracting data for 2024-11-30\nThe file /home/chetana/fsca/final_output/2024-11-30_output.csv exists. skip.\nextracting data for 2024-12-01\nThe file /home/chetana/fsca/final_output/2024-12-01_output.csv exists. skip.\nextracting data for 2024-12-02\nThe file /home/chetana/fsca/final_output/2024-12-02_output.csv exists. skip.\nextracting data for 2024-12-03\nThe file /home/chetana/fsca/final_output/2024-12-03_output.csv exists. skip.\nextracting data for 2024-12-04\nThe file /home/chetana/fsca/final_output/2024-12-04_output.csv exists. skip.\nextracting data for 2024-12-05\nThe file /home/chetana/fsca/final_output/2024-12-05_output.csv exists. skip.\nextracting data for 2024-12-06\nThe file /home/chetana/fsca/final_output/2024-12-06_output.csv exists. skip.\nextracting data for 2024-12-07\nThe file /home/chetana/fsca/final_output/2024-12-07_output.csv exists. skip.\nextracting data for 2024-12-08\nThe file /home/chetana/fsca/final_output/2024-12-08_output.csv exists. skip.\nextracting data for 2024-12-09\nThe file /home/chetana/fsca/final_output/2024-12-09_output.csv exists. skip.\nextracting data for 2024-12-10\nThe file /home/chetana/fsca/final_output/2024-12-10_output.csv exists. skip.\nextracting data for 2024-12-11\nThe file /home/chetana/fsca/final_output/2024-12-11_output.csv exists. skip.\nextracting data for 2024-12-12\nOpening /home/chetana/fsca/final_output//2024-12-12__snow_cover.tif\nSaving CSV file: /home/chetana/fsca/final_output/2024-12-12_output.csv\nadd_time_series_columns target csv: /home/chetana/fsca/final_output//2024-12-12_output_with_time_series.csv\nnew_df.columns =  Index(['Latitude', 'Longitude', 'fsca_2024-10-01', 'fsca_2024-10-02',\n       'fsca_2024-10-03', 'fsca_2024-10-04', 'fsca_2024-10-05',\n       'fsca_2024-10-06', 'fsca_2024-10-07', 'fsca_2024-10-08',\n       'fsca_2024-10-09', 'fsca_2024-10-10', 'fsca_2024-10-11',\n       'fsca_2024-10-12', 'fsca_2024-10-13', 'fsca_2024-10-14',\n       'fsca_2024-10-15', 'fsca_2024-10-16', 'fsca_2024-10-17',\n       'fsca_2024-10-18', 'fsca_2024-10-19', 'fsca_2024-10-20',\n       'fsca_2024-10-21', 'fsca_2024-10-22', 'fsca_2024-10-23',\n       'fsca_2024-10-24', 'fsca_2024-10-25', 'fsca_2024-10-26',\n       'fsca_2024-10-27', 'fsca_2024-10-28', 'fsca_2024-10-29',\n       'fsca_2024-10-30', 'fsca_2024-10-31', 'fsca_2024-11-01',\n       'fsca_2024-11-02', 'fsca_2024-11-03', 'fsca_2024-11-04',\n       'fsca_2024-11-05', 'fsca_2024-11-06', 'fsca_2024-11-07',\n       'fsca_2024-11-08', 'fsca_2024-11-09', 'fsca_2024-11-10',\n       'fsca_2024-11-11', 'fsca_2024-11-12', 'fsca_2024-11-13',\n       'fsca_2024-11-14', 'fsca_2024-11-15', 'fsca_2024-11-16',\n       'fsca_2024-11-17', 'fsca_2024-11-18', 'fsca_2024-11-19',\n       'fsca_2024-11-20', 'fsca_2024-11-21', 'fsca_2024-11-22',\n       'fsca_2024-11-23', 'fsca_2024-11-24', 'fsca_2024-11-25',\n       'fsca_2024-11-26', 'fsca_2024-11-27', 'fsca_2024-11-28',\n       'fsca_2024-11-29', 'fsca_2024-11-30', 'fsca_2024-12-01',\n       'fsca_2024-12-02', 'fsca_2024-12-03', 'fsca_2024-12-04',\n       'fsca_2024-12-05', 'fsca_2024-12-06', 'fsca_2024-12-07',\n       'fsca_2024-12-08', 'fsca_2024-12-09', 'fsca_2024-12-10',\n       'fsca_2024-12-11', 'fsca'],\n      dtype='object')\n   Latitude  Longitude  fsca_2024-10-01  ...  fsca_2024-12-10  fsca_2024-12-11  fsca\n0      49.0   -125.000                0  ...              239              239   239\n1      49.0   -124.964              239  ...              239              239   239\n2      49.0   -124.928              239  ...              239              239   239\n3      49.0   -124.892              250  ...              250              250   250\n4      49.0   -124.856                0  ...              250              250   250\n[5 rows x 75 columns]\nAll current columns:  Index(['Latitude', 'Longitude', 'fsca_2024-10-01', 'fsca_2024-10-02',\n       'fsca_2024-10-03', 'fsca_2024-10-04', 'fsca_2024-10-05',\n       'fsca_2024-10-06', 'fsca_2024-10-07', 'fsca_2024-10-08',\n       'fsca_2024-10-09', 'fsca_2024-10-10', 'fsca_2024-10-11',\n       'fsca_2024-10-12', 'fsca_2024-10-13', 'fsca_2024-10-14',\n       'fsca_2024-10-15', 'fsca_2024-10-16', 'fsca_2024-10-17',\n       'fsca_2024-10-18', 'fsca_2024-10-19', 'fsca_2024-10-20',\n       'fsca_2024-10-21', 'fsca_2024-10-22', 'fsca_2024-10-23',\n       'fsca_2024-10-24', 'fsca_2024-10-25', 'fsca_2024-10-26',\n       'fsca_2024-10-27', 'fsca_2024-10-28', 'fsca_2024-10-29',\n       'fsca_2024-10-30', 'fsca_2024-10-31', 'fsca_2024-11-01',\n       'fsca_2024-11-02', 'fsca_2024-11-03', 'fsca_2024-11-04',\n       'fsca_2024-11-05', 'fsca_2024-11-06', 'fsca_2024-11-07',\n       'fsca_2024-11-08', 'fsca_2024-11-09', 'fsca_2024-11-10',\n       'fsca_2024-11-11', 'fsca_2024-11-12', 'fsca_2024-11-13',\n       'fsca_2024-11-14', 'fsca_2024-11-15', 'fsca_2024-11-16',\n       'fsca_2024-11-17', 'fsca_2024-11-18', 'fsca_2024-11-19',\n       'fsca_2024-11-20', 'fsca_2024-11-21', 'fsca_2024-11-22',\n       'fsca_2024-11-23', 'fsca_2024-11-24', 'fsca_2024-11-25',\n       'fsca_2024-11-26', 'fsca_2024-11-27', 'fsca_2024-11-28',\n       'fsca_2024-11-29', 'fsca_2024-11-30', 'fsca_2024-12-01',\n       'fsca_2024-12-02', 'fsca_2024-12-03', 'fsca_2024-12-04',\n       'fsca_2024-12-05', 'fsca_2024-12-06', 'fsca_2024-12-07',\n       'fsca_2024-12-08', 'fsca_2024-12-09', 'fsca_2024-12-10',\n       'fsca_2024-12-11', 'fsca'],\n      dtype='object')\nStart to fill in the missing values\nall the df shape:  (462204, 75)\nIndex(['fsca_2024-10-01', 'fsca_2024-10-02', 'fsca_2024-10-03',\n       'fsca_2024-10-04', 'fsca_2024-10-05', 'fsca_2024-10-06',\n       'fsca_2024-10-07', 'fsca_2024-10-08', 'fsca_2024-10-09',\n       'fsca_2024-10-10', 'fsca_2024-10-11', 'fsca_2024-10-12',\n       'fsca_2024-10-13', 'fsca_2024-10-14', 'fsca_2024-10-15',\n       'fsca_2024-10-16', 'fsca_2024-10-17', 'fsca_2024-10-18',\n       'fsca_2024-10-19', 'fsca_2024-10-20', 'fsca_2024-10-21',\n       'fsca_2024-10-22', 'fsca_2024-10-23', 'fsca_2024-10-24',\n       'fsca_2024-10-25', 'fsca_2024-10-26', 'fsca_2024-10-27',\n       'fsca_2024-10-28', 'fsca_2024-10-29', 'fsca_2024-10-30',\n       'fsca_2024-10-31', 'fsca_2024-11-01', 'fsca_2024-11-02',\n       'fsca_2024-11-03', 'fsca_2024-11-04', 'fsca_2024-11-05',\n       'fsca_2024-11-06', 'fsca_2024-11-07', 'fsca_2024-11-08',\n       'fsca_2024-11-09', 'fsca_2024-11-10', 'fsca_2024-11-11',\n       'fsca_2024-11-12', 'fsca_2024-11-13', 'fsca_2024-11-14',\n       'fsca_2024-11-15', 'fsca_2024-11-16', 'fsca_2024-11-17',\n       'fsca_2024-11-18', 'fsca_2024-11-19', 'fsca_2024-11-20',\n       'fsca_2024-11-21', 'fsca_2024-11-22', 'fsca_2024-11-23',\n       'fsca_2024-11-24', 'fsca_2024-11-25', 'fsca_2024-11-26',\n       'fsca_2024-11-27', 'fsca_2024-11-28', 'fsca_2024-11-29',\n       'fsca_2024-11-30', 'fsca_2024-12-01', 'fsca_2024-12-02',\n       'fsca_2024-12-03', 'fsca_2024-12-04', 'fsca_2024-12-05',\n       'fsca_2024-12-06', 'fsca_2024-12-07', 'fsca_2024-12-08',\n       'fsca_2024-12-09', 'fsca_2024-12-10', 'fsca_2024-12-11', 'fsca'],\n      dtype='object')\nFinished correctly     Latitude  Longitude  fsca_2024-10-01  ...  fsca  cumulative_fsca        date\n0      49.0   -125.000              0.0  ...   0.0              0.0  2024-12-12\n1      49.0   -124.964              0.0  ...  26.0           1378.0  2024-12-12\n2      49.0   -124.928              0.0  ...   0.0              0.0  2024-12-12\n3      49.0   -124.892              0.0  ...  13.0             78.0  2024-12-12\n4      49.0   -124.856              0.0  ...   0.0            622.5  2024-12-12\n[5 rows x 77 columns]\nNew filled values csv is saved to /home/chetana/fsca/final_output//2024-12-12_output_with_time_series.csv_gap_filled.csv\n['2024-12-12']\n(462204, 77)\ncount    462204.000000\nmean          7.240619\nstd          18.067353\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           0.000000\nmax          97.000000\nName: fsca, dtype: float64\nNew data is saved to /home/chetana/fsca/final_output//2024-12-12_output_with_time_series.csv\nFile is backed up to /home/chetana/fsca/final_output//2024-12-12_output_with_time_series_backup.csv\n",
  "history_begin_time" : 1734235214916,
  "history_end_time" : 1734235515290,
  "history_notes" : null,
  "history_process" : "c2qa9u",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "nlqusb6vh5m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214258,
  "history_end_time" : 1734235214258,
  "history_notes" : null,
  "history_process" : "lnrsop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sffr0a0cjal",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214275,
  "history_end_time" : 1734235214275,
  "history_notes" : null,
  "history_process" : "c8isgf",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iawhrxnz7g5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214301,
  "history_end_time" : 1734235214301,
  "history_notes" : null,
  "history_process" : "16qpco",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mlxst0iewrz",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "/home/chetana\ntoday date = 2024-12-15\n2024-12-12\ntest start date:  2024-12-12\ntest end date:  2024-5-19\nget test_start_date =  2024-12-12\n2024-12-12 00:00:00\nThe start_date of the water year 2024-10-01 00:00:00\nfile_size_bytes: 507830\nThe file /home/chetana/water_mask/final_output//2024__water_mask.tif exists. skip.\nextracting data for 2024-10-01\nThe file /home/chetana/water_mask/final_output/2024_output.csv exists. skip.\n",
  "history_begin_time" : 1734235214917,
  "history_end_time" : 1734235236697,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ns7y6rbbpp0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214335,
  "history_end_time" : 1734235214335,
  "history_notes" : null,
  "history_process" : "uw1w1u",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e5t0f7mxo0j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214349,
  "history_end_time" : 1734235214349,
  "history_notes" : null,
  "history_process" : "14bhpn",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vyo69ao2bdr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214372,
  "history_end_time" : 1734235214372,
  "history_notes" : null,
  "history_process" : "pyn9xn",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9iqueasccvf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1734235214386,
  "history_end_time" : 1734235214386,
  "history_notes" : null,
  "history_process" : "h1952i",
  "host_id" : "100001",
  "indicator" : "Skipped"
}]
