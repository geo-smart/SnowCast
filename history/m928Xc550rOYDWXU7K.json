[{
  "history_id" : "1sr4n7b3fmb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613334,
  "history_end_time" : 1676063613334,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "377d6s6ztrb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613350,
  "history_end_time" : 1676063613350,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gmxx0wj0lab",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613352,
  "history_end_time" : 1676063613352,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u6xd310vl91",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613353,
  "history_end_time" : 1676063613353,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lji48d5biwk",
  "history_input" : "# Find the best model\nprint(\"model comparison script\")\nprint(\"hello world\")",
  "history_output" : "model comparison script\nhello world\n",
  "history_begin_time" : 1676063636732,
  "history_end_time" : 1676063636787,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "slev0n13pap",
  "history_input" : "# Deploy model to service\n\nprint(\"deploy model to service\")\n",
  "history_output" : "deploy model to service\n",
  "history_begin_time" : 1676063638589,
  "history_end_time" : 1676063638664,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ywxyqhfrhsj",
  "history_input" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit()  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/ywxyqhfrhsj/service_prediction.py\", line 18, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1676063639817,
  "history_end_time" : 1676063642515,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "d3ib9r9b5fy",
  "history_input" : "from model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\n\nprint(\"Train Models\")\n\nworm_holes = [RandomForestHole(), XGBoostHole()]\n\nfor hole in worm_holes:\n  hole.preprocessing()\n  print(hole.train_x.shape)\n  print(hole.train_y.shape)\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  \nprint(\"Finished training and validating all the models.\")\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/d3ib9r9b5fy/model_train_validate.py\", line 1, in <module>\n    from model_creation_rf import RandomForestHole\n  File \"/Users/gokulprathin/gw-workspace/d3ib9r9b5fy/model_creation_rf.py\", line 16, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1676063620879,
  "history_end_time" : 1676063631657,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "vdqkjk1y6zy",
  "history_input" : "# Test models\n\n# Random Forest model creation and save to file\n\nfrom sklearn.ensemble import RandomForestRegressor,ExtraTreesRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom datetime import datetime\n\ndef turn_doy_to_date(year, doy):\n  doy = int(doy)\n  doy = str(doy)\n  doy.rjust(3 + len(doy), '0')\n  #res = datetime.strptime(str(year) + \"-\" + doy, \"%Y-%j\").strftime(\"%m/%d/%Y\")\n  res = datetime.strptime(str(year) + \"-\" + doy, \"%Y-%j\").strftime(\"%Y-%m-%d\")\n  return res\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\ntest_ready_file = f\"{github_dir}/data/ready_for_testing/all_ready_3.csv\"\ntest_ready_pd = pd.read_csv(test_ready_file, header=0, index_col=0)\nsubmission_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_pd = pd.read_csv(submission_file, header=0, index_col=0)\npredicted_file = f\"{homedir}/Documents/GitHub/SnowCast/data/results/wormhole_output_4.csv\"\n\ntrain_cols = ['year','m','doy','ndsi','grd','eto','pr','rmax','rmin','tmmn','tmmx','vpd','vs','lat','lon','elevation','aspect','curvature','slope','eastness','northness']\n\nprint(test_ready_pd.shape)\npd_to_clean = test_ready_pd[train_cols]\nprint(\"PD shape: \", pd_to_clean.shape)\n\ndoy_list = test_ready_pd[\"doy\"].unique()\nprint(doy_list)\n\ndate_list = [turn_doy_to_date(2022, doy_list[i]) for i in range(len(doy_list)) ]\nprint(date_list)\n\nall_features = pd_to_clean.to_numpy()\nall_features = np.nan_to_num(all_features)\nprint(\"train feature shape: \", all_features.shape)\n#all_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\n#all_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\n#base_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic_v2.joblib\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/SnowCast/model/wormhole_20221305163806.joblib\")\ny_predicted = best_random.predict(all_features)\nprint(y_predicted) #first got daily prediction\n\ntarget_dates = [\"2022-01-13\",\"2022-01-20\",\"2022-01-27\",\"2022-02-03\",\"2022-02-10\",\"2022-02-17\",\"2022-02-24\",\"2022-03-03\",\"2022-03-10\",\"2022-03-17\",\"2022-03-24\",\"2022-03-31\",\"2022-04-07\",\"2022-04-14\",\"2022-04-21\",\"2022-04-28\",\"2022-05-05\",\"2022-05-12\",\"2022-05-19\",\"2022-05-26\",\"2022-06-02\",\"2022-06-09\",\"2022-06-16\",\"2022-06-23\",\"2022-06-30\"]\nprint(\"taregt date list: \", len(target_dates))\n\ndaily_predictions = pd.DataFrame(columns = target_dates, index = submission_pd.index)\nfor i in range(len(y_predicted)):\n  doy = all_features[i][2]\n  #print(doy)\n  ndate = turn_doy_to_date(2022, doy)\n  if ndate in target_dates:\n    cell_id = test_ready_pd[\"cell_id\"].iloc[i]\n    daily_predictions.at[cell_id, ndate] = y_predicted[i]\n  #print(ndate, cell_id)\n  #print(y_predicted[i])\n  \nprint(daily_predictions.shape)\n#daily_predictions = daily_predictions[[\"2022-01-13\"]]\n\nif os.path.exists(predicted_file):\n  os.remove(predicted_file)\n  \ndaily_predictions.fillna(0.0, inplace=True)\ndaily_predictions.to_csv(predicted_file, date_format=\"%Y-%d-%m\")\n\n\n# turn daily into weekly using mean values\n\n\n\n\n\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/vdqkjk1y6zy/model_test.py\", line 20, in <module>\n    import geojson\nModuleNotFoundError: No module named 'geojson'\n",
  "history_begin_time" : 1676063632925,
  "history_end_time" : 1676063635566,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ohkevsnpg9o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613378,
  "history_end_time" : 1676063613378,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dfvds1c9y0v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613381,
  "history_end_time" : 1676063613381,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1lg51cyq4gv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613384,
  "history_end_time" : 1676063613384,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f1tztl8f27m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613390,
  "history_end_time" : 1676063613390,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "igom1w0p1rd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613392,
  "history_end_time" : 1676063613392,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kxfjbp2xnd6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613397,
  "history_end_time" : 1676063613397,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p7o268bj8ab",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613400,
  "history_end_time" : 1676063613400,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w12hjn2d0xo",
  "history_input" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\nfor var in var_list:\n\n    var_name = var\n    column_name = var\n\n    dfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n    if not os.path.exists(dfolder):\n        os.makedirs(dfolder)\n\n    all_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\n    for ind in station_cell_mapper_df.index:\n\n        try:\n\n          current_cell_id = station_cell_mapper_df['cell_id'][ind]\n          print(\"collecting \", current_cell_id)\n          single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n          if os.path.exists(single_csv_file):\n              print(\"exists skipping..\")\n              continue\n\n          longitude = station_cell_mapper_df['lon'][ind]\n          latitude = station_cell_mapper_df['lat'][ind]\n\n          # identify a 500 meter buffer around our Point Of Interest (POI)\n          poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n          viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_name)\n\n          def poi_mean(img):\n              reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n              mean = reducer.get(var_name)\n              return img.set('date', img.date().format()).set(column_name,mean)\n\n\n          poi_reduced_imgs = viirs.map(poi_mean)\n\n          nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n          # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n          df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n          df['date'] = pd.to_datetime(df['date'])\n          df = df.set_index('date')\n\n          df['cell_id'] = current_cell_id\n          df['latitude'] = latitude\n          df['longitude'] = longitude\n          df.to_csv(single_csv_file)\n\n          df_list = [all_cell_df, df]\n          all_cell_df = pd.concat(df_list) # merge into big dataframe\n\n        except Exception as e:\n\n          print(e)\n          pass\n    \n    all_cell_df.to_csv(f\"{dfolder}/{column_name}.csv\")  ",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/w12hjn2d0xo/data_gee_gridmet_station_only.py\", line 3, in <module>\n    import ee\nModuleNotFoundError: No module named 'ee'\n",
  "history_begin_time" : 1676063614102,
  "history_end_time" : 1676063616832,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "nodufxk2ud0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613407,
  "history_end_time" : 1676063613407,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9rxtzs4ydnx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613410,
  "history_end_time" : 1676063613410,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nthmzpcci3p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613417,
  "history_end_time" : 1676063613417,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9z7vfkk2d0w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613419,
  "history_end_time" : 1676063613419,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mcgfd761b0y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613423,
  "history_end_time" : 1676063613423,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5jkkcfjmgzb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613426,
  "history_end_time" : 1676063613426,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jr7qpmlo1t1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613428,
  "history_end_time" : 1676063613428,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yh6595he25s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613432,
  "history_end_time" : 1676063613432,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "upca9ngfgxi",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/gokulprathin/gw-workspace/upca9ngfgxi/data_nsidc_4km_swe.py\", line 24, in <module>\n    import netCDF4 as nc\nModuleNotFoundError: No module named 'netCDF4'\n",
  "history_begin_time" : 1676063614076,
  "history_end_time" : 1676063616832,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0osyvj4ndiq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613437,
  "history_end_time" : 1676063613437,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1i3h2akv1gv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1676063613441,
  "history_end_time" : 1676063613441,
  "history_notes" : null,
  "history_process" : "8ovvln",
  "host_id" : "100001",
  "indicator" : "Skipped"
}]
