[{
  "history_id" : "yb1pv5xonmy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389844,
  "history_end_time" : 1677352389844,
  "history_notes" : null,
  "history_process" : "78vedq",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "11y64z9j4yr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389848,
  "history_end_time" : 1677352389848,
  "history_notes" : null,
  "history_process" : "mxpyqt",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "mjmfhdicx7u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389852,
  "history_end_time" : 1677352389852,
  "history_notes" : null,
  "history_process" : "c2xkhz",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zs12l7y6taq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389855,
  "history_end_time" : 1677352389855,
  "history_notes" : null,
  "history_process" : "rauqsh",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "c6eq6j5maro",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389858,
  "history_end_time" : 1677352389858,
  "history_notes" : null,
  "history_process" : "mi3e5n",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "nqek1jijwul",
  "history_input" : "# Integrate all the datasets into one training dataset\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom math import radians\nfrom sklearn import neighbors as sk\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom datetime import datetime,timedelta\n\nprint(\"integrating datasets into one dataset\")\n# pd.set_option('display.max_columns', None)'''\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nfile = f'{github_dir}/data/snowcast_provided/grid_cells.geojson'\n\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\n\n# example_mod_file = f\"{github_dir}/data/modis/mod10a1_ndsi_f191fe19-0e81-4bc9-9980-29738a05a49b.csv\"\n\ntraining_feature_pd = pd.read_csv(training_feature_file, header=0, index_col=0)\ntesting_feature_pd = pd.read_csv(testing_feature_file, header=0, index_col=0)\ntrain_labels_pd = pd.read_csv(train_labels_file, header=0, index_col=0)\nprint(train_labels_pd.head())\n# if \"2ca6a37f-67f5-4905-864b-ddf98d956ebb\" in train_labels_pd.index and \"2013-01-02\" in train_labels_pd.columns:\n#   print(\"Check one value: \", train_labels_pd.loc[\"2ca6a37f-67f5-4905-864b-ddf98d956ebb\"][\"2013-01-02\"])\n# else:\n#   print(\"Key not existed\")\n\nstation_cell_mapper_pd = pd.read_csv(station_cell_mapper_file, header=0, index_col=0)\n\n\n# print(station_cell_mapper_pd.head())\n\n# example_mod_pd = pd.read_csv(example_mod_file, header=0, index_col=0)\n# print(example_mod_pd.shape)\n\n\ndef getDateStr(x):\n    return x.split(\" \")[0]\n\n\ndef integrate_modis():\n    \"\"\"\n  Integrate all MODIS data into mod_all.csv\n  \"\"\"\n    all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n    if os.path.isfile(all_mod_file):\n        return\n    dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n    mod_all_df = pd.DataFrame(columns=[\"date\"])\n    mod_all_df['date'] = dates\n\n    # print(mod_all_df.head())\n    for ind in station_cell_mapper_pd.index:\n        current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n        print(current_cell_id)\n        mod_single_file = f\"{github_dir}/data/sat_training/modis/mod10a1_ndsi_{current_cell_id}.csv\"\n        if os.path.isfile(mod_single_file):\n            mod_single_pd = pd.read_csv(mod_single_file, header=0)\n            mod_single_pd = mod_single_pd[[\"date\", \"mod10a1_ndsi\"]]\n            mod_single_pd = mod_single_pd.rename(columns={\"mod10a1_ndsi\": current_cell_id})\n            mod_single_pd['date'] = pd.to_datetime(mod_single_pd['date']).astype(str)\n            print(mod_all_df.shape)\n            mod_all_df = pd.merge(mod_all_df, mod_single_pd, how='left', on=\"date\")\n    mod_all_df.to_csv(all_mod_file)\n\n\ndef integrate_sentinel1():\n    \"\"\"\n  Integrate all Sentinel 1 data into sentinel1_all.csv\n  \"\"\"\n    all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n    if os.path.isfile(all_sentinel1_file):\n        return\n    dates = pd.date_range(start='1/1/2013', end='12/31/2021', freq='D').astype(str)\n    sentinel1_all_df = pd.DataFrame(columns=[\"date\"])\n    sentinel1_all_df['date'] = dates\n    # print(mod_all_df.head())\n\n    for ind in station_cell_mapper_pd.index:\n        current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n        print(current_cell_id)\n        sentinel1_single_file = f\"{github_dir}/data/sat_training/sentinel1/s1_grd_vv_{current_cell_id}.csv\"\n        if os.path.isfile(sentinel1_single_file) and current_cell_id not in sentinel1_all_df:\n            sentinel1_single_pd = pd.read_csv(sentinel1_single_file, header=0)\n            sentinel1_single_pd = sentinel1_single_pd[[\"date\", \"s1_grd_vv\"]]\n            sentinel1_single_pd = sentinel1_single_pd.rename(columns={\"s1_grd_vv\": current_cell_id})\n            # sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n            sentinel1_single_pd['date'] = pd.to_datetime(sentinel1_single_pd['date']).dt.round(\"D\").astype(str)\n            print(\"sentinel1_single_pd: \", sentinel1_single_pd.head())\n            print(\"sentinel1_single_pd check value: \", sentinel1_single_pd[sentinel1_single_pd[\"date\"] == \"2015-04-01\"])\n            sentinel1_single_pd = sentinel1_single_pd.drop_duplicates(subset=['date'],\n                                                                      keep='first')  # this will remove all the other values of the same day\n\n            sentinel1_all_df = pd.merge(sentinel1_all_df, sentinel1_single_pd, how='left', on=\"date\")\n            print(\"sentinel1_all_df check value: \", sentinel1_all_df[sentinel1_all_df[\"date\"] == \"2015-04-01\"])\n            print(\"sentinel1_all_df: \", sentinel1_all_df.shape)\n\n    print(sentinel1_all_df.shape)\n    sentinel1_all_df.to_csv(all_sentinel1_file)\n\n\ndef integrate_gridmet():\n    \"\"\"\n  Integrate all gridMET data into gridmet_all.csv\n  \"\"\"\n\n    dates = pd.date_range(start='10/1/2018', end='09/30/2019', freq='D').astype(str)\n\n    # print(mod_all_df.head())\n    var_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\n    for var in var_list:\n        gridmet_all_df = pd.DataFrame(columns=[\"date\"])\n        gridmet_all_df['date'] = dates\n        all_gridmet_file = f\"{github_dir}/data/ready_for_training/gridmet_{var}_all.csv\"\n        if os.path.isfile(all_gridmet_file):\n            return\n        for ind in station_cell_mapper_pd.index:\n            current_cell_id = station_cell_mapper_pd[\"cell_id\"][ind]\n            print(current_cell_id)\n            gridmet_single_file = f\"{github_dir}/data/sim_training/gridmet/{var}_{current_cell_id}.csv\"\n            if os.path.isfile(gridmet_single_file) and current_cell_id not in gridmet_all_df:\n                gridmet_single_pd = pd.read_csv(gridmet_single_file, header=0)\n                gridmet_single_pd = gridmet_single_pd[[\"date\", var]]\n                gridmet_single_pd = gridmet_single_pd.rename(columns={var: current_cell_id})\n                # sentinel1_single_pd['date'] = sentinel1_single_pd['date'].astype('datetime64[ns]')\n                gridmet_single_pd['date'] = pd.to_datetime(gridmet_single_pd['date']).dt.round(\"D\").astype(str)\n                print(\"gridmet_single_pd: \", gridmet_single_pd.head())\n                print(\"gridmet_single_pd check value: \", gridmet_single_pd[gridmet_single_pd[\"date\"] == \"2015-04-01\"])\n                gridmet_single_pd = gridmet_single_pd.drop_duplicates(subset=['date'],\n                                                                      keep='first')  # this will remove all the other values of the same day\n\n                gridmet_all_df = pd.merge(gridmet_all_df, gridmet_single_pd, how='left', on=\"date\")\n                print(\"gridmet_all_df check value: \", gridmet_all_df[gridmet_all_df[\"date\"] == \"2015-04-01\"])\n                print(\"gridmet_all_df: \", gridmet_all_df.shape)\n\n        print(gridmet_all_df.shape)\n        gridmet_all_df.to_csv(all_gridmet_file)\n\n\ndef prepare_training_csv():\n    \"\"\"\n  MOD model:\n    input columns: [m, doy, ndsi]\n    output column: [swe]\n  Sentinel1 model:\n    input columns: [m, doy, grd]\n    output column: [swe]\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n    all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready.csv\"\n    if os.path.isfile(all_ready_file):\n        return\n\n    all_mod_file = f\"{github_dir}/data/ready_for_training/modis_all.csv\"\n    modis_all_pd = pd.read_csv(all_mod_file, header=0)\n    all_sentinel1_file = f\"{github_dir}/data/ready_for_training/sentinel1_all.csv\"\n    sentinel1_all_pd = pd.read_csv(all_sentinel1_file, header=0)\n    all_gridmet_eto_file = f\"{github_dir}/data/ready_for_training/gridmet_eto_all.csv\"\n    gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col=0)\n    all_gridmet_pr_file = f\"{github_dir}/data/ready_for_training/gridmet_pr_all.csv\"\n    gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col=0)\n    all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_training/gridmet_rmax_all.csv\"\n    gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col=0)\n    all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_training/gridmet_rmin_all.csv\"\n    gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col=0)\n    all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmn_all.csv\"\n    gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col=0)\n    all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmx_all.csv\"\n    gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col=0)\n    all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_training/gridmet_vpd_all.csv\"\n    gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col=0)\n    all_gridmet_vs_file = f\"{github_dir}/data/ready_for_training/gridmet_vs_all.csv\"\n    gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col=0)\n\n    grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n    grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col=1)\n\n    print(\"modis_all_size: \", modis_all_pd.shape)\n    print(\"station size: \", station_cell_mapper_pd.shape)\n    print(\"training_feature_pd size: \", training_feature_pd.shape)\n    print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n\n    all_training_pd = pd.DataFrame(\n        columns=[\"cell_id\", \"year\", \"m\", \"doy\", \"ndsi\", \"grd\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\",\n                 \"lat\", \"lon\", \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe\"])\n    all_training_pd = all_training_pd.reset_index()\n    for index, row in modis_all_pd.iterrows():\n        dt = datetime.strptime(row['date'], '%Y-%m-%d')\n        month = dt.month\n        year = dt.year\n        doy = dt.timetuple().tm_yday\n        print(f\"Dealing {year} {doy}\")\n        for i in range(3, len(row.index)):\n            cell_id = row.index[i][:-2]\n            if cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n                ndsi = row.values[i]\n                swe = train_labels_pd.loc[cell_id, row['date']]\n                grd = sentinel1_all_pd.loc[index, cell_id]\n                eto = gridmet_eto_all_pd.loc[index, cell_id]\n                pr = gridmet_pr_all_pd.loc[index, cell_id]\n                rmax = gridmet_rmax_all_pd.loc[index, cell_id]\n                rmin = gridmet_rmin_all_pd.loc[index, cell_id]\n                tmmn = gridmet_tmmn_all_pd.loc[index, cell_id]\n                tmmx = gridmet_tmmx_all_pd.loc[index, cell_id]\n                vpd = gridmet_vpd_all_pd.loc[index, cell_id]\n                vs = gridmet_vs_all_pd.loc[index, cell_id]\n                lat = grid_terrain_pd.loc[cell_id, \"Longitude [deg]\"]\n                lon = grid_terrain_pd.loc[cell_id, \"Latitude [deg]\"]\n                elevation = grid_terrain_pd.loc[cell_id, \"Elevation [m]\"]\n                aspect = grid_terrain_pd.loc[cell_id, \"Aspect [deg]\"]\n                curvature = grid_terrain_pd.loc[cell_id, \"Curvature [ratio]\"]\n                slope = grid_terrain_pd.loc[cell_id, \"Slope [deg]\"]\n                eastness = grid_terrain_pd.loc[cell_id, \"Eastness [unitCirc.]\"]\n                northness = grid_terrain_pd.loc[cell_id, \"Northness [unitCirc.]\"]\n\n                if not np.isnan(swe):\n                    json_kv = {\"cell_id\": cell_id, \"year\": year, \"m\": month, \"doy\": doy, \"ndsi\": ndsi, \"grd\": grd,\n                               \"eto\": eto,\n                               \"pr\": pr, \"rmax\": rmax, \"rmin\": rmin, \"tmmn\": tmmn, \"tmmx\": tmmx, \"vpd\": vpd, \"vs\": vs,\n                               \"lat\": lat,\n                               \"lon\": lon, \"elevation\": elevation, \"aspect\": aspect, \"curvature\": curvature,\n                               \"slope\": slope,\n                               \"eastness\": eastness, \"northness\": northness, \"swe\": swe}\n                    # print(json_kv)\n                    all_training_pd = all_training_pd.append(json_kv, ignore_index=True)\n\n    print(all_training_pd.shape)\n    all_training_pd.to_csv(all_ready_file)\n\n\ndef loc_closest_gridcell_id(find_lat, find_lon, valid_cols):\n    grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n    grid_lat_lon = pd.read_csv(grid_terrain_file, header=0, usecols=['cell_id', 'Latitude [deg]', 'Longitude [deg]']).loc[lambda df: df['cell_id'].isin(valid_cols)]\n    # print(grid_lat_lon.shape)\n    # print(grid_lat_lon)\n    grid_lat_lon_npy = grid_lat_lon.to_numpy()\n    grid_lat_lon_rad = np.array([[radians(x[2]), radians(x[1])] for x in grid_lat_lon_npy])\n    ball_tree = sk.BallTree(grid_lat_lon_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lon))], return_distance=True)\n    # print(dist)\n    print(ind[0][0])\n    print(\"cell id: \", grid_lat_lon.iloc[ind[0][0]]['cell_id'])\n    return ind[0][0], grid_lat_lon.iloc[ind[0][0]]['cell_id']\n\n\ndef prepare_training_csv_nsidc():\n    \"\"\"\n  gridMET model:\n    input columns: [m, doy, tmmn, tmmx, pr, vpd, eto, rmax, rmin, vs]\n    output column: [swe]\n  \"\"\"\n    all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_new.csv\"\n    if os.path.isfile(all_ready_file):\n        print(\"The file already exists. Exiting..\")\n        return\n    all_gridmet_eto_file = f\"{github_dir}/data/ready_for_training/gridmet_eto_all.csv\"\n    gridmet_eto_all_pd = pd.read_csv(all_gridmet_eto_file, header=0, index_col=0)\n    all_gridmet_pr_file = f\"{github_dir}/data/ready_for_training/gridmet_pr_all.csv\"\n    gridmet_pr_all_pd = pd.read_csv(all_gridmet_pr_file, header=0, index_col=0)\n    all_gridmet_rmax_file = f\"{github_dir}/data/ready_for_training/gridmet_rmax_all.csv\"\n    gridmet_rmax_all_pd = pd.read_csv(all_gridmet_rmax_file, header=0, index_col=0)\n    all_gridmet_rmin_file = f\"{github_dir}/data/ready_for_training/gridmet_rmin_all.csv\"\n    gridmet_rmin_all_pd = pd.read_csv(all_gridmet_rmin_file, header=0, index_col=0)\n    all_gridmet_tmmn_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmn_all.csv\"\n    gridmet_tmmn_all_pd = pd.read_csv(all_gridmet_tmmn_file, header=0, index_col=0)\n    all_gridmet_tmmx_file = f\"{github_dir}/data/ready_for_training/gridmet_tmmx_all.csv\"\n    gridmet_tmmx_all_pd = pd.read_csv(all_gridmet_tmmx_file, header=0, index_col=0)\n    all_gridmet_vpd_file = f\"{github_dir}/data/ready_for_training/gridmet_vpd_all.csv\"\n    gridmet_vpd_all_pd = pd.read_csv(all_gridmet_vpd_file, header=0, index_col=0)\n    all_gridmet_vs_file = f\"{github_dir}/data/ready_for_training/gridmet_vs_all.csv\"\n    gridmet_vs_all_pd = pd.read_csv(all_gridmet_vs_file, header=0, index_col=0)\n    all_nsidc_file = f\"{github_dir}/data/sim_training/nsidc/2019nsidc_data.csv\"\n    nsidc_all_pd = pd.read_csv(all_nsidc_file, header=0, index_col=0)\n\n    # print(nsidc_all_pd.shape)\n    # print(nsidc_all_pd)\n\n    grid_terrain_file = f\"{github_dir}/data/terrain/gridcells_terrainData.csv\"\n    grid_terrain_pd = pd.read_csv(grid_terrain_file, header=0, index_col=0)\n\n    # print(grid_terrain_pd.shape)\n    # print(grid_terrain_pd)\n\n    print(\"station size: \", station_cell_mapper_pd.shape)\n    print(\"training_feature_pd size: \", training_feature_pd.shape)\n    print(\"testing_feature_pd size: \", testing_feature_pd.shape)\n    all_valid_columns = gridmet_eto_all_pd.columns.values\n    all_training_pd = pd.DataFrame(\n        columns=[\"cell_id\", \"year\", \"m\", \"day\", \"eto\", \"pr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"lat\", \"lon\",\n                 \"elevation\", \"aspect\", \"curvature\", \"slope\", \"eastness\", \"northness\", \"swe_0719\", \"depth_0719\", \"swe_snotel\"])\n    all_training_pd = all_training_pd.reset_index()\n    for index, row in nsidc_all_pd.iterrows():\n        month = row['Month']\n        year = row['Year']\n        day = row['Day']\n#         print(f\"Dealing {year} {month} {day}\")\n        lat = row['Lat']\n        lon = row['Lon']\n#         print(\"lat lon: \", lat, \" \", lon)\n        ind, cell_id = loc_closest_gridcell_id(lat, lon, all_valid_columns)\n        swe = row['SWE']\n        depth = row['Depth']\n        index = index % 365\n        eto = gridmet_eto_all_pd.iloc[index][cell_id]\n        pr = gridmet_pr_all_pd.iloc[index][cell_id]\n        rmax = gridmet_rmax_all_pd.iloc[index][cell_id]\n        rmin = gridmet_rmin_all_pd.iloc[index][cell_id]\n        tmmn = gridmet_tmmn_all_pd.iloc[index][cell_id]\n        tmmx = gridmet_tmmx_all_pd.iloc[index][cell_id]\n        vpd = gridmet_vpd_all_pd.iloc[index][cell_id]\n        vs = gridmet_vs_all_pd.iloc[index][cell_id]\n        lat = grid_terrain_pd.loc[ind, \"Latitude [deg]\"]\n        lon = grid_terrain_pd.loc[ind, \"Longitude [deg]\"]\n        elevation = grid_terrain_pd.loc[ind, \"Elevation [m]\"]\n        aspect = grid_terrain_pd.loc[ind, \"Aspect [deg]\"]\n        curvature = grid_terrain_pd.loc[ind, \"Curvature [ratio]\"]\n        slope = grid_terrain_pd.loc[ind, \"Slope [deg]\"]\n        eastness = grid_terrain_pd.loc[ind, \"Eastness [unitCirc.]\"]\n        northness = grid_terrain_pd.loc[ind, \"Northness [unitCirc.]\"]\n        cdate = datetime(year=int(year), month=int(month), day=int(day))\n        current_date = cdate.strftime(\"%Y-%m-%d\")\n        \n        if cell_id in train_labels_pd.index and current_date in train_labels_pd.columns:\n#           print(\"Check one value: \", train_labels_pd.loc[cell_id][current_date])\n          swe_snotel = train_labels_pd.loc[cell_id][current_date]\n        else:\n          swe_snotel = -1\n#           print(\"Key not existed\")\n\n        if not np.isnan(swe):\n            json_kv = {\"cell_id\":cell_id,\"year\":year, \"m\":month, \"day\": day, \"eto\":eto, \"pr\":pr, \"rmax\":rmax, \"rmin\":rmin, \"tmmn\":tmmn, \"tmmx\":tmmx, \"vpd\":vpd, \"vs\":vs, \"lat\":lat, \"lon\":lon, \"elevation\":elevation, \"aspect\":aspect, \"curvature\":curvature, \"slope\":slope, \"eastness\":eastness, \"northness\":northness, \"swe_0719\":swe, \"depth_0719\":depth, \"swe_snotel\": swe_snotel}\n#             print(json_kv)\n            all_training_pd = all_training_pd.append(json_kv, ignore_index=True)\n#             print(all_training_pd.shape)\n\n    print(all_training_pd.shape)\n    all_training_pd.to_csv(all_ready_file)\n\n    \"\"\"\n  grd_all_pd = pd.DataFrame(columns=[\"year\", \"m\", \"doy\", \"grd\", \"swe\"])\n  grd_all_pd = grd_all_pd.reset_index()\n  for index, row in sentinel1_all_pd.iterrows():\n    dt = datetime.strptime(row['date'], '%Y-%m-%d')\n    year = dt.year\n    month = dt.month\n    doy = dt.timetuple().tm_yday\n    for i in range(3,len(row.index)):\n      cell_id = row.index[i]\n      grd = row.values[i]\n      if not np.isnan(grd) and cell_id in train_labels_pd.index and row['date'] in train_labels_pd:\n        swe = train_labels_pd.loc[cell_id, row['date']]\n        if not np.isnan(swe):\n          print([month, doy, grd, swe])\n          grd_all_pd = grd_all_pd.append({\"year\": year, \"m\":month, \"doy\": doy, \"grd\": grd, \"swe\": swe}, ignore_index = True)\n  \n  print(grd_all_pd.shape)\n  grd_all_pd.to_csv(f\"{github_dir}/data/ready_for_training/sentinel1_ready.csv\")\n  \"\"\"\n\n\n# exit() # done already\n\n# integrate_modis()\n# integrate_sentinel1()\n# integrate_gridmet()\n# prepare_training_csv()\nprepare_training_csv_nsidc()",
  "history_output" : "",
  "history_begin_time" : 1677356964558,
  "history_end_time" : 1677360080491,
  "history_notes" : null,
  "history_process" : "u7xh2p",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "7ppt2voi65q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389867,
  "history_end_time" : 1677352389867,
  "history_notes" : null,
  "history_process" : "2wkl6e",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "xxpmfofo97b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389872,
  "history_end_time" : 1677352389872,
  "history_notes" : null,
  "history_process" : "i2fynz",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "1zdhi1w2tli",
  "history_input" : "from model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\n\nprint(\"Train Models\")\n\nworm_holes = [RandomForestHole(), XGBoostHole()]\n\nfor hole in worm_holes:\n  hole.preprocessing()\n  print(hole.train_x.shape)\n  print(hole.train_y.shape)\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  \nprint(\"Finished training and validating all the models.\")\n",
  "history_output" : "Train Models\nall columns:  ['year', 'm', 'day', 'eto', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lat', 'lon', 'elevation', 'aspect', 'curvature', 'slope', 'eastness', 'northness', 'swe_0719', 'depth_0719', 'swe_snotel']\n(4713, 22)\n(4713, 1)\n/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/pipeline.py:405: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  self._final_estimator.fit(Xt, y, **fit_params_last_step)\nThe random forest model performance for testing set\n--------------------------------------\nMAE is 0.08095082178540712\nMSE is 0.0760352297315518\nR2 score is 0.9997501618703859\nRMSE is 0.27574486347265253\nSaving model to /home/chetana/Documents/GitHub/SnowCast/model/wormhole_RandomForestHole_20232502212126.joblib\nall columns:  ['year', 'm', 'day', 'eto', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lat', 'lon', 'elevation', 'aspect', 'curvature', 'slope', 'eastness', 'northness', 'swe_0719', 'depth_0719', 'swe_snotel']\n(4713, 22)\n(4713, 1)\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/1zdhi1w2tli/model_train_validate.py\", line 12, in <module>\n    hole.train()\n  File \"/home/chetana/gw-workspace/1zdhi1w2tli/base_hole.py\", line 57, in train\n    self.classifier.fit(self.train_x, self.train_y)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 340, in fit\n    self._validate_params()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/base.py\", line 581, in _validate_params\n    validate_parameter_constraints(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/sklearn/utils/_param_validation.py\", line 97, in validate_parameter_constraints\n    raise InvalidParameterError(\nsklearn.utils._param_validation.InvalidParameterError: The 'criterion' parameter of ExtraTreesRegressor must be a str among {'absolute_error', 'friedman_mse', 'poisson', 'squared_error'}. Got 'mse' instead.\n",
  "history_begin_time" : 1677360082287,
  "history_end_time" : 1677360089311,
  "history_notes" : null,
  "history_process" : "e8k4wq",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "s3ttmt44dcr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389885,
  "history_end_time" : 1677352389885,
  "history_notes" : null,
  "history_process" : "h1qp9v",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "b7i1beg5l20",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389887,
  "history_end_time" : 1677352389887,
  "history_notes" : null,
  "history_process" : "urd0nk",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "w3b43msnn0q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389892,
  "history_end_time" : 1677352389892,
  "history_notes" : null,
  "history_process" : "525l8q",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "x16rhvy5it5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389896,
  "history_end_time" : 1677352389896,
  "history_notes" : null,
  "history_process" : "7temiv",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "i541anxbobz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389902,
  "history_end_time" : 1677352389902,
  "history_notes" : null,
  "history_process" : "rmxece",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "6rr51q94237",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389907,
  "history_end_time" : 1677352389907,
  "history_notes" : null,
  "history_process" : "illwc1",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "rx4rgpryxw4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389910,
  "history_end_time" : 1677352389910,
  "history_notes" : null,
  "history_process" : "sjs5by",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "vcfcrnqd4hh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389913,
  "history_end_time" : 1677352389913,
  "history_notes" : null,
  "history_process" : "y7nb46",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "8bvslidwm8d",
  "history_input" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\n# exit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = os.path.join(homedir, 'Documents', 'GitHub', 'SnowCast')\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'gridmet'\nproduct_name = 'IDAHO_EPSCOR/GRIDMET'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\nvar_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'eto', 'rmax', 'rmin', 'vs']\n\nfor var in var_list:\n\n    var_name = var\n    column_name = var\n\n    dfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\n    if not os.path.exists(dfolder):\n        os.makedirs(dfolder)\n\n    all_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\n    for ind in station_cell_mapper_df.index:\n\n        try:\n\n          current_cell_id = station_cell_mapper_df['cell_id'][ind]\n          print(\"collecting \", current_cell_id)\n          single_csv_file = f\"{dfolder}/{column_name}_{current_cell_id}.csv\"\n\n          if os.path.exists(single_csv_file):\n              print(\"exists skipping..\")\n              continue\n\n          longitude = station_cell_mapper_df['lon'][ind]\n          latitude = station_cell_mapper_df['lat'][ind]\n\n          # identify a 500 meter buffer around our Point Of Interest (POI)\n          poi = ee.Geometry.Point(longitude, latitude).buffer(1000)\n          viirs = ee.ImageCollection(product_name).filterDate(start_date, end_date).filterBounds(poi).select(var_name)\n\n          def poi_mean(img):\n              reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=1000)\n              mean = reducer.get(var_name)\n              return img.set('date', img.date().format()).set(column_name,mean)\n\n\n          poi_reduced_imgs = viirs.map(poi_mean)\n\n          nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n          # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n          df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n          df['date'] = pd.to_datetime(df['date'])\n          df = df.set_index('date')\n\n          df['cell_id'] = current_cell_id\n          df['latitude'] = latitude\n          df['longitude'] = longitude\n          df.to_csv(single_csv_file)\n\n          df_list = [all_cell_df, df]\n          all_cell_df = pd.concat(df_list) # merge into big dataframe\n\n        except Exception as e:\n\n          print(e)\n          pass\n    \n    all_cell_df.to_csv(f\"{dfolder}/{column_name}.csv\")  ",
  "history_output" : "Fetching credentials using gcloud\nsh: gcloud: command not found\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8bvslidwm8d/data_gee_gridmet_station_only.py\", line 15, in <module>\n    ee.Initialize()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/ee/__init__.py\", line 131, in Initialize\n    credentials = data.get_persistent_credentials()\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/ee/data.py\", line 221, in get_persistent_credentials\n    return Credentials(None, **oauth.get_credentials_arguments())\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/ee/oauth.py\", line 78, in get_credentials_arguments\n    args['refresh_token'] = stored['refresh_token']  # Must be present\nKeyError: 'refresh_token'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/8bvslidwm8d/data_gee_gridmet_station_only.py\", line 17, in <module>\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/ee/__init__.py\", line 104, in Authenticate\n    return oauth.authenticate(authorization_code, quiet, code_verifier, auth_mode,\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/ee/oauth.py\", line 381, in authenticate\n    _load_app_default_credentials(auth_mode == 'gcloud', scopes, quiet)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/ee/oauth.py\", line 291, in _load_app_default_credentials\n    raise Exception('gcloud failed. Please check for any errors above '\nException: gcloud failed. Please check for any errors above and install gcloud if needed.\n",
  "history_begin_time" : 1677352390477,
  "history_end_time" : 1677352395045,
  "history_notes" : null,
  "history_process" : "a8p3n7",
  "host_id" : "45wlwr",
  "indicator" : "Failed"
},{
  "history_id" : "uk80m6dnzdh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389919,
  "history_end_time" : 1677352389919,
  "history_notes" : null,
  "history_process" : "smsdr0",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "ecn7a7gofc6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389922,
  "history_end_time" : 1677352389922,
  "history_notes" : null,
  "history_process" : "4i0sop",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "t6e2f8r6yez",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389925,
  "history_end_time" : 1677352389925,
  "history_notes" : null,
  "history_process" : "b63prf",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "2m05xmksyoj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389927,
  "history_end_time" : 1677352389927,
  "history_notes" : null,
  "history_process" : "zh38b6",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "mkvg4bjs1hq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389929,
  "history_end_time" : 1677352389929,
  "history_notes" : null,
  "history_process" : "wdh394",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "scbm287x3hw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389931,
  "history_end_time" : 1677352389931,
  "history_notes" : null,
  "history_process" : "p87wh1",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "t25yz7ze5pd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389934,
  "history_end_time" : 1677352389934,
  "history_notes" : null,
  "history_process" : "ilbqzg",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "kskk33y6kkr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389938,
  "history_end_time" : 1677352389938,
  "history_notes" : null,
  "history_process" : "do86ae",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
},{
  "history_id" : "zypkbej32yq",
  "history_input" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n'''\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n'''\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    \n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude, )\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \nturn_nsidc_nc_to_csv()",
  "history_output" : "/home/chetana\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00036589]]\n[[180957]]\n[[[  37.708332 -119.125   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00033864]]\n[[164419]]\n[[[  37.083332 -118.75    ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00015398]]\n[[164425]]\n[[[  37.083332 -118.5     ]]]\n[[0.00031948]]\n[[154543]]\n[[[  36.708332 -118.833336]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00015498]]\n[[193151]]\n[[[  38.166668 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00024992]]\n[[173209]]\n[[[  37.416668 -119.5     ]]]\n[[0.00013956]]\n[[147950]]\n[[[  36.458332 -118.541664]]]\n[[0.00020465]]\n[[156750]]\n[[[  36.791668 -118.416664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00037612]]\n[[186492]]\n[[[  37.916668 -119.25    ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00017002]]\n[[178752]]\n[[[  37.625    -119.083336]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00015417]]\n[[146849]]\n[[[  36.416668 -118.583336]]]\n[[0.00016601]]\n[[183167]]\n[[[  37.791668 -119.208336]]]\n[[0.00032432]]\n[[182046]]\n[[[  37.75     -119.791664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.0003812]]\n[[151250]]\n[[[  36.583332 -118.75    ]]]\n[[0.00032729]]\n[[194278]]\n[[[  38.208332 -119.875   ]]]\n[[0.00032678]]\n[[176538]]\n[[[  37.541668 -119.25    ]]]\n[[0.00022214]]\n[[174321]]\n[[[  37.458332 -119.291664]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025497]]\n[[167711]]\n[[[  37.208332 -119.208336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00038418]]\n[[239381]]\n[[[  39.833332 -121.333336]]]\n[[0.00025197]]\n[[193160]]\n[[[  38.166668 -119.666664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00021309]]\n[[267031]]\n[[[  40.791668 -121.791664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00024862]]\n[[277959]]\n[[[  41.166668 -121.958336]]]\n[[0.00019348]]\n[[178753]]\n[[[  37.625    -119.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.00017594]]\n[[155643]]\n[[[  36.75     -118.708336]]]\n[[0.00032588]]\n[[189821]]\n[[[  38.041668 -119.666664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00017579]]\n[[145757]]\n[[[  36.375    -118.291664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035749]]\n[[144646]]\n[[[  36.333332 -118.625   ]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[0.00018052]]\n[[234637]]\n[[[  39.666668 -120.625   ]]]\n[[0.00020109]]\n[[174334]]\n[[[  37.458332 -118.75    ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00035973]]\n[[279150]]\n[[[  41.208332 -122.791664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[0.0002774]]\n[[279156]]\n[[[  41.208332 -122.541664]]]\n[[0.00027356]]\n[[196529]]\n[[[  38.291668 -119.625   ]]]\n[[0.0002125]]\n[[202215]]\n[[[  38.5      -119.791664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00034271]]\n[[166626]]\n[[[  37.166668 -118.541664]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00021444]]\n[[166610]]\n[[[  37.166668 -119.208336]]]\n[[0.00010017]]\n[[184265]]\n[[[  37.833332 -119.458336]]]\n[[0.00013865]]\n[[185376]]\n[[[  37.875    -119.333336]]]\n[[0.00038687]]\n[[167717]]\n[[[  37.208332 -118.958336]]]\n[[0.00031303]]\n[[153453]]\n[[[  36.666668 -118.416664]]]\n[[0.00019106]]\n[[172122]]\n[[[  37.375    -118.916664]]]\n[[0.00022016]]\n[[189830]]\n[[[  38.041668 -119.291664]]]\n[[9.7935995e-05]]\n[[213680]]\n[[[  38.916668 -120.375   ]]]\n[[4.44927463e-05]]\n[[213684]]\n[[[  38.916668 -120.208336]]]\n[[0.00025795]]\n[[185368]]\n[[[  37.875    -119.666664]]]\n[[0.00018855]]\n[[144647]]\n[[[  36.333332 -118.583336]]]\n[[0.00013633]]\n[[163319]]\n[[[  37.041668 -118.916664]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00045162]]\n[[185710]]\n[[[  37.875    -105.416664]]]\n[[0.00025435]]\n[[429279]]\n[[[  46.916668 -110.875   ]]]\n[[0.00021788]]\n[[322295]]\n[[[  42.708332 -120.791664]]]\n[[0.00026951]]\n[[462456]]\n[[[  48.708332 -121.916664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00040204]]\n[[388480]]\n[[[  45.166668 -115.958336]]]\n[[0.00014205]]\n[[136228]]\n[[[  36.       -106.541664]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.0003979]]\n[[256536]]\n[[[  40.416668 -105.833336]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00033677]]\n[[268603]]\n[[[  40.833332 -106.75    ]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00039466]]\n[[260006]]\n[[[  40.541668 -111.666664]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00035303]]\n[[336197]]\n[[[  43.208332 -122.125   ]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00042276]]\n[[284429]]\n[[[  41.375    -106.208336]]]\n[[0.00030705]]\n[[71813]]\n[[[  33.375    -107.833336]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[9.13684685e-05]]\n[[202219]]\n[[[  38.5   -119.625]]]\n[[0.00022309]]\n[[466617]]\n[[[  48.958332 -115.958336]]]\n[[0.00041662]]\n[[270934]]\n[[[  40.916668 -111.833336]]]\n[[0.00020189]]\n[[173518]]\n[[[  37.416668 -106.625   ]]]\n[[0.00041858]]\n[[256517]]\n[[[  40.416668 -106.625   ]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[0.00030302]]\n[[176687]]\n[[[  37.541668 -113.041664]]]\n[[3.15345512e-05]]\n[[207931]]\n[[[  38.708332 -120.041664]]]\n[[0.00030815]]\n[[436457]]\n[[[  47.291668 -121.333336]]]\n[[0.00022181]]\n[[324594]]\n[[[  42.791668 -121.958336]]]\n[[0.00035873]]\n[[413974]]\n[[[  46.25  -117.375]]]\n[[0.00026204]]\n[[346610]]\n[[[  43.583332 -111.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00029972]]\n[[432364]]\n[[[  47.083332 -121.583336]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.0001205]]\n[[208256]]\n[[[  38.708332 -106.416664]]]\n[[0.00013927]]\n[[224389]]\n[[[  39.291668 -106.541664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00025921]]\n[[389637]]\n[[[  45.208332 -110.25    ]]]\n[[0.00012545]]\n[[444361]]\n[[[  47.708332 -123.458336]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00031334]]\n[[143618]]\n[[[  36.291668 -115.666664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00032815]]\n[[292970]]\n[[[  41.666668 -111.416664]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00035358]]\n[[232605]]\n[[[  39.583332 -106.5     ]]]\n[[0.00026446]]\n[[109297]]\n[[[  34.958332 -111.5     ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00034654]]\n[[256538]]\n[[[  40.416668 -105.75    ]]]\n[[0.00029183]]\n[[185681]]\n[[[  37.875 -106.625]]]\n[[0.00038069]]\n[[78266]]\n[[[  33.666668 -109.291664]]]\n[[0.00016855]]\n[[202538]]\n[[[  38.5      -106.333336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00012986]]\n[[373281]]\n[[[  44.583332 -107.208336]]]\n[[3.87134852e-05]]\n[[362136]]\n[[[  44.166668 -107.125   ]]]\n[[0.00015419]]\n[[330682]]\n[[[  43.   -109.75]]]\n[[0.00023674]]\n[[319102]]\n[[[  42.583332 -108.833336]]]\n[[8.83810871e-05]]\n[[267286]]\n[[[  40.791668 -110.875   ]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00034885]]\n[[116759]]\n[[[  35.25 -108.25]]]\n[[0.00034215]]\n[[107190]]\n[[[  34.875 -111.625]]]\n[[0.0001517]]\n[[104042]]\n[[[  34.75     -111.416664]]]\n[[0.00013821]]\n[[216303]]\n[[[  39.   -106.75]]]\n[[0.00016356]]\n[[174581]]\n[[[  37.458332 -108.458336]]]\n[[0.00033874]]\n[[457740]]\n[[[  48.416668 -113.916664]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00032894]]\n[[232381]]\n[[[  39.583332 -115.833336]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00035391]]\n[[177932]]\n[[[  37.583332 -107.25    ]]]\n[[0.00019507]]\n[[260145]]\n[[[  40.541668 -105.875   ]]]\n[[0.00012476]]\n[[269738]]\n[[[  40.875    -110.541664]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.0003001]]\n[[374050]]\n[[[  44.625    -122.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00032386]]\n[[148270]]\n[[[  36.458332 -105.208336]]]\n[[0.00039874]]\n[[459718]]\n[[[  48.541668 -120.75    ]]]\n[[0.00028356]]\n[[162526]]\n[[[  37.       -106.291664]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00014601]]\n[[183432]]\n[[[  37.791668 -108.166664]]]\n[[0.00031911]]\n[[237369]]\n[[[  39.75     -105.916664]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00019668]]\n[[266072]]\n[[[  40.75  -110.625]]]\n[[0.00034733]]\n[[255062]]\n[[[  40.375 -116.875]]]\n[[9.15488263e-05]]\n[[283188]]\n[[[  41.333332 -106.375   ]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022445]]\n[[185613]]\n[[[  37.875    -109.458336]]]\n[[0.00025986]]\n[[240815]]\n[[[  39.875 -111.25 ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00012938]]\n[[252783]]\n[[[  40.291668 -111.25    ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00020894]]\n[[263633]]\n[[[  40.666668 -110.958336]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00021975]]\n[[225438]]\n[[[  39.333332 -111.5     ]]]\n[[0.00031999]]\n[[219632]]\n[[[  39.125    -111.458336]]]\n[[0.00030129]]\n[[416899]]\n[[[  46.375    -121.083336]]]\n[[0.00026128]]\n[[234882]]\n[[[  39.666668 -110.416664]]]\n[[0.00014923]]\n[[213691]]\n[[[  38.916668 -119.916664]]]\n[[0.0003611]]\n[[269634]]\n[[[  40.875    -115.208336]]]\n[[0.00030221]]\n[[272081]]\n[[[  40.958332 -115.083336]]]\n[[0.00017289]]\n[[258783]]\n[[[  40.5   -112.625]]]\n[[0.00022015]]\n[[177796]]\n[[[  37.583332 -112.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00026176]]\n[[250509]]\n[[[  40.208332 -105.583336]]]\n[[0.00025279]]\n[[275892]]\n[[[  41.083332 -106.958336]]]\n[[0.00020455]]\n[[139488]]\n[[[  36.125    -105.541664]]]\n[[0.00010963]]\n[[447727]]\n[[[  47.875    -117.083336]]]\n[[0.00012719]]\n[[426421]]\n[[[  46.791668 -121.75    ]]]\n[[0.00019681]]\n[[302446]]\n[[[  42.       -120.166664]]]\n[[0.00028668]]\n[[462486]]\n[[[  48.708332 -120.666664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00027775]]\n[[296540]]\n[[[  41.791668 -116.041664]]]\n[[0.00026125]]\n[[411879]]\n[[[  46.166668 -121.916664]]]\n[[0.00029815]]\n[[187786]]\n[[[  37.958332 -111.833336]]]\n[[0.00036997]]\n[[84911]]\n[[[  33.958332 -109.5     ]]]\n[[7.66642558e-05]]\n[[292876]]\n[[[  41.666668 -115.333336]]]\n[[0.00010902]]\n[[238201]]\n[[[  39.791668 -120.875   ]]]\n[[0.00035619]]\n[[202393]]\n[[[  38.5   -112.375]]]\n[[0.00024296]]\n[[230122]]\n[[[  39.5      -111.708336]]]\n[[0.00011195]]\n[[400403]]\n[[[  45.666668 -113.958336]]]\n[[0.00033222]]\n[[369791]]\n[[[  44.458332 -113.      ]]]\nfinished\n",
  "history_begin_time" : 1677352391500,
  "history_end_time" : 1677356964559,
  "history_notes" : null,
  "history_process" : "gkhtc0",
  "host_id" : "45wlwr",
  "indicator" : "Done"
},{
  "history_id" : "66ng73o3gvf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1677352389952,
  "history_end_time" : 1677352389952,
  "history_notes" : null,
  "history_process" : "lbd6cp",
  "host_id" : "45wlwr",
  "indicator" : "Skipped"
}]
