[{
  "history_id" : "rr51gb5ujlj",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-06-04\n2024-06-01\ntest start date:  2024-06-01\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-06-01\n2024-06-01 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\n",
  "history_begin_time" : 1717473610528,
  "history_end_time" : 1717473618054,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "kpupwzc6bod",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-06-03\n2024-05-31\ntest start date:  2024-05-31\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-05-31\n2024-05-31 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1717387238793,
  "history_end_time" : 1717387270303,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gzatjoprwtp",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-06-02\n2024-05-30\ntest start date:  2024-05-30\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-05-30\n2024-05-30 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1717300810983,
  "history_end_time" : 1717300818718,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mtfdpv404mw",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-06-01\n2024-05-29\ntest start date:  2024-05-29\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-05-29\n2024-05-29 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1717214411105,
  "history_end_time" : 1717214419217,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "5v0zc4j8sw4",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-31\n2024-05-28\ntest start date:  2024-05-28\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-05-28\n2024-05-28 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1717128011050,
  "history_end_time" : 1717128018829,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "7tpqpt362m4",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-30\n2024-05-27\ntest start date:  2024-05-27\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-05-27\n2024-05-27 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1717041610791,
  "history_end_time" : 1717041618757,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "77kwqkd17m8",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-29\n2024-05-26\ntest start date:  2024-05-26\ntest end date:  2024-5-19\n/home/chetana\nget test_start_date =  2024-05-26\n2024-05-26 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716955210772,
  "history_end_time" : 1716955218838,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "efm9bh5dnki",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-28\n2024-05-25\ntest start date:  2024-05-25\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-25\n2024-05-25 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716868811666,
  "history_end_time" : 1716868819113,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2biwxcz11g3",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-27\n2024-05-24\ntest start date:  2024-05-24\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-24\n2024-05-24 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\n",
  "history_begin_time" : 1716817416784,
  "history_end_time" : 1716817425256,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "pdieeccfa2u",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-27\ntest start date:  2024-05-19\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-19\n2024-05-19 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716797561900,
  "history_end_time" : 1716797568534,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "epvmidqmkqq",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-27\ntest start date:  2024-05-22\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-22\n2024-05-22 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716782411960,
  "history_end_time" : 1716782418974,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8xxf4oysg6z",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-27\ntest start date:  2024-05-22\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-22\n2024-05-22 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\n",
  "history_begin_time" : 1716777610649,
  "history_end_time" : 1716777617076,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "qyosa7qln2t",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-27\ntest start date:  2024-05-21\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-21\n2024-05-21 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\n",
  "history_begin_time" : 1716774916807,
  "history_end_time" : 1716774923289,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ti8vjo55vh5",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\n2024-05-23\ntest start date:  2024-05-23\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-23\n2024-05-23 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\n",
  "history_begin_time" : 1716754065903,
  "history_end_time" : 1716754072377,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "NnUYhuxdUGXF",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\n2024-05-23\ntest start date:  2024-05-23\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-23\n2024-05-23 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716753740326,
  "history_end_time" : 1716753742591,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Xaa6eSXwqzeL",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{end_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\n2024-05-23\ntest start date:  2024-05-23\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-23\n2024-05-23 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2024__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2024144\ntarget_date[:4] =  2024\ntif_files =  []\nuh-oh, didn't find HDFs for date 2024-05-23\ngenerate a new csv file with empty values for each point\nsaved the merged tifs to /home/chetana/water_mask/final_output//2024__water_mask.tif\nextracting data for 2024-05-23\nOpening /home/chetana/water_mask/final_output//2024__water_mask.tif\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2024__water_mask.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Xaa6eSXwqzeL/mod_water_mask.py\", line 669, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/Xaa6eSXwqzeL/mod_water_mask.py\", line 663, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  File \"/home/chetana/gw-workspace/Xaa6eSXwqzeL/mod_water_mask.py\", line 279, in process_file\n    with rasterio.open(file_path) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2024__water_mask.tif: No such file or directory\n",
  "history_begin_time" : 1716753509346,
  "history_end_time" : 1716753514883,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9sYR0ckb9FPI",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\n2024-05-23\ntest start date:  2024-05-23\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-23\n2024-05-23 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2024-05-23\nOpening /home/chetana/water_mask/final_output//2024__water_mask.tif\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2024__water_mask.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/9sYR0ckb9FPI/mod_water_mask.py\", line 669, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/9sYR0ckb9FPI/mod_water_mask.py\", line 663, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  File \"/home/chetana/gw-workspace/9sYR0ckb9FPI/mod_water_mask.py\", line 279, in process_file\n    with rasterio.open(file_path) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2024__water_mask.tif: No such file or directory\n",
  "history_begin_time" : 1716753352072,
  "history_end_time" : 1716753354663,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8e6csh8d98y",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-12-07\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2023-12-07\n2023-12-07 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-12-07\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716751218516,
  "history_end_time" : 1716751225148,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ubupf4lo1kp",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\n2024-05-23\ntest start date:  2024-05-23\ntest end date:  2024-10-19\n/home/chetana\nget test_start_date =  2024-05-23\n2024-05-23 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2024-05-23\nOpening /home/chetana/water_mask/final_output//2024__water_mask.tif\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2024__water_mask.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ubupf4lo1kp/mod_water_mask.py\", line 669, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/ubupf4lo1kp/mod_water_mask.py\", line 663, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  File \"/home/chetana/gw-workspace/ubupf4lo1kp/mod_water_mask.py\", line 279, in process_file\n    with rasterio.open(file_path) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2024__water_mask.tif: No such file or directory\n",
  "history_begin_time" : 1716750881996,
  "history_end_time" : 1716751199727,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "c0wnowpvt8n",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-04\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716746641714,
  "history_end_time" : 1716746648578,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "iMNV8epcGO8P",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1527.78it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 92601.52it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 159158.86it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nCopying nodata values from source /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\nextracting data for 2023-10-04\nOpening /home/chetana/water_mask/final_output//2023__water_mask.tif\nSaving CSV file: /home/chetana/water_mask/final_output/2023_output.csv\n",
  "history_begin_time" : 1716741367948,
  "history_end_time" : 1716741456397,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LMAKmjkwoM9q",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date.year in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = end_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          end_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1583.39it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 18853.30it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 162052.65it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  subdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/LMAKmjkwoM9q/mod_water_mask.py\", line 668, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/LMAKmjkwoM9q/mod_water_mask.py\", line 653, in extract_data_for_testing\n    download_tiles_and_merge(start_date, end_date)\n  File \"/home/chetana/gw-workspace/LMAKmjkwoM9q/mod_water_mask.py\", line 229, in download_tiles_and_merge\n    merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n  File \"/home/chetana/gw-workspace/LMAKmjkwoM9q/mod_water_mask.py\", line 107, in merge_tifs\n    tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n  File \"/home/chetana/gw-workspace/LMAKmjkwoM9q/mod_water_mask.py\", line 108, in <listcomp>\n    and \"assessment\" not in f and target_date.year in f]\nAttributeError: 'str' object has no attribute 'year'\n",
  "history_begin_time" : 1716741290907,
  "history_end_time" : 1716741300459,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Vk1qg42IaBFn",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date.year in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1344.13it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 37686.66it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 102742.32it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  subdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Vk1qg42IaBFn/mod_water_mask.py\", line 668, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/Vk1qg42IaBFn/mod_water_mask.py\", line 653, in extract_data_for_testing\n    download_tiles_and_merge(start_date, end_date)\n  File \"/home/chetana/gw-workspace/Vk1qg42IaBFn/mod_water_mask.py\", line 229, in download_tiles_and_merge\n    merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n  File \"/home/chetana/gw-workspace/Vk1qg42IaBFn/mod_water_mask.py\", line 107, in merge_tifs\n    tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n  File \"/home/chetana/gw-workspace/Vk1qg42IaBFn/mod_water_mask.py\", line 108, in <listcomp>\n    and \"assessment\" not in f and target_date.year in f]\nAttributeError: 'str' object has no attribute 'year'\n",
  "history_begin_time" : 1716741178898,
  "history_end_time" : 1716741190696,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "24SXYxXqN4Xa",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = end_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{end_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{end_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(end_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-04\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716741112174,
  "history_end_time" : 1716741114633,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "LC5rbOZiUkSZ",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nOpening /home/chetana/water_mask/final_output//2023__water_mask.tif\nSaving CSV file: /home/chetana/water_mask/final_output/2023_output.csv\n",
  "history_begin_time" : 1716740090732,
  "history_end_time" : 1716740153449,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "lnKI0GszGcLl",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1917.01it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 23424.17it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 160955.23it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  subdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nCopying nodata values from source /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\nextracting data for 2023-10-01\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/lnKI0GszGcLl/mod_water_mask.py\", line 668, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/lnKI0GszGcLl/mod_water_mask.py\", line 662, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  File \"/home/chetana/gw-workspace/lnKI0GszGcLl/mod_water_mask.py\", line 274, in process_file\n    station_df = pd.read_csv(mapper_file)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n    self.handles = get_handle(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 856, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/water_mask/final_output/modis_to_dem_mapper.csv'\n",
  "history_begin_time" : 1716740003179,
  "history_end_time" : 1716740033312,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Es0saPtSPzet",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nThe file /home/chetana/water_mask/final_output/2023_output.csv exists. skip.\n",
  "history_begin_time" : 1716737802806,
  "history_end_time" : 1716737805079,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fhP01w6nR3Cd",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nOpening /home/chetana/water_mask/final_output//2023__water_mask.tif\nSaving CSV file: /home/chetana/water_mask/final_output/2023_output.csv\n",
  "history_begin_time" : 1716737338963,
  "history_end_time" : 1716737402014,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GGU16f1KRy9n",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nOpening /home/chetana/water_mask/final_output//2023__water_mask.tif\nSaving CSV file: /home/chetana/water_mask/final_output/2023-10-01_output.csv\n",
  "history_begin_time" : 1716737216848,
  "history_end_time" : 1716737280368,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6XgD8eQGgup7",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nOpening /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/6XgD8eQGgup7/mod_water_mask.py\", line 669, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/6XgD8eQGgup7/mod_water_mask.py\", line 663, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{current_date}__water_mask.tif', current_date)\n  File \"/home/chetana/gw-workspace/6XgD8eQGgup7/mod_water_mask.py\", line 278, in process_file\n    with rasterio.open(file_path) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif: No such file or directory\n",
  "history_begin_time" : 1716737161892,
  "history_end_time" : 1716737164478,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "HjhJViBcPaGZ",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/HjhJViBcPaGZ/mod_water_mask.py\", line 669, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/HjhJViBcPaGZ/mod_water_mask.py\", line 663, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{current_date}__water_mask.tif', current_date)\n  File \"/home/chetana/gw-workspace/HjhJViBcPaGZ/mod_water_mask.py\", line 274, in process_file\n    station_df = pd.read_csv(mapper_file)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n    self.handles = get_handle(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 856, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/water_mask/final_output/modis_to_dem_mapper.csv'\n",
  "history_begin_time" : 1716737049554,
  "history_end_time" : 1716737051820,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "iXZyId1OeLS4",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507832\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\nextracting data for 2023-10-01\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/iXZyId1OeLS4/mod_water_mask.py\", line 669, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/iXZyId1OeLS4/mod_water_mask.py\", line 663, in extract_data_for_testing\n    process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  File \"/home/chetana/gw-workspace/iXZyId1OeLS4/mod_water_mask.py\", line 274, in process_file\n    station_df = pd.read_csv(mapper_file)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 211, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 950, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 605, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1442, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\", line 1735, in _make_engine\n    self.handles = get_handle(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\", line 856, in get_handle\n    handle = open(\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/water_mask/final_output/modis_to_dem_mapper.csv'\n",
  "history_begin_time" : 1716737011747,
  "history_end_time" : 1716737014147,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "wlMEQ3XUqSi2",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1813.78it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 22557.16it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 157750.37it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  subdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3_assessment.tif is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1', '[2400x2400] LC_Prop1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2', '[2400x2400] LC_Prop2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3', '[2400x2400] LC_Prop3 MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:QC', '[2400x2400] QC MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LW', '[2400x2400] LW MCD12Q1 (8-bit unsigned integer)')\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nCopying nodata values from source /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\n",
  "history_begin_time" : 1716736753292,
  "history_end_time" : 1716736783412,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "J5P29ZP0augI",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n              else:\n                gdal.Translate(output_path, ds)\n              print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n              break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1629.97it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 11595.90it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 158804.38it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif_assessment.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif_assessment.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif_assessment.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif_assessment.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif_assessment.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif_assessment.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif_assessment.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif_assessment.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nCopying nodata values from source /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\n",
  "history_begin_time" : 1716736490776,
  "history_end_time" : 1716736534374,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Isa5nKa0Vhpj",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              gdal.Translate(output_path, ds)\n              print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n              break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2127.50it/s]\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 26253.01it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 183771.05it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v06.061.2023243092049.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v04.061.2023243115711.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v06.061.2023243063834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v05.061.2023243054834.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v04.061.2023243062305.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v05.061.2023243093838.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v05.061.2023243094009.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v04.061.2023243085508.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v04.061.2023243105251.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h09v06.061.2023243075235.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v06.061.2023243073808.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h08v05.061.2023243090807.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h11v04.061.2023243112318.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h07v06.061.2023243061908.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h12v05.061.2023243080033.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type1', '[2400x2400] LC_Type1 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type2', '[2400x2400] LC_Type2 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type3', '[2400x2400] LC_Type3 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type4', '[2400x2400] LC_Type4 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Type5', '[2400x2400] LC_Type5 MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop1_Assessment', '[2400x2400] LC_Prop1_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h13v04.061.2023243101116.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif\n('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop2_Assessment', '[2400x2400] LC_Prop2_Assessment MCD12Q1 (8-bit unsigned integer)')\nsubdataset =  ('HDF4_EOS:EOS_GRID:\"/home/chetana/water_mask/temp/MCD12Q1.A2022001.h10v05.061.2023243103913.hdf\":MCD12Q1:LC_Prop3_Assessment', '[2400x2400] LC_Prop3_Assessment MCD12Q1 (8-bit unsigned integer)')\nThe layer MCD12Q1:LC_Prop3 is extracted to /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nCopying nodata values from source /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\n",
  "history_begin_time" : 1716736197712,
  "history_end_time" : 1716736225141,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "FMZxpchoJwY3",
  "history_input" : "# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nfile_size_bytes: 507434\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif exists. skip.\n",
  "history_begin_time" : 1716734395266,
  "history_end_time" : 1716734397541,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pttWbfOFZLlv",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1618.98it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloadedFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 22068.45it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 174335.37it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nRunning  /usr/bin/gdalwarp -t_srs EPSG:4326 -tr 0.036 0.036 -cutline /home/chetana/gridmet_test_run/template.shp -crop_to_cutline -overwrite /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nCreating output file that is 755P x 666L.\nProcessing input file /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\nCopying nodata values from source /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\n",
  "history_begin_time" : 1716716094842,
  "history_end_time" : 1716716121313,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "swViQh3zTFE7",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') ]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 418.72it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | :   6%|▌         | 1/17 [00:02<00:42,  2.67s/it]\nPROCESSING TASKS | :  18%|█▊        | 3/17 [00:02<00:10,  1.34it/s]\nPROCESSING TASKS | :  29%|██▉       | 5/17 [00:03<00:05,  2.13it/s]\nPROCESSING TASKS | :  53%|█████▎    | 9/17 [00:05<00:04,  1.93it/s]\nPROCESSING TASKS | :  59%|█████▉    | 10/17 [00:05<00:03,  2.21it/s]\nPROCESSING TASKS | :  65%|██████▍   | 11/17 [00:05<00:02,  2.33it/s]\nPROCESSING TASKS | :  71%|███████   | 12/17 [00:06<00:01,  2.69it/s]\nPROCESSING TASKS | :  76%|███████▋  | 13/17 [00:06<00:01,  2.92it/s]\nPROCESSING TASKS | :  88%|████████▊ | 15/17 [00:06<00:00,  4.37it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:08<00:00,  1.86it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:08<00:00,  1.98it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 44592.35it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  ['/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v04.061.2022153230033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v06.061.2022168151426.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v06.061.2022166151608.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v05.061.2022160065528.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v04.061.2022158211115.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v05.061.2022164175416.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v05.061.2022159184300.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v04.061.2022162010807.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v06.061.2022168141319.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v06.061.2022165214445.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v06.061.2022153125643.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v05.061.2022165203837.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v05.061.2022160074557.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v04.061.2022216015823.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v05.061.2022146040113.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v06.061.2022151161214.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v06.061.2022164160305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v06.061.2022162021504.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v04.061.2022168033428.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v06.061.2022159174334.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v05.061.2022168025349.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v06.061.2022164164813.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v06.061.2022152014317.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v06.061.2022168025826.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v06.061.2022152220612.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v06.061.2022166153208.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v06.061.2022165065712.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v05.061.2022216030750.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v05.061.2022151161531.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v04.061.2022162035938.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v04.061.2022158231134.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v04.061.2022215203426.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v04.061.2022169160414.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v05.061.2022168174253.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v05.061.2022153122432.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v06.061.2022152014235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v06.061.2022215212351.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v05.061.2022154031246.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v06.061.2022146033902.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v04.061.2022165215345.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h13v04.061.2022169161048.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v04.061.2022162025604.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v04.061.2022146050354.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v04.061.2022162021540.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v05.061.2022147205502.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v06.061.2022215233619.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v04.061.2022165082137.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v04.061.2022151161906.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v06.061.2022151161736.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v06.061.2022153211515.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v06.061.2022160064847.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v04.061.2022165225511.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v06.061.2022164164813.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h13v04.061.2022148083352.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v06.061.2022146031055.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v06.061.2022215202155.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v05.061.2022165082044.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v05.061.2022162024506.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v05.061.2022158221052.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v06.061.2022152211338.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v05.061.2022146033917.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v04.061.2022161140708.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v05.061.2022152121656.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h13v04.061.2022146060649.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v04.061.2022159184020.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v04.061.2022160063857.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h13v04.061.2022166180416.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v06.061.2022152124844.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v04.061.2022152014252.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v06.061.2022147204000.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v05.061.2022168001816.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v05.061.2022166164515.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v04.061.2022165230140.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v04.061.2022169160935.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v06.061.2022147203753.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v04.061.2022158220628.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v06.061.2022161133402.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v04.061.2022146034934.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v06.061.2022146040126.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v06.061.2022216003122.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v06.061.2022164162410.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v06.061.2022202150716.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v06.061.2022158215642.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v04.061.2022164172811.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v05.061.2022166152129.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v05.061.2022160071156.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v06.061.2022161131106.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v06.061.2022158215757.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v06.061.2022216025520.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v05.061.2022154060738.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v05.061.2022147215523.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v05.061.2022166160910.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v05.061.2022151161918.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v05.061.2022171160153.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v05.061.2022153083959.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v05.061.2022159180020.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v05.061.2022151161738.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v06.061.2022165193642.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v06.061.2022202150327.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v05.061.2022171174410.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v05.061.2022166151313.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v05.061.2022171161201.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v04.061.2022165065242.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v04.061.2022158215957.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v05.061.2022216071653.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v06.061.2022162003933.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v06.061.2022166153713.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v05.061.2022158221132.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v04.061.2022147204353.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v05.061.2022147203812.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v04.061.2022165202021.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v06.061.2022169160433.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h13v04.061.2022152050602.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h13v04.061.2022162035643.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v05.061.2022159193439.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h13v04.061.2022159001357.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v04.061.2022161140208.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v05.061.2022169160942.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v06.061.2022153115802.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v04.061.2022168173053.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v06.061.2022168174923.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v06.061.2022151161416.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v04.061.2022153233438.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v05.061.2022202150656.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v04.061.2022147201909.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v06.061.2022165051616.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v04.061.2022161130111.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v06.061.2022164160634.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v06.061.2022160071951.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v06.061.2022160065224.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v05.061.2022169160344.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v05.061.2022152014115.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v04.061.2022152141617.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v06.061.2022158211027.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v06.061.2022166153314.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v04.061.2022160075223.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v05.061.2022158212002.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v06.061.2022168001822.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v06.061.2022152022235.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v05.061.2022147203929.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v06.061.2022165055649.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v05.061.2022153084625.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v04.061.2022169160645.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v05.061.2022153204112.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v04.061.2022169160759.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v04.061.2022164162715.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v05.061.2022153114232.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v05.061.2022160050421.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v06.061.2022165072106.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v05.061.2022162040109.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v05.061.2022158212252.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v06.061.2022165195905.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v05.061.2022168152848.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h13v04.061.2022168153530.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v06.061.2022158211828.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v04.061.2022151161351.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v04.061.2022168002003.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v06.061.2022146035452.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v05.061.2022153205724.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v05.061.2022165225415.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v06.061.2022159182449.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v04.061.2022171151922.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v05.061.2022168173357.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v06.061.2022152022413.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v04.061.2022160070952.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v04.061.2022202150652.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v06.061.2022166144709.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v06.061.2022169160549.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v05.061.2022162012243.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v04.061.2022158212527.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v06.061.2022147204132.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v05.061.2022168030750.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v05.061.2022146045521.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v05.061.2022158225932.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v04.061.2022171161358.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v05.061.2022161124337.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v05.061.2022202150408.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v04.061.2022147215712.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v06.061.2022151161601.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v05.061.2022165220945.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v06.061.2022202150433.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v05.061.2022168002006.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v04.061.2022147201926.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v04.061.2022159184319.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v06.061.2022202150900.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h13v04.061.2022151162053.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v06.061.2022153125459.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v05.061.2022161140603.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v06.061.2022162022641.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v04.061.2022165074211.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v05.061.2022215203800.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v04.061.2022165062521.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v04.061.2022154032745.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v05.061.2022152115041.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v04.061.2022164160625.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v06.061.2022161133506.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h13v04.061.2022216011046.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v06.061.2022171151818.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v06.061.2022165204322.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v05.061.2022168031256.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v06.061.2022168030220.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v06.061.2022161132907.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v05.061.2022152041846.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v04.061.2022164173746.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v04.061.2022202151028.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v04.061.2022202150536.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v06.061.2022171160428.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v04.061.2022160051756.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v04.061.2022160081327.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v05.061.2022152134513.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h13v04.061.2022202151230.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v06.061.2022158220427.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v05.061.2022166154009.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v04.061.2022159194525.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v04.061.2022146030108.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h13v04.061.2022160091153.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v06.061.2022159175519.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v05.061.2022164172325.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v05.061.2022168172423.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v06.061.2022168165354.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v04.061.2022216052627.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v04.061.2022153091803.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v04.061.2022152220556.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v05.061.2022169160425.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v04.061.2022171155354.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v04.061.2022202150359.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v06.061.2022171153157.tif', '/home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v06.061.2022168001851.tif']\npyproj.datadir.get_data_dir() =  /home/chetana/anaconda3/lib/python3.9/site-packages/pyproj/proj_dir/share/proj\nRunning  /usr/bin/gdalwarp -r min /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v04.061.2022153230033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v06.061.2022168151426.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v06.061.2022166151608.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v05.061.2022160065528.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v04.061.2022158211115.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v05.061.2022164175416.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v05.061.2022159184300.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v04.061.2022162010807.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v06.061.2022168141319.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v06.061.2022165214445.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v06.061.2022153125643.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v05.061.2022165203837.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v05.061.2022160074557.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v04.061.2022216015823.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v05.061.2022146040113.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v06.061.2022151161214.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v06.061.2022164160305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v06.061.2022162021504.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v04.061.2022168033428.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v06.061.2022159174334.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v05.061.2022168025349.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v06.061.2022164164813.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v06.061.2022152014317.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v06.061.2022168025826.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v06.061.2022152220612.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v06.061.2022166153208.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v06.061.2022165065712.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v05.061.2022216030750.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v05.061.2022151161531.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v04.061.2022162035938.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v04.061.2022158231134.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v04.061.2022215203426.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v04.061.2022169160414.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v05.061.2022168174253.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v05.061.2022153122432.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v06.061.2022152014235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v06.061.2022215212351.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v05.061.2022154031246.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v06.061.2022146033902.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v04.061.2022165215345.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h13v04.061.2022169161048.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v04.061.2022162025604.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v04.061.2022146050354.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v04.061.2022162021540.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v05.061.2022147205502.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v06.061.2022215233619.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v04.061.2022165082137.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v04.061.2022151161906.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v06.061.2022151161736.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v06.061.2022153211515.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v06.061.2022160064847.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v04.061.2022165225511.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v06.061.2022164164813.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h13v04.061.2022148083352.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v06.061.2022146031055.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v06.061.2022215202155.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v05.061.2022165082044.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v05.061.2022162024506.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v05.061.2022158221052.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v06.061.2022152211338.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v05.061.2022146033917.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v04.061.2022161140708.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v05.061.2022152121656.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h13v04.061.2022146060649.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v04.061.2022159184020.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v04.061.2022160063857.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h13v04.061.2022166180416.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v06.061.2022152124844.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v04.061.2022152014252.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v06.061.2022147204000.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v05.061.2022168001816.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v05.061.2022166164515.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v04.061.2022165230140.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v04.061.2022169160935.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v06.061.2022147203753.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v04.061.2022158220628.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v06.061.2022161133402.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v04.061.2022146034934.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v06.061.2022146040126.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v06.061.2022216003122.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v06.061.2022164162410.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v06.061.2022202150716.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v06.061.2022158215642.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v04.061.2022164172811.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v05.061.2022166152129.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v05.061.2022160071156.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v06.061.2022161131106.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v06.061.2022158215757.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v06.061.2022216025520.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v05.061.2022154060738.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v05.061.2022147215523.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v05.061.2022166160910.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v05.061.2022151161918.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v05.061.2022171160153.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v05.061.2022153083959.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v05.061.2022159180020.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v05.061.2022151161738.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v06.061.2022165193642.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v06.061.2022202150327.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v05.061.2022171174410.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v05.061.2022166151313.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v05.061.2022171161201.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v04.061.2022165065242.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v04.061.2022158215957.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v05.061.2022216071653.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v06.061.2022162003933.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v06.061.2022166153713.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v05.061.2022158221132.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v04.061.2022147204353.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v05.061.2022147203812.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v04.061.2022165202021.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v06.061.2022169160433.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h13v04.061.2022152050602.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h13v04.061.2022162035643.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v05.061.2022159193439.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h13v04.061.2022159001357.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v04.061.2022161140208.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v05.061.2022169160942.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v06.061.2022153115802.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v04.061.2022168173053.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v06.061.2022168174923.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v06.061.2022151161416.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v04.061.2022153233438.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v05.061.2022202150656.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v04.061.2022147201909.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v06.061.2022165051616.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v04.061.2022161130111.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v06.061.2022164160634.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v06.061.2022160071951.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v06.061.2022160065224.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v05.061.2022169160344.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v05.061.2022152014115.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v04.061.2022152141617.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v06.061.2022158211027.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v06.061.2022166153314.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v04.061.2022160075223.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v05.061.2022158212002.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v06.061.2022168001822.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v06.061.2022152022235.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v05.061.2022147203929.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v06.061.2022165055649.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v05.061.2022153084625.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v04.061.2022169160645.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v05.061.2022153204112.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v04.061.2022169160759.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v04.061.2022164162715.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v05.061.2022153114232.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v05.061.2022160050421.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v06.061.2022165072106.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v05.061.2022162040109.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v05.061.2022158212252.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v06.061.2022165195905.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v05.061.2022168152848.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h13v04.061.2022168153530.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v06.061.2022158211828.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v04.061.2022151161351.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v04.061.2022168002003.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v06.061.2022146035452.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v05.061.2022153205724.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v05.061.2022165225415.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v06.061.2022159182449.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v04.061.2022171151922.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v05.061.2022168173357.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v06.061.2022152022413.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v04.061.2022160070952.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v04.061.2022202150652.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v06.061.2022166144709.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v06.061.2022169160549.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v05.061.2022162012243.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v04.061.2022158212527.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v06.061.2022147204132.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v05.061.2022168030750.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v05.061.2022146045521.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v05.061.2022158225932.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v04.061.2022171161358.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v05.061.2022161124337.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v05.061.2022202150408.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v04.061.2022147215712.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v06.061.2022151161601.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v05.061.2022165220945.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v06.061.2022202150433.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v05.061.2022168002006.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v04.061.2022147201926.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v04.061.2022159184319.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v06.061.2022202150900.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h13v04.061.2022151162053.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v06.061.2022153125459.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v05.061.2022161140603.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v06.061.2022162022641.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v04.061.2022165074211.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v05.061.2022215203800.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v04.061.2022165062521.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v04.061.2022154032745.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v05.061.2022152115041.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v04.061.2022164160625.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v06.061.2022161133506.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h13v04.061.2022216011046.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v06.061.2022171151818.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v06.061.2022165204322.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v05.061.2022168031256.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v06.061.2022168030220.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v06.061.2022161132907.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v05.061.2022152041846.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v04.061.2022164173746.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v04.061.2022202151028.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v04.061.2022202150536.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v06.061.2022171160428.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v04.061.2022160051756.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v04.061.2022160081327.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v05.061.2022152134513.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h13v04.061.2022202151230.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v06.061.2022158220427.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v05.061.2022166154009.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v04.061.2022159194525.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v04.061.2022146030108.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h13v04.061.2022160091153.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v06.061.2022159175519.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v05.061.2022164172325.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v05.061.2022168172423.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v06.061.2022168165354.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v04.061.2022216052627.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v04.061.2022153091803.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v04.061.2022152220556.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v05.061.2022169160425.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v04.061.2022171155354.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v04.061.2022202150359.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v06.061.2022171153157.tif /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v06.061.2022168001851.tif /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nCreating output file that is 16800P x 7200L.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif.\nCopying nodata values from source /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif to destination /home/chetana/water_mask/final_output//2023__water_mask.tif_500m.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif.\n0...10...20...30...40...50...60...70...80...90...100 - done.\nProcessing input file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif.\nUsing internal nodata values (e.g. 255) for image /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif.\n\nStream closed",
  "history_begin_time" : 1716715943112,
  "history_end_time" : 1716716065882,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "27y4v9mRM4pY",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1904.16it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloadedFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 13550.58it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 138452.75it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v05.061.2022153083959.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v06.061.2022146031055.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v05.061.2022154060738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v04.061.2022153091803.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v06.061.2022202150900.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v06.061.2022153125459.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v06.061.2022166151608.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v06.061.2022165193642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v04.061.2022147204353.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v05.061.2022215203800.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v06.061.2022161133402.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v05.061.2022160065528.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v05.061.2022168031256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h13v04.061.2022160091153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v06.061.2022165204322.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v05.061.2022168173357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v05.061.2022160074557.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v06.061.2022146035452.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v04.061.2022160063857.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v06.061.2022161133506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v05.061.2022171161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v04.061.2022165215345.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v06.061.2022153211515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v05.061.2022168174253.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v04.061.2022216052627.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v06.061.2022151161601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v05.061.2022169160344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v06.061.2022215233619.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v04.061.2022169160414.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v06.061.2022216003122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v06.061.2022152014317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v06.061.2022168151426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v06.061.2022164160305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v04.061.2022147201909.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v06.061.2022168001822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v05.061.2022161124337.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v06.061.2022158211027.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v04.061.2022202150359.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v06.061.2022165072106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v05.061.2022166154009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v04.061.2022158211115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v05.061.2022171160153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v06.061.2022152124844.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h13v04.061.2022152050602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v04.061.2022164173746.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v06.061.2022165195905.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v05.061.2022168152848.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v04.061.2022153230033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v06.061.2022165065712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v06.061.2022166153314.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v04.061.2022171161358.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v06.061.2022161131106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v05.061.2022164172325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v05.061.2022165225415.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v05.061.2022168172423.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v05.061.2022168025349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v06.061.2022158211828.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v04.061.2022168173053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v04.061.2022162021540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v05.061.2022216030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v04.061.2022164172811.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v06.061.2022215212351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v06.061.2022152220612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v05.061.2022169160942.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v04.061.2022161140208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v06.061.2022171151818.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v06.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v04.061.2022168033428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v04.061.2022171151922.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v04.061.2022160081327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v05.061.2022168001816.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v04.061.2022168002003.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v05.061.2022202150408.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v06.061.2022158220427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v05.061.2022153084625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v05.061.2022164175416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v04.061.2022152141617.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v06.061.2022159182449.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v05.061.2022153204112.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v06.061.2022151161736.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v05.061.2022146040113.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v05.061.2022169160425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v05.061.2022166160910.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v05.061.2022171174410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v05.061.2022158221132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h13v04.061.2022166180416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v04.061.2022158212527.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v04.061.2022151161351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v04.061.2022202150652.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v04.061.2022162010807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v05.061.2022154031246.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v06.061.2022168001851.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v04.061.2022216015823.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v04.061.2022151161906.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v06.061.2022162021504.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v04.061.2022202151028.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v06.061.2022168030220.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v05.061.2022162012243.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v06.061.2022168174923.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v05.061.2022147205502.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v04.061.2022158215957.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h13v04.061.2022148083352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v05.061.2022158212002.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v06.061.2022168165354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v06.061.2022165051616.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v04.061.2022165225511.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h13v04.061.2022216011046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v04.061.2022161140708.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v04.061.2022164162715.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v04.061.2022165082137.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v05.061.2022161140603.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v04.061.2022146050354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v04.061.2022162035938.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v04.061.2022159194525.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v06.061.2022169160549.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v06.061.2022153125643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v05.061.2022147215523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v04.061.2022147201926.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v06.061.2022151161214.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v05.061.2022159193439.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v04.061.2022160075223.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v04.061.2022215203426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v04.061.2022169160759.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v06.061.2022166153713.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v06.061.2022152022235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v05.061.2022147203812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v05.061.2022159180020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v05.061.2022152121656.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v04.061.2022165230140.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v06.061.2022165055649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v04.061.2022152220556.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v04.061.2022165202021.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v04.061.2022169160645.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v05.061.2022146033917.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v06.061.2022164162410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v06.061.2022169160433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v05.061.2022165220945.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v05.061.2022158212252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v05.061.2022160071156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h13v04.061.2022151162053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v06.061.2022151161416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v04.061.2022146034934.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v06.061.2022168141319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v04.061.2022147215712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v06.061.2022158215642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v05.061.2022152041846.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v06.061.2022171160428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v06.061.2022164160634.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v06.061.2022160071951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v05.061.2022146045521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v05.061.2022153122432.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v04.061.2022160051756.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v06.061.2022165214445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h13v04.061.2022159001357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v05.061.2022152014115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v05.061.2022162024506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v06.061.2022160065224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v06.061.2022216025520.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v04.061.2022158231134.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v05.061.2022151161531.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v04.061.2022169160935.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v05.061.2022159184300.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v05.061.2022165082044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v05.061.2022216071653.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v04.061.2022165065242.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v05.061.2022158221052.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v06.061.2022147204132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v06.061.2022146040126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v04.061.2022152014252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v06.061.2022162003933.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v04.061.2022171155354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v05.061.2022168002006.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v05.061.2022168030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v05.061.2022152134513.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v05.061.2022151161738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v06.061.2022171153157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v04.061.2022162025604.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v04.061.2022202150536.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v04.061.2022158220628.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v05.061.2022166152129.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v06.061.2022160064847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v04.061.2022146030108.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v05.061.2022162040109.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v06.061.2022147204000.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v06.061.2022166144709.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v06.061.2022202150433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v06.061.2022147203753.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v06.061.2022162022641.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v04.061.2022159184319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v06.061.2022152022413.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v06.061.2022168025826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v05.061.2022166164515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v04.061.2022153233438.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v06.061.2022166153208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v05.061.2022147203929.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v05.061.2022153114232.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v05.061.2022151161918.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v04.061.2022164160625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v06.061.2022152014235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v06.061.2022152211338.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v06.061.2022146033902.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h13v04.061.2022169161048.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h13v04.061.2022168153530.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v06.061.2022161132907.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v06.061.2022215202155.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v04.061.2022160070952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v05.061.2022153205724.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v06.061.2022202150716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v04.061.2022154032745.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v05.061.2022165203837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v05.061.2022166151313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v04.061.2022159184020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h13v04.061.2022162035643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v04.061.2022161130111.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v06.061.2022159175519.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v06.061.2022153115802.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v05.061.2022152115041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v05.061.2022158225932.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h13v04.061.2022146060649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v06.061.2022158215757.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h13v04.061.2022202151230.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v06.061.2022202150327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v05.061.2022160050421.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v04.061.2022165074211.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v04.061.2022165062521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v05.061.2022202150656.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\n",
  "history_begin_time" : 1716715843731,
  "history_end_time" : 1716715849755,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2TESwOrTcwSh",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          current_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2TESwOrTcwSh/mod_water_mask.py\", line 663, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/2TESwOrTcwSh/mod_water_mask.py\", line 646, in extract_data_for_testing\n    download_tiles_and_merge(start_date, end_date)\n  File \"/home/chetana/gw-workspace/2TESwOrTcwSh/mod_water_mask.py\", line 215, in download_tiles_and_merge\n    current_date.strftime(\"%Y-%m-%d\")))\nAttributeError: 'str' object has no attribute 'strftime'\n",
  "history_begin_time" : 1716715816451,
  "history_end_time" : 1716715821303,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "QkVfmM2KiE96",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    # Subtract one year from the current date\n    one_year_ago = i - timedelta(days=365)\n    target_output_tif = f'{modis_day_wise}/{i.year}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(one_year_ago, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nThe provided start date was not recognized\nGranules found: 374\n Getting 374 granules, approx download size: 2.81 GB\nQUEUEING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]\nQUEUEING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 6312.78it/s]\nPROCESSING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]\nPROCESSING TASKS | :   0%|          | 1/374 [00:02<17:35,  2.83s/it]\nPROCESSING TASKS | :   1%|          | 3/374 [00:02<04:55,  1.26it/s]\nPROCESSING TASKS | :   1%|          | 4/374 [00:03<03:49,  1.61it/s]\nPROCESSING TASKS | :   2%|▏         | 6/374 [00:03<02:05,  2.94it/s]\nPROCESSING TASKS | :   2%|▏         | 8/374 [00:03<01:27,  4.20it/s]\nPROCESSING TASKS | :   3%|▎         | 10/374 [00:06<04:05,  1.49it/s]\nPROCESSING TASKS | :   3%|▎         | 11/374 [00:06<03:30,  1.72it/s]\nPROCESSING TASKS | :   3%|▎         | 12/374 [00:06<02:56,  2.05it/s]\nPROCESSING TASKS | :   4%|▍         | 15/374 [00:06<01:39,  3.63it/s]\nPROCESSING TASKS | :   4%|▍         | 16/374 [00:07<01:54,  3.11it/s]\nPROCESSING TASKS | :   5%|▍         | 17/374 [00:09<03:57,  1.50it/s]\nPROCESSING TASKS | :   5%|▌         | 19/374 [00:09<02:34,  2.30it/s]\nPROCESSING TASKS | :   6%|▌         | 21/374 [00:09<02:04,  2.84it/s]\nPROCESSING TASKS | :   6%|▌         | 22/374 [00:10<01:48,  3.24it/s]\nPROCESSING TASKS | :   6%|▋         | 24/374 [00:10<01:33,  3.75it/s]\nPROCESSING TASKS | :   7%|▋         | 25/374 [00:12<03:27,  1.68it/s]\nPROCESSING TASKS | :   7%|▋         | 26/374 [00:12<03:08,  1.84it/s]\nPROCESSING TASKS | :   7%|▋         | 28/374 [00:12<02:01,  2.85it/s]\nPROCESSING TASKS | :   8%|▊         | 29/374 [00:13<01:51,  3.09it/s]\nPROCESSING TASKS | :   8%|▊         | 30/374 [00:13<02:12,  2.61it/s]\nPROCESSING TASKS | :   9%|▊         | 32/374 [00:13<01:25,  4.01it/s]\nPROCESSING TASKS | :   9%|▉         | 33/374 [00:15<03:07,  1.82it/s]\nPROCESSING TASKS | :   9%|▉         | 34/374 [00:15<02:49,  2.01it/s]\nPROCESSING TASKS | :   9%|▉         | 35/374 [00:15<02:28,  2.28it/s]\nPROCESSING TASKS | :  10%|▉         | 37/374 [00:16<01:38,  3.42it/s]\nPROCESSING TASKS | :  10%|█         | 38/374 [00:16<01:48,  3.09it/s]\nPROCESSING TASKS | :  10%|█         | 39/374 [00:16<01:47,  3.12it/s]\nPROCESSING TASKS | :  11%|█         | 41/374 [00:18<03:00,  1.84it/s]\nPROCESSING TASKS | :  11%|█         | 42/374 [00:18<02:30,  2.20it/s]\nPROCESSING TASKS | :  12%|█▏        | 44/374 [00:18<01:38,  3.36it/s]\nPROCESSING TASKS | :  12%|█▏        | 45/374 [00:19<01:53,  2.90it/s]\nPROCESSING TASKS | :  12%|█▏        | 46/374 [00:19<01:52,  2.92it/s]\nPROCESSING TASKS | :  13%|█▎        | 47/374 [00:19<01:34,  3.48it/s]\nPROCESSING TASKS | :  13%|█▎        | 48/374 [00:20<01:32,  3.52it/s]\nPROCESSING TASKS | :  13%|█▎        | 49/374 [00:21<03:04,  1.76it/s]\nPROCESSING TASKS | :  13%|█▎        | 50/374 [00:21<02:47,  1.93it/s]\nPROCESSING TASKS | :  14%|█▍        | 52/374 [00:21<01:41,  3.17it/s]\nPROCESSING TASKS | :  14%|█▍        | 53/374 [00:22<01:31,  3.52it/s]\nPROCESSING TASKS | :  14%|█▍        | 54/374 [00:22<02:03,  2.60it/s]\nPROCESSING TASKS | :  15%|█▍        | 56/374 [00:22<01:22,  3.87it/s]\nPROCESSING TASKS | :  15%|█▌        | 57/374 [00:24<02:42,  1.95it/s]\nPROCESSING TASKS | :  16%|█▌        | 58/374 [00:24<02:12,  2.39it/s]\nPROCESSING TASKS | :  16%|█▌        | 60/374 [00:25<02:14,  2.34it/s]\nPROCESSING TASKS | :  16%|█▋        | 61/374 [00:25<02:11,  2.39it/s]\nPROCESSING TASKS | :  17%|█▋        | 62/374 [00:25<01:58,  2.64it/s]\nPROCESSING TASKS | :  17%|█▋        | 64/374 [00:26<01:30,  3.42it/s]\nPROCESSING TASKS | :  17%|█▋        | 65/374 [00:27<02:25,  2.13it/s]\nPROCESSING TASKS | :  18%|█▊        | 67/374 [00:27<01:39,  3.08it/s]\nPROCESSING TASKS | :  18%|█▊        | 68/374 [00:28<02:19,  2.20it/s]\nPROCESSING TASKS | :  18%|█▊        | 69/374 [00:28<02:05,  2.43it/s]\nPROCESSING TASKS | :  19%|█▉        | 71/374 [00:28<01:25,  3.56it/s]\nPROCESSING TASKS | :  19%|█▉        | 72/374 [00:29<01:13,  4.12it/s]\nPROCESSING TASKS | :  20%|█▉        | 73/374 [00:30<02:22,  2.11it/s]\nPROCESSING TASKS | :  20%|█▉        | 74/374 [00:30<02:10,  2.30it/s]\nPROCESSING TASKS | :  20%|██        | 76/374 [00:31<02:14,  2.22it/s]\nPROCESSING TASKS | :  21%|██        | 77/374 [00:31<01:50,  2.69it/s]\nPROCESSING TASKS | :  21%|██        | 79/374 [00:32<01:30,  3.25it/s]\nPROCESSING TASKS | :  22%|██▏       | 81/374 [00:33<02:04,  2.35it/s]\nPROCESSING TASKS | :  22%|██▏       | 83/374 [00:33<01:39,  2.93it/s]\nPROCESSING TASKS | :  22%|██▏       | 84/374 [00:34<02:00,  2.40it/s]\nPROCESSING TASKS | :  23%|██▎       | 85/374 [00:34<01:43,  2.79it/s]\nPROCESSING TASKS | :  23%|██▎       | 87/374 [00:34<01:08,  4.16it/s]\nPROCESSING TASKS | :  24%|██▎       | 88/374 [00:34<01:15,  3.76it/s]\nPROCESSING TASKS | :  24%|██▍       | 89/374 [00:36<02:06,  2.26it/s]\nPROCESSING TASKS | :  24%|██▍       | 91/374 [00:36<01:44,  2.70it/s]\nPROCESSING TASKS | :  25%|██▍       | 92/374 [00:37<02:10,  2.15it/s]\nPROCESSING TASKS | :  25%|██▍       | 93/374 [00:37<02:08,  2.19it/s]\nPROCESSING TASKS | :  25%|██▌       | 95/374 [00:37<01:23,  3.35it/s]\nPROCESSING TASKS | :  26%|██▌       | 96/374 [00:38<01:31,  3.05it/s]\nPROCESSING TASKS | :  26%|██▌       | 97/374 [00:38<01:18,  3.53it/s]\nPROCESSING TASKS | :  26%|██▌       | 98/374 [00:38<01:09,  3.98it/s]\nPROCESSING TASKS | :  26%|██▋       | 99/374 [00:39<01:55,  2.39it/s]\nPROCESSING TASKS | :  27%|██▋       | 100/374 [00:40<02:32,  1.79it/s]\nPROCESSING TASKS | :  27%|██▋       | 101/374 [00:40<02:06,  2.16it/s]\nPROCESSING TASKS | :  27%|██▋       | 102/374 [00:40<01:46,  2.57it/s]\nPROCESSING TASKS | :  28%|██▊       | 104/374 [00:41<01:27,  3.08it/s]\nPROCESSING TASKS | :  28%|██▊       | 106/374 [00:41<00:59,  4.52it/s]\nPROCESSING TASKS | :  29%|██▊       | 107/374 [00:42<01:34,  2.82it/s]\nPROCESSING TASKS | :  29%|██▉       | 108/374 [00:43<02:04,  2.14it/s]\nPROCESSING TASKS | :  29%|██▉       | 109/374 [00:43<01:56,  2.28it/s]\nPROCESSING TASKS | :  29%|██▉       | 110/374 [00:43<01:52,  2.34it/s]\nPROCESSING TASKS | :  30%|██▉       | 111/374 [00:43<01:29,  2.93it/s]\nPROCESSING TASKS | :  30%|██▉       | 112/374 [00:44<01:17,  3.38it/s]\nPROCESSING TASKS | :  30%|███       | 113/374 [00:44<01:08,  3.82it/s]\nPROCESSING TASKS | :  31%|███       | 115/374 [00:45<01:25,  3.03it/s]\nPROCESSING TASKS | :  31%|███       | 116/374 [00:45<01:52,  2.30it/s]\nPROCESSING TASKS | :  31%|███▏      | 117/374 [00:46<02:02,  2.09it/s]\nPROCESSING TASKS | :  32%|███▏      | 118/374 [00:46<01:42,  2.50it/s]\nPROCESSING TASKS | :  32%|███▏      | 119/374 [00:46<01:27,  2.90it/s]\nPROCESSING TASKS | :  32%|███▏      | 120/374 [00:47<01:16,  3.33it/s]\nPROCESSING TASKS | :  32%|███▏      | 121/374 [00:47<01:12,  3.49it/s]\nPROCESSING TASKS | :  33%|███▎      | 122/374 [00:47<01:22,  3.05it/s]\nPROCESSING TASKS | :  33%|███▎      | 123/374 [00:48<01:38,  2.54it/s]\nPROCESSING TASKS | :  33%|███▎      | 124/374 [00:48<01:49,  2.27it/s]\nPROCESSING TASKS | :  33%|███▎      | 125/374 [00:49<01:58,  2.11it/s]\nPROCESSING TASKS | :  34%|███▎      | 126/374 [00:49<01:53,  2.18it/s]\nPROCESSING TASKS | :  34%|███▍      | 128/374 [00:50<01:08,  3.58it/s]\nPROCESSING TASKS | :  35%|███▍      | 130/374 [00:50<01:18,  3.10it/s]\nPROCESSING TASKS | :  35%|███▌      | 131/374 [00:51<01:52,  2.16it/s]\nPROCESSING TASKS | :  35%|███▌      | 132/374 [00:51<01:36,  2.50it/s]\nPROCESSING TASKS | :  36%|███▌      | 133/374 [00:52<01:27,  2.74it/s]\nPROCESSING TASKS | :  36%|███▌      | 134/374 [00:52<01:33,  2.58it/s]\nPROCESSING TASKS | :  36%|███▋      | 136/374 [00:52<01:01,  3.86it/s]\nPROCESSING TASKS | :  37%|███▋      | 138/374 [00:53<01:14,  3.16it/s]\nPROCESSING TASKS | :  37%|███▋      | 139/374 [00:54<01:46,  2.20it/s]\nPROCESSING TASKS | :  37%|███▋      | 140/374 [00:54<01:35,  2.45it/s]\nPROCESSING TASKS | :  38%|███▊      | 141/374 [00:55<01:21,  2.84it/s]\nPROCESSING TASKS | :  38%|███▊      | 142/374 [00:55<01:34,  2.46it/s]\nPROCESSING TASKS | :  38%|███▊      | 143/374 [00:55<01:14,  3.09it/s]\nPROCESSING TASKS | :  39%|███▊      | 144/374 [00:56<01:20,  2.86it/s]\nPROCESSING TASKS | :  39%|███▉      | 146/374 [00:56<01:07,  3.39it/s]\nPROCESSING TASKS | :  39%|███▉      | 147/374 [00:57<01:34,  2.41it/s]\nPROCESSING TASKS | :  40%|███▉      | 148/374 [00:57<01:41,  2.23it/s]\nPROCESSING TASKS | :  40%|███▉      | 149/374 [00:58<01:22,  2.73it/s]\nPROCESSING TASKS | :  40%|████      | 150/374 [00:58<01:26,  2.60it/s]\nPROCESSING TASKS | :  40%|████      | 151/374 [00:58<01:19,  2.79it/s]\nPROCESSING TASKS | :  41%|████      | 152/374 [00:59<01:18,  2.83it/s]\nPROCESSING TASKS | :  41%|████      | 154/374 [00:59<00:47,  4.61it/s]\nPROCESSING TASKS | :  41%|████▏     | 155/374 [01:00<01:31,  2.40it/s]\nPROCESSING TASKS | :  42%|████▏     | 156/374 [01:00<01:24,  2.59it/s]\nPROCESSING TASKS | :  42%|████▏     | 157/374 [01:00<01:16,  2.83it/s]\nPROCESSING TASKS | :  42%|████▏     | 158/374 [01:01<01:33,  2.32it/s]\nPROCESSING TASKS | :  43%|████▎     | 159/374 [01:01<01:26,  2.48it/s]\nPROCESSING TASKS | :  43%|████▎     | 161/374 [01:02<00:58,  3.62it/s]\nPROCESSING TASKS | :  43%|████▎     | 162/374 [01:02<01:26,  2.45it/s]\nPROCESSING TASKS | :  44%|████▎     | 163/374 [01:03<01:23,  2.52it/s]\nPROCESSING TASKS | :  44%|████▍     | 164/374 [01:03<01:14,  2.80it/s]\nPROCESSING TASKS | :  44%|████▍     | 165/374 [01:03<01:13,  2.86it/s]\nPROCESSING TASKS | :  44%|████▍     | 166/374 [01:04<01:34,  2.20it/s]\nPROCESSING TASKS | :  45%|████▍     | 167/374 [01:04<01:17,  2.69it/s]\nPROCESSING TASKS | :  45%|████▍     | 168/374 [01:04<01:04,  3.21it/s]\nPROCESSING TASKS | :  45%|████▌     | 170/374 [01:05<01:08,  2.96it/s]\nPROCESSING TASKS | :  46%|████▌     | 171/374 [01:06<01:23,  2.43it/s]\nPROCESSING TASKS | :  46%|████▌     | 172/374 [01:06<01:08,  2.95it/s]\nPROCESSING TASKS | :  46%|████▋     | 173/374 [01:06<00:55,  3.60it/s]\nPROCESSING TASKS | :  47%|████▋     | 174/374 [01:07<01:16,  2.62it/s]\nPROCESSING TASKS | :  47%|████▋     | 175/374 [01:07<01:19,  2.49it/s]\nPROCESSING TASKS | :  47%|████▋     | 176/374 [01:07<01:02,  3.18it/s]\nPROCESSING TASKS | :  47%|████▋     | 177/374 [01:08<01:07,  2.90it/s]\nPROCESSING TASKS | :  48%|████▊     | 178/374 [01:08<01:20,  2.44it/s]\nPROCESSING TASKS | :  48%|████▊     | 179/374 [01:08<01:12,  2.68it/s]\nPROCESSING TASKS | :  48%|████▊     | 180/374 [01:09<01:08,  2.82it/s]\nPROCESSING TASKS | :  48%|████▊     | 181/374 [01:09<00:57,  3.38it/s]\nPROCESSING TASKS | :  49%|████▊     | 182/374 [01:10<01:27,  2.19it/s]\nPROCESSING TASKS | :  49%|████▉     | 183/374 [01:10<01:11,  2.69it/s]\nPROCESSING TASKS | :  49%|████▉     | 184/374 [01:10<01:10,  2.71it/s]\nPROCESSING TASKS | :  49%|████▉     | 185/374 [01:11<01:04,  2.95it/s]\nPROCESSING TASKS | :  50%|████▉     | 186/374 [01:11<01:15,  2.50it/s]\nPROCESSING TASKS | :  50%|█████     | 187/374 [01:11<01:14,  2.52it/s]\nPROCESSING TASKS | :  50%|█████     | 188/374 [01:12<01:06,  2.79it/s]\nPROCESSING TASKS | :  51%|█████     | 189/374 [01:12<00:57,  3.20it/s]\nPROCESSING TASKS | :  51%|█████     | 190/374 [01:13<01:27,  2.11it/s]\nPROCESSING TASKS | :  51%|█████▏    | 192/374 [01:13<01:07,  2.72it/s]\nPROCESSING TASKS | :  52%|█████▏    | 193/374 [01:14<01:02,  2.91it/s]\nPROCESSING TASKS | :  52%|█████▏    | 194/374 [01:14<00:55,  3.23it/s]\nPROCESSING TASKS | :  52%|█████▏    | 195/374 [01:15<01:15,  2.36it/s]\nPROCESSING TASKS | :  52%|█████▏    | 196/374 [01:15<00:59,  3.00it/s]\nPROCESSING TASKS | :  53%|█████▎    | 198/374 [01:15<01:02,  2.82it/s]\nPROCESSING TASKS | :  53%|█████▎    | 199/374 [01:16<01:10,  2.49it/s]\nPROCESSING TASKS | :  53%|█████▎    | 200/374 [01:16<01:03,  2.76it/s]\nPROCESSING TASKS | :  54%|█████▎    | 201/374 [01:16<00:53,  3.26it/s]\nPROCESSING TASKS | :  54%|█████▍    | 202/374 [01:17<00:49,  3.46it/s]\nPROCESSING TASKS | :  54%|█████▍    | 203/374 [01:17<01:12,  2.34it/s]\nPROCESSING TASKS | :  55%|█████▍    | 204/374 [01:18<01:09,  2.45it/s]\nPROCESSING TASKS | :  55%|█████▌    | 206/374 [01:18<00:49,  3.41it/s]\nPROCESSING TASKS | :  55%|█████▌    | 207/374 [01:19<01:13,  2.27it/s]\nPROCESSING TASKS | :  56%|█████▌    | 208/374 [01:19<00:58,  2.83it/s]\nPROCESSING TASKS | :  56%|█████▌    | 209/374 [01:19<00:50,  3.29it/s]\nPROCESSING TASKS | :  56%|█████▌    | 210/374 [01:20<00:53,  3.04it/s]\nPROCESSING TASKS | :  56%|█████▋    | 211/374 [01:20<01:10,  2.32it/s]\nPROCESSING TASKS | :  57%|█████▋    | 213/374 [01:21<00:51,  3.15it/s]\nPROCESSING TASKS | :  57%|█████▋    | 214/374 [01:21<00:56,  2.86it/s]\nPROCESSING TASKS | :  57%|█████▋    | 215/374 [01:22<01:16,  2.07it/s]\nPROCESSING TASKS | :  58%|█████▊    | 217/374 [01:22<00:51,  3.05it/s]\nPROCESSING TASKS | :  58%|█████▊    | 218/374 [01:23<00:55,  2.79it/s]\nPROCESSING TASKS | :  59%|█████▊    | 219/374 [01:23<01:06,  2.34it/s]\nPROCESSING TASKS | :  59%|█████▉    | 220/374 [01:23<00:55,  2.79it/s]\nPROCESSING TASKS | :  59%|█████▉    | 222/374 [01:24<00:51,  2.96it/s]\nPROCESSING TASKS | :  60%|█████▉    | 223/374 [01:25<01:02,  2.40it/s]\nPROCESSING TASKS | :  60%|█████▉    | 224/374 [01:25<01:00,  2.47it/s]\nPROCESSING TASKS | :  60%|██████    | 226/374 [01:26<00:47,  3.13it/s]\nPROCESSING TASKS | :  61%|██████    | 227/374 [01:26<00:49,  2.96it/s]\nPROCESSING TASKS | :  61%|██████    | 228/374 [01:26<00:45,  3.22it/s]\nPROCESSING TASKS | :  61%|██████    | 229/374 [01:27<00:54,  2.65it/s]\nPROCESSING TASKS | :  61%|██████▏   | 230/374 [01:27<00:58,  2.45it/s]\nPROCESSING TASKS | :  62%|██████▏   | 231/374 [01:28<01:05,  2.17it/s]\nPROCESSING TASKS | :  63%|██████▎   | 234/374 [01:29<00:47,  2.95it/s]\nPROCESSING TASKS | :  63%|██████▎   | 235/374 [01:29<00:50,  2.74it/s]\nPROCESSING TASKS | :  63%|██████▎   | 236/374 [01:29<00:47,  2.92it/s]\nPROCESSING TASKS | :  63%|██████▎   | 237/374 [01:30<00:48,  2.85it/s]\nPROCESSING TASKS | :  64%|██████▎   | 238/374 [01:30<01:00,  2.25it/s]\nPROCESSING TASKS | :  64%|██████▍   | 239/374 [01:30<00:50,  2.70it/s]\nPROCESSING TASKS | :  65%|██████▍   | 242/374 [01:31<00:41,  3.14it/s]\nPROCESSING TASKS | :  65%|██████▍   | 243/374 [01:32<00:53,  2.47it/s]\nPROCESSING TASKS | :  65%|██████▌   | 244/374 [01:32<00:44,  2.92it/s]\nPROCESSING TASKS | :  66%|██████▌   | 246/374 [01:33<00:57,  2.23it/s]\nPROCESSING TASKS | :  67%|██████▋   | 249/374 [01:34<00:37,  3.33it/s]\nPROCESSING TASKS | :  67%|██████▋   | 250/374 [01:35<00:54,  2.29it/s]\nPROCESSING TASKS | :  67%|██████▋   | 251/374 [01:35<00:47,  2.58it/s]\nPROCESSING TASKS | :  68%|██████▊   | 253/374 [01:35<00:35,  3.38it/s]\nPROCESSING TASKS | :  68%|██████▊   | 254/374 [01:36<00:54,  2.20it/s]\nPROCESSING TASKS | :  68%|██████▊   | 256/374 [01:37<00:36,  3.25it/s]\nPROCESSING TASKS | :  69%|██████▊   | 257/374 [01:37<00:36,  3.25it/s]\nPROCESSING TASKS | :  69%|██████▉   | 258/374 [01:38<00:57,  2.03it/s]\nPROCESSING TASKS | :  69%|██████▉   | 259/374 [01:38<00:45,  2.52it/s]\nPROCESSING TASKS | :  70%|██████▉   | 261/374 [01:38<00:35,  3.15it/s]\nPROCESSING TASKS | :  70%|███████   | 262/374 [01:39<00:54,  2.07it/s]\nPROCESSING TASKS | :  71%|███████   | 265/374 [01:40<00:31,  3.43it/s]\nPROCESSING TASKS | :  71%|███████   | 266/374 [01:41<00:45,  2.38it/s]\nPROCESSING TASKS | :  72%|███████▏  | 268/374 [01:41<00:31,  3.32it/s]\nPROCESSING TASKS | :  72%|███████▏  | 270/374 [01:42<00:42,  2.42it/s]\nPROCESSING TASKS | :  73%|███████▎  | 272/374 [01:42<00:31,  3.20it/s]\nPROCESSING TASKS | :  73%|███████▎  | 274/374 [01:44<00:41,  2.43it/s]\nPROCESSING TASKS | :  74%|███████▎  | 275/374 [01:44<00:37,  2.63it/s]\nPROCESSING TASKS | :  74%|███████▍  | 277/374 [01:44<00:34,  2.83it/s]\nPROCESSING TASKS | :  74%|███████▍  | 278/374 [01:45<00:36,  2.60it/s]\nPROCESSING TASKS | :  75%|███████▍  | 279/374 [01:45<00:33,  2.83it/s]\nPROCESSING TASKS | :  75%|███████▍  | 280/374 [01:45<00:31,  3.01it/s]\nPROCESSING TASKS | :  75%|███████▌  | 282/374 [01:47<00:38,  2.37it/s]\nPROCESSING TASKS | :  76%|███████▌  | 283/374 [01:47<00:33,  2.68it/s]\nPROCESSING TASKS | :  76%|███████▌  | 284/374 [01:47<00:32,  2.77it/s]\nPROCESSING TASKS | :  76%|███████▌  | 285/374 [01:47<00:29,  3.01it/s]\nPROCESSING TASKS | :  76%|███████▋  | 286/374 [01:48<00:27,  3.25it/s]\nPROCESSING TASKS | :  77%|███████▋  | 287/374 [01:48<00:38,  2.28it/s]\nPROCESSING TASKS | :  78%|███████▊  | 290/374 [01:50<00:35,  2.38it/s]\nPROCESSING TASKS | :  78%|███████▊  | 291/374 [01:50<00:30,  2.72it/s]\nPROCESSING TASKS | :  78%|███████▊  | 293/374 [01:50<00:26,  3.02it/s]\nPROCESSING TASKS | :  79%|███████▊  | 294/374 [01:51<00:30,  2.60it/s]\nPROCESSING TASKS | :  79%|███████▉  | 295/374 [01:51<00:27,  2.92it/s]\nPROCESSING TASKS | :  79%|███████▉  | 296/374 [01:51<00:22,  3.42it/s]\nPROCESSING TASKS | :  79%|███████▉  | 297/374 [01:52<00:25,  3.08it/s]\nPROCESSING TASKS | :  80%|███████▉  | 298/374 [01:52<00:30,  2.49it/s]\nPROCESSING TASKS | :  80%|███████▉  | 299/374 [01:53<00:27,  2.77it/s]\nPROCESSING TASKS | :  80%|████████  | 300/374 [01:53<00:29,  2.54it/s]\nPROCESSING TASKS | :  80%|████████  | 301/374 [01:53<00:28,  2.54it/s]\nPROCESSING TASKS | :  81%|████████  | 302/374 [01:54<00:27,  2.61it/s]\nPROCESSING TASKS | :  81%|████████  | 303/374 [01:54<00:27,  2.61it/s]\nPROCESSING TASKS | :  82%|████████▏ | 305/374 [01:55<00:22,  3.10it/s]\nPROCESSING TASKS | :  82%|████████▏ | 306/374 [01:55<00:28,  2.38it/s]\nPROCESSING TASKS | :  82%|████████▏ | 308/374 [01:56<00:29,  2.26it/s]\nPROCESSING TASKS | :  83%|████████▎ | 309/374 [01:56<00:24,  2.65it/s]\nPROCESSING TASKS | :  83%|████████▎ | 310/374 [01:57<00:25,  2.49it/s]\nPROCESSING TASKS | :  83%|████████▎ | 312/374 [01:57<00:17,  3.55it/s]\nPROCESSING TASKS | :  84%|████████▎ | 313/374 [01:58<00:23,  2.58it/s]\nPROCESSING TASKS | :  84%|████████▍ | 314/374 [01:58<00:23,  2.51it/s]\nPROCESSING TASKS | :  84%|████████▍ | 315/374 [01:59<00:20,  2.89it/s]\nPROCESSING TASKS | :  84%|████████▍ | 316/374 [01:59<00:24,  2.37it/s]\nPROCESSING TASKS | :  85%|████████▍ | 317/374 [01:59<00:22,  2.52it/s]\nPROCESSING TASKS | :  85%|████████▌ | 318/374 [02:00<00:18,  3.05it/s]\nPROCESSING TASKS | :  86%|████████▌ | 320/374 [02:00<00:11,  4.62it/s]\nPROCESSING TASKS | :  86%|████████▌ | 321/374 [02:01<00:23,  2.29it/s]\nPROCESSING TASKS | :  86%|████████▌ | 322/374 [02:01<00:23,  2.22it/s]\nPROCESSING TASKS | :  86%|████████▋ | 323/374 [02:02<00:19,  2.66it/s]\nPROCESSING TASKS | :  87%|████████▋ | 324/374 [02:02<00:20,  2.47it/s]\nPROCESSING TASKS | :  87%|████████▋ | 325/374 [02:02<00:17,  2.81it/s]\nPROCESSING TASKS | :  87%|████████▋ | 327/374 [02:03<00:12,  3.88it/s]\nPROCESSING TASKS | :  88%|████████▊ | 328/374 [02:03<00:10,  4.34it/s]\nPROCESSING TASKS | :  88%|████████▊ | 329/374 [02:04<00:18,  2.40it/s]\nPROCESSING TASKS | :  88%|████████▊ | 330/374 [02:04<00:22,  1.99it/s]\nPROCESSING TASKS | :  89%|████████▊ | 331/374 [02:05<00:17,  2.40it/s]\nPROCESSING TASKS | :  89%|████████▉ | 332/374 [02:05<00:16,  2.59it/s]\nPROCESSING TASKS | :  89%|████████▉ | 333/374 [02:05<00:16,  2.55it/s]\nPROCESSING TASKS | :  89%|████████▉ | 334/374 [02:06<00:14,  2.69it/s]\nPROCESSING TASKS | :  90%|████████▉ | 336/374 [02:07<00:15,  2.39it/s]\nPROCESSING TASKS | :  90%|█████████ | 337/374 [02:07<00:19,  1.91it/s]\nPROCESSING TASKS | :  90%|█████████ | 338/374 [02:08<00:16,  2.16it/s]\nPROCESSING TASKS | :  91%|█████████ | 339/374 [02:08<00:12,  2.74it/s]\nPROCESSING TASKS | :  91%|█████████ | 340/374 [02:08<00:11,  2.98it/s]\nPROCESSING TASKS | :  91%|█████████ | 341/374 [02:09<00:13,  2.37it/s]\nPROCESSING TASKS | :  92%|█████████▏| 343/374 [02:09<00:08,  3.74it/s]\nPROCESSING TASKS | :  92%|█████████▏| 344/374 [02:10<00:12,  2.39it/s]\nPROCESSING TASKS | :  92%|█████████▏| 345/374 [02:10<00:13,  2.12it/s]\nPROCESSING TASKS | :  93%|█████████▎| 347/374 [02:11<00:08,  3.05it/s]\nPROCESSING TASKS | :  93%|█████████▎| 348/374 [02:11<00:08,  3.10it/s]\nPROCESSING TASKS | :  93%|█████████▎| 349/374 [02:12<00:09,  2.65it/s]\nPROCESSING TASKS | :  94%|█████████▎| 350/374 [02:12<00:08,  2.93it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | :  99%|█████████▊| 369/374 [02:13<00:00, 12.70it/s]\nPROCESSING TASKS | :  99%|█████████▉| 370/374 [02:13<00:00,  8.82it/s]\nPROCESSING TASKS | :  99%|█████████▉| 371/374 [02:14<00:00,  6.72it/s]\nPROCESSING TASKS | : 100%|█████████▉| 373/374 [02:14<00:00,  6.41it/s]\nPROCESSING TASKS | : 100%|██████████| 374/374 [02:15<00:00,  5.33it/s]\nPROCESSING TASKS | : 100%|██████████| 374/374 [02:15<00:00,  2.77it/s]\nCOLLECTING RESULTS | :   0%|          | 0/374 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 374/374 [00:00<00:00, 53730.77it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nThe provided start date was not recognized\nGranules found: 374\n Getting 374 granules, approx download size: 2.81 GB\nQUEUEING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]File MCD12Q1.A2001001.h08v04.061.2022146034934.hdf already downloaded\nFile MCD12Q1.A2001001.h12v05.061.2022146045521.hdf already downloaded\nFile MCD12Q1.A2001001.h09v06.061.2022146040126.hdf already downloadedFile MCD12Q1.A2001001.h07v05.061.2022146033046.hdf already downloaded\nFile MCD12Q1.A2001001.h08v05.061.2022146033917.hdf already downloaded\nFile MCD12Q1.A2001001.h08v06.061.2022146035452.hdf already downloadedFile MCD12Q1.A2001001.h13v04.061.2022146060649.hdf already downloaded\nFile MCD12Q1.A2001001.h11v04.061.2022146060256.hdf already downloadedFile MCD12Q1.A2001001.h09v05.061.2022146040113.hdf already downloaded\nFile MCD12Q1.A2001001.h07v06.061.2022146033902.hdf already downloadedFile MCD12Q1.A2001001.h10v04.061.2022146040552.hdf already downloaded\nFile MCD12Q1.A2001001.h11v06.061.2022146031055.hdf already downloadedFile MCD12Q1.A2001001.h10v05.061.2022146030344.hdf already downloaded\nFile MCD12Q1.A2001001.h09v04.061.2022146030108.hdf already downloaded\nFile MCD12Q1.A2001001.h11v05.061.2022146031022.hdf already downloaded\nFile MCD12Q1.A2001001.h12v04.061.2022146050354.hdf already downloaded\nFile MCD12Q1.A2001001.h10v06.061.2022146030552.hdf already downloaded\nFile MCD12Q1.A2002001.h11v06.061.2022147204132.hdf already downloaded\nFile MCD12Q1.A2002001.h07v05.061.2022147201847.hdf already downloaded\nFile MCD12Q1.A2002001.h12v05.061.2022147215523.hdf already downloaded\nFile MCD12Q1.A2002001.h07v06.061.2022147203753.hdf already downloadedFile MCD12Q1.A2002001.h10v06.061.2022147204000.hdf already downloaded\nFile MCD12Q1.A2002001.h09v06.061.2022147202005.hdf already downloaded\nFile MCD12Q1.A2002001.h08v04.061.2022147201909.hdf already downloadedFile MCD12Q1.A2002001.h11v04.061.2022147204553.hdf already downloaded\nFile MCD12Q1.A2002001.h10v04.061.2022147204353.hdf already downloaded\nFile MCD12Q1.A2002001.h09v05.061.2022147203952.hdf already downloaded\nFile MCD12Q1.A2002001.h10v05.061.2022147203812.hdf already downloaded\nFile MCD12Q1.A2002001.h08v05.061.2022147203929.hdf already downloaded\nFile MCD12Q1.A2002001.h13v04.061.2022148083352.hdf already downloaded\nFile MCD12Q1.A2002001.h09v04.061.2022147201926.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 15093.81it/s]\nPROCESSING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]File MCD12Q1.A2002001.h08v06.061.2022147204405.hdf already downloaded\nFile MCD12Q1.A2002001.h12v04.061.2022147215712.hdf already downloaded\nFile MCD12Q1.A2003001.h09v06.061.2022151161416.hdf already downloaded\nFile MCD12Q1.A2003001.h11v04.061.2022151161716.hdf already downloaded\nFile MCD12Q1.A2003001.h08v04.061.2022151161158.hdf already downloaded\nFile MCD12Q1.A2003001.h12v05.061.2022151161918.hdf already downloadedFile MCD12Q1.A2002001.h11v05.061.2022147205502.hdf already downloaded\nFile MCD12Q1.A2003001.h13v04.061.2022151162053.hdf already downloaded\nFile MCD12Q1.A2003001.h11v06.061.2022151161736.hdf already downloaded\nFile MCD12Q1.A2003001.h07v06.061.2022151161138.hdf already downloaded\nFile MCD12Q1.A2003001.h07v05.061.2022151161122.hdf already downloadedFile MCD12Q1.A2003001.h08v05.061.2022151161201.hdf already downloadedFile MCD12Q1.A2003001.h08v06.061.2022151161214.hdf already downloaded\nFile MCD12Q1.A2003001.h10v06.061.2022151161601.hdf already downloaded\nFile MCD12Q1.A2003001.h09v05.061.2022151161409.hdf already downloaded\nFile MCD12Q1.A2003001.h09v04.061.2022151161351.hdf already downloaded\nFile MCD12Q1.A2003001.h10v05.061.2022151161531.hdf already downloadedFile MCD12Q1.A2004001.h07v05.061.2022152014117.hdf already downloaded\nFile MCD12Q1.A2004001.h09v06.061.2022152022342.hdf already downloaded\nFile MCD12Q1.A2004001.h13v04.061.2022152050602.hdf already downloaded\nFile MCD12Q1.A2004001.h10v06.061.2022152022413.hdf already downloaded\nFile MCD12Q1.A2004001.h12v05.061.2022152041846.hdf already downloaded\nFile MCD12Q1.A2004001.h07v06.061.2022152014317.hdf already downloadedFile MCD12Q1.A2003001.h10v04.061.2022151161523.hdf already downloadedFile MCD12Q1.A2004001.h08v04.061.2022152013916.hdf already downloaded\nFile MCD12Q1.A2004001.h11v05.061.2022152014115.hdf already downloaded\nFile MCD12Q1.A2004001.h09v04.061.2022152023313.hdf already downloaded\nFile MCD12Q1.A2004001.h11v06.061.2022152022235.hdf already downloaded\nFile MCD12Q1.A2004001.h10v05.061.2022152023406.hdf already downloaded\nFile MCD12Q1.A2004001.h11v04.061.2022152014252.hdf already downloadedFile MCD12Q1.A2003001.h12v04.061.2022151161906.hdf already downloaded\nFile MCD12Q1.A2004001.h08v05.061.2022152014143.hdf already downloaded\nFile MCD12Q1.A2004001.h08v06.061.2022152014235.hdf already downloaded\nFile MCD12Q1.A2004001.h09v05.061.2022152023143.hdf already downloaded\nFile MCD12Q1.A2005001.h07v05.061.2022152115041.hdf already downloaded\nFile MCD12Q1.A2005001.h07v06.061.2022152124844.hdf already downloaded\nFile MCD12Q1.A2004001.h10v04.061.2022152022837.hdf already downloaded\nFile MCD12Q1.A2005001.h08v05.061.2022152121656.hdf already downloaded\nFile MCD12Q1.A2005001.h11v06.061.2022152130904.hdf already downloadedFile MCD12Q1.A2005001.h10v06.061.2022152132706.hdf already downloaded\nFile MCD12Q1.A2005001.h09v05.061.2022153084625.hdf already downloaded\nFile MCD12Q1.A2003001.h11v05.061.2022151161738.hdf already downloadedFile MCD12Q1.A2005001.h11v04.061.2022152220556.hdf already downloaded\nFile MCD12Q1.A2005001.h09v06.061.2022152220612.hdf already downloadedFile MCD12Q1.A2005001.h08v06.061.2022152211338.hdf already downloaded\nFile MCD12Q1.A2005001.h08v04.061.2022152141617.hdf already downloaded\nFile MCD12Q1.A2005001.h10v04.061.2022153091803.hdf already downloaded\nFile MCD12Q1.A2005001.h09v04.061.2022153082156.hdf already downloaded\nFile MCD12Q1.A2005001.h10v05.061.2022153083959.hdf already downloaded\nFile MCD12Q1.A2004001.h12v04.061.2022152043252.hdf already downloaded\nFile MCD12Q1.A2005001.h12v05.061.2022152134513.hdf already downloaded\nFile MCD12Q1.A2005001.h11v05.061.2022152215559.hdf already downloaded\nFile MCD12Q1.A2005001.h13v04.061.2022153191607.hdf already downloaded\nFile MCD12Q1.A2006001.h08v04.061.2022202150359.hdf already downloadedFile MCD12Q1.A2005001.h12v04.061.2022152140219.hdf already downloaded\nFile MCD12Q1.A2006001.h11v06.061.2022202150900.hdf already downloaded\nFile MCD12Q1.A2006001.h11v04.061.2022202150826.hdf already downloadedFile MCD12Q1.A2006001.h12v04.061.2022202151028.hdf already downloaded\nFile MCD12Q1.A2006001.h10v05.061.2022202150656.hdf already downloaded\nFile MCD12Q1.A2006001.h12v05.061.2022202151035.hdf already downloaded\nFile MCD12Q1.A2006001.h13v04.061.2022202151230.hdf already downloaded\nFile MCD12Q1.A2006001.h08v06.061.2022202150433.hdf already downloadedFile MCD12Q1.A2006001.h07v06.061.2022202150327.hdf already downloaded\nFile MCD12Q1.A2006001.h10v06.061.2022202150716.hdf already downloaded\nFile MCD12Q1.A2006001.h09v05.061.2022202150538.hdf already downloaded\nFile MCD12Q1.A2006001.h09v06.061.2022202150602.hdf already downloadedFile MCD12Q1.A2006001.h09v04.061.2022202150536.hdf already downloadedFile MCD12Q1.A2006001.h10v04.061.2022202150652.hdf already downloaded\nFile MCD12Q1.A2006001.h08v05.061.2022202150408.hdf already downloaded\nFile MCD12Q1.A2006001.h11v05.061.2022202150843.hdf already downloaded\nFile MCD12Q1.A2007001.h12v05.061.2022153122432.hdf already downloaded\nFile MCD12Q1.A2007001.h09v06.061.2022153223012.hdf already downloaded\nFile MCD12Q1.A2007001.h08v06.061.2022153211515.hdf already downloaded\nFile MCD12Q1.A2006001.h07v05.061.2022202150325.hdf already downloadedFile MCD12Q1.A2007001.h13v04.061.2022154052639.hdf already downloadedFile MCD12Q1.A2007001.h08v05.061.2022153205724.hdf already downloaded\nFile MCD12Q1.A2007001.h08v04.061.2022153230033.hdf already downloaded\nFile MCD12Q1.A2007001.h10v04.061.2022154052218.hdf already downloaded\nFile MCD12Q1.A2007001.h09v05.061.2022154060738.hdf already downloaded\nFile MCD12Q1.A2007001.h07v05.061.2022153114232.hdf already downloaded\nFile MCD12Q1.A2007001.h11v04.061.2022154032745.hdf already downloaded\nFile MCD12Q1.A2007001.h07v06.061.2022153115802.hdf already downloaded\nFile MCD12Q1.A2007001.h11v05.061.2022153204112.hdf already downloaded\nFile MCD12Q1.A2007001.h12v04.061.2022153233438.hdf already downloaded\nFile MCD12Q1.A2007001.h10v06.061.2022153125643.hdf already downloaded\nFile MCD12Q1.A2008001.h11v04.061.2022158212527.hdf already downloaded\nFile MCD12Q1.A2007001.h11v06.061.2022153125459.hdf already downloadedFile MCD12Q1.A2007001.h10v05.061.2022154031246.hdf already downloaded\nFile MCD12Q1.A2008001.h08v05.061.2022158212252.hdf already downloaded\nFile MCD12Q1.A2008001.h09v04.061.2022158215957.hdf already downloadedFile MCD12Q1.A2007001.h09v04.061.2022154033740.hdf already downloaded\nFile MCD12Q1.A2008001.h09v05.061.2022158221132.hdf already downloaded\nFile MCD12Q1.A2008001.h07v06.061.2022158211027.hdf already downloaded\nFile MCD12Q1.A2008001.h10v06.061.2022158215757.hdf already downloadedFile MCD12Q1.A2008001.h08v04.061.2022158211115.hdf already downloaded\nFile MCD12Q1.A2008001.h10v05.061.2022158221052.hdf already downloaded\nFile MCD12Q1.A2008001.h11v06.061.2022158215642.hdf already downloadedFile MCD12Q1.A2008001.h08v06.061.2022158211828.hdf already downloaded\nFile MCD12Q1.A2008001.h09v06.061.2022158220427.hdf already downloaded\nFile MCD12Q1.A2008001.h13v04.061.2022159001357.hdf already downloaded\nFile MCD12Q1.A2008001.h12v04.061.2022158231134.hdf already downloaded\nFile MCD12Q1.A2009001.h08v04.061.2022159174334.hdf already downloaded\nFile MCD12Q1.A2009001.h07v06.061.2022159174334.hdf already downloadedFile MCD12Q1.A2008001.h07v05.061.2022158211012.hdf already downloadedFile MCD12Q1.A2009001.h11v06.061.2022159182449.hdf already downloaded\nFile MCD12Q1.A2009001.h09v04.061.2022159175354.hdf already downloaded\nFile MCD12Q1.A2009001.h11v04.061.2022159184319.hdf already downloadedFile MCD12Q1.A2008001.h10v04.061.2022158220628.hdf already downloaded\nFile MCD12Q1.A2009001.h08v06.061.2022159175519.hdf already downloaded\nFile MCD12Q1.A2008001.h12v05.061.2022158225932.hdf already downloaded\nFile MCD12Q1.A2009001.h09v06.061.2022159182729.hdf already downloaded\nFile MCD12Q1.A2008001.h11v05.061.2022158212002.hdf already downloaded\nFile MCD12Q1.A2009001.h10v04.061.2022159184020.hdf already downloaded\nFile MCD12Q1.A2009001.h07v05.061.2022159174239.hdf already downloadedFile MCD12Q1.A2009001.h08v05.061.2022159175354.hdf already downloaded\nFile MCD12Q1.A2009001.h10v06.061.2022159182554.hdf already downloaded\nFile MCD12Q1.A2009001.h09v05.061.2022159180020.hdf already downloaded\nFile MCD12Q1.A2010001.h12v05.061.2022160074557.hdf already downloaded\nFile MCD12Q1.A2009001.h12v04.061.2022159194525.hdf already downloadedFile MCD12Q1.A2009001.h11v05.061.2022159183224.hdf already downloaded\nFile MCD12Q1.A2009001.h10v05.061.2022159184300.hdf already downloaded\nFile MCD12Q1.A2010001.h11v06.061.2022160071951.hdf already downloaded\nFile MCD12Q1.A2009001.h13v04.061.2022159204454.hdf already downloaded\nFile MCD12Q1.A2010001.h09v04.061.2022160063857.hdf already downloaded\nFile MCD12Q1.A2010001.h09v06.061.2022160064847.hdf already downloaded\nFile MCD12Q1.A2010001.h10v06.061.2022160064326.hdf already downloadedFile MCD12Q1.A2010001.h08v04.061.2022160051756.hdf already downloaded\nFile MCD12Q1.A2010001.h07v06.061.2022160051624.hdf already downloaded\nFile MCD12Q1.A2010001.h08v06.061.2022160065224.hdf already downloaded\nFile MCD12Q1.A2010001.h13v04.061.2022160091153.hdf already downloadedFile MCD12Q1.A2010001.h08v05.061.2022160064327.hdf already downloadedFile MCD12Q1.A2009001.h12v05.061.2022159193439.hdf already downloaded\nFile MCD12Q1.A2010001.h11v05.061.2022160065528.hdf already downloaded\nFile MCD12Q1.A2010001.h12v04.061.2022160081327.hdf already downloaded\nFile MCD12Q1.A2010001.h10v05.061.2022160071156.hdf already downloaded\nFile MCD12Q1.A2010001.h07v05.061.2022160050421.hdf already downloaded\nFile MCD12Q1.A2011001.h08v04.061.2022161124637.hdf already downloaded\nFile MCD12Q1.A2011001.h07v05.061.2022161124337.hdf already downloaded\nFile MCD12Q1.A2011001.h08v05.061.2022161130812.hdf already downloaded\nFile MCD12Q1.A2011001.h11v06.061.2022161132907.hdf already downloaded\nFile MCD12Q1.A2011001.h09v04.061.2022161130111.hdf already downloaded\nFile MCD12Q1.A2011001.h07v06.061.2022161124349.hdf already downloadedFile MCD12Q1.A2010001.h09v05.061.2022160070317.hdf already downloaded\nFile MCD12Q1.A2011001.h08v06.061.2022161131106.hdf already downloaded\nFile MCD12Q1.A2011001.h11v04.061.2022161140708.hdf already downloaded\nFile MCD12Q1.A2011001.h12v05.061.2022161143539.hdf already downloadedFile MCD12Q1.A2010001.h10v04.061.2022160075223.hdf already downloaded\nFile MCD12Q1.A2010001.h11v04.061.2022160070952.hdf already downloaded\nFile MCD12Q1.A2011001.h11v05.061.2022161134833.hdf already downloaded\nFile MCD12Q1.A2011001.h10v04.061.2022161140208.hdf already downloaded\nFile MCD12Q1.A2011001.h13v04.061.2022161154912.hdf already downloaded\nFile MCD12Q1.A2011001.h10v05.061.2022161140603.hdf already downloaded\nFile MCD12Q1.A2011001.h12v04.061.2022161144809.hdf already downloadedFile MCD12Q1.A2011001.h09v06.061.2022161133402.hdf already downloaded\nFile MCD12Q1.A2012001.h10v04.061.2022162025604.hdf already downloaded\nFile MCD12Q1.A2012001.h10v05.061.2022162040109.hdf already downloadedFile MCD12Q1.A2012001.h11v04.061.2022162040638.hdf already downloaded\nFile MCD12Q1.A2012001.h13v04.061.2022162035643.hdf already downloaded\nFile MCD12Q1.A2012001.h12v04.061.2022162035938.hdf already downloaded\nFile MCD12Q1.A2012001.h07v06.061.2022162003933.hdf already downloaded\nFile MCD12Q1.A2012001.h09v06.061.2022162022641.hdf already downloaded\nFile MCD12Q1.A2011001.h10v06.061.2022161133506.hdf already downloaded\nFile MCD12Q1.A2012001.h08v04.061.2022162010807.hdf already downloaded\nFile MCD12Q1.A2012001.h07v05.061.2022162012243.hdf already downloaded\nFile MCD12Q1.A2012001.h09v05.061.2022162023037.hdf already downloaded\nFile MCD12Q1.A2012001.h11v05.061.2022162024041.hdf already downloaded\nFile MCD12Q1.A2012001.h08v06.061.2022162021504.hdf already downloaded\nFile MCD12Q1.A2012001.h11v06.061.2022162025807.hdf already downloadedFile MCD12Q1.A2012001.h10v06.061.2022162031538.hdf already downloaded\nFile MCD12Q1.A2013001.h07v06.061.2022164160305.hdf already downloadedFile MCD12Q1.A2012001.h12v05.061.2022162024506.hdf already downloaded\nFile MCD12Q1.A2013001.h08v04.061.2022164160625.hdf already downloadedFile MCD12Q1.A2012001.h08v05.061.2022162022037.hdf already downloaded\nFile MCD12Q1.A2013001.h08v06.061.2022164162410.hdf already downloadedFile MCD12Q1.A2013001.h08v05.061.2022164163351.hdf already downloaded\nFile MCD12Q1.A2013001.h07v05.061.2022164160213.hdf already downloaded\nFile MCD12Q1.A2013001.h11v06.061.2022164160634.hdf already downloaded\nFile MCD12Q1.A2013001.h12v05.061.2022164175416.hdf already downloadedFile MCD12Q1.A2011001.h09v05.061.2022161131738.hdf already downloadedFile MCD12Q1.A2013001.h09v04.061.2022164162715.hdf already downloaded\nFile MCD12Q1.A2013001.h11v04.061.2022164173746.hdf already downloaded\nFile MCD12Q1.A2013001.h10v04.061.2022164172811.hdf already downloaded\nFile MCD12Q1.A2013001.h09v05.061.2022164172325.hdf already downloaded\nFile MCD12Q1.A2013001.h09v06.061.2022164164813.hdf already downloaded\nFile MCD12Q1.A2013001.h13v04.061.2022164191941.hdf already downloadedFile MCD12Q1.A2013001.h12v04.061.2022164182417.hdf already downloaded\nFile MCD12Q1.A2013001.h10v06.061.2022164164813.hdf already downloaded\nFile MCD12Q1.A2012001.h09v04.061.2022162021540.hdf already downloaded\nFile MCD12Q1.A2014001.h11v06.061.2022165055649.hdf already downloaded\nFile MCD12Q1.A2014001.h09v04.061.2022165065242.hdf already downloaded\nFile MCD12Q1.A2014001.h08v06.061.2022165065712.hdf already downloadedFile MCD12Q1.A2013001.h11v05.061.2022164170219.hdf already downloaded\nFile MCD12Q1.A2014001.h09v06.061.2022165072106.hdf already downloaded\nFile MCD12Q1.A2014001.h11v05.061.2022165075505.hdf already downloaded\nFile MCD12Q1.A2014001.h07v06.061.2022165051616.hdf already downloaded\nFile MCD12Q1.A2014001.h08v05.061.2022165072154.hdf already downloaded\nFile MCD12Q1.A2014001.h12v04.061.2022165083049.hdf already downloaded\nFile MCD12Q1.A2013001.h10v05.061.2022164173115.hdf already downloaded\nFile MCD12Q1.A2014001.h08v04.061.2022165062521.hdf already downloaded\nFile MCD12Q1.A2014001.h10v05.061.2022165082044.hdf already downloaded\nFile MCD12Q1.A2014001.h07v05.061.2022165051115.hdf already downloaded\nFile MCD12Q1.A2014001.h12v05.061.2022165071913.hdf already downloaded\nFile MCD12Q1.A2014001.h10v06.061.2022165072912.hdf already downloaded\nFile MCD12Q1.A2014001.h09v05.061.2022165081147.hdf already downloaded\nFile MCD12Q1.A2015001.h08v04.061.2022165193640.hdf already downloaded\nFile MCD12Q1.A2015001.h11v06.061.2022165195905.hdf already downloaded\nFile MCD12Q1.A2015001.h12v05.061.2022165203837.hdf already downloaded\nFile MCD12Q1.A2015001.h09v06.061.2022165204539.hdf already downloaded\nFile MCD12Q1.A2015001.h09v04.061.2022165202021.hdf already downloadedFile MCD12Q1.A2014001.h11v04.061.2022165074211.hdf already downloadedFile MCD12Q1.A2014001.h13v04.061.2022165084441.hdf already downloaded\nFile MCD12Q1.A2014001.h10v04.061.2022165082137.hdf already downloaded\nFile MCD12Q1.A2015001.h09v05.061.2022165224410.hdf already downloaded\nFile MCD12Q1.A2015001.h07v06.061.2022165193642.hdf already downloaded\nFile MCD12Q1.A2015001.h08v06.061.2022165204322.hdf already downloaded\nFile MCD12Q1.A2015001.h13v04.061.2022165221239.hdf already downloaded\nFile MCD12Q1.A2015001.h08v05.061.2022165214015.hdf already downloaded\nFile MCD12Q1.A2015001.h10v04.061.2022165225511.hdf already downloaded\nFile MCD12Q1.A2015001.h10v06.061.2022165214445.hdf already downloaded\nFile MCD12Q1.A2015001.h12v04.061.2022165230140.hdf already downloaded\nFile MCD12Q1.A2016001.h12v04.061.2022166171445.hdf already downloaded\nFile MCD12Q1.A2015001.h10v05.061.2022165225415.hdf already downloaded\nFile MCD12Q1.A2015001.h07v05.061.2022165193841.hdf already downloadedFile MCD12Q1.A2015001.h11v05.061.2022165220945.hdf already downloaded\nFile MCD12Q1.A2015001.h11v04.061.2022165215345.hdf already downloaded\nFile MCD12Q1.A2016001.h11v04.061.2022166160447.hdf already downloaded\nFile MCD12Q1.A2016001.h09v05.061.2022166152129.hdf already downloaded\nFile MCD12Q1.A2016001.h08v05.061.2022166151313.hdf already downloaded\nFile MCD12Q1.A2016001.h10v04.061.2022166160540.hdf already downloaded\nFile MCD12Q1.A2016001.h09v04.061.2022166151038.hdf already downloaded\nFile MCD12Q1.A2016001.h11v05.061.2022166154009.hdf already downloaded\nFile MCD12Q1.A2016001.h12v05.061.2022166164515.hdf already downloaded\nFile MCD12Q1.A2016001.h07v05.061.2022166145612.hdf already downloaded\nFile MCD12Q1.A2016001.h09v06.061.2022166153314.hdf already downloaded\nFile MCD12Q1.A2016001.h11v06.061.2022166153208.hdf already downloaded\nFile MCD12Q1.A2016001.h13v04.061.2022166180416.hdf already downloaded\nFile MCD12Q1.A2017001.h08v04.061.2022168001839.hdf already downloaded\nFile MCD12Q1.A2016001.h08v04.061.2022166145044.hdf already downloaded\nFile MCD12Q1.A2017001.h07v06.061.2022168001822.hdf already downloaded\nFile MCD12Q1.A2016001.h10v05.061.2022166160910.hdf already downloaded\nFile MCD12Q1.A2016001.h08v06.061.2022166151608.hdf already downloadedFile MCD12Q1.A2017001.h07v05.061.2022168001816.hdf already downloaded\nFile MCD12Q1.A2017001.h13v04.061.2022168022420.hdf already downloaded\nFile MCD12Q1.A2017001.h11v06.061.2022168025451.hdf already downloaded\nFile MCD12Q1.A2017001.h11v05.061.2022168031256.hdf already downloadedFile MCD12Q1.A2017001.h11v04.061.2022168022951.hdf already downloadedFile MCD12Q1.A2017001.h08v05.061.2022168001850.hdf already downloaded\nFile MCD12Q1.A2017001.h08v06.061.2022168001851.hdf already downloaded\nFile MCD12Q1.A2017001.h10v04.061.2022168030221.hdf already downloaded\nFile MCD12Q1.A2016001.h07v06.061.2022166144709.hdf already downloadedFile MCD12Q1.A2017001.h10v06.061.2022168025826.hdf already downloaded\nFile MCD12Q1.A2017001.h10v05.061.2022168025349.hdf already downloaded\nFile MCD12Q1.A2017001.h09v06.061.2022168030220.hdf already downloaded\nFile MCD12Q1.A2016001.h10v06.061.2022166153713.hdf already downloadedFile MCD12Q1.A2017001.h12v04.061.2022168033428.hdf already downloaded\nFile MCD12Q1.A2018001.h13v04.061.2022168153530.hdf already downloaded\nFile MCD12Q1.A2017001.h09v05.061.2022168002006.hdf already downloaded\nFile MCD12Q1.A2017001.h09v04.061.2022168002003.hdf already downloadedFile MCD12Q1.A2017001.h12v05.061.2022168030750.hdf already downloaded\nFile MCD12Q1.A2018001.h07v06.061.2022168141319.hdf already downloaded\nFile MCD12Q1.A2018001.h09v05.061.2022168162537.hdf already downloaded\nFile MCD12Q1.A2018001.h12v05.061.2022168174253.hdf already downloaded\nFile MCD12Q1.A2018001.h12v04.061.2022168173053.hdf already downloaded\nFile MCD12Q1.A2018001.h08v06.061.2022168151426.hdf already downloaded\nFile MCD12Q1.A2018001.h08v04.061.2022168135023.hdf already downloaded\nFile MCD12Q1.A2018001.h11v06.061.2022168165354.hdf already downloaded\nFile MCD12Q1.A2018001.h07v05.061.2022168144825.hdf already downloadedFile MCD12Q1.A2018001.h09v06.061.2022168174923.hdf already downloaded\nFile MCD12Q1.A2018001.h08v05.061.2022168152848.hdf already downloaded\nFile MCD12Q1.A2018001.h10v06.061.2022168170157.hdf already downloadedFile MCD12Q1.A2018001.h10v05.061.2022168173357.hdf already downloadedFile MCD12Q1.A2018001.h10v04.061.2022168175454.hdf already downloaded\nFile MCD12Q1.A2018001.h11v05.061.2022168172423.hdf already downloaded\nFile MCD12Q1.A2019001.h09v06.061.2022169160549.hdf already downloaded\nFile MCD12Q1.A2019001.h09v05.061.2022169160540.hdf already downloaded\nFile MCD12Q1.A2019001.h09v04.061.2022169160534.hdf already downloaded\nFile MCD12Q1.A2019001.h13v04.061.2022169161048.hdf already downloaded\nFile MCD12Q1.A2019001.h12v04.061.2022169160935.hdf already downloaded\nFile MCD12Q1.A2018001.h11v04.061.2022168164651.hdf already downloaded\nFile MCD12Q1.A2019001.h11v05.061.2022169160820.hdf already downloaded\nFile MCD12Q1.A2019001.h10v04.061.2022169160645.hdf already downloaded\nFile MCD12Q1.A2019001.h11v04.061.2022169160759.hdf already downloaded\nFile MCD12Q1.A2019001.h10v06.061.2022169160700.hdf already downloaded\nFile MCD12Q1.A2019001.h08v04.061.2022169160414.hdf already downloaded\nFile MCD12Q1.A2019001.h07v05.061.2022169160344.hdf already downloaded\nFile MCD12Q1.A2019001.h08v06.061.2022169160433.hdf already downloaded\nFile MCD12Q1.A2018001.h09v04.061.2022168150425.hdf already downloadedFile MCD12Q1.A2019001.h10v05.061.2022169160646.hdf already downloadedFile MCD12Q1.A2019001.h08v05.061.2022169160425.hdf already downloaded\nFile MCD12Q1.A2020001.h10v06.061.2022171153457.hdf already downloaded\nFile MCD12Q1.A2020001.h08v05.061.2022171160153.hdf already downloaded\nFile MCD12Q1.A2020001.h07v06.061.2022171160856.hdf already downloaded\nFile MCD12Q1.A2019001.h11v06.061.2022169160822.hdf already downloaded\nFile MCD12Q1.A2020001.h07v05.061.2022171160801.hdf already downloaded\nFile MCD12Q1.A2019001.h12v05.061.2022169160942.hdf already downloadedFile MCD12Q1.A2020001.h09v04.061.2022171155354.hdf already downloaded\nFile MCD12Q1.A2020001.h12v05.061.2022171174410.hdf already downloadedFile MCD12Q1.A2019001.h07v06.061.2022169160348.hdf already downloaded\nFile MCD12Q1.A2020001.h08v04.061.2022171151922.hdf already downloaded\nFile MCD12Q1.A2020001.h10v04.061.2022171161358.hdf already downloaded\nFile MCD12Q1.A2020001.h11v06.061.2022171151818.hdf already downloaded\nFile MCD12Q1.A2020001.h09v06.061.2022171153157.hdf already downloadedFile MCD12Q1.A2020001.h10v05.061.2022171161201.hdf already downloaded\nFile MCD12Q1.A2020001.h12v04.061.2022171175121.hdf already downloaded\nFile MCD12Q1.A2020001.h11v05.061.2022171162126.hdf already downloaded\nFile MCD12Q1.A2020001.h08v06.061.2022171160428.hdf already downloaded\nFile MCD12Q1.A2021001.h09v05.061.2022216063601.hdf already downloaded\nFile MCD12Q1.A2021001.h13v04.061.2022216011046.hdf already downloaded\nFile MCD12Q1.A2020001.h11v04.061.2022171170901.hdf already downloaded\nFile MCD12Q1.A2021001.h08v05.061.2022216030750.hdf already downloaded\nFile MCD12Q1.A2021001.h09v06.061.2022216003122.hdf already downloaded\nFile MCD12Q1.A2020001.h13v04.061.2022171190218.hdf already downloadedFile MCD12Q1.A2021001.h12v05.061.2022215202352.hdf already downloaded\nFile MCD12Q1.A2021001.h07v06.061.2022215212351.hdf already downloaded\nFile MCD12Q1.A2020001.h09v05.061.2022171161548.hdf already downloaded\nFile MCD12Q1.A2021001.h10v06.061.2022215233619.hdf already downloaded\nFile MCD12Q1.A2021001.h11v06.061.2022215202155.hdf already downloaded\nFile MCD12Q1.A2021001.h07v05.061.2022215203800.hdf already downloaded\nFile MCD12Q1.A2021001.h11v05.061.2022216013427.hdf already downloaded\nFile MCD12Q1.A2021001.h08v04.061.2022215203426.hdf already downloadedFile MCD12Q1.A2021001.h08v06.061.2022216025520.hdf already downloaded\nFile MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloadedFile MCD12Q1.A2021001.h11v04.061.2022216043057.hdf already downloaded\nFile MCD12Q1.A2021001.h10v05.061.2022216071653.hdf already downloaded\nFile MCD12Q1.A2021001.h12v04.061.2022216040516.hdf already downloaded\nFile MCD12Q1.A2021001.h10v04.061.2022216052627.hdf already downloadedFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2021001.h09v04.061.2022216015823.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 3808.25it/s]\nCOLLECTING RESULTS | :   0%|          | 0/374 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 374/374 [00:00<00:00, 83033.54it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v05.061.2022153083959.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v06.061.2022146031055.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v05.061.2022154060738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v04.061.2022153091803.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v06.061.2022202150900.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v06.061.2022153125459.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v06.061.2022166151608.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v06.061.2022165193642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v04.061.2022147204353.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v05.061.2022215203800.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v06.061.2022161133402.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v05.061.2022160065528.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v05.061.2022168031256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h13v04.061.2022160091153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v06.061.2022165204322.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v05.061.2022168173357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v05.061.2022160074557.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v06.061.2022146035452.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v04.061.2022160063857.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v06.061.2022161133506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v05.061.2022171161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v04.061.2022165215345.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v06.061.2022153211515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v05.061.2022168174253.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v04.061.2022216052627.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v06.061.2022151161601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v05.061.2022169160344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v06.061.2022215233619.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v04.061.2022169160414.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v06.061.2022216003122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v06.061.2022152014317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v06.061.2022168151426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v06.061.2022164160305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v04.061.2022147201909.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v06.061.2022168001822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v05.061.2022161124337.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v06.061.2022158211027.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v04.061.2022202150359.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v06.061.2022165072106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v05.061.2022166154009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v04.061.2022158211115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v05.061.2022171160153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v06.061.2022152124844.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h13v04.061.2022152050602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v04.061.2022164173746.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v06.061.2022165195905.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v05.061.2022168152848.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v04.061.2022153230033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v06.061.2022165065712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v06.061.2022166153314.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v04.061.2022171161358.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v06.061.2022161131106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v05.061.2022164172325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v05.061.2022165225415.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v05.061.2022168172423.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v05.061.2022168025349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v06.061.2022158211828.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v04.061.2022168173053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v04.061.2022162021540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v05.061.2022216030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v04.061.2022164172811.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v06.061.2022215212351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v06.061.2022152220612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v05.061.2022169160942.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v04.061.2022161140208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v06.061.2022171151818.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v06.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v04.061.2022168033428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v04.061.2022171151922.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v04.061.2022160081327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v05.061.2022168001816.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v04.061.2022168002003.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v05.061.2022202150408.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v06.061.2022158220427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v05.061.2022153084625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v05.061.2022164175416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v04.061.2022152141617.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v06.061.2022159182449.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v05.061.2022153204112.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v06.061.2022151161736.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v05.061.2022146040113.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v05.061.2022169160425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v05.061.2022166160910.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v05.061.2022171174410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v05.061.2022158221132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h13v04.061.2022166180416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v04.061.2022158212527.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v04.061.2022151161351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v04.061.2022202150652.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v04.061.2022162010807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v05.061.2022154031246.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v06.061.2022168001851.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v04.061.2022216015823.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v04.061.2022151161906.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v06.061.2022162021504.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v04.061.2022202151028.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v06.061.2022168030220.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v05.061.2022162012243.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v06.061.2022168174923.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v05.061.2022147205502.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v04.061.2022158215957.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h13v04.061.2022148083352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v05.061.2022158212002.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v06.061.2022168165354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v06.061.2022165051616.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v04.061.2022165225511.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h13v04.061.2022216011046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v04.061.2022161140708.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v04.061.2022164162715.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v04.061.2022165082137.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v05.061.2022161140603.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v04.061.2022146050354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v04.061.2022162035938.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v04.061.2022159194525.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v06.061.2022169160549.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v06.061.2022153125643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v05.061.2022147215523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v04.061.2022147201926.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v06.061.2022151161214.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v05.061.2022159193439.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v04.061.2022160075223.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v04.061.2022215203426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v04.061.2022169160759.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v06.061.2022166153713.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v06.061.2022152022235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v05.061.2022147203812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v05.061.2022159180020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v05.061.2022152121656.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v04.061.2022165230140.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v06.061.2022165055649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v04.061.2022152220556.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v04.061.2022165202021.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v04.061.2022169160645.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v05.061.2022146033917.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v06.061.2022164162410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v06.061.2022169160433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v05.061.2022165220945.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v05.061.2022158212252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v05.061.2022160071156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h13v04.061.2022151162053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v06.061.2022151161416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v04.061.2022146034934.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v06.061.2022168141319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v04.061.2022147215712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v06.061.2022158215642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v05.061.2022152041846.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v06.061.2022171160428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v06.061.2022164160634.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v06.061.2022160071951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v05.061.2022146045521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v05.061.2022153122432.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v04.061.2022160051756.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v06.061.2022165214445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h13v04.061.2022159001357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v05.061.2022152014115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v05.061.2022162024506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v06.061.2022160065224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v06.061.2022216025520.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v04.061.2022158231134.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v05.061.2022151161531.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v04.061.2022169160935.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v05.061.2022159184300.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v05.061.2022165082044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v05.061.2022216071653.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v04.061.2022165065242.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v05.061.2022158221052.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v06.061.2022147204132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v06.061.2022146040126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v04.061.2022152014252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v06.061.2022162003933.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v04.061.2022171155354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v05.061.2022168002006.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v05.061.2022168030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v05.061.2022152134513.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v05.061.2022151161738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v06.061.2022171153157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v04.061.2022162025604.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v04.061.2022202150536.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v04.061.2022158220628.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v05.061.2022166152129.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v06.061.2022160064847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v04.061.2022146030108.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v05.061.2022162040109.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v06.061.2022147204000.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v06.061.2022166144709.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v06.061.2022202150433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v06.061.2022147203753.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v06.061.2022162022641.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v04.061.2022159184319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v06.061.2022152022413.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v06.061.2022168025826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v05.061.2022166164515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v04.061.2022153233438.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v06.061.2022166153208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v05.061.2022147203929.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v05.061.2022153114232.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v05.061.2022151161918.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v04.061.2022164160625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v06.061.2022152014235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v06.061.2022152211338.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v06.061.2022146033902.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h13v04.061.2022169161048.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h13v04.061.2022168153530.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v06.061.2022161132907.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v06.061.2022215202155.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v04.061.2022160070952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v05.061.2022153205724.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v06.061.2022202150716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v04.061.2022154032745.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v05.061.2022165203837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v05.061.2022166151313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v04.061.2022159184020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h13v04.061.2022162035643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v04.061.2022161130111.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v06.061.2022159175519.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v06.061.2022153115802.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v05.061.2022152115041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v05.061.2022158225932.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h13v04.061.2022146060649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v06.061.2022158215757.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h13v04.061.2022202151230.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v06.061.2022202150327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v05.061.2022160050421.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v04.061.2022165074211.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v04.061.2022165062521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v05.061.2022202150656.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nThe provided start date was not recognized\nGranules found: 374\n Getting 374 granules, approx download size: 2.81 GB\nQUEUEING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]File MCD12Q1.A2001001.h08v04.061.2022146034934.hdf already downloaded\nFile MCD12Q1.A2001001.h12v05.061.2022146045521.hdf already downloaded\nFile MCD12Q1.A2001001.h07v05.061.2022146033046.hdf already downloaded\nFile MCD12Q1.A2001001.h09v06.061.2022146040126.hdf already downloadedFile MCD12Q1.A2001001.h08v05.061.2022146033917.hdf already downloaded\nFile MCD12Q1.A2001001.h08v06.061.2022146035452.hdf already downloaded\nFile MCD12Q1.A2001001.h13v04.061.2022146060649.hdf already downloadedFile MCD12Q1.A2001001.h10v04.061.2022146040552.hdf already downloaded\nFile MCD12Q1.A2001001.h11v04.061.2022146060256.hdf already downloadedFile MCD12Q1.A2001001.h11v06.061.2022146031055.hdf already downloaded\nFile MCD12Q1.A2001001.h09v05.061.2022146040113.hdf already downloadedFile MCD12Q1.A2001001.h07v06.061.2022146033902.hdf already downloaded\nFile MCD12Q1.A2001001.h10v05.061.2022146030344.hdf already downloaded\nFile MCD12Q1.A2001001.h09v04.061.2022146030108.hdf already downloaded\nFile MCD12Q1.A2002001.h11v06.061.2022147204132.hdf already downloaded\nFile MCD12Q1.A2001001.h10v06.061.2022146030552.hdf already downloadedFile MCD12Q1.A2001001.h12v04.061.2022146050354.hdf already downloaded\nFile MCD12Q1.A2002001.h12v05.061.2022147215523.hdf already downloaded\nFile MCD12Q1.A2002001.h07v06.061.2022147203753.hdf already downloaded\nFile MCD12Q1.A2001001.h11v05.061.2022146031022.hdf already downloaded\nFile MCD12Q1.A2002001.h07v05.061.2022147201847.hdf already downloaded\nFile MCD12Q1.A2002001.h10v06.061.2022147204000.hdf already downloaded\nFile MCD12Q1.A2002001.h09v06.061.2022147202005.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 22957.26it/s]\nFile MCD12Q1.A2002001.h08v04.061.2022147201909.hdf already downloadedFile MCD12Q1.A2002001.h11v04.061.2022147204553.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]File MCD12Q1.A2002001.h10v04.061.2022147204353.hdf already downloaded\nFile MCD12Q1.A2002001.h09v05.061.2022147203952.hdf already downloaded\nFile MCD12Q1.A2002001.h08v05.061.2022147203929.hdf already downloadedFile MCD12Q1.A2002001.h10v05.061.2022147203812.hdf already downloadedFile MCD12Q1.A2002001.h09v04.061.2022147201926.hdf already downloaded\nFile MCD12Q1.A2002001.h13v04.061.2022148083352.hdf already downloaded\nFile MCD12Q1.A2002001.h11v05.061.2022147205502.hdf already downloadedFile MCD12Q1.A2002001.h12v04.061.2022147215712.hdf already downloaded\nFile MCD12Q1.A2003001.h11v04.061.2022151161716.hdf already downloaded\nFile MCD12Q1.A2002001.h08v06.061.2022147204405.hdf already downloaded\nFile MCD12Q1.A2003001.h07v05.061.2022151161122.hdf already downloaded\nFile MCD12Q1.A2003001.h09v06.061.2022151161416.hdf already downloadedFile MCD12Q1.A2003001.h12v05.061.2022151161918.hdf already downloaded\nFile MCD12Q1.A2003001.h08v04.061.2022151161158.hdf already downloaded\nFile MCD12Q1.A2003001.h13v04.061.2022151162053.hdf already downloadedFile MCD12Q1.A2003001.h10v04.061.2022151161523.hdf already downloaded\nFile MCD12Q1.A2003001.h08v05.061.2022151161201.hdf already downloaded\nFile MCD12Q1.A2003001.h07v06.061.2022151161138.hdf already downloaded\nFile MCD12Q1.A2003001.h10v06.061.2022151161601.hdf already downloadedFile MCD12Q1.A2003001.h09v05.061.2022151161409.hdf already downloaded\nFile MCD12Q1.A2003001.h08v06.061.2022151161214.hdf already downloaded\nFile MCD12Q1.A2003001.h11v06.061.2022151161736.hdf already downloadedFile MCD12Q1.A2003001.h09v04.061.2022151161351.hdf already downloaded\nFile MCD12Q1.A2003001.h11v05.061.2022151161738.hdf already downloaded\nFile MCD12Q1.A2003001.h10v05.061.2022151161531.hdf already downloaded\nFile MCD12Q1.A2004001.h10v06.061.2022152022413.hdf already downloaded\nFile MCD12Q1.A2004001.h13v04.061.2022152050602.hdf already downloadedFile MCD12Q1.A2004001.h09v06.061.2022152022342.hdf already downloaded\nFile MCD12Q1.A2004001.h07v05.061.2022152014117.hdf already downloaded\nFile MCD12Q1.A2004001.h12v05.061.2022152041846.hdf already downloaded\nFile MCD12Q1.A2004001.h11v06.061.2022152022235.hdf already downloaded\nFile MCD12Q1.A2004001.h08v04.061.2022152013916.hdf already downloaded\nFile MCD12Q1.A2004001.h07v06.061.2022152014317.hdf already downloaded\nFile MCD12Q1.A2004001.h11v04.061.2022152014252.hdf already downloaded\nFile MCD12Q1.A2003001.h12v04.061.2022151161906.hdf already downloadedFile MCD12Q1.A2004001.h10v04.061.2022152022837.hdf already downloaded\nFile MCD12Q1.A2004001.h10v05.061.2022152023406.hdf already downloaded\nFile MCD12Q1.A2004001.h09v05.061.2022152023143.hdf already downloaded\nFile MCD12Q1.A2004001.h11v05.061.2022152014115.hdf already downloadedFile MCD12Q1.A2004001.h08v05.061.2022152014143.hdf already downloaded\nFile MCD12Q1.A2004001.h09v04.061.2022152023313.hdf already downloaded\nFile MCD12Q1.A2005001.h08v04.061.2022152141617.hdf already downloaded\nFile MCD12Q1.A2005001.h07v06.061.2022152124844.hdf already downloaded\nFile MCD12Q1.A2005001.h11v06.061.2022152130904.hdf already downloaded\nFile MCD12Q1.A2004001.h08v06.061.2022152014235.hdf already downloaded\nFile MCD12Q1.A2005001.h11v04.061.2022152220556.hdf already downloaded\nFile MCD12Q1.A2005001.h09v05.061.2022153084625.hdf already downloadedFile MCD12Q1.A2004001.h12v04.061.2022152043252.hdf already downloaded\nFile MCD12Q1.A2005001.h09v04.061.2022153082156.hdf already downloaded\nFile MCD12Q1.A2005001.h08v05.061.2022152121656.hdf already downloaded\nFile MCD12Q1.A2005001.h07v05.061.2022152115041.hdf already downloadedFile MCD12Q1.A2005001.h08v06.061.2022152211338.hdf already downloaded\nFile MCD12Q1.A2005001.h10v05.061.2022153083959.hdf already downloaded\nFile MCD12Q1.A2005001.h12v04.061.2022152140219.hdf already downloaded\nFile MCD12Q1.A2005001.h13v04.061.2022153191607.hdf already downloaded\nFile MCD12Q1.A2006001.h08v04.061.2022202150359.hdf already downloadedFile MCD12Q1.A2005001.h10v06.061.2022152132706.hdf already downloaded\nFile MCD12Q1.A2005001.h09v06.061.2022152220612.hdf already downloadedFile MCD12Q1.A2005001.h10v04.061.2022153091803.hdf already downloaded\nFile MCD12Q1.A2005001.h11v05.061.2022152215559.hdf already downloaded\nFile MCD12Q1.A2006001.h12v04.061.2022202151028.hdf already downloaded\nFile MCD12Q1.A2006001.h12v05.061.2022202151035.hdf already downloaded\nFile MCD12Q1.A2006001.h09v06.061.2022202150602.hdf already downloadedFile MCD12Q1.A2006001.h07v06.061.2022202150327.hdf already downloaded\nFile MCD12Q1.A2006001.h11v06.061.2022202150900.hdf already downloaded\nFile MCD12Q1.A2006001.h10v05.061.2022202150656.hdf already downloaded\nFile MCD12Q1.A2006001.h09v04.061.2022202150536.hdf already downloaded\nFile MCD12Q1.A2006001.h10v06.061.2022202150716.hdf already downloaded\nFile MCD12Q1.A2005001.h12v05.061.2022152134513.hdf already downloaded\nFile MCD12Q1.A2006001.h10v04.061.2022202150652.hdf already downloaded\nFile MCD12Q1.A2006001.h09v05.061.2022202150538.hdf already downloaded\nFile MCD12Q1.A2006001.h07v05.061.2022202150325.hdf already downloaded\nFile MCD12Q1.A2006001.h13v04.061.2022202151230.hdf already downloaded\nFile MCD12Q1.A2006001.h11v04.061.2022202150826.hdf already downloadedFile MCD12Q1.A2006001.h11v05.061.2022202150843.hdf already downloaded\nFile MCD12Q1.A2006001.h08v06.061.2022202150433.hdf already downloadedFile MCD12Q1.A2007001.h09v06.061.2022153223012.hdf already downloaded\nFile MCD12Q1.A2007001.h08v05.061.2022153205724.hdf already downloaded\nFile MCD12Q1.A2007001.h12v05.061.2022153122432.hdf already downloaded\nFile MCD12Q1.A2007001.h09v05.061.2022154060738.hdf already downloaded\nFile MCD12Q1.A2007001.h07v05.061.2022153114232.hdf already downloadedFile MCD12Q1.A2007001.h08v06.061.2022153211515.hdf already downloaded\nFile MCD12Q1.A2006001.h08v05.061.2022202150408.hdf already downloaded\nFile MCD12Q1.A2007001.h08v04.061.2022153230033.hdf already downloaded\nFile MCD12Q1.A2007001.h11v04.061.2022154032745.hdf already downloaded\nFile MCD12Q1.A2007001.h07v06.061.2022153115802.hdf already downloaded\nFile MCD12Q1.A2007001.h10v05.061.2022154031246.hdf already downloadedFile MCD12Q1.A2007001.h13v04.061.2022154052639.hdf already downloaded\nFile MCD12Q1.A2007001.h11v05.061.2022153204112.hdf already downloaded\nFile MCD12Q1.A2007001.h10v04.061.2022154052218.hdf already downloaded\nFile MCD12Q1.A2007001.h10v06.061.2022153125643.hdf already downloadedFile MCD12Q1.A2007001.h09v04.061.2022154033740.hdf already downloaded\nFile MCD12Q1.A2008001.h08v04.061.2022158211115.hdf already downloaded\nFile MCD12Q1.A2008001.h08v05.061.2022158212252.hdf already downloaded\nFile MCD12Q1.A2008001.h09v04.061.2022158215957.hdf already downloaded\nFile MCD12Q1.A2008001.h10v06.061.2022158215757.hdf already downloadedFile MCD12Q1.A2008001.h08v06.061.2022158211828.hdf already downloaded\nFile MCD12Q1.A2007001.h11v06.061.2022153125459.hdf already downloaded\nFile MCD12Q1.A2008001.h11v05.061.2022158212002.hdf already downloaded\nFile MCD12Q1.A2008001.h07v06.061.2022158211027.hdf already downloaded\nFile MCD12Q1.A2008001.h09v05.061.2022158221132.hdf already downloaded\nFile MCD12Q1.A2008001.h10v05.061.2022158221052.hdf already downloaded\nFile MCD12Q1.A2008001.h12v04.061.2022158231134.hdf already downloaded\nFile MCD12Q1.A2008001.h12v05.061.2022158225932.hdf already downloaded\nFile MCD12Q1.A2007001.h12v04.061.2022153233438.hdf already downloaded\nFile MCD12Q1.A2008001.h09v06.061.2022158220427.hdf already downloadedFile MCD12Q1.A2009001.h07v05.061.2022159174239.hdf already downloaded\nFile MCD12Q1.A2009001.h08v04.061.2022159174334.hdf already downloadedFile MCD12Q1.A2008001.h10v04.061.2022158220628.hdf already downloadedFile MCD12Q1.A2008001.h11v04.061.2022158212527.hdf already downloaded\nFile MCD12Q1.A2008001.h13v04.061.2022159001357.hdf already downloaded\nFile MCD12Q1.A2009001.h11v06.061.2022159182449.hdf already downloaded\nFile MCD12Q1.A2009001.h07v06.061.2022159174334.hdf already downloaded\nFile MCD12Q1.A2008001.h07v05.061.2022158211012.hdf already downloadedFile MCD12Q1.A2009001.h09v06.061.2022159182729.hdf already downloaded\nFile MCD12Q1.A2009001.h08v06.061.2022159175519.hdf already downloaded\nFile MCD12Q1.A2009001.h09v04.061.2022159175354.hdf already downloaded\nFile MCD12Q1.A2009001.h12v04.061.2022159194525.hdf already downloaded\nFile MCD12Q1.A2009001.h11v04.061.2022159184319.hdf already downloaded\nFile MCD12Q1.A2009001.h10v06.061.2022159182554.hdf already downloaded\nFile MCD12Q1.A2009001.h09v05.061.2022159180020.hdf already downloaded\nFile MCD12Q1.A2009001.h11v05.061.2022159183224.hdf already downloaded\nFile MCD12Q1.A2008001.h11v06.061.2022158215642.hdf already downloaded\nFile MCD12Q1.A2009001.h10v04.061.2022159184020.hdf already downloadedFile MCD12Q1.A2010001.h09v04.061.2022160063857.hdf already downloaded\nFile MCD12Q1.A2009001.h13v04.061.2022159204454.hdf already downloaded\nFile MCD12Q1.A2009001.h08v05.061.2022159175354.hdf already downloaded\nFile MCD12Q1.A2010001.h08v04.061.2022160051756.hdf already downloadedFile MCD12Q1.A2010001.h09v06.061.2022160064847.hdf already downloaded\nFile MCD12Q1.A2010001.h12v05.061.2022160074557.hdf already downloaded\nFile MCD12Q1.A2010001.h07v06.061.2022160051624.hdf already downloaded\nFile MCD12Q1.A2010001.h08v06.061.2022160065224.hdf already downloadedFile MCD12Q1.A2010001.h08v05.061.2022160064327.hdf already downloaded\nFile MCD12Q1.A2010001.h10v06.061.2022160064326.hdf already downloaded\nFile MCD12Q1.A2010001.h11v06.061.2022160071951.hdf already downloaded\nFile MCD12Q1.A2010001.h13v04.061.2022160091153.hdf already downloaded\nFile MCD12Q1.A2010001.h12v04.061.2022160081327.hdf already downloaded\nFile MCD12Q1.A2009001.h10v05.061.2022159184300.hdf already downloaded\nFile MCD12Q1.A2009001.h12v05.061.2022159193439.hdf already downloaded\nFile MCD12Q1.A2010001.h09v05.061.2022160070317.hdf already downloaded\nFile MCD12Q1.A2010001.h10v04.061.2022160075223.hdf already downloadedFile MCD12Q1.A2010001.h10v05.061.2022160071156.hdf already downloaded\nFile MCD12Q1.A2010001.h11v05.061.2022160065528.hdf already downloaded\nFile MCD12Q1.A2011001.h08v05.061.2022161130812.hdf already downloaded\nFile MCD12Q1.A2011001.h07v06.061.2022161124349.hdf already downloadedFile MCD12Q1.A2011001.h08v04.061.2022161124637.hdf already downloaded\nFile MCD12Q1.A2010001.h07v05.061.2022160050421.hdf already downloaded\nFile MCD12Q1.A2010001.h11v04.061.2022160070952.hdf already downloaded\nFile MCD12Q1.A2011001.h07v05.061.2022161124337.hdf already downloaded\nFile MCD12Q1.A2011001.h09v05.061.2022161131738.hdf already downloaded\nFile MCD12Q1.A2011001.h12v05.061.2022161143539.hdf already downloadedFile MCD12Q1.A2011001.h11v04.061.2022161140708.hdf already downloaded\nFile MCD12Q1.A2011001.h10v04.061.2022161140208.hdf already downloaded\nFile MCD12Q1.A2011001.h09v04.061.2022161130111.hdf already downloaded\nFile MCD12Q1.A2011001.h13v04.061.2022161154912.hdf already downloaded\nFile MCD12Q1.A2012001.h10v04.061.2022162025604.hdf already downloaded\nFile MCD12Q1.A2011001.h10v05.061.2022161140603.hdf already downloaded\nFile MCD12Q1.A2011001.h12v04.061.2022161144809.hdf already downloaded\nFile MCD12Q1.A2012001.h11v04.061.2022162040638.hdf already downloaded\nFile MCD12Q1.A2012001.h13v04.061.2022162035643.hdf already downloadedFile MCD12Q1.A2011001.h10v06.061.2022161133506.hdf already downloaded\nFile MCD12Q1.A2011001.h11v05.061.2022161134833.hdf already downloaded\nFile MCD12Q1.A2012001.h12v04.061.2022162035938.hdf already downloaded\nFile MCD12Q1.A2012001.h07v06.061.2022162003933.hdf already downloaded\nFile MCD12Q1.A2012001.h10v06.061.2022162031538.hdf already downloadedFile MCD12Q1.A2012001.h09v06.061.2022162022641.hdf already downloadedFile MCD12Q1.A2011001.h08v06.061.2022161131106.hdf already downloadedFile MCD12Q1.A2012001.h10v05.061.2022162040109.hdf already downloaded\nFile MCD12Q1.A2011001.h09v06.061.2022161133402.hdf already downloaded\nFile MCD12Q1.A2011001.h11v06.061.2022161132907.hdf already downloaded\nFile MCD12Q1.A2012001.h09v05.061.2022162023037.hdf already downloaded\nFile MCD12Q1.A2012001.h11v05.061.2022162024041.hdf already downloadedFile MCD12Q1.A2012001.h07v05.061.2022162012243.hdf already downloaded\nFile MCD12Q1.A2012001.h08v04.061.2022162010807.hdf already downloaded\nFile MCD12Q1.A2012001.h11v06.061.2022162025807.hdf already downloaded\nFile MCD12Q1.A2012001.h12v05.061.2022162024506.hdf already downloaded\nFile MCD12Q1.A2012001.h08v06.061.2022162021504.hdf already downloaded\nFile MCD12Q1.A2013001.h07v06.061.2022164160305.hdf already downloaded\nFile MCD12Q1.A2013001.h08v06.061.2022164162410.hdf already downloaded\nFile MCD12Q1.A2012001.h09v04.061.2022162021540.hdf already downloaded\nFile MCD12Q1.A2013001.h08v05.061.2022164163351.hdf already downloaded\nFile MCD12Q1.A2013001.h07v05.061.2022164160213.hdf already downloadedFile MCD12Q1.A2012001.h08v05.061.2022162022037.hdf already downloadedFile MCD12Q1.A2013001.h08v04.061.2022164160625.hdf already downloaded\nFile MCD12Q1.A2013001.h09v05.061.2022164172325.hdf already downloaded\nFile MCD12Q1.A2013001.h10v04.061.2022164172811.hdf already downloaded\nFile MCD12Q1.A2013001.h09v04.061.2022164162715.hdf already downloaded\nFile MCD12Q1.A2013001.h09v06.061.2022164164813.hdf already downloadedFile MCD12Q1.A2013001.h12v05.061.2022164175416.hdf already downloaded\nFile MCD12Q1.A2013001.h12v04.061.2022164182417.hdf already downloaded\nFile MCD12Q1.A2013001.h10v05.061.2022164173115.hdf already downloaded\nFile MCD12Q1.A2013001.h11v05.061.2022164170219.hdf already downloaded\nFile MCD12Q1.A2013001.h11v06.061.2022164160634.hdf already downloaded\nFile MCD12Q1.A2013001.h11v04.061.2022164173746.hdf already downloaded\nFile MCD12Q1.A2014001.h11v06.061.2022165055649.hdf already downloaded\nFile MCD12Q1.A2013001.h13v04.061.2022164191941.hdf already downloaded\nFile MCD12Q1.A2013001.h10v06.061.2022164164813.hdf already downloadedFile MCD12Q1.A2014001.h07v05.061.2022165051115.hdf already downloadedFile MCD12Q1.A2014001.h09v04.061.2022165065242.hdf already downloaded\nFile MCD12Q1.A2014001.h08v06.061.2022165065712.hdf already downloaded\nFile MCD12Q1.A2014001.h09v06.061.2022165072106.hdf already downloaded\nFile MCD12Q1.A2014001.h11v05.061.2022165075505.hdf already downloaded\nFile MCD12Q1.A2014001.h08v04.061.2022165062521.hdf already downloaded\nFile MCD12Q1.A2014001.h12v05.061.2022165071913.hdf already downloaded\nFile MCD12Q1.A2014001.h08v05.061.2022165072154.hdf already downloaded\nFile MCD12Q1.A2014001.h07v06.061.2022165051616.hdf already downloaded\nFile MCD12Q1.A2014001.h12v04.061.2022165083049.hdf already downloadedFile MCD12Q1.A2014001.h11v04.061.2022165074211.hdf already downloaded\nFile MCD12Q1.A2014001.h10v05.061.2022165082044.hdf already downloaded\nFile MCD12Q1.A2014001.h10v04.061.2022165082137.hdf already downloaded\nFile MCD12Q1.A2015001.h08v04.061.2022165193640.hdf already downloaded\nFile MCD12Q1.A2015001.h07v06.061.2022165193642.hdf already downloadedFile MCD12Q1.A2014001.h10v06.061.2022165072912.hdf already downloaded\nFile MCD12Q1.A2014001.h09v05.061.2022165081147.hdf already downloaded\nFile MCD12Q1.A2015001.h11v06.061.2022165195905.hdf already downloaded\nFile MCD12Q1.A2015001.h12v05.061.2022165203837.hdf already downloaded\nFile MCD12Q1.A2015001.h09v06.061.2022165204539.hdf already downloaded\nFile MCD12Q1.A2015001.h11v05.061.2022165220945.hdf already downloaded\nFile MCD12Q1.A2015001.h09v05.061.2022165224410.hdf already downloaded\nFile MCD12Q1.A2015001.h09v04.061.2022165202021.hdf already downloaded\nFile MCD12Q1.A2015001.h08v05.061.2022165214015.hdf already downloadedFile MCD12Q1.A2014001.h13v04.061.2022165084441.hdf already downloaded\nFile MCD12Q1.A2015001.h10v06.061.2022165214445.hdf already downloaded\nFile MCD12Q1.A2015001.h13v04.061.2022165221239.hdf already downloaded\nFile MCD12Q1.A2015001.h10v05.061.2022165225415.hdf already downloaded\nFile MCD12Q1.A2015001.h08v06.061.2022165204322.hdf already downloaded\nFile MCD12Q1.A2016001.h10v04.061.2022166160540.hdf already downloadedFile MCD12Q1.A2015001.h07v05.061.2022165193841.hdf already downloaded\nFile MCD12Q1.A2016001.h12v04.061.2022166171445.hdf already downloaded\nFile MCD12Q1.A2016001.h11v04.061.2022166160447.hdf already downloaded\nFile MCD12Q1.A2016001.h09v05.061.2022166152129.hdf already downloaded\nFile MCD12Q1.A2015001.h10v04.061.2022165225511.hdf already downloaded\nFile MCD12Q1.A2016001.h08v05.061.2022166151313.hdf already downloaded\nFile MCD12Q1.A2015001.h11v04.061.2022165215345.hdf already downloadedFile MCD12Q1.A2016001.h09v04.061.2022166151038.hdf already downloaded\nFile MCD12Q1.A2016001.h08v06.061.2022166151608.hdf already downloaded\nFile MCD12Q1.A2016001.h10v05.061.2022166160910.hdf already downloaded\nFile MCD12Q1.A2016001.h11v05.061.2022166154009.hdf already downloaded\nFile MCD12Q1.A2016001.h12v05.061.2022166164515.hdf already downloaded\nFile MCD12Q1.A2016001.h07v06.061.2022166144709.hdf already downloadedFile MCD12Q1.A2016001.h07v05.061.2022166145612.hdf already downloaded\nFile MCD12Q1.A2016001.h10v06.061.2022166153713.hdf already downloaded\nFile MCD12Q1.A2016001.h13v04.061.2022166180416.hdf already downloaded\nFile MCD12Q1.A2016001.h08v04.061.2022166145044.hdf already downloaded\nFile MCD12Q1.A2016001.h11v06.061.2022166153208.hdf already downloaded\nFile MCD12Q1.A2015001.h12v04.061.2022165230140.hdf already downloaded\nFile MCD12Q1.A2016001.h09v06.061.2022166153314.hdf already downloaded\nFile MCD12Q1.A2017001.h13v04.061.2022168022420.hdf already downloadedFile MCD12Q1.A2017001.h08v04.061.2022168001839.hdf already downloaded\nFile MCD12Q1.A2017001.h07v05.061.2022168001816.hdf already downloaded\nFile MCD12Q1.A2017001.h11v04.061.2022168022951.hdf already downloaded\nFile MCD12Q1.A2017001.h09v04.061.2022168002003.hdf already downloaded\nFile MCD12Q1.A2017001.h11v06.061.2022168025451.hdf already downloaded\nFile MCD12Q1.A2017001.h10v04.061.2022168030221.hdf already downloaded\nFile MCD12Q1.A2017001.h12v04.061.2022168033428.hdf already downloaded\nFile MCD12Q1.A2017001.h08v06.061.2022168001851.hdf already downloaded\nFile MCD12Q1.A2017001.h10v06.061.2022168025826.hdf already downloaded\nFile MCD12Q1.A2017001.h11v05.061.2022168031256.hdf already downloaded\nFile MCD12Q1.A2017001.h10v05.061.2022168025349.hdf already downloaded\nFile MCD12Q1.A2017001.h09v06.061.2022168030220.hdf already downloaded\nFile MCD12Q1.A2017001.h12v05.061.2022168030750.hdf already downloaded\nFile MCD12Q1.A2017001.h07v06.061.2022168001822.hdf already downloadedFile MCD12Q1.A2018001.h07v06.061.2022168141319.hdf already downloaded\nFile MCD12Q1.A2018001.h09v04.061.2022168150425.hdf already downloadedFile MCD12Q1.A2018001.h13v04.061.2022168153530.hdf already downloaded\nFile MCD12Q1.A2018001.h08v05.061.2022168152848.hdf already downloaded\nFile MCD12Q1.A2017001.h08v05.061.2022168001850.hdf already downloadedFile MCD12Q1.A2017001.h09v05.061.2022168002006.hdf already downloaded\nFile MCD12Q1.A2018001.h08v04.061.2022168135023.hdf already downloaded\nFile MCD12Q1.A2018001.h07v05.061.2022168144825.hdf already downloaded\nFile MCD12Q1.A2018001.h10v05.061.2022168173357.hdf already downloaded\nFile MCD12Q1.A2018001.h09v06.061.2022168174923.hdf already downloadedFile MCD12Q1.A2018001.h08v06.061.2022168151426.hdf already downloaded\nFile MCD12Q1.A2018001.h12v04.061.2022168173053.hdf already downloaded\nFile MCD12Q1.A2018001.h09v05.061.2022168162537.hdf already downloaded\nFile MCD12Q1.A2018001.h11v04.061.2022168164651.hdf already downloaded\nFile MCD12Q1.A2018001.h10v06.061.2022168170157.hdf already downloaded\nFile MCD12Q1.A2018001.h10v04.061.2022168175454.hdf already downloadedFile MCD12Q1.A2018001.h12v05.061.2022168174253.hdf already downloaded\nFile MCD12Q1.A2019001.h09v04.061.2022169160534.hdf already downloaded\nFile MCD12Q1.A2019001.h10v05.061.2022169160646.hdf already downloaded\nFile MCD12Q1.A2019001.h09v06.061.2022169160549.hdf already downloaded\nFile MCD12Q1.A2018001.h11v06.061.2022168165354.hdf already downloaded\nFile MCD12Q1.A2019001.h10v04.061.2022169160645.hdf already downloadedFile MCD12Q1.A2019001.h13v04.061.2022169161048.hdf already downloaded\nFile MCD12Q1.A2019001.h09v05.061.2022169160540.hdf already downloaded\nFile MCD12Q1.A2019001.h12v05.061.2022169160942.hdf already downloaded\nFile MCD12Q1.A2019001.h11v06.061.2022169160822.hdf already downloaded\nFile MCD12Q1.A2019001.h11v04.061.2022169160759.hdf already downloaded\nFile MCD12Q1.A2019001.h12v04.061.2022169160935.hdf already downloaded\nFile MCD12Q1.A2019001.h10v06.061.2022169160700.hdf already downloaded\nFile MCD12Q1.A2019001.h07v05.061.2022169160344.hdf already downloaded\nFile MCD12Q1.A2019001.h08v04.061.2022169160414.hdf already downloaded\nFile MCD12Q1.A2019001.h08v05.061.2022169160425.hdf already downloaded\nFile MCD12Q1.A2020001.h09v06.061.2022171153157.hdf already downloaded\nFile MCD12Q1.A2020001.h10v06.061.2022171153457.hdf already downloadedFile MCD12Q1.A2019001.h07v06.061.2022169160348.hdf already downloaded\nFile MCD12Q1.A2020001.h11v06.061.2022171151818.hdf already downloaded\nFile MCD12Q1.A2019001.h08v06.061.2022169160433.hdf already downloaded\nFile MCD12Q1.A2020001.h08v05.061.2022171160153.hdf already downloaded\nFile MCD12Q1.A2020001.h07v06.061.2022171160856.hdf already downloaded\nFile MCD12Q1.A2019001.h11v05.061.2022169160820.hdf already downloaded\nFile MCD12Q1.A2020001.h08v06.061.2022171160428.hdf already downloaded\nFile MCD12Q1.A2020001.h11v05.061.2022171162126.hdf already downloadedFile MCD12Q1.A2018001.h11v05.061.2022168172423.hdf already downloaded\nFile MCD12Q1.A2020001.h08v04.061.2022171151922.hdf already downloaded\nFile MCD12Q1.A2020001.h13v04.061.2022171190218.hdf already downloaded\nFile MCD12Q1.A2020001.h12v05.061.2022171174410.hdf already downloaded\nFile MCD12Q1.A2020001.h10v04.061.2022171161358.hdf already downloaded\nFile MCD12Q1.A2020001.h09v05.061.2022171161548.hdf already downloaded\nFile MCD12Q1.A2020001.h07v05.061.2022171160801.hdf already downloaded\nFile MCD12Q1.A2020001.h09v04.061.2022171155354.hdf already downloadedFile MCD12Q1.A2021001.h13v04.061.2022216011046.hdf already downloaded\nFile MCD12Q1.A2020001.h11v04.061.2022171170901.hdf already downloaded\nFile MCD12Q1.A2020001.h12v04.061.2022171175121.hdf already downloaded\nFile MCD12Q1.A2020001.h10v05.061.2022171161201.hdf already downloadedFile MCD12Q1.A2021001.h12v05.061.2022215202352.hdf already downloaded\nFile MCD12Q1.A2021001.h07v06.061.2022215212351.hdf already downloaded\nFile MCD12Q1.A2021001.h08v05.061.2022216030750.hdf already downloaded\nFile MCD12Q1.A2021001.h09v05.061.2022216063601.hdf already downloaded\nFile MCD12Q1.A2021001.h08v06.061.2022216025520.hdf already downloadedFile MCD12Q1.A2021001.h11v06.061.2022215202155.hdf already downloaded\nFile MCD12Q1.A2021001.h11v05.061.2022216013427.hdf already downloaded\nFile MCD12Q1.A2021001.h09v06.061.2022216003122.hdf already downloaded\nFile MCD12Q1.A2021001.h08v04.061.2022215203426.hdf already downloaded\nFile MCD12Q1.A2021001.h07v05.061.2022215203800.hdf already downloaded\nFile MCD12Q1.A2021001.h10v05.061.2022216071653.hdf already downloaded\nFile MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2021001.h09v04.061.2022216015823.hdf already downloadedFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2021001.h10v06.061.2022215233619.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2021001.h12v04.061.2022216040516.hdf already downloadedFile MCD12Q1.A2021001.h10v04.061.2022216052627.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2021001.h11v04.061.2022216043057.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 4743.84it/s]\nCOLLECTING RESULTS | :   0%|          | 0/374 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 374/374 [00:00<00:00, 84724.26it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v05.061.2022153083959.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v06.061.2022146031055.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v05.061.2022154060738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v04.061.2022153091803.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v06.061.2022202150900.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v06.061.2022153125459.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v06.061.2022166151608.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v06.061.2022165193642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v04.061.2022147204353.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v05.061.2022215203800.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v06.061.2022161133402.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v05.061.2022160065528.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v05.061.2022168031256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h13v04.061.2022160091153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v06.061.2022165204322.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v05.061.2022168173357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v05.061.2022160074557.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v06.061.2022146035452.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v04.061.2022160063857.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v06.061.2022161133506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v05.061.2022171161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v04.061.2022165215345.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v06.061.2022153211515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v05.061.2022168174253.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v04.061.2022216052627.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v06.061.2022151161601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v05.061.2022169160344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v06.061.2022215233619.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v04.061.2022169160414.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v06.061.2022216003122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v06.061.2022152014317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v06.061.2022168151426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v06.061.2022164160305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v04.061.2022147201909.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v06.061.2022168001822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v05.061.2022161124337.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v06.061.2022158211027.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v04.061.2022202150359.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v06.061.2022165072106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v05.061.2022166154009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v04.061.2022158211115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v05.061.2022171160153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v06.061.2022152124844.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h13v04.061.2022152050602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v04.061.2022164173746.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v06.061.2022165195905.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v05.061.2022168152848.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v04.061.2022153230033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v06.061.2022165065712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v06.061.2022166153314.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v04.061.2022171161358.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v06.061.2022161131106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v05.061.2022164172325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v05.061.2022165225415.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v05.061.2022168172423.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v05.061.2022168025349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v06.061.2022158211828.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v04.061.2022168173053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v04.061.2022162021540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v05.061.2022216030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v04.061.2022164172811.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v06.061.2022215212351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v06.061.2022152220612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v05.061.2022169160942.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v04.061.2022161140208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v06.061.2022171151818.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v06.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v04.061.2022168033428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v04.061.2022171151922.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v04.061.2022160081327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v05.061.2022168001816.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v04.061.2022168002003.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v05.061.2022202150408.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v06.061.2022158220427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v05.061.2022153084625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v05.061.2022164175416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v04.061.2022152141617.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v06.061.2022159182449.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v05.061.2022153204112.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v06.061.2022151161736.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v05.061.2022146040113.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v05.061.2022169160425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v05.061.2022166160910.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v05.061.2022171174410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v05.061.2022158221132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h13v04.061.2022166180416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v04.061.2022158212527.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v04.061.2022151161351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v04.061.2022202150652.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v04.061.2022162010807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v05.061.2022154031246.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v06.061.2022168001851.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v04.061.2022216015823.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v04.061.2022151161906.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v06.061.2022162021504.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v04.061.2022202151028.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v06.061.2022168030220.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v05.061.2022162012243.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v06.061.2022168174923.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v05.061.2022147205502.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v04.061.2022158215957.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h13v04.061.2022148083352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v05.061.2022158212002.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v06.061.2022168165354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v06.061.2022165051616.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v04.061.2022165225511.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h13v04.061.2022216011046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v04.061.2022161140708.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v04.061.2022164162715.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v04.061.2022165082137.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v05.061.2022161140603.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v04.061.2022146050354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v04.061.2022162035938.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v04.061.2022159194525.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v06.061.2022169160549.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v06.061.2022153125643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v05.061.2022147215523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v04.061.2022147201926.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v06.061.2022151161214.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v05.061.2022159193439.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v04.061.2022160075223.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v04.061.2022215203426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v04.061.2022169160759.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v06.061.2022166153713.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v06.061.2022152022235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v05.061.2022147203812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v05.061.2022159180020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v05.061.2022152121656.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v04.061.2022165230140.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v06.061.2022165055649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v04.061.2022152220556.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v04.061.2022165202021.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v04.061.2022169160645.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v05.061.2022146033917.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v06.061.2022164162410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v06.061.2022169160433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v05.061.2022165220945.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v05.061.2022158212252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v05.061.2022160071156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h13v04.061.2022151162053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v06.061.2022151161416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v04.061.2022146034934.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v06.061.2022168141319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v04.061.2022147215712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v06.061.2022158215642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v05.061.2022152041846.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v06.061.2022171160428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v06.061.2022164160634.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v06.061.2022160071951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v05.061.2022146045521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v05.061.2022153122432.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v04.061.2022160051756.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v06.061.2022165214445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h13v04.061.2022159001357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v05.061.2022152014115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v05.061.2022162024506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v06.061.2022160065224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v06.061.2022216025520.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v04.061.2022158231134.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v05.061.2022151161531.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v04.061.2022169160935.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v05.061.2022159184300.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v05.061.2022165082044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v05.061.2022216071653.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v04.061.2022165065242.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v05.061.2022158221052.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v06.061.2022147204132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v06.061.2022146040126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v04.061.2022152014252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v06.061.2022162003933.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v04.061.2022171155354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v05.061.2022168002006.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v05.061.2022168030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v05.061.2022152134513.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v05.061.2022151161738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v06.061.2022171153157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v04.061.2022162025604.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v04.061.2022202150536.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v04.061.2022158220628.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v05.061.2022166152129.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v06.061.2022160064847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v04.061.2022146030108.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v05.061.2022162040109.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v06.061.2022147204000.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v06.061.2022166144709.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v06.061.2022202150433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v06.061.2022147203753.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v06.061.2022162022641.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v04.061.2022159184319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v06.061.2022152022413.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v06.061.2022168025826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v05.061.2022166164515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v04.061.2022153233438.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v06.061.2022166153208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v05.061.2022147203929.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v05.061.2022153114232.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v05.061.2022151161918.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v04.061.2022164160625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v06.061.2022152014235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v06.061.2022152211338.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v06.061.2022146033902.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h13v04.061.2022169161048.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h13v04.061.2022168153530.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v06.061.2022161132907.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v06.061.2022215202155.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v04.061.2022160070952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v05.061.2022153205724.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v06.061.2022202150716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v04.061.2022154032745.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v05.061.2022165203837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v05.061.2022166151313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v04.061.2022159184020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h13v04.061.2022162035643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v04.061.2022161130111.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v06.061.2022159175519.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v06.061.2022153115802.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v05.061.2022152115041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v05.061.2022158225932.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h13v04.061.2022146060649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v06.061.2022158215757.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h13v04.061.2022202151230.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v06.061.2022202150327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v05.061.2022160050421.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v04.061.2022165074211.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v04.061.2022165062521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v05.061.2022202150656.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023__water_mask.tif does not exist.\nstart to download files from NASA server to local\nThe provided start date was not recognized\nGranules found: 374\n Getting 374 granules, approx download size: 2.81 GB\nQUEUEING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]File MCD12Q1.A2001001.h08v04.061.2022146034934.hdf already downloadedFile MCD12Q1.A2001001.h12v05.061.2022146045521.hdf already downloaded\nFile MCD12Q1.A2001001.h09v06.061.2022146040126.hdf already downloaded\nFile MCD12Q1.A2001001.h07v05.061.2022146033046.hdf already downloaded\nFile MCD12Q1.A2001001.h08v05.061.2022146033917.hdf already downloaded\nFile MCD12Q1.A2001001.h08v06.061.2022146035452.hdf already downloaded\nFile MCD12Q1.A2001001.h13v04.061.2022146060649.hdf already downloaded\nFile MCD12Q1.A2001001.h11v04.061.2022146060256.hdf already downloaded\nFile MCD12Q1.A2001001.h09v05.061.2022146040113.hdf already downloaded\nFile MCD12Q1.A2001001.h10v04.061.2022146040552.hdf already downloaded\nFile MCD12Q1.A2001001.h07v06.061.2022146033902.hdf already downloaded\nFile MCD12Q1.A2001001.h12v04.061.2022146050354.hdf already downloadedFile MCD12Q1.A2001001.h11v06.061.2022146031055.hdf already downloaded\nFile MCD12Q1.A2001001.h10v05.061.2022146030344.hdf already downloaded\nFile MCD12Q1.A2001001.h10v06.061.2022146030552.hdf already downloaded\nFile MCD12Q1.A2002001.h11v06.061.2022147204132.hdf already downloaded\nFile MCD12Q1.A2002001.h12v05.061.2022147215523.hdf already downloaded\nFile MCD12Q1.A2001001.h09v04.061.2022146030108.hdf already downloadedFile MCD12Q1.A2001001.h11v05.061.2022146031022.hdf already downloadedFile MCD12Q1.A2002001.h07v05.061.2022147201847.hdf already downloaded\nFile MCD12Q1.A2002001.h07v06.061.2022147203753.hdf already downloadedFile MCD12Q1.A2002001.h09v06.061.2022147202005.hdf already downloaded\nFile MCD12Q1.A2002001.h10v06.061.2022147204000.hdf already downloaded\nFile MCD12Q1.A2002001.h10v04.061.2022147204353.hdf already downloaded\nFile MCD12Q1.A2002001.h08v04.061.2022147201909.hdf already downloaded\nFile MCD12Q1.A2002001.h10v05.061.2022147203812.hdf already downloaded\nFile MCD12Q1.A2002001.h08v05.061.2022147203929.hdf already downloaded\nFile MCD12Q1.A2002001.h13v04.061.2022148083352.hdf already downloaded\nFile MCD12Q1.A2002001.h09v05.061.2022147203952.hdf already downloaded\nFile MCD12Q1.A2002001.h11v05.061.2022147205502.hdf already downloaded\nFile MCD12Q1.A2002001.h12v04.061.2022147215712.hdf already downloaded\nFile MCD12Q1.A2002001.h08v06.061.2022147204405.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 12435.74it/s]\nPROCESSING TASKS | :   0%|          | 0/374 [00:00<?, ?it/s]File MCD12Q1.A2002001.h09v04.061.2022147201926.hdf already downloaded\nFile MCD12Q1.A2003001.h11v04.061.2022151161716.hdf already downloaded\nFile MCD12Q1.A2003001.h09v06.061.2022151161416.hdf already downloaded\nFile MCD12Q1.A2002001.h11v04.061.2022147204553.hdf already downloaded\nFile MCD12Q1.A2003001.h07v05.061.2022151161122.hdf already downloaded\nFile MCD12Q1.A2003001.h12v05.061.2022151161918.hdf already downloadedFile MCD12Q1.A2003001.h08v04.061.2022151161158.hdf already downloaded\nFile MCD12Q1.A2003001.h10v04.061.2022151161523.hdf already downloaded\nFile MCD12Q1.A2003001.h11v06.061.2022151161736.hdf already downloadedFile MCD12Q1.A2003001.h13v04.061.2022151162053.hdf already downloaded\nFile MCD12Q1.A2003001.h08v05.061.2022151161201.hdf already downloaded\nFile MCD12Q1.A2003001.h10v06.061.2022151161601.hdf already downloaded\nFile MCD12Q1.A2003001.h09v05.061.2022151161409.hdf already downloadedFile MCD12Q1.A2003001.h08v06.061.2022151161214.hdf already downloadedFile MCD12Q1.A2003001.h10v05.061.2022151161531.hdf already downloaded\nFile MCD12Q1.A2004001.h09v06.061.2022152022342.hdf already downloaded\nFile MCD12Q1.A2003001.h12v04.061.2022151161906.hdf already downloaded\nFile MCD12Q1.A2003001.h07v06.061.2022151161138.hdf already downloaded\nFile MCD12Q1.A2004001.h07v06.061.2022152014317.hdf already downloaded\nFile MCD12Q1.A2004001.h07v05.061.2022152014117.hdf already downloaded\nFile MCD12Q1.A2004001.h12v05.061.2022152041846.hdf already downloadedFile MCD12Q1.A2003001.h09v04.061.2022151161351.hdf already downloadedFile MCD12Q1.A2004001.h10v06.061.2022152022413.hdf already downloaded\nFile MCD12Q1.A2004001.h13v04.061.2022152050602.hdf already downloaded\nFile MCD12Q1.A2004001.h10v05.061.2022152023406.hdf already downloaded\nFile MCD12Q1.A2004001.h11v04.061.2022152014252.hdf already downloaded\nFile MCD12Q1.A2004001.h11v06.061.2022152022235.hdf already downloadedFile MCD12Q1.A2004001.h12v04.061.2022152043252.hdf already downloaded\nFile MCD12Q1.A2004001.h11v05.061.2022152014115.hdf already downloaded\nFile MCD12Q1.A2004001.h10v04.061.2022152022837.hdf already downloaded\nFile MCD12Q1.A2004001.h08v04.061.2022152013916.hdf already downloaded\nFile MCD12Q1.A2004001.h09v04.061.2022152023313.hdf already downloaded\nFile MCD12Q1.A2005001.h08v04.061.2022152141617.hdf already downloaded\nFile MCD12Q1.A2004001.h08v05.061.2022152014143.hdf already downloaded\nFile MCD12Q1.A2004001.h08v06.061.2022152014235.hdf already downloadedFile MCD12Q1.A2004001.h09v05.061.2022152023143.hdf already downloaded\nFile MCD12Q1.A2003001.h11v05.061.2022151161738.hdf already downloadedFile MCD12Q1.A2005001.h07v05.061.2022152115041.hdf already downloaded\nFile MCD12Q1.A2005001.h07v06.061.2022152124844.hdf already downloaded\nFile MCD12Q1.A2005001.h11v06.061.2022152130904.hdf already downloaded\nFile MCD12Q1.A2005001.h09v05.061.2022153084625.hdf already downloaded\nFile MCD12Q1.A2005001.h09v06.061.2022152220612.hdf already downloaded\nFile MCD12Q1.A2005001.h10v06.061.2022152132706.hdf already downloaded\nFile MCD12Q1.A2005001.h11v04.061.2022152220556.hdf already downloaded\nFile MCD12Q1.A2005001.h10v04.061.2022153091803.hdf already downloadedFile MCD12Q1.A2005001.h12v05.061.2022152134513.hdf already downloaded\nFile MCD12Q1.A2005001.h12v04.061.2022152140219.hdf already downloaded\nFile MCD12Q1.A2005001.h10v05.061.2022153083959.hdf already downloaded\nFile MCD12Q1.A2005001.h08v05.061.2022152121656.hdf already downloadedFile MCD12Q1.A2005001.h11v05.061.2022152215559.hdf already downloaded\nFile MCD12Q1.A2006001.h11v06.061.2022202150900.hdf already downloaded\nFile MCD12Q1.A2006001.h09v06.061.2022202150602.hdf already downloadedFile MCD12Q1.A2006001.h08v04.061.2022202150359.hdf already downloaded\nFile MCD12Q1.A2006001.h12v04.061.2022202151028.hdf already downloaded\nFile MCD12Q1.A2006001.h12v05.061.2022202151035.hdf already downloaded\nFile MCD12Q1.A2005001.h09v04.061.2022153082156.hdf already downloaded\nFile MCD12Q1.A2005001.h13v04.061.2022153191607.hdf already downloaded\nFile MCD12Q1.A2005001.h08v06.061.2022152211338.hdf already downloaded\nFile MCD12Q1.A2006001.h13v04.061.2022202151230.hdf already downloaded\nFile MCD12Q1.A2006001.h10v05.061.2022202150656.hdf already downloaded\nFile MCD12Q1.A2006001.h07v06.061.2022202150327.hdf already downloaded\nFile MCD12Q1.A2006001.h07v05.061.2022202150325.hdf already downloaded\nFile MCD12Q1.A2006001.h10v06.061.2022202150716.hdf already downloaded\nFile MCD12Q1.A2006001.h09v04.061.2022202150536.hdf already downloaded\nFile MCD12Q1.A2006001.h08v05.061.2022202150408.hdf already downloaded\nFile MCD12Q1.A2007001.h13v04.061.2022154052639.hdf already downloaded\nFile MCD12Q1.A2006001.h11v05.061.2022202150843.hdf already downloaded\nFile MCD12Q1.A2006001.h11v04.061.2022202150826.hdf already downloaded\nFile MCD12Q1.A2006001.h09v05.061.2022202150538.hdf already downloaded\nFile MCD12Q1.A2006001.h08v06.061.2022202150433.hdf already downloaded\nFile MCD12Q1.A2007001.h09v06.061.2022153223012.hdf already downloaded\nFile MCD12Q1.A2007001.h08v06.061.2022153211515.hdf already downloaded\nFile MCD12Q1.A2007001.h08v05.061.2022153205724.hdf already downloaded\nFile MCD12Q1.A2007001.h07v05.061.2022153114232.hdf already downloaded\nFile MCD12Q1.A2007001.h12v05.061.2022153122432.hdf already downloaded\nFile MCD12Q1.A2007001.h10v04.061.2022154052218.hdf already downloaded\nFile MCD12Q1.A2007001.h11v06.061.2022153125459.hdf already downloaded\nFile MCD12Q1.A2007001.h08v04.061.2022153230033.hdf already downloadedFile MCD12Q1.A2007001.h09v05.061.2022154060738.hdf already downloadedFile MCD12Q1.A2007001.h12v04.061.2022153233438.hdf already downloadedFile MCD12Q1.A2007001.h11v04.061.2022154032745.hdf already downloaded\nFile MCD12Q1.A2007001.h11v05.061.2022153204112.hdf already downloaded\nFile MCD12Q1.A2006001.h10v04.061.2022202150652.hdf already downloadedFile MCD12Q1.A2007001.h10v05.061.2022154031246.hdf already downloaded\nFile MCD12Q1.A2007001.h09v04.061.2022154033740.hdf already downloaded\nFile MCD12Q1.A2008001.h08v04.061.2022158211115.hdf already downloaded\nFile MCD12Q1.A2008001.h08v05.061.2022158212252.hdf already downloadedFile MCD12Q1.A2008001.h09v04.061.2022158215957.hdf already downloaded\nFile MCD12Q1.A2007001.h07v06.061.2022153115802.hdf already downloaded\nFile MCD12Q1.A2007001.h10v06.061.2022153125643.hdf already downloadedFile MCD12Q1.A2008001.h08v06.061.2022158211828.hdf already downloaded\nFile MCD12Q1.A2008001.h11v05.061.2022158212002.hdf already downloaded\nFile MCD12Q1.A2008001.h10v06.061.2022158215757.hdf already downloaded\nFile MCD12Q1.A2008001.h11v04.061.2022158212527.hdf already downloaded\nFile MCD12Q1.A2008001.h09v05.061.2022158221132.hdf already downloaded\nFile MCD12Q1.A2008001.h07v06.061.2022158211027.hdf already downloaded\nFile MCD12Q1.A2008001.h09v06.061.2022158220427.hdf already downloaded\nFile MCD12Q1.A2008001.h12v04.061.2022158231134.hdf already downloaded\nFile MCD12Q1.A2008001.h11v06.061.2022158215642.hdf already downloadedFile MCD12Q1.A2008001.h10v05.061.2022158221052.hdf already downloaded\nFile MCD12Q1.A2008001.h12v05.061.2022158225932.hdf already downloaded\nFile MCD12Q1.A2008001.h10v04.061.2022158220628.hdf already downloaded\nFile MCD12Q1.A2009001.h07v06.061.2022159174334.hdf already downloadedFile MCD12Q1.A2009001.h07v05.061.2022159174239.hdf already downloadedFile MCD12Q1.A2008001.h13v04.061.2022159001357.hdf already downloaded\nFile MCD12Q1.A2009001.h08v04.061.2022159174334.hdf already downloaded\nFile MCD12Q1.A2009001.h09v04.061.2022159175354.hdf already downloadedFile MCD12Q1.A2009001.h11v04.061.2022159184319.hdf already downloaded\nFile MCD12Q1.A2009001.h11v06.061.2022159182449.hdf already downloaded\nFile MCD12Q1.A2008001.h07v05.061.2022158211012.hdf already downloaded\nFile MCD12Q1.A2009001.h12v05.061.2022159193439.hdf already downloaded\nFile MCD12Q1.A2009001.h08v06.061.2022159175519.hdf already downloaded\nFile MCD12Q1.A2009001.h10v06.061.2022159182554.hdf already downloaded\nFile MCD12Q1.A2009001.h10v04.061.2022159184020.hdf already downloadedFile MCD12Q1.A2009001.h09v06.061.2022159182729.hdf already downloaded\nFile MCD12Q1.A2009001.h13v04.061.2022159204454.hdf already downloaded\nFile MCD12Q1.A2009001.h08v05.061.2022159175354.hdf already downloaded\nFile MCD12Q1.A2009001.h09v05.061.2022159180020.hdf already downloadedFile MCD12Q1.A2009001.h11v05.061.2022159183224.hdf already downloaded\nFile MCD12Q1.A2010001.h09v04.061.2022160063857.hdf already downloaded\nFile MCD12Q1.A2010001.h11v06.061.2022160071951.hdf already downloaded\nFile MCD12Q1.A2010001.h08v04.061.2022160051756.hdf already downloaded\nFile MCD12Q1.A2009001.h10v05.061.2022159184300.hdf already downloaded\nFile MCD12Q1.A2010001.h07v06.061.2022160051624.hdf already downloaded\nFile MCD12Q1.A2009001.h12v04.061.2022159194525.hdf already downloadedFile MCD12Q1.A2010001.h08v05.061.2022160064327.hdf already downloaded\nFile MCD12Q1.A2010001.h12v05.061.2022160074557.hdf already downloadedFile MCD12Q1.A2010001.h09v06.061.2022160064847.hdf already downloaded\nFile MCD12Q1.A2010001.h10v06.061.2022160064326.hdf already downloaded\nFile MCD12Q1.A2010001.h10v05.061.2022160071156.hdf already downloaded\nFile MCD12Q1.A2010001.h11v05.061.2022160065528.hdf already downloaded\nFile MCD12Q1.A2010001.h08v06.061.2022160065224.hdf already downloadedFile MCD12Q1.A2010001.h12v04.061.2022160081327.hdf already downloaded\nFile MCD12Q1.A2010001.h11v04.061.2022160070952.hdf already downloadedFile MCD12Q1.A2010001.h13v04.061.2022160091153.hdf already downloaded\nFile MCD12Q1.A2010001.h10v04.061.2022160075223.hdf already downloaded\nFile MCD12Q1.A2010001.h07v05.061.2022160050421.hdf already downloaded\nFile MCD12Q1.A2011001.h07v05.061.2022161124337.hdf already downloadedFile MCD12Q1.A2010001.h09v05.061.2022160070317.hdf already downloaded\nFile MCD12Q1.A2011001.h08v04.061.2022161124637.hdf already downloaded\nFile MCD12Q1.A2011001.h09v04.061.2022161130111.hdf already downloaded\nFile MCD12Q1.A2011001.h12v05.061.2022161143539.hdf already downloaded\nFile MCD12Q1.A2011001.h11v04.061.2022161140708.hdf already downloaded\nFile MCD12Q1.A2011001.h08v05.061.2022161130812.hdf already downloaded\nFile MCD12Q1.A2011001.h08v06.061.2022161131106.hdf already downloaded\nFile MCD12Q1.A2011001.h07v06.061.2022161124349.hdf already downloaded\nFile MCD12Q1.A2011001.h11v05.061.2022161134833.hdf already downloaded\nFile MCD12Q1.A2011001.h09v06.061.2022161133402.hdf already downloaded\nFile MCD12Q1.A2011001.h10v05.061.2022161140603.hdf already downloaded\nFile MCD12Q1.A2011001.h11v06.061.2022161132907.hdf already downloadedFile MCD12Q1.A2011001.h09v05.061.2022161131738.hdf already downloadedFile MCD12Q1.A2011001.h10v04.061.2022161140208.hdf already downloaded\nFile MCD12Q1.A2012001.h10v04.061.2022162025604.hdf already downloaded\nFile MCD12Q1.A2011001.h12v04.061.2022161144809.hdf already downloaded\nFile MCD12Q1.A2012001.h13v04.061.2022162035643.hdf already downloaded\nFile MCD12Q1.A2012001.h08v04.061.2022162010807.hdf already downloaded\nFile MCD12Q1.A2011001.h13v04.061.2022161154912.hdf already downloadedFile MCD12Q1.A2012001.h11v04.061.2022162040638.hdf already downloaded\nFile MCD12Q1.A2012001.h07v06.061.2022162003933.hdf already downloaded\nFile MCD12Q1.A2012001.h10v06.061.2022162031538.hdf already downloadedFile MCD12Q1.A2012001.h12v04.061.2022162035938.hdf already downloaded\nFile MCD12Q1.A2012001.h10v05.061.2022162040109.hdf already downloadedFile MCD12Q1.A2012001.h11v06.061.2022162025807.hdf already downloaded\nFile MCD12Q1.A2012001.h09v05.061.2022162023037.hdf already downloaded\nFile MCD12Q1.A2012001.h08v06.061.2022162021504.hdf already downloaded\nFile MCD12Q1.A2012001.h09v06.061.2022162022641.hdf already downloaded\nFile MCD12Q1.A2012001.h11v05.061.2022162024041.hdf already downloaded\nFile MCD12Q1.A2012001.h08v05.061.2022162022037.hdf already downloaded\nFile MCD12Q1.A2013001.h07v06.061.2022164160305.hdf already downloadedFile MCD12Q1.A2013001.h08v04.061.2022164160625.hdf already downloaded\nFile MCD12Q1.A2013001.h08v06.061.2022164162410.hdf already downloaded\nFile MCD12Q1.A2012001.h09v04.061.2022162021540.hdf already downloaded\nFile MCD12Q1.A2012001.h12v05.061.2022162024506.hdf already downloaded\nFile MCD12Q1.A2013001.h07v05.061.2022164160213.hdf already downloaded\nFile MCD12Q1.A2013001.h09v04.061.2022164162715.hdf already downloaded\nFile MCD12Q1.A2013001.h11v06.061.2022164160634.hdf already downloaded\nFile MCD12Q1.A2013001.h12v05.061.2022164175416.hdf already downloaded\nFile MCD12Q1.A2013001.h09v05.061.2022164172325.hdf already downloaded\nFile MCD12Q1.A2013001.h11v04.061.2022164173746.hdf already downloaded\nFile MCD12Q1.A2013001.h12v04.061.2022164182417.hdf already downloadedFile MCD12Q1.A2013001.h10v04.061.2022164172811.hdf already downloaded\nFile MCD12Q1.A2013001.h09v06.061.2022164164813.hdf already downloaded\nFile MCD12Q1.A2013001.h10v06.061.2022164164813.hdf already downloaded\nFile MCD12Q1.A2011001.h10v06.061.2022161133506.hdf already downloaded\nFile MCD12Q1.A2013001.h10v05.061.2022164173115.hdf already downloaded\nFile MCD12Q1.A2013001.h11v05.061.2022164170219.hdf already downloaded\nFile MCD12Q1.A2014001.h11v06.061.2022165055649.hdf already downloaded\nFile MCD12Q1.A2014001.h09v04.061.2022165065242.hdf already downloaded\nFile MCD12Q1.A2014001.h08v06.061.2022165065712.hdf already downloaded\nFile MCD12Q1.A2013001.h13v04.061.2022164191941.hdf already downloaded\nFile MCD12Q1.A2013001.h08v05.061.2022164163351.hdf already downloaded\nFile MCD12Q1.A2014001.h08v05.061.2022165072154.hdf already downloaded\nFile MCD12Q1.A2012001.h07v05.061.2022162012243.hdf already downloadedFile MCD12Q1.A2014001.h07v05.061.2022165051115.hdf already downloaded\nFile MCD12Q1.A2014001.h11v05.061.2022165075505.hdf already downloadedFile MCD12Q1.A2014001.h07v06.061.2022165051616.hdf already downloaded\nFile MCD12Q1.A2014001.h09v06.061.2022165072106.hdf already downloaded\nFile MCD12Q1.A2014001.h12v04.061.2022165083049.hdf already downloaded\nFile MCD12Q1.A2014001.h11v04.061.2022165074211.hdf already downloaded\nFile MCD12Q1.A2014001.h10v04.061.2022165082137.hdf already downloaded\nFile MCD12Q1.A2014001.h08v04.061.2022165062521.hdf already downloaded\nFile MCD12Q1.A2014001.h13v04.061.2022165084441.hdf already downloaded\nFile MCD12Q1.A2014001.h09v05.061.2022165081147.hdf already downloaded\nFile MCD12Q1.A2014001.h12v05.061.2022165071913.hdf already downloaded\nFile MCD12Q1.A2014001.h10v05.061.2022165082044.hdf already downloaded\nFile MCD12Q1.A2015001.h11v06.061.2022165195905.hdf already downloaded\nFile MCD12Q1.A2015001.h08v04.061.2022165193640.hdf already downloaded\nFile MCD12Q1.A2014001.h10v06.061.2022165072912.hdf already downloaded\nFile MCD12Q1.A2015001.h09v04.061.2022165202021.hdf already downloaded\nFile MCD12Q1.A2015001.h11v04.061.2022165215345.hdf already downloaded\nFile MCD12Q1.A2015001.h10v06.061.2022165214445.hdf already downloaded\nFile MCD12Q1.A2015001.h07v06.061.2022165193642.hdf already downloaded\nFile MCD12Q1.A2015001.h09v06.061.2022165204539.hdf already downloaded\nFile MCD12Q1.A2015001.h09v05.061.2022165224410.hdf already downloaded\nFile MCD12Q1.A2015001.h12v05.061.2022165203837.hdf already downloadedFile MCD12Q1.A2015001.h07v05.061.2022165193841.hdf already downloaded\nFile MCD12Q1.A2015001.h11v05.061.2022165220945.hdf already downloaded\nFile MCD12Q1.A2015001.h08v05.061.2022165214015.hdf already downloaded\nFile MCD12Q1.A2015001.h08v06.061.2022165204322.hdf already downloadedFile MCD12Q1.A2015001.h10v04.061.2022165225511.hdf already downloaded\nFile MCD12Q1.A2015001.h12v04.061.2022165230140.hdf already downloaded\nFile MCD12Q1.A2016001.h11v04.061.2022166160447.hdf already downloaded\nFile MCD12Q1.A2015001.h13v04.061.2022165221239.hdf already downloadedFile MCD12Q1.A2015001.h10v05.061.2022165225415.hdf already downloadedFile MCD12Q1.A2016001.h10v05.061.2022166160910.hdf already downloaded\nFile MCD12Q1.A2016001.h10v04.061.2022166160540.hdf already downloaded\nFile MCD12Q1.A2016001.h12v04.061.2022166171445.hdf already downloadedFile MCD12Q1.A2016001.h09v05.061.2022166152129.hdf already downloaded\nFile MCD12Q1.A2016001.h09v04.061.2022166151038.hdf already downloaded\nFile MCD12Q1.A2016001.h10v06.061.2022166153713.hdf already downloaded\nFile MCD12Q1.A2016001.h07v06.061.2022166144709.hdf already downloaded\nFile MCD12Q1.A2016001.h08v04.061.2022166145044.hdf already downloaded\nFile MCD12Q1.A2016001.h12v05.061.2022166164515.hdf already downloaded\nFile MCD12Q1.A2016001.h11v05.061.2022166154009.hdf already downloadedFile MCD12Q1.A2016001.h11v06.061.2022166153208.hdf already downloaded\nFile MCD12Q1.A2016001.h08v06.061.2022166151608.hdf already downloaded\nFile MCD12Q1.A2016001.h09v06.061.2022166153314.hdf already downloaded\nFile MCD12Q1.A2016001.h08v05.061.2022166151313.hdf already downloaded\nFile MCD12Q1.A2016001.h13v04.061.2022166180416.hdf already downloaded\nFile MCD12Q1.A2017001.h07v05.061.2022168001816.hdf already downloaded\nFile MCD12Q1.A2017001.h08v04.061.2022168001839.hdf already downloaded\nFile MCD12Q1.A2017001.h07v06.061.2022168001822.hdf already downloaded\nFile MCD12Q1.A2017001.h08v05.061.2022168001850.hdf already downloaded\nFile MCD12Q1.A2017001.h11v05.061.2022168031256.hdf already downloaded\nFile MCD12Q1.A2017001.h11v04.061.2022168022951.hdf already downloadedFile MCD12Q1.A2017001.h10v06.061.2022168025826.hdf already downloaded\nFile MCD12Q1.A2017001.h09v04.061.2022168002003.hdf already downloaded\nFile MCD12Q1.A2017001.h08v06.061.2022168001851.hdf already downloaded\nFile MCD12Q1.A2017001.h13v04.061.2022168022420.hdf already downloaded\nFile MCD12Q1.A2017001.h10v04.061.2022168030221.hdf already downloaded\nFile MCD12Q1.A2017001.h12v04.061.2022168033428.hdf already downloaded\nFile MCD12Q1.A2017001.h10v05.061.2022168025349.hdf already downloaded\nFile MCD12Q1.A2017001.h11v06.061.2022168025451.hdf already downloaded\nFile MCD12Q1.A2017001.h12v05.061.2022168030750.hdf already downloaded\nFile MCD12Q1.A2017001.h09v06.061.2022168030220.hdf already downloaded\nFile MCD12Q1.A2018001.h07v06.061.2022168141319.hdf already downloaded\nFile MCD12Q1.A2018001.h08v05.061.2022168152848.hdf already downloaded\nFile MCD12Q1.A2018001.h13v04.061.2022168153530.hdf already downloaded\nFile MCD12Q1.A2018001.h09v05.061.2022168162537.hdf already downloaded\nFile MCD12Q1.A2016001.h07v05.061.2022166145612.hdf already downloadedFile MCD12Q1.A2018001.h08v04.061.2022168135023.hdf already downloaded\nFile MCD12Q1.A2017001.h09v05.061.2022168002006.hdf already downloaded\nFile MCD12Q1.A2018001.h08v06.061.2022168151426.hdf already downloaded\nFile MCD12Q1.A2018001.h09v04.061.2022168150425.hdf already downloaded\nFile MCD12Q1.A2018001.h12v05.061.2022168174253.hdf already downloaded\nFile MCD12Q1.A2018001.h07v05.061.2022168144825.hdf already downloaded\nFile MCD12Q1.A2018001.h12v04.061.2022168173053.hdf already downloaded\nFile MCD12Q1.A2018001.h10v05.061.2022168173357.hdf already downloaded\nFile MCD12Q1.A2018001.h11v06.061.2022168165354.hdf already downloaded\nFile MCD12Q1.A2018001.h10v06.061.2022168170157.hdf already downloaded\nFile MCD12Q1.A2018001.h11v05.061.2022168172423.hdf already downloaded\nFile MCD12Q1.A2018001.h11v04.061.2022168164651.hdf already downloaded\nFile MCD12Q1.A2018001.h09v06.061.2022168174923.hdf already downloaded\nFile MCD12Q1.A2018001.h10v04.061.2022168175454.hdf already downloaded\nFile MCD12Q1.A2019001.h09v04.061.2022169160534.hdf already downloadedFile MCD12Q1.A2019001.h10v05.061.2022169160646.hdf already downloaded\nFile MCD12Q1.A2019001.h09v06.061.2022169160549.hdf already downloaded\nFile MCD12Q1.A2019001.h09v05.061.2022169160540.hdf already downloaded\nFile MCD12Q1.A2019001.h10v06.061.2022169160700.hdf already downloaded\nFile MCD12Q1.A2019001.h10v04.061.2022169160645.hdf already downloaded\nFile MCD12Q1.A2019001.h11v04.061.2022169160759.hdf already downloaded\nFile MCD12Q1.A2019001.h13v04.061.2022169161048.hdf already downloaded\nFile MCD12Q1.A2019001.h12v05.061.2022169160942.hdf already downloaded\nFile MCD12Q1.A2019001.h12v04.061.2022169160935.hdf already downloaded\nFile MCD12Q1.A2019001.h08v06.061.2022169160433.hdf already downloaded\nFile MCD12Q1.A2019001.h07v06.061.2022169160348.hdf already downloaded\nFile MCD12Q1.A2019001.h08v04.061.2022169160414.hdf already downloaded\nFile MCD12Q1.A2020001.h10v06.061.2022171153457.hdf already downloaded\nFile MCD12Q1.A2019001.h11v06.061.2022169160822.hdf already downloaded\nFile MCD12Q1.A2019001.h07v05.061.2022169160344.hdf already downloaded\nFile MCD12Q1.A2019001.h11v05.061.2022169160820.hdf already downloaded\nFile MCD12Q1.A2020001.h11v06.061.2022171151818.hdf already downloaded\nFile MCD12Q1.A2019001.h08v05.061.2022169160425.hdf already downloaded\nFile MCD12Q1.A2020001.h07v06.061.2022171160856.hdf already downloaded\nFile MCD12Q1.A2020001.h07v05.061.2022171160801.hdf already downloaded\nFile MCD12Q1.A2020001.h09v04.061.2022171155354.hdf already downloaded\nFile MCD12Q1.A2020001.h08v06.061.2022171160428.hdf already downloaded\nFile MCD12Q1.A2020001.h11v05.061.2022171162126.hdf already downloaded\nFile MCD12Q1.A2020001.h09v06.061.2022171153157.hdf already downloaded\nFile MCD12Q1.A2020001.h12v05.061.2022171174410.hdf already downloaded\nFile MCD12Q1.A2020001.h08v05.061.2022171160153.hdf already downloadedFile MCD12Q1.A2020001.h08v04.061.2022171151922.hdf already downloaded\nFile MCD12Q1.A2020001.h10v05.061.2022171161201.hdf already downloaded\nFile MCD12Q1.A2020001.h10v04.061.2022171161358.hdf already downloaded\nFile MCD12Q1.A2020001.h12v04.061.2022171175121.hdf already downloaded\nFile MCD12Q1.A2020001.h11v04.061.2022171170901.hdf already downloaded\nFile MCD12Q1.A2020001.h13v04.061.2022171190218.hdf already downloaded\nFile MCD12Q1.A2020001.h09v05.061.2022171161548.hdf already downloaded\nFile MCD12Q1.A2021001.h13v04.061.2022216011046.hdf already downloaded\nFile MCD12Q1.A2021001.h09v05.061.2022216063601.hdf already downloaded\nFile MCD12Q1.A2021001.h08v05.061.2022216030750.hdf already downloaded\nFile MCD12Q1.A2021001.h08v06.061.2022216025520.hdf already downloaded\nFile MCD12Q1.A2021001.h09v06.061.2022216003122.hdf already downloaded\nFile MCD12Q1.A2021001.h11v06.061.2022215202155.hdf already downloaded\nFile MCD12Q1.A2021001.h12v05.061.2022215202352.hdf already downloaded\nFile MCD12Q1.A2021001.h07v06.061.2022215212351.hdf already downloaded\nFile MCD12Q1.A2021001.h07v05.061.2022215203800.hdf already downloaded\nFile MCD12Q1.A2021001.h10v04.061.2022216052627.hdf already downloaded\nFile MCD12Q1.A2021001.h08v04.061.2022215203426.hdf already downloaded\nFile MCD12Q1.A2021001.h11v05.061.2022216013427.hdf already downloaded\nFile MCD12Q1.A2021001.h10v06.061.2022215233619.hdf already downloaded\nFile MCD12Q1.A2021001.h09v04.061.2022216015823.hdf already downloaded\nFile MCD12Q1.A2021001.h10v05.061.2022216071653.hdf already downloaded\nFile MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2021001.h12v04.061.2022216040516.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2021001.h11v04.061.2022216043057.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 374/374 [00:00<00:00, 3790.75it/s]\nCOLLECTING RESULTS | :   0%|          | 0/374 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 374/374 [00:00<00:00, 83275.98it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v05.061.2022153083959.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v06.061.2022146031055.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v05.061.2022154060738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v04.061.2022153091803.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v06.061.2022202150900.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v06.061.2022146030552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v04.061.2022168001839.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v06.061.2022153125459.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v05.061.2022152014117.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v06.061.2022166151608.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v04.061.2022165193640.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v05.061.2022168162537.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v04.061.2022147204553.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v06.061.2022165193642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v04.061.2022147204353.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v05.061.2022215203800.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v06.061.2022153223012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v06.061.2022161133402.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v05.061.2022160065528.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v05.061.2022168031256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h13v04.061.2022160091153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v06.061.2022165204322.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v05.061.2022168173357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v04.061.2022152022837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v05.061.2022160074557.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v06.061.2022146035452.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v05.061.2022169160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v04.061.2022160063857.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v06.061.2022147204405.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v06.061.2022161133506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v05.061.2022171161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v04.061.2022165215345.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v06.061.2022153211515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v05.061.2022146031022.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v05.061.2022162024041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v05.061.2022168174253.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v04.061.2022216052627.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v06.061.2022151161601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v05.061.2022169160344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v06.061.2022215233619.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v06.061.2022159182729.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v04.061.2022169160414.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v06.061.2022216003122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h07v06.061.2022152014317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v06.061.2022168151426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v06.061.2022164160305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v05.061.2022147203952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v05.061.2022165072154.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v04.061.2022147201909.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v04.061.2022165083049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v06.061.2022168001822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v05.061.2022161124337.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v05.061.2022215202352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v06.061.2022158211027.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v04.061.2022202150359.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v06.061.2022165072106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v05.061.2022166154009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v04.061.2022158211115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v05.061.2022171160153.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v06.061.2022152124844.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h13v04.061.2022152050602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v04.061.2022164173746.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v06.061.2022165195905.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v05.061.2022168152848.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v04.061.2022153230033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v06.061.2022165065712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v05.061.2022171160801.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h07v06.061.2022169160348.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h13v04.061.2022168022420.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v06.061.2022166153314.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v05.061.2022151161201.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v04.061.2022171161358.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h13v04.061.2022159204454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v05.061.2022171162126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v06.061.2022161131106.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v05.061.2022164172325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v04.061.2022164182417.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v05.061.2022165225415.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v05.061.2022171161548.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v06.061.2022162031538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v04.061.2022202150826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v05.061.2022168172423.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v05.061.2022168025349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v04.061.2022153082156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v05.061.2022168001850.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v04.061.2022166171445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v05.061.2022152023143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v06.061.2022158211828.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v05.061.2022164163351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h12v04.061.2022168173053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v04.061.2022162021540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v05.061.2022216030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v04.061.2022164172811.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h07v06.061.2022215212351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v06.061.2022152220612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v05.061.2022169160942.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v04.061.2022161140208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v05.061.2022216063601.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v06.061.2022171151818.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v06.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v04.061.2022168033428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v04.061.2022171151922.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v04.061.2022166160447.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v06.061.2022160064326.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v06.061.2022165204539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h12v04.061.2022160081327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h07v05.061.2022168001816.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v04.061.2022168002003.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h13v04.061.2022164191941.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h07v05.061.2022158211012.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h07v05.061.2022165193841.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v05.061.2022202150408.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v06.061.2022158220427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v04.061.2022162040638.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h09v05.061.2022153084625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h12v05.061.2022164175416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v04.061.2022152141617.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v06.061.2022202150602.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v06.061.2022159182449.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v05.061.2022153204112.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v05.061.2022164170219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v06.061.2022151161736.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v05.061.2022146040113.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v05.061.2022169160425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v04.061.2022166160540.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v05.061.2022160070317.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v05.061.2022166160910.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v05.061.2022165051115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v05.061.2022171174410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v05.061.2022158221132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v05.061.2022202150538.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h13v04.061.2022166180416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v04.061.2022158212527.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v04.061.2022151161351.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v04.061.2022202150652.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v04.061.2022162010807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v05.061.2022154031246.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h08v06.061.2022168001851.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h11v04.061.2022171170901.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h09v04.061.2022216015823.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v04.061.2022151161906.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h07v05.061.2022164160213.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v06.061.2022162021504.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h10v06.061.2022171153457.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h07v05.061.2022159174239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h12v05.061.2022165071913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v04.061.2022202151028.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v04.061.2022146040552.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v06.061.2022159182554.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h08v05.061.2022165214015.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v06.061.2022152022342.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v06.061.2022168030220.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v05.061.2022162012243.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h13v04.061.2022153191607.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v06.061.2022168174923.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h13v04.061.2022165084441.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h08v04.061.2022168135023.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v05.061.2022147205502.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h09v04.061.2022158215957.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h09v04.061.2022168150425.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v05.061.2022165081147.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h13v04.061.2022148083352.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v05.061.2022158212002.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v04.061.2022161124637.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v06.061.2022168165354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h07v06.061.2022165051616.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h11v06.061.2022162025807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v04.061.2022165225511.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h13v04.061.2022216011046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v05.061.2022152215559.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h08v05.061.2022162022037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v04.061.2022152043252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v04.061.2022161144809.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v04.061.2022161140708.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v05.061.2022169160646.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v05.061.2022216013427.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v04.061.2022164162715.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v06.061.2022152130904.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v04.061.2022166145044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v04.061.2022165082137.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h10v05.061.2022161140603.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v04.061.2022146050354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v04.061.2022162035938.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h08v05.061.2022161130812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v04.061.2022159194525.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v06.061.2022169160549.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v04.061.2022159174334.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v06.061.2022153125643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v05.061.2022147215523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v05.061.2022147201847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v05.061.2022165075505.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v06.061.2022151161138.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v04.061.2022147201926.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v06.061.2022151161214.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h12v05.061.2022159193439.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v04.061.2022160075223.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v04.061.2022215203426.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v04.061.2022169160759.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h10v06.061.2022166153713.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v06.061.2022152022235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v05.061.2022147203812.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v05.061.2022159180020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v05.061.2022152121656.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v06.061.2022160051624.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v04.061.2022165230140.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v05.061.2022160064327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h13v04.061.2022171190218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v06.061.2022165055649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h11v04.061.2022152220556.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h12v05.061.2022202151035.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v04.061.2022165202021.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v04.061.2022169160645.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v05.061.2022146033917.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v06.061.2022164162410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h08v06.061.2022169160433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h11v05.061.2022165220945.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h08v05.061.2022158212252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h10v05.061.2022160071156.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h13v04.061.2022151162053.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v06.061.2022151161416.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h08v04.061.2022146034934.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h12v04.061.2022171175121.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v06.061.2022168141319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h12v04.061.2022147215712.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h12v04.061.2022216040516.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h11v06.061.2022158215642.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v05.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h12v05.061.2022152041846.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h08v06.061.2022171160428.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h11v06.061.2022164160634.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v06.061.2022160071951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h09v06.061.2022164164813.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v05.061.2022161131738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h10v06.061.2022152132706.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h12v05.061.2022146045521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v05.061.2022153122432.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v04.061.2022160051756.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h10v06.061.2022165214445.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h13v04.061.2022159001357.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v05.061.2022152014115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h12v05.061.2022162024506.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h08v06.061.2022160065224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h09v05.061.2022165224410.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h08v06.061.2022216025520.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v04.061.2022158231134.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v05.061.2022166145612.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v05.061.2022151161531.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h09v04.061.2022169160534.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h12v04.061.2022169160935.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h07v05.061.2022151161122.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v05.061.2022159184300.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v05.061.2022165082044.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h10v05.061.2022216071653.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h09v04.061.2022165065242.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v05.061.2022158221052.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h11v06.061.2022147204132.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v06.061.2022146040126.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v04.061.2022216043057.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h10v04.061.2022154052218.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h09v04.061.2022154033740.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h13v04.061.2022161154912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h11v04.061.2022152014252.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h07v06.061.2022162003933.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v04.061.2022171155354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h09v05.061.2022168002006.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h12v05.061.2022168030750.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v05.061.2022152134513.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v05.061.2022151161738.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h10v06.061.2022169160700.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v04.061.2022168030221.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h10v05.061.2022164173115.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h07v06.061.2022161124349.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h09v06.061.2022171153157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v04.061.2022162025604.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h09v04.061.2022202150536.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h09v04.061.2022152023313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v04.061.2022158220628.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v05.061.2022166152129.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v05.061.2022159183224.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h09v06.061.2022160064847.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h07v05.061.2022168144825.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h09v04.061.2022146030108.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h10v05.061.2022162040109.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h09v05.061.2022151161409.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h10v06.061.2022147204000.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h09v04.061.2022159175354.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h07v06.061.2022166144709.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h08v06.061.2022202150433.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v06.061.2022168025451.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h07v06.061.2022147203753.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v06.061.2022162022641.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h11v04.061.2022159184319.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v06.061.2022152022413.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h10v06.061.2022168025826.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h09v05.061.2022162023037.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v05.061.2022146033046.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h12v05.061.2022166164515.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h12v04.061.2022153233438.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h11v04.061.2022146060256.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v05.061.2022161134833.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h11v06.061.2022166153208.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h08v05.061.2022147203929.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v04.061.2022152013916.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2017001.h11v04.061.2022168022951.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h10v06.061.2022165072912.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h10v04.061.2022151161523.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h08v04.061.2022151161158.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2020001.h07v06.061.2022171160856.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h11v05.061.2022202150843.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v05.061.2022153114232.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v05.061.2022169160820.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h12v05.061.2022151161918.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2013001.h08v04.061.2022164160625.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v06.061.2022152014235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h08v06.061.2022152211338.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h10v05.061.2022152023406.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h07v06.061.2022146033902.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h13v04.061.2022169161048.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v04.061.2022168175454.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h13v04.061.2022168153530.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h11v06.061.2022161132907.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2021001.h11v06.061.2022215202155.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h12v04.061.2022152140219.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h11v04.061.2022160070952.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h08v05.061.2022153205724.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h10v06.061.2022168170157.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h13v04.061.2022154052639.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h13v04.061.2022165221239.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v06.061.2022202150716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h11v04.061.2022154032745.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2002001.h09v06.061.2022147202005.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2019001.h11v06.061.2022169160822.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2015001.h12v05.061.2022165203837.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h08v05.061.2022166151313.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h10v04.061.2022159184020.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2012001.h13v04.061.2022162035643.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h09v04.061.2022161130111.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2009001.h08v06.061.2022159175519.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2007001.h07v06.061.2022153115802.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2003001.h11v04.061.2022151161716.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2005001.h07v05.061.2022152115041.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h12v05.061.2022158225932.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h13v04.061.2022146060649.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2008001.h10v06.061.2022158215757.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2011001.h12v05.061.2022161143539.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2001001.h10v05.061.2022146030344.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h13v04.061.2022202151230.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v06.061.2022202150327.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2010001.h07v05.061.2022160050421.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h11v04.061.2022165074211.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2014001.h08v04.061.2022165062521.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2018001.h11v04.061.2022168164651.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2004001.h08v05.061.2022152014143.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h07v05.061.2022202150325.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2016001.h09v04.061.2022166151038.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2006001.h10v05.061.2022202150656.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023__water_mask.tif\n",
  "history_begin_time" : 1716715353532,
  "history_end_time" : 1716715511176,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QN1jnncp7IXt",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    # Subtract one year from the current date\n    one_year_ago = i - timedelta(days=365)\n    target_output_tif = f'{modis_day_wise}/{current_date.year}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(one_year_ago, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QN1jnncp7IXt/mod_water_mask.py\", line 662, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/QN1jnncp7IXt/mod_water_mask.py\", line 645, in extract_data_for_testing\n    download_tiles_and_merge(start_date, end_date)\n  File \"/home/chetana/gw-workspace/QN1jnncp7IXt/mod_water_mask.py\", line 201, in download_tiles_and_merge\n    target_output_tif = f'{modis_day_wise}/{current_date.year}__water_mask.tif'\nAttributeError: 'str' object has no attribute 'year'\n",
  "history_begin_time" : 1716715339844,
  "history_end_time" : 1716715342102,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eXcnVmM2R4Tt",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    # Subtract one year from the current date\n    one_year_ago = current_date - timedelta(days=365)\n    target_output_tif = f'{modis_day_wise}/{current_date.year}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(one_year_ago, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/eXcnVmM2R4Tt/mod_water_mask.py\", line 662, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/eXcnVmM2R4Tt/mod_water_mask.py\", line 645, in extract_data_for_testing\n    download_tiles_and_merge(start_date, end_date)\n  File \"/home/chetana/gw-workspace/eXcnVmM2R4Tt/mod_water_mask.py\", line 200, in download_tiles_and_merge\n    one_year_ago = current_date - timedelta(days=365)\nTypeError: unsupported operand type(s) for -: 'str' and 'datetime.timedelta'\n",
  "history_begin_time" : 1716715296266,
  "history_end_time" : 1716715298553,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "txx2A0Ojzsyp",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716715179407,
  "history_end_time" : 1716715188905,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wHz2UH3BdxUt",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2022-12-01\", \"2024-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1956.19it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 21993.57it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 159158.86it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloadedFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2327.43it/s]File MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 11770.08it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 63720.44it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1758.75it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 62382.47it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 123361.88it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1828.15it/s]\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloadedFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 19391.67it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 181895.84it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\ntif_files =  []\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716715049442,
  "history_end_time" : 1716715060555,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dgJoucNp7anb",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2022-12-01\", \"2024-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1574.40it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 23493.63it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 144631.17it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2415.17it/s]\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloadedFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 13545.43it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 165053.63it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2278.06it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloadedFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 10063.96it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 168964.85it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1751.10it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloadedFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 17618.77it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 169366.19it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716714639332,
  "history_end_time" : 1716714649564,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SGqovkwxsXLG",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2022-12-01\", \"2024-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2075.36it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 22282.24it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 163915.33it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloadedFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1967.85it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 36194.50it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 173065.94it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2118.08it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 22324.10it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 54975.46it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2625.88it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloadedFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloadedFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 8273.75it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 178704.68it/s]\ndone with downloading, start to convert HDF to geotiff..\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v05.061.2023243090807.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v04.061.2023243105251.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v05.061.2023243054834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v06.061.2023243075235.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v05.061.2023243103913.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v06.061.2023243092049.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h08v04.061.2023243062305.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v05.061.2023243080033.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h12v04.061.2023243115711.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h07v06.061.2023243061908.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v04.061.2023243085508.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v04.061.2023243112318.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h13v04.061.2023243101116.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v06.061.2023243063834.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h10v06.061.2023243073808.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h09v05.061.2023243094009.tif exists. skip.\nThe file /home/chetana/water_mask/output_folder/MCD12Q1.A2022001.h11v05.061.2023243093838.tif exists. skip.\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716713824201,
  "history_end_time" : 1716713835981,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oUp6TKqGpnxj",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2022-12-01\", \"2024-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloadedFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1899.14it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 18046.87it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 152683.44it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloadedFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2971.09it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 10846.24it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 173487.03it/s]\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloadedFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloadedFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1700.61it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloadedFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 19861.61it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 130353.14it/s]\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloadedFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloadedFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloadedFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2716.73it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloadedFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 9290.32it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 56589.82it/s]\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716712866699,
  "history_end_time" : 1716712882197,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZFFJFNuX292K",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2022-12-01\", \"2024-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 396.99it/s]\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | :   6%|▌         | 1/17 [00:03<00:49,  3.07s/it]\nPROCESSING TASKS | :  12%|█▏        | 2/17 [00:03<00:20,  1.38s/it]\nPROCESSING TASKS | :  24%|██▎       | 4/17 [00:03<00:07,  1.79it/s]\nPROCESSING TASKS | :  29%|██▉       | 5/17 [00:03<00:05,  2.24it/s]\nPROCESSING TASKS | :  41%|████      | 7/17 [00:03<00:02,  3.82it/s]\nPROCESSING TASKS | :  53%|█████▎    | 9/17 [00:06<00:04,  1.60it/s]\nPROCESSING TASKS | :  59%|█████▉    | 10/17 [00:06<00:03,  1.91it/s]\nPROCESSING TASKS | :  65%|██████▍   | 11/17 [00:06<00:02,  2.30it/s]\nPROCESSING TASKS | :  71%|███████   | 12/17 [00:06<00:01,  2.76it/s]\nPROCESSING TASKS | :  82%|████████▏ | 14/17 [00:06<00:00,  3.47it/s]\nPROCESSING TASKS | :  88%|████████▊ | 15/17 [00:07<00:00,  3.66it/s]\nPROCESSING TASKS | :  94%|█████████▍| 16/17 [00:07<00:00,  2.74it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:09<00:00,  1.43it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:09<00:00,  1.81it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 42569.06it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloadedFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 1746.56it/s]\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloadedFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 15806.51it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 169769.45it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloaded\nFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloadedFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloadedFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloadedFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloadedFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2311.51it/s]\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 20977.69it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 177371.06it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 17\n Getting 17 granules, approx download size: 0.13 GB\nQUEUEING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]File MCD12Q1.A2022001.h12v05.061.2023243080033.hdf already downloaded\nFile MCD12Q1.A2022001.h09v05.061.2023243094009.hdf already downloaded\nFile MCD12Q1.A2022001.h11v05.061.2023243093838.hdf already downloadedFile MCD12Q1.A2022001.h10v04.061.2023243105251.hdf already downloadedFile MCD12Q1.A2022001.h08v06.061.2023243092049.hdf already downloaded\nFile MCD12Q1.A2022001.h08v04.061.2023243062305.hdf already downloaded\nFile MCD12Q1.A2022001.h07v05.061.2023243054834.hdf already downloaded\nFile MCD12Q1.A2022001.h13v04.061.2023243101116.hdf already downloaded\nFile MCD12Q1.A2022001.h07v06.061.2023243061908.hdf already downloaded\nFile MCD12Q1.A2022001.h10v06.061.2023243073808.hdf already downloaded\nFile MCD12Q1.A2022001.h11v04.061.2023243112318.hdf already downloaded\nFile MCD12Q1.A2022001.h11v06.061.2023243063834.hdf already downloadedFile MCD12Q1.A2022001.h09v04.061.2023243085508.hdf already downloaded\nFile MCD12Q1.A2022001.h08v05.061.2023243090807.hdf already downloaded\nFile MCD12Q1.A2022001.h09v06.061.2023243075235.hdf already downloadedFile MCD12Q1.A2022001.h10v05.061.2023243103913.hdf already downloaded\nQUEUEING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 2312.71it/s]\nFile MCD12Q1.A2022001.h12v04.061.2023243115711.hdf already downloaded\nPROCESSING TASKS | :   0%|          | 0/17 [00:00<?, ?it/s]\nPROCESSING TASKS | : 100%|██████████| 17/17 [00:00<00:00, 29871.46it/s]\nCOLLECTING RESULTS | :   0%|          | 0/17 [00:00<?, ?it/s]\nCOLLECTING RESULTS | : 100%|██████████| 17/17 [00:00<00:00, 56188.47it/s]\ndone with downloading, start to convert HDF to geotiff..\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ntoday date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711841876,
  "history_end_time" : 1716711874605,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "NOvnZbT0bX4A",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2023-01-01\", \"2024-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711745532,
  "history_end_time" : 1716711755160,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z7aWZgtK8nTv",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(\"2023-01-01\", \"2023-01-01\"))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711728552,
  "history_end_time" : 1716711737681,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IIFl21A5wfiS",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711628136,
  "history_end_time" : 1716711637778,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7Ra1DT45KNef",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD12Q1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711476360,
  "history_end_time" : 1716711485797,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Bp7U5OwMNQ7z",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W1\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711453173,
  "history_end_time" : 1716711463771,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KUVo497Zo5zC",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711376749,
  "history_end_time" : 1716711387075,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8UUfl0SVHD9B",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716711156208,
  "history_end_time" : 1716711166092,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "t87rVUHcxOCJ",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716710919577,
  "history_end_time" : 1716710929322,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "34P0MCFOWhOZ",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/water_mask_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//water_mask_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//water_mask_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716708988176,
  "history_end_time" : 1716708999037,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5V4YS7K4KIK4",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023274\nuh-oh, didn't find HDFs for date 2023-10-01\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//fsca_template.tif /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nEnvironment variable PROJ_LIB removed.\nEnvironment variable GDAL_DATA removed.\nERROR 4: /home/chetana/water_mask/final_output//fsca_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023275\nuh-oh, didn't find HDFs for date 2023-10-02\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//fsca_template.tif /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//fsca_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-02__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023276\nuh-oh, didn't find HDFs for date 2023-10-03\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//fsca_template.tif /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//fsca_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-03__water_mask.tif\nThe file /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\ndone with conversion, start to merge geotiff tiles to one tif per day..\ntarget julian date 2023277\nuh-oh, didn't find HDFs for date 2023-10-04\ngenerate a new csv file with empty values for each point\nRunning  /usr/bin/gdal_translate -b 1 -outsize 100% 100% -scale 0 255 200 200 /home/chetana/water_mask/final_output//fsca_template.tif /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\nERROR 4: /home/chetana/water_mask/final_output//fsca_template.tif: No such file or directory\nsaved the merged tifs to /home/chetana/water_mask/final_output//2023-10-04__water_mask.tif\n",
  "history_begin_time" : 1716708916707,
  "history_end_time" : 1716708927277,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KHYOgF9d3vGt",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/water_mask/final_output//2023-10-01__water_mask.tif does not exist.\nstart to download files from NASA server to local\nGranules found: 0\nList of URLs or DataGranule isntances expected\ndone with downloading, start to convert HDF to geotiff..\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/KHYOgF9d3vGt/mod_water_mask.py\", line 658, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/KHYOgF9d3vGt/mod_water_mask.py\", line 641, in extract_data_for_testing\n    download_tiles_and_merge(start_date, end_date)\n  File \"/home/chetana/gw-workspace/KHYOgF9d3vGt/mod_water_mask.py\", line 214, in download_tiles_and_merge\n    convert_all_hdf_in_folder(input_folder, output_folder)\n  File \"/home/chetana/gw-workspace/KHYOgF9d3vGt/mod_water_mask.py\", line 71, in convert_all_hdf_in_folder\n    for file in os.listdir(folder_path):\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/water_mask/temp/'\n",
  "history_begin_time" : 1716708875048,
  "history_end_time" : 1716708881288,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "VqwHoCWsMYLn",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nstart to generate /home/chetana/water_mask/final_output/modis_to_dem_mapper.csv\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2022-10-01_water_mask.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/VqwHoCWsMYLn/mod_water_mask.py\", line 658, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/VqwHoCWsMYLn/mod_water_mask.py\", line 639, in extract_data_for_testing\n    prepare_modis_grid_mapper()\n  File \"/home/chetana/gw-workspace/VqwHoCWsMYLn/mod_water_mask.py\", line 596, in prepare_modis_grid_mapper\n    with rasterio.open(sample_modis_tif) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2022-10-01_water_mask.tif: No such file or directory\n",
  "history_begin_time" : 1716708752855,
  "history_end_time" : 1716708755561,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "r62iLo2DDLGG",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__water_mask.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nstart to generate /home/chetana/water_mask/final_output/modis_to_dem_mapper.csv\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2022-10-01__snow_cover.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/r62iLo2DDLGG/mod_water_mask.py\", line 658, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/r62iLo2DDLGG/mod_water_mask.py\", line 639, in extract_data_for_testing\n    prepare_modis_grid_mapper()\n  File \"/home/chetana/gw-workspace/r62iLo2DDLGG/mod_water_mask.py\", line 596, in prepare_modis_grid_mapper\n    with rasterio.open(sample_modis_tif) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2022-10-01__snow_cover.tif: No such file or directory\n",
  "history_begin_time" : 1716708673039,
  "history_end_time" : 1716708675798,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Ie57KSZB0RWe",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nstart to generate /home/chetana/water_mask/final_output/modis_to_dem_mapper.csv\nTraceback (most recent call last):\n  File \"rasterio/_base.pyx\", line 310, in rasterio._base.DatasetBase.__init__\n  File \"rasterio/_base.pyx\", line 221, in rasterio._base.open_dataset\n  File \"rasterio/_err.pyx\", line 221, in rasterio._err.exc_wrap_pointer\nrasterio._err.CPLE_OpenFailedError: /home/chetana/water_mask/final_output//2022-10-01__snow_cover.tif: No such file or directory\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/Ie57KSZB0RWe/mod_water_mask.py\", line 658, in <module>\n    extract_data_for_testing()\n  File \"/home/chetana/gw-workspace/Ie57KSZB0RWe/mod_water_mask.py\", line 639, in extract_data_for_testing\n    prepare_modis_grid_mapper()\n  File \"/home/chetana/gw-workspace/Ie57KSZB0RWe/mod_water_mask.py\", line 596, in prepare_modis_grid_mapper\n    with rasterio.open(sample_modis_tif) as src:\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/env.py\", line 451, in wrapper\n    return f(*args, **kwds)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/rasterio/__init__.py\", line 304, in open\n    dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n  File \"rasterio/_base.pyx\", line 312, in rasterio._base.DatasetBase.__init__\nrasterio.errors.RasterioIOError: /home/chetana/water_mask/final_output//2022-10-01__snow_cover.tif: No such file or directory\n",
  "history_begin_time" : 1716708646308,
  "history_end_time" : 1716708649185,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9nOxxiU2Ljgq",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nos.chdir(f\"{homedir}/water_mask/\")\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/9nOxxiU2Ljgq/mod_water_mask.py\", line 28, in <module>\n    os.chdir(f\"{homedir}/water_mask/\")\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/water_mask/'\n",
  "history_begin_time" : 1716708176191,
  "history_end_time" : 1716708178494,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "UeiIxnJi7Kqs",
  "history_input" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nos.chdir(f\"{homedir}/fsca/\")\n\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD44W\",\n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n#   date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n#   for i in date_list:\n#     current_date = i.strftime(\"%Y-%m-%d\")\n#     print(f\"extracting data for {current_date}\")\n#     outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n#     if os.path.exists(outfile):\n#       print(f\"The file {outfile} exists. skip.\")\n#     else:\n#       process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n#   add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "history_output" : "today date = 2024-05-26\ntest start date:  2023-10-04\ntest end date:  2023-10-19\n/home/chetana\nget test_start_date =  2023-10-04\n2023-10-04 00:00:00\nThe start_date of the water year 2023-10-01 00:00:00\nThe file /home/chetana/fsca/final_output/modis_to_dem_mapper.csv exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2023-10-01__snow_cover.tif exists. skip.\nfile_size_bytes: 509344\nThe file /home/chetana/fsca/final_output//2023-10-02__snow_cover.tif exists. skip.\nfile_size_bytes: 509484\nThe file /home/chetana/fsca/final_output//2023-10-03__snow_cover.tif exists. skip.\nfile_size_bytes: 509308\nThe file /home/chetana/fsca/final_output//2023-10-04__snow_cover.tif exists. skip.\n",
  "history_begin_time" : 1716708134875,
  "history_end_time" : 1716708137188,
  "history_notes" : null,
  "history_process" : "1xdwd6",
  "host_id" : null,
  "indicator" : "Done"
},]
