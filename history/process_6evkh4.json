[{
  "history_id" : "e30ormxhrvr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696863953338,
  "history_end_time" : 1696863953338,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4zic7v5cnfl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696862402981,
  "history_end_time" : 1696862402981,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wwv8trcb7rf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696832263694,
  "history_end_time" : 1696832263694,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pxs99pn0s2z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696831867409,
  "history_end_time" : 1696831867409,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "669szc9kfkc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696830174417,
  "history_end_time" : 1696830174417,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "05w40kib5n0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696787541979,
  "history_end_time" : 1696787541979,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iapry4kaljk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696786838282,
  "history_end_time" : 1696786838282,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yxqxhkj9no9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696771780951,
  "history_end_time" : 1696771780951,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rhfv0yvwhjf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696602943962,
  "history_end_time" : 1696602943962,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pid6zyedq2x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432484358,
  "history_end_time" : 1696432484358,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y13m78q769r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1696432299777,
  "history_end_time" : 1696432482237,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yrj37n3lj8u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827991152,
  "history_end_time" : 1695827991152,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o52s9v7vgiz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827889186,
  "history_end_time" : 1695827965229,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kn7szvjjvl5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695827855653,
  "history_end_time" : 1695827867013,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sjlk4ztvadz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695696616127,
  "history_end_time" : 1695696616127,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sziwjv537m6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695694257337,
  "history_end_time" : 1695694257337,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vghhz5xzblo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693585756,
  "history_end_time" : 1695693585756,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1hr1mc1v810",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695693149443,
  "history_end_time" : 1695693149443,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "453iefjjqkv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695580915855,
  "history_end_time" : 1695580915855,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u7u0y3e68vi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695576291673,
  "history_end_time" : 1695576291673,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xgavhccipnm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695575931036,
  "history_end_time" : 1695575931036,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xwql0w70gxj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535769220,
  "history_end_time" : 1695535769220,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f1ebmramh28",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535478716,
  "history_end_time" : 1695535478716,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p1wqsmujuhp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695535214032,
  "history_end_time" : 1695535214032,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6t850o59txh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534943597,
  "history_end_time" : 1695534943597,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p4v0l5g5t0z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695534671836,
  "history_end_time" : 1695534671836,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8yuambl2szs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695533024175,
  "history_end_time" : 1695533024175,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "njkujvvdzqt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695529187876,
  "history_end_time" : 1695529187876,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fisgm9h8si1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695528505205,
  "history_end_time" : 1695528505205,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fu1iaw4xr5e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695515862564,
  "history_end_time" : 1695515862564,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5yyvh5ll501",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695506423867,
  "history_end_time" : 1695506423867,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "soa7syqvrpa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695418741351,
  "history_end_time" : 1695418741351,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g7sxhavbyoj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417619687,
  "history_end_time" : 1695417619687,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bqrw9g887cr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417171290,
  "history_end_time" : 1695417171290,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yhiw2wmwh38",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695417052740,
  "history_end_time" : 1695417052740,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hpmi8vtnh8p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695416916064,
  "history_end_time" : 1695416916064,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6ojdf2b10ra",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106488990,
  "history_end_time" : 1695106488990,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wb0sp8j8eb6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695106316228,
  "history_end_time" : 1695106316228,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x659ha31xvw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054045037,
  "history_end_time" : 1695054045037,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t75j7o72pp7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695054019805,
  "history_end_time" : 1695054033337,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "52t5c13ogxf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053979924,
  "history_end_time" : 1695054019279,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zzaxov9f3x8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053793464,
  "history_end_time" : 1695053793464,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0j006y5e7j8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1695053733482,
  "history_end_time" : 1695053733482,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "shy9b1q9ana",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694971144856,
  "history_end_time" : 1694972839692,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7b2up00iv1w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970707954,
  "history_end_time" : 1694970707954,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "884p4ag7xz2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970594775,
  "history_end_time" : 1694970594775,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kt9kz06uj1m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694970131665,
  "history_end_time" : 1694970131665,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "83qtstgj60k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694969350148,
  "history_end_time" : 1694969350148,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nigsnac36dc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694905307743,
  "history_end_time" : 1694905307743,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "68kkcsza87z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1694897887154,
  "history_end_time" : 1694897887154,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "humE6a4jdsEp",
  "history_input" : "from snowcast_utils import work_dir\nimport dask.dataframe as dd\n\n\ndef clip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file):\n  # Load CSV files into Dask DataFrames\n  gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n  snotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\n  terrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\n  amsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n  # Filter and rename columns for each DataFrame in a single step\n  snotel_df = (snotel_df\n               .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n               .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n               .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n               )\n  print(\"snotel_df.head()\", snotel_df.head())\n\n  gridmet_df = (gridmet_df\n                .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n                .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n                )\n  print(\"gridmet_df.head(): \", gridmet_df.head())\n\n  terrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n  print(\"terrain_df.head(): \", terrain_df.head())\n\n  amsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n  amsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n  amsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\n  amsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n  print(\"amsr_df.head(): \", amsr_df.head())\n\n  gridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\n  amsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n  snotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n  \n\ngridmet_20_years_file = f\"{work_dir}/climatology_data.csv\"\nsnotel_20_years_file = f\"{work_dir}/training_ready_snotel_data.csv\"\nterrain_file = f'{work_dir}/training_ready_terrain.csv'\namsr_3_years_file = f\"{work_dir}/training_amsr_data.csv\"\noutput_file = f\"{work_dir}/training_ready_merged_data_dd.csv\"\n\nclip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file)\n\n# Merge all DataFrames in a single step using inner join\n# merged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n#              .merge(terrain_df, on=['lat', 'lon'], how='inner')\n#              .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n#              )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\n# merged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py:8069: UserWarning: Insufficient elements for `head`. 5 elements requested, only 0 elements available. Try passing larger `npartitions` to `head`.\n  warnings.warn(\ntoday date = 2023-08-13\n/home/chetana\nsnotel_df.head()             date   SWE        lat         lon\n6207  2019-01-01  10.9  41.993149 -120.178715\n6208  2019-01-02  11.0  41.993149 -120.178715\n6209  2019-01-03  11.0  41.993149 -120.178715\n6210  2019-01-04  11.1  41.993149 -120.178715\n6211  2019-01-05  11.1  41.993149 -120.178715\ngridmet_df.head():  Empty DataFrame\nColumns: [date, lat, lon, cell_id, station_id, etr, pr, rmax, rmin, tmmn, tmmx, vpd, vs]\nIndex: []\nterrain_df.head():     elevation      slope  curvature  ...  northness        lat         lon\n0  2290.8364  89.988850 -9401.7705  ...   0.649194  41.993149 -120.178715\n1  2955.2370  89.991880 -5259.1160  ...   0.661430  37.727154 -119.136669\n2  2481.0059  89.992966 -9113.7150  ...   0.649554  38.918144 -120.205665\n3  3329.7136  89.975500 -7727.0957  ...   0.193519  37.070608 -118.768361\n4  2851.1318  89.975540  2352.6350  ...  -0.509422  36.364939 -118.292254\n[5 rows x 8 columns]\namsr_df.head():           date        lat         lon  SWE  Flag\n0  2020-11-25  41.993149 -120.178715  255   255\n1  2020-11-25  37.727154 -119.136669  255   255\n2  2020-11-25  38.918144 -120.205665  255   255\n3  2020-11-25  37.070608 -118.768361  255   255\n4  2020-11-25  36.364939 -118.292254  255   255\n",
  "history_begin_time" : 1691903268357,
  "history_end_time" : 1691903321560,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SK6ZStRD5LPZ",
  "history_input" : "from snowcast_utils import work_dir\nimport dask.dataframe as dd\n\n\ndef clip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file):\n  # Load CSV files into Dask DataFrames\n  gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n  snotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\n  terrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\n  amsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n  # Filter and rename columns for each DataFrame in a single step\n  snotel_df = (snotel_df\n               .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n               .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n               .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n               )\n  print(\"snotel_df.head()\", snotel_df.head())\n\n  gridmet_df = (gridmet_df\n                .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n                .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n                )\n  print(\"gridmet_df.head(): \", gridmet_df.head())\n\n  terrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n  print(\"terrain_df.head(): \", terrain_df.head())\n\n  amsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n  amsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n  amsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\n  amsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n  print(\"amsr_df.head(): \", amsr_df.head())\n\n  gridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\n  amsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n  snotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n  \n\ngridmet_20_years_file = f\"{work_dir}climatology_data.csv\"\nsnotel_20_years_file = f\"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = f'{work_dir}/training_ready_terrain.csv'\namsr_3_years_file = f\"{work_dir}/training_amsr_data.csv\"\noutput_file = f\"{work_dir}/training_ready_merged_data_dd.csv\"\n\nclip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file)\n\n# Merge all DataFrames in a single step using inner join\n# merged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n#              .merge(terrain_df, on=['lat', 'lon'], how='inner')\n#              .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n#              )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\n# merged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "today date = 2023-08-13\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 136, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 761, in read\n    return read_pandas(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 561, in read_pandas\n    b_out = read_bytes(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/bytes/core.py\", line 111, in read_bytes\n    size = fs.info(path)[\"size\"]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 87, in info\n    out = os.stat(path, follow_symlinks=False)\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_runclimatology_data.csv'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/SK6ZStRD5LPZ/training_data_range.py\", line 46, in <module>\n    clip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file)\n  File \"/home/chetana/gw-workspace/SK6ZStRD5LPZ/training_data_range.py\", line 7, in clip_csv_using_time_range\n    gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/backends.py\", line 138, in wrapper\n    raise type(e)(\nFileNotFoundError: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_runclimatology_data.csv'\n",
  "history_begin_time" : 1691903235788,
  "history_end_time" : 1691903240166,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "wYZpmAyxC7VK",
  "history_input" : "\nimport dask.dataframe as dd\n\n\ndef clip_csv_using_time_range():\n  # Load CSV files into Dask DataFrames\n  gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n  snotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\n  terrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\n  amsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n  # Filter and rename columns for each DataFrame in a single step\n  snotel_df = (snotel_df\n               .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n               .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n               .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n               )\n  print(\"snotel_df.head()\", snotel_df.head())\n\n  gridmet_df = (gridmet_df\n                .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n                .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n                )\n  print(\"gridmet_df.head(): \", gridmet_df.head())\n\n  terrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n  print(\"terrain_df.head(): \", terrain_df.head())\n\n  amsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n  amsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\n  amsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\n  amsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n  print(\"amsr_df.head(): \", amsr_df.head())\n\n  gridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\n  amsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n  snotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n  \n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data_dd.csv\"\n\n\n\n# Merge all DataFrames in a single step using inner join\n# merged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n#              .merge(terrain_df, on=['lat', 'lon'], how='inner')\n#              .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n#              )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\n# merged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "",
  "history_begin_time" : 1691873541086,
  "history_end_time" : 1691873542398,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ku6b0uCewU0E",
  "history_input" : "\nimport dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data_dd.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\nprint(\"snotel_df.head()\", snotel_df.head())\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\nprint(\"gridmet_df.head(): \", gridmet_df.head())\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\nprint(\"terrain_df.head(): \", terrain_df.head())\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\nprint(\"amsr_df.head(): \", amsr_df.head())\n\ngridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\namsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\nsnotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n\n# Merge all DataFrames in a single step using inner join\n# merged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n#              .merge(terrain_df, on=['lat', 'lon'], how='inner')\n#              .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n#              )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\n# merged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py:8069: UserWarning: Insufficient elements for `head`. 5 elements requested, only 0 elements available. Try passing larger `npartitions` to `head`.\n  warnings.warn(\nsnotel_df.head()             date   SWE        lat         lon\n6207  2019-01-01  10.9  41.993149 -120.178715\n6208  2019-01-02  11.0  41.993149 -120.178715\n6209  2019-01-03  11.0  41.993149 -120.178715\n6210  2019-01-04  11.1  41.993149 -120.178715\n6211  2019-01-05  11.1  41.993149 -120.178715\ngridmet_df.head():  Empty DataFrame\nColumns: [date, lat, lon, cell_id, station_id, etr, pr, rmax, rmin, tmmn, tmmx, vpd, vs]\nIndex: []\nterrain_df.head():     elevation      slope  curvature  ...  northness        lat         lon\n0  2290.8364  89.988850 -9401.7705  ...   0.649194  41.993149 -120.178715\n1  2955.2370  89.991880 -5259.1160  ...   0.661430  37.727154 -119.136669\n2  2481.0059  89.992966 -9113.7150  ...   0.649554  38.918144 -120.205665\n3  3329.7136  89.975500 -7727.0957  ...   0.193519  37.070608 -118.768361\n4  2851.1318  89.975540  2352.6350  ...  -0.509422  36.364939 -118.292254\n[5 rows x 8 columns]\namsr_df.head():           date        lat         lon  SWE  Flag\n0  2020-11-25  41.993149 -120.178715  255   255\n1  2020-11-25  37.727154 -119.136669  255   255\n2  2020-11-25  38.918144 -120.205665  255   255\n3  2020-11-25  37.070608 -118.768361  255   255\n4  2020-11-25  36.364939 -118.292254  255   255\n",
  "history_begin_time" : 1691865435489,
  "history_end_time" : 1691865491510,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0p3i2wf8uqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531335833,
  "history_end_time" : 1691531335833,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "fm5f63prs6o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531292842,
  "history_end_time" : 1691531292842,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "0kapdz9xtjr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531254770,
  "history_end_time" : 1691531284904,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "qjawq7y86rj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531163959,
  "history_end_time" : 1691531163959,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "s9yh91u32df",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531120996,
  "history_end_time" : 1691531120996,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "bvfd1n4f10a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691531061018,
  "history_end_time" : 1691531061018,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "vrt0vu75z1b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530848385,
  "history_end_time" : 1691530848385,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "re8pxz4pkca",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530717764,
  "history_end_time" : 1691530721109,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "eqt9ytzpg0b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530690272,
  "history_end_time" : 1691530716753,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "wugmd9aubrh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530621140,
  "history_end_time" : 1691530622443,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "bni82l003o9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530617328,
  "history_end_time" : 1691530617328,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Skipped"
},{
  "history_id" : "x6x2xi4cf98",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1691530599902,
  "history_end_time" : 1691530614288,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : "tq3z35",
  "indicator" : "Stopped"
},{
  "history_id" : "tnvxFdY0LDBa",
  "history_input" : "\nimport dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data_dd.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\ngridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\namsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\nsnotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n\n# Merge all DataFrames in a single step using inner join\n# merged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n#              .merge(terrain_df, on=['lat', 'lon'], how='inner')\n#              .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n#              )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\n# merged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "",
  "history_begin_time" : 1691353532688,
  "history_end_time" : 1691353577319,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "F3ZYK2KhbEnQ",
  "history_input" : "\nimport dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data_dd.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\ngridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\nterrain_df.to_csv('/home/chetana/gridmet_test_run/training_ready_terrain_3_yrs.csv', index=False, single_file=True)\namsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n\n# Merge all DataFrames in a single step using inner join\n# merged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n#              .merge(terrain_df, on=['lat', 'lon'], how='inner')\n#              .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n#              )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\n# merged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "",
  "history_begin_time" : 1691353035833,
  "history_end_time" : 1691353076431,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "xVTFMnL06dD8",
  "history_input" : "# This process is to aggreate all the collected data into training.csv.\n# the reason why it is separated from the data_integration step is because it assume the 20 years of Gridmet and Snotel data are already collected, but AMSR only has data after 2012. So this process will make subset of gridmet and snotel to comply with the time range of AMSR.\n\nimport dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data_dd.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge all DataFrames in a single step using inner join\nmerged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n             .merge(terrain_df, on=['lat', 'lon'], how='inner')\n             .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n             )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "",
  "history_begin_time" : 1691302416790,
  "history_end_time" : 1691306383162,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bsE3FcqGyz4h",
  "history_input" : "# This process is to aggreate all the collected data into training.csv.\n# the reason why it is separated from the data_integration step is because it assume the 20 years of Gridmet and Snotel data are already collected, but AMSR only has data after 2012. So this process will make subset of gridmet and snotel to comply with the time range of AMSR.\n\nimport dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge all DataFrames in a single step using inner join\nmerged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n             .merge(terrain_df, on=['lat', 'lon'], how='inner')\n             .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n             )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "",
  "history_begin_time" : 1691287362201,
  "history_end_time" : 1691288036053,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "jBrpqm6orIsT",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge all DataFrames in a single step using inner join\nmerged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n             .merge(terrain_df, on=['lat', 'lon'], how='inner')\n             .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n             )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "",
  "history_begin_time" : 1691267466936,
  "history_end_time" : 1691271012437,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ljE7C8nMJ96p",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge all DataFrames in a single step using inner join\nmerged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n             .merge(terrain_df, on=['lat', 'lon'], how='inner')\n             .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n             )\n\n# Drop duplicate columns from the merge\n#merged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False, single_file=True)\n\n# Persist DataFrames to memory\n#merged_df = merged_df.persist()\n#merged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ljE7C8nMJ96p/training_data_range.py\", line 43, in <module>\n    merged_df.to_csv(output_file, index=False, single_file=True)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 224, in execute_task\n    result = _execute_task(task, data)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 792, in _write_csv\n    with fil as f:\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 102, in __enter__\n    f = self.fs.open(self.path, mode=mode)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/spec.py\", line 1241, in open\n    f = self._open(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 184, in _open\n    return LocalFileOpener(path, mode, fs=self, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 315, in __init__\n    self._open()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 320, in _open\n    self.f = open(self.path, mode=self.mode)\nIsADirectoryError: [Errno 21] Is a directory: '/home/chetana/gridmet_test_run/training_ready_merged_data.csv'\n",
  "history_begin_time" : 1691267158805,
  "history_end_time" : 1691267305558,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "O1dBk4WovuEW",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\nsnotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\nterrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\namsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n# Filter and rename columns for each DataFrame in a single step\nsnotel_df = (snotel_df\n             .loc[snotel_df['Date'].between('2019-01-01', '2022-12-31')]\n             .loc[:, ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n             .rename(columns={'Date': 'date', 'Snow Water Equivalent (in) Start of Day Values': 'SWE'})\n             )\n\ngridmet_df = (gridmet_df\n              .loc[gridmet_df['date'].between('2019-01-01', '2022-12-31')]\n              .loc[:, ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n              )\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge all DataFrames in a single step using inner join\nmerged_df = (dd.merge(gridmet_df, snotel_df, on=['date', 'lat', 'lon'], how='inner')\n             .merge(terrain_df, on=['lat', 'lon'], how='inner')\n             .merge(amsr_df, on=['date', 'lat', 'lon'], how='inner')\n             )\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n\n# Persist DataFrames to memory\nmerged_df = merged_df.persist()\nmerged_df.compute()  # Trigger computation and wait for the result\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6852, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 774, in drop_by_shallow_copy\n    df2.drop(columns=columns, inplace=True, errors=errors)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Date'] not found in axis\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/O1dBk4WovuEW/training_data_range.py\", line 40, in <module>\n    merged_df = merged_df.drop(['Date'], axis=1)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 5575, in drop\n    return self.map_partitions(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 1006, in map_partitions\n    return map_partitions(func, self, *args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6922, in map_partitions\n    meta = _get_meta_map_partitions(args, dfs, func, kwargs, meta, parent_meta)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 7033, in _get_meta_map_partitions\n    meta = _emulate(func, *args, udf=True, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6852, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/home/chetana/anaconda3/lib/python3.9/contextlib.py\", line 137, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 214, in raise_on_meta_error\n    raise ValueError(msg) from e\nValueError: Metadata inference failed in `drop_by_shallow_copy`.\n\nYou have supplied a custom function and Dask is unable to \ndetermine the type of output that that function returns. \n\nTo resolve this please provide a meta= keyword.\nThe docstring of the Dask function you ran should have more information.\n\nOriginal error is below:\n------------------------\nKeyError(\"['Date'] not found in axis\")\n\nTraceback:\n---------\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 193, in raise_on_meta_error\n    yield\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 6852, in _emulate\n    return func(*_extract_meta(args, True), **_extract_meta(kwargs, True))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/utils.py\", line 774, in drop_by_shallow_copy\n    df2.drop(columns=columns, inplace=True, errors=errors)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\n\n",
  "history_begin_time" : 1691265997856,
  "history_end_time" : 1691266012295,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0l3CsParJoVY",
  "history_input" : "#import dask.dataframe as dd\nimport pandas as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge the DataFrames using inner join\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='inner')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='inner')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='inner')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/0l3CsParJoVY/training_data_range.py\", line 32, in <module>\n    merged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='inner')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 124, in merge\n    return op.get_result(copy=copy)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 775, in get_result\n    result = self._reindex_and_concat(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 766, in _reindex_and_concat\n    result = concat([left, right], axis=1, copy=copy)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 381, in concat\n    return op.get_result()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 616, in get_result\n    new_data = concatenate_managers(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 14.5 GiB for an array with shape (8, 243366075) and data type float64\n",
  "history_begin_time" : 1691263822126,
  "history_end_time" : 1691263970552,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cKPIBA5yid3G",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge the DataFrames using inner join\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='inner')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='inner')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='inner')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "sh: line 1: 30926 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python training_data_range.py\n",
  "history_begin_time" : 1691253868602,
  "history_end_time" : 1691254054108,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Pin25sz4EJOT",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge the DataFrames\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "sh: line 1: 29778 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python training_data_range.py\n",
  "history_begin_time" : 1691250840482,
  "history_end_time" : 1691251319846,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "KguAvkdnreGM",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge the DataFrames\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/KguAvkdnreGM/training_data_range.py\", line 37, in <module>\n    merged_df.to_csv(output_file, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 994, in to_csv\n    return list(dask.compute(*values, **compute_kwargs))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/threaded.py\", line 89, in get\n    results = get_async(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 511, in get_async\n    raise_exception(exc, tb)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/local.py\", line 319, in reraise\n    raise exc\nsh: line 1: 46822 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python training_data_range.py\n",
  "history_begin_time" : 1691217365853,
  "history_end_time" : 1691217773013,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pOR1cIpIUD09",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_ready_merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge the DataFrames\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "sh: line 1: 46090 Killed                  /home/chetana/gridmet_test_run/pycrate/bin/python training_data_range.py\n",
  "history_begin_time" : 1691216557074,
  "history_end_time" : 1691217040162,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VHhnXv8rjsN9",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df = amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'})\n\n# Merge the DataFrames\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/VHhnXv8rjsN9/training_data_range.py\", line 37, in <module>\n    merged_df.to_csv(output_file, index=False)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/core.py\", line 1856, in to_csv\n    return to_csv(self, filename, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/dask/dataframe/io/csv.py\", line 952, in to_csv\n    files = open_files(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in open_files\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/core.py\", line 293, in <listcomp>\n    [fs.makedirs(parent, exist_ok=True) for parent in parents]\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/fsspec/implementations/local.py\", line 54, in makedirs\n    os.makedirs(path, exist_ok=exist_ok)\n  File \"/home/chetana/anaconda3/lib/python3.9/os.py\", line 225, in makedirs\n    mkdir(name, mode)\nFileExistsError: [Errno 17] File exists: '/home/chetana/gridmet_test_run/merged_data.csv'\n",
  "history_begin_time" : 1691216530678,
  "history_end_time" : 1691216534568,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SNIwBeWDXtmm",
  "history_input" : "import dask.dataframe as dd\n\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/merged_data.csv\"\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Merge the DataFrames\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/SNIwBeWDXtmm/training_data_range.py\", line 26, in <module>\n    amsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\nTypeError: rename() got an unexpected keyword argument 'inplace'\n",
  "history_begin_time" : 1691216397749,
  "history_end_time" : 1691216400068,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pcrc6jiu69m1",
  "history_input" : "import dask.dataframe as dd\n\n# Load CSV files into Dask DataFrames\ngridmet_df = dd.read_csv(gridmet_20_years_file)\nsnotel_df = dd.read_csv(snotel_20_years_file)\nterrain_df = dd.read_csv(terrain_file)\namsr_df = dd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = dd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Merge the DataFrames\nmerged_df = dd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = dd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df = merged_df.drop(['Date'], axis=1)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/pcrc6jiu69m1/training_data_range.py\", line 4, in <module>\n    gridmet_df = dd.read_csv(gridmet_20_years_file)\nNameError: name 'gridmet_20_years_file' is not defined\n",
  "history_begin_time" : 1691215920874,
  "history_end_time" : 1691215933049,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "FKVzeukLpabC",
  "history_input" : "import pandas as pd\n\n# Define file paths\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/merged_data.csv\"\n\n# Load CSV files into pandas DataFrames\ngridmet_df = pd.read_csv(gridmet_20_years_file)\nsnotel_df = pd.read_csv(snotel_20_years_file)\nterrain_df = pd.read_csv(terrain_file)\namsr_df = pd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = pd.to_datetime(amsr_df['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d')\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Merge the DataFrames\nmerged_df = pd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df.drop(['Date'], axis=1, inplace=True)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/FKVzeukLpabC/training_data_range.py\", line 32, in <module>\n    merged_df = pd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 124, in merge\n    return op.get_result(copy=copy)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 775, in get_result\n    result = self._reindex_and_concat(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 766, in _reindex_and_concat\n    result = concat([left, right], axis=1, copy=copy)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 381, in concat\n    return op.get_result()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 616, in get_result\n    new_data = concatenate_managers(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/internals/concat.py\", line 212, in concatenate_managers\n    values = values.copy()\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 14.5 GiB for an array with shape (8, 243763544) and data type float64\n",
  "history_begin_time" : 1691215125439,
  "history_end_time" : 1691215420446,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qMt8yoC9glmr",
  "history_input" : "import pandas as pd\n\n# Define file paths\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/merged_data.csv\"\n\n# Load CSV files into pandas DataFrames\ngridmet_df = pd.read_csv(gridmet_20_years_file)\nsnotel_df = pd.read_csv(snotel_20_years_file)\nterrain_df = pd.read_csv(terrain_file)\namsr_df = pd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Merge the DataFrames\nmerged_df = pd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df.drop(['Date'], axis=1, inplace=True)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691213656882,
  "history_end_time" : 1691214156034,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "7JYNg2N8zO2B",
  "history_input" : "import pandas as pd\n\n# Define file paths\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/merged_data.csv\"\n\n# Load CSV files into pandas DataFrames\ngridmet_df = pd.read_csv(gridmet_20_years_file)\nsnotel_df = pd.read_csv(snotel_20_years_file)\nterrain_df = pd.read_csv(terrain_file)\namsr_df = pd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df['Date'] = pd.to_datetime(amsr_df['Date'], format=\"%d/%m/%Y %I:%M:%S %p\").dt.date\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\namsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Merge the DataFrames\nmerged_df = pd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df.drop(['Date'], axis=1, inplace=True)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/7JYNg2N8zO2B/training_data_range.py\", line 25, in <module>\n    amsr_df['Date'] = pd.to_datetime(amsr_df['Date'], format=\"%d/%m/%Y %I:%M:%S %p\").dt.date\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/tools/datetimes.py\", line 1064, in to_datetime\n    cache_array = _maybe_cache(arg, format, cache, convert_listlike)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/tools/datetimes.py\", line 229, in _maybe_cache\n    cache_dates = convert_listlike(unique_dates, format)\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/tools/datetimes.py\", line 430, in _convert_listlike_datetimes\n    res = _to_datetime_with_format(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/tools/datetimes.py\", line 538, in _to_datetime_with_format\n    res = _array_strptime_with_fallback(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/tools/datetimes.py\", line 473, in _array_strptime_with_fallback\n    result, timezones = array_strptime(arg, fmt, exact=exact, errors=errors)\n  File \"pandas/_libs/tslibs/strptime.pyx\", line 150, in pandas._libs.tslibs.strptime.array_strptime\nValueError: time data '2020-11-25 00:00:00' does not match format '%d/%m/%Y %I:%M:%S %p' (match)\n",
  "history_begin_time" : 1691211943681,
  "history_end_time" : 1691211968679,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "bB5hdnSPhWgf",
  "history_input" : "import pandas as pd\n\n# Define file paths\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/merged_data.csv\"\n\n# Load CSV files into pandas DataFrames\ngridmet_df = pd.read_csv(gridmet_20_years_file)\nsnotel_df = pd.read_csv(snotel_20_years_file)\nterrain_df = pd.read_csv(terrain_file)\namsr_df = pd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\n\n# Rename columns to match case\namsr_df.rename(columns={'Latitude': 'lat', 'Longitude': 'lon'}, inplace=True)\n\n# Merge the DataFrames\nmerged_df = pd.merge(gridmet_df, snotel_df, left_on=['date', 'lat', 'lon'], right_on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, amsr_df, on=['Date', 'lat', 'lon'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df.drop(['Date'], axis=1, inplace=True)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "",
  "history_begin_time" : 1691206252532,
  "history_end_time" : 1691206819280,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gewLvGQKw2TZ",
  "history_input" : "import pandas as pd\n\n# Define file paths\ngridmet_20_years_file = \"/home/chetana/gridmet_test_run/climatology_data.csv\"\nsnotel_20_years_file = \"/home/chetana/gridmet_test_run/training_ready_snotel_data.csv\"\nterrain_file = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\namsr_3_years_file = \"/home/chetana/gridmet_test_run/training_amsr_data.csv\"\noutput_file = \"/home/chetana/gridmet_test_run/training_3_yrs_merged_data.csv\"\n\n# Load CSV files into pandas DataFrames\ngridmet_df = pd.read_csv(gridmet_20_years_file)\nsnotel_df = pd.read_csv(snotel_20_years_file)\nterrain_df = pd.read_csv(terrain_file)\namsr_df = pd.read_csv(amsr_3_years_file)\n\n# Filter columns for each DataFrame\nsnotel_df = snotel_df.loc[snotel_df['Date'].between('2019-01-01', '2022-12-31'),\n                          ['Date', 'Snow Water Equivalent (in) Start of Day Values', 'lat', 'lon']]\n\ngridmet_df = gridmet_df.loc[gridmet_df['date'].between('2019-01-01', '2022-12-31'),\n                            ['date', 'lat', 'lon', 'cell_id', 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs']]\n\nterrain_df = terrain_df[['elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness', 'lat', 'lon']]\n\namsr_df = amsr_df[['Date', 'Latitude', 'Longitude', 'SWE', 'Flag']]\n\n# Merge the DataFrames\nmerged_df = pd.merge(gridmet_df, snotel_df, on=['Date', 'lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, terrain_df, on=['lat', 'lon'], how='outer')\nmerged_df = pd.merge(merged_df, amsr_df, left_on=['Date', 'lat', 'lon'], right_on=['Date', 'Latitude', 'Longitude'], how='outer')\n\n# Drop duplicate columns from the merge\nmerged_df.drop(['Latitude', 'Longitude'], axis=1, inplace=True)\n\n# Save the merged DataFrame to a new CSV file\nmerged_df.to_csv(output_file, index=False)\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/gewLvGQKw2TZ/training_data_range.py\", line 28, in <module>\n    merged_df = pd.merge(gridmet_df, snotel_df, on=['Date', 'lat', 'lon'], how='outer')\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 110, in merge\n    op = _MergeOperation(\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 703, in __init__\n    ) = self._get_merge_keys()\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 1179, in _get_merge_keys\n    left_keys.append(left._get_label_or_level_values(lk))\n  File \"/home/chetana/gridmet_test_run/pycrate/lib/python3.9/site-packages/pandas/core/generic.py\", line 1850, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'Date'\n",
  "history_begin_time" : 1691206111226,
  "history_end_time" : 1691206150085,
  "history_notes" : null,
  "history_process" : "6evkh4",
  "host_id" : null,
  "indicator" : "Done"
},]
