[{
  "history_id" : "j9nkby1y2w7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714977336242,
  "history_end_time" : 1714977336242,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "84b4bzvptav",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714968024445,
  "history_end_time" : 1714968024445,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mtz6l2bqg1n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714956083752,
  "history_end_time" : 1714956344018,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "vbx6zc",
  "indicator" : "Stopped"
},{
  "history_id" : "al4ha05rh1g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714944537623,
  "history_end_time" : 1714944767580,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "vbx6zc",
  "indicator" : "Stopped"
},{
  "history_id" : "fj4mu4mmwqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714943546229,
  "history_end_time" : 1714943546229,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pixk1zpm28z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714897025218,
  "history_end_time" : 1714897025218,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a7j7y3t78z1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714894044203,
  "history_end_time" : 1714895139380,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nudo3e4ci4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714881609916,
  "history_end_time" : 1714881609916,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "avi4xn0g9p8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714409076795,
  "history_end_time" : 1714409076795,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d5k9zpqmi6z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714407283853,
  "history_end_time" : 1714407283853,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hew4jrpl8sy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714404800143,
  "history_end_time" : 1714404800143,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n2804sgdh11",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714363210314,
  "history_end_time" : 1714363210314,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4k3yvitfm6x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714362032733,
  "history_end_time" : 1714362032733,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "udf7zgdm5ct",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714355595261,
  "history_end_time" : 1714355595261,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "go2so1vcfal",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714351396916,
  "history_end_time" : 1714351396916,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8csomj8vrl1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714348539324,
  "history_end_time" : 1714348539324,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "63d2nzlmfy1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714345973293,
  "history_end_time" : 1714345973293,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "euorvex51ko",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714343583925,
  "history_end_time" : 1714343583925,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rr0cmfrvh73",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714340841832,
  "history_end_time" : 1714340841832,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "daoy9wk4i0v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714332798202,
  "history_end_time" : 1714332798202,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ivjl1i0ae2c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714328816241,
  "history_end_time" : 1714328816241,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "61q78iucpi8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714323529116,
  "history_end_time" : 1714323529116,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b4siavfiylp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714320354978,
  "history_end_time" : 1714320354978,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1etbte89st9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714315863144,
  "history_end_time" : 1714315863144,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vzfz6knykcn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714283543380,
  "history_end_time" : 1714315862070,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vqh74dlrit0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714281870998,
  "history_end_time" : 1714283515041,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mf48oh7y2be",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714276838499,
  "history_end_time" : 1714315876918,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a71ir0l992s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714274662767,
  "history_end_time" : 1714281869553,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ygtcadjjhb1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714272894760,
  "history_end_time" : 1714274352452,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h331atixxkd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714265253466,
  "history_end_time" : 1714272892785,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rqzlvhfjc5l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714265112678,
  "history_end_time" : 1714265250945,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j6fdk1nzq5w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714192736446,
  "history_end_time" : 1714265111668,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xj4ygmn1g50",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714190418473,
  "history_end_time" : 1714274351056,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zrqe1najsgw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714104009626,
  "history_end_time" : 1714274350454,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fzrjjwxz4j4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714017609955,
  "history_end_time" : 1714274349940,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h9f0sbu37km",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713931210261,
  "history_end_time" : 1714274349398,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "unmh0sh4u65",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713844809939,
  "history_end_time" : 1714274348795,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kcqibv1l0gy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713758409514,
  "history_end_time" : 1714274347362,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ly731n0ihhe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713672009960,
  "history_end_time" : 1714274346369,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8r9acaazuc6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713585609786,
  "history_end_time" : 1714274345856,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0zoseknqg0s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713499209994,
  "history_end_time" : 1714274345302,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ee7764646l0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713412809224,
  "history_end_time" : 1714274344729,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nvjv5kruav2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713326409346,
  "history_end_time" : 1714274343306,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e6c6plq0crc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713283461365,
  "history_end_time" : 1713283461365,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iwtto3sf8va",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1713240009306,
  "history_end_time" : 1714274342602,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2uhnln02nlu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711771209955,
  "history_end_time" : 1711771209955,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1gt8pr1xsb1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711684813967,
  "history_end_time" : 1714282476123,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jphsmbw9vcp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711598415388,
  "history_end_time" : 1714282474717,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dsqncct8si6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592280538,
  "history_end_time" : 1711592280538,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bt6ve11vus6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592081952,
  "history_end_time" : 1711592081952,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "y2na7s317tz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711512016692,
  "history_end_time" : 1711512016692,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "muuuis5adr1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711425614455,
  "history_end_time" : 1714282483208,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rqobansvubw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711339213020,
  "history_end_time" : 1714282483871,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9dqapmj5d8k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711252813780,
  "history_end_time" : 1711252813780,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fjowl8rhb03",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711166414398,
  "history_end_time" : 1711166414398,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2ciomkyu9is",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711080009362,
  "history_end_time" : 1711080009362,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g4fvk6bq28y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710993610012,
  "history_end_time" : 1714282484657,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5dl134rirgg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710907209457,
  "history_end_time" : 1714282485305,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9qt00dom2oi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710820809108,
  "history_end_time" : 1714282486311,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ah7vsqcsa94",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710734409835,
  "history_end_time" : 1714282486977,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "86499rd6fgh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710690214218,
  "history_end_time" : 1710690214218,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ersebhpwckb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710648009530,
  "history_end_time" : 1714282488984,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xhbtmxc4bn9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710561608999,
  "history_end_time" : 1714282489557,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "py7mab9tipo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710475209609,
  "history_end_time" : 1714282490331,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1d7mo4mueut",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710388809594,
  "history_end_time" : 1714282490853,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7109fnw40vd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710302409679,
  "history_end_time" : 1714282491525,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5oow5dyhbsx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710216009756,
  "history_end_time" : 1714282492143,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kl8uhi5mnm6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710172046545,
  "history_end_time" : 1710172046545,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "rsoep8znk72",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710129609424,
  "history_end_time" : 1714282493756,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kq2haklituc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710080234081,
  "history_end_time" : 1710080234081,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "5i7on8yai19",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710043209065,
  "history_end_time" : 1714282495099,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xtd2vw39jxg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709998716305,
  "history_end_time" : 1709998716305,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gsngdhjzhoh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709956809354,
  "history_end_time" : 1714282495739,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "65r71obk4ed",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709924987611,
  "history_end_time" : 1709924987611,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tojoy4gzioq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709870410352,
  "history_end_time" : 1714282496863,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l9c2adbs42v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709845595336,
  "history_end_time" : 1709845595336,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ixwhhzkuj86",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709844621986,
  "history_end_time" : 1709844621986,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mbkmenp3bj6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709842923413,
  "history_end_time" : 1709842923413,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ohf41olrvuc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709827652628,
  "history_end_time" : 1709844621187,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "fevhgsbgsx2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709826138724,
  "history_end_time" : 1709826138724,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2rfyb4mdomr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709797240863,
  "history_end_time" : 1709797240863,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ouf5lp6wyb8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709791538007,
  "history_end_time" : 1709791538007,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "faw5vleuwla",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709784009821,
  "history_end_time" : 1714282504747,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "03kq6dk0jms",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709778056681,
  "history_end_time" : 1709778056681,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cgczvp1amqg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709774843637,
  "history_end_time" : 1709774843637,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5mzfealvvxf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709765244695,
  "history_end_time" : 1709774842952,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3zotto2jw7v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709763117646,
  "history_end_time" : 1709765243451,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3sdc9ynlqt4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751527401,
  "history_end_time" : 1709751527401,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uas9zoncoz8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751443207,
  "history_end_time" : 1709751495341,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tdgswc07tvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709697609645,
  "history_end_time" : 1714282506692,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8fbnr545ny9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709611209055,
  "history_end_time" : 1714282507225,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4yx4kxwmmu3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709524809255,
  "history_end_time" : 1714282507738,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "glye1td3wqt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709438409032,
  "history_end_time" : 1709438409032,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wr79cfxurlo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709352009417,
  "history_end_time" : 1709352009417,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8lpb0bdpseh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709265609795,
  "history_end_time" : 1709265609795,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "puuvyiph2ex",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709179209745,
  "history_end_time" : 1709179209745,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "venvl6dzm95",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709137312996,
  "history_end_time" : 1709137312996,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "svdrojwssvi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709092809385,
  "history_end_time" : 1709092809385,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "piwmmu7mtnc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709085684232,
  "history_end_time" : 1709085684232,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "dfwjr7qxkek",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709078942170,
  "history_end_time" : 1709085672815,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "f34a550t4bc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038894877,
  "history_end_time" : 1709038894877,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "oirgb7t34gh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038873001,
  "history_end_time" : 1709038879075,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "1ptjioq0tsf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709006408956,
  "history_end_time" : 1709006408956,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5kx2488zh7b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708971826231,
  "history_end_time" : 1708971826231,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ze7udqvoa2d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708958410902,
  "history_end_time" : 1708958410902,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "gt2vkpk5tg0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708954624233,
  "history_end_time" : 1708954624233,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "wpkwwhamt8q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708920009776,
  "history_end_time" : 1708920009776,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kboy44mxahn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708833609462,
  "history_end_time" : 1708833609462,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mws8jf912m9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708747209220,
  "history_end_time" : 1708747209220,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fe2qh8lq539",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708660809335,
  "history_end_time" : 1708660809335,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ynojb7lz4pu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708574410013,
  "history_end_time" : 1708574410013,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mwoom42woi5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708488010522,
  "history_end_time" : 1708488010522,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b45hqkfm3mw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708401609246,
  "history_end_time" : 1708401609246,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tq6eu9iw0hg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708352215571,
  "history_end_time" : 1708352215571,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "fcbuw7pku7n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708348193309,
  "history_end_time" : 1708352214878,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "6khb34wy8kj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708315209402,
  "history_end_time" : 1708315209402,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "26t61dvzm3s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708312689984,
  "history_end_time" : 1708312689984,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ptva9tb5g7i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708305309950,
  "history_end_time" : 1708312689141,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "b8zdio704q5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708242679332,
  "history_end_time" : 1708242679332,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "lx9dixd9wb4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708240733544,
  "history_end_time" : 1708240733544,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "upnkby5gty9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708238770029,
  "history_end_time" : 1708238770029,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "y21lfvr3fhd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708237144907,
  "history_end_time" : 1708237144907,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "3ptlqdrw3th",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708235187258,
  "history_end_time" : 1708235187258,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "o56ke8n0qhz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708233874708,
  "history_end_time" : 1708233874708,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "6khznllt9ql",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708228809515,
  "history_end_time" : 1708228809515,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "36chf7koe88",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708227613836,
  "history_end_time" : 1708227613836,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "b18yjd72uyp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708142409726,
  "history_end_time" : 1708142409726,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "43u7ogn78w5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708056009568,
  "history_end_time" : 1708056009568,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7d85odht510",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707969609354,
  "history_end_time" : 1707969609354,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "embcl009w84",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707883209132,
  "history_end_time" : 1707883209132,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ru5bbd1qffv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707796809645,
  "history_end_time" : 1707796809645,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8zb3kdayl7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707750669788,
  "history_end_time" : 1707750669788,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "gsixyf4cqdx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707710409693,
  "history_end_time" : 1707710409693,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "23lhuu4u9me",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707624009761,
  "history_end_time" : 1707624009761,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "59j32cblcww",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707537608921,
  "history_end_time" : 1707537608921,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7vp5sjdf18b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707491746729,
  "history_end_time" : 1707491746729,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "gpf20ll0grd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707484657966,
  "history_end_time" : 1707484657966,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "qjh5d7h6qfh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707451209983,
  "history_end_time" : 1707451209983,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xrt92q7mp5q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707434644211,
  "history_end_time" : 1707434644211,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "n4348cdu1or",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707432571828,
  "history_end_time" : 1707432571828,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "eb0rip0ez3g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707431129266,
  "history_end_time" : 1707432053961,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "o7whr6m6gzh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707418188665,
  "history_end_time" : 1707418188665,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "w8ih2bnqs44",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707413610371,
  "history_end_time" : 1707413610371,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "y3rv8gh8sax",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707364809006,
  "history_end_time" : 1707364809006,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "up364w1cr0w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707278409234,
  "history_end_time" : 1707278409234,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w5ag0de85do",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192718902,
  "history_end_time" : 1707192718902,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5c951pbg2if",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192009423,
  "history_end_time" : 1707448888657,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ljxp0v00njz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707189399146,
  "history_end_time" : 1707189399146,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c35t79kijjf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707105609455,
  "history_end_time" : 1707750639197,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "awxouyf0q88",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707019209464,
  "history_end_time" : 1707750639666,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7lvhpletfe7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706932809346,
  "history_end_time" : 1707750640210,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qa9mz8t4bhq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706846409821,
  "history_end_time" : 1707750640644,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9l503cdxt8x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706760010092,
  "history_end_time" : 1707750643301,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w8whz64crp7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706673609617,
  "history_end_time" : 1707750643806,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9t2kd1tsgb5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706587209540,
  "history_end_time" : 1707750644989,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xx0jar4tl1b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706500809129,
  "history_end_time" : 1707750645666,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "erayw6u20aq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706414409459,
  "history_end_time" : 1707750646111,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hzcrn3gmtqe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706366105920,
  "history_end_time" : 1706366105920,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jl4yvu0snad",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706364888474,
  "history_end_time" : 1706364888474,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w5mee0d9jey",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706328009671,
  "history_end_time" : 1707750646910,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2y7dkenj8ky",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706280497985,
  "history_end_time" : 1706280497985,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zdu43cwdudx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706244881249,
  "history_end_time" : 1706244881249,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wkz958z1gyv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706241609873,
  "history_end_time" : 1706244810659,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t5777o4eb80",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706155209741,
  "history_end_time" : 1706244801320,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r3h0k806avu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706068809250,
  "history_end_time" : 1706244800347,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hz3t05xupa7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705982409138,
  "history_end_time" : 1706244799980,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7n04h0wrhbm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896759337,
  "history_end_time" : 1706244799016,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xyskhuc12tm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896009936,
  "history_end_time" : 1706244798536,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ddoq5nub256",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705849064185,
  "history_end_time" : 1706244798023,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "skc6s3sq7wx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705809609077,
  "history_end_time" : 1705849649607,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hulxl2ow68u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705793527239,
  "history_end_time" : 1705849647065,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t0i13vkvfv2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705790835111,
  "history_end_time" : 1705790835111,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "464tqld8m5u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705770628243,
  "history_end_time" : 1705849642252,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kw3ckzr3ryh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705762761029,
  "history_end_time" : 1705849640896,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3fjdesavlu0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705723209659,
  "history_end_time" : 1705789738455,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "msygdef1m22",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705636808964,
  "history_end_time" : 1705770636240,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "61rexa8zyg2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705550409444,
  "history_end_time" : 1705770635555,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sdxq78ljh3k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705464009022,
  "history_end_time" : 1705770635068,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "f5rrw7p3bj7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705422422540,
  "history_end_time" : 1705422422540,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q9ymais9vox",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705377609593,
  "history_end_time" : 1705770633062,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pfivechpqhv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705291209227,
  "history_end_time" : 1705770632183,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q668lo7qk5s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705278850598,
  "history_end_time" : 1705278850598,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nt5ixird5l5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705270952807,
  "history_end_time" : 1705270952807,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vq3hiluro4t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705204809672,
  "history_end_time" : 1705789662012,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "v9vw2i6jxzb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705169057238,
  "history_end_time" : 1705169057238,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8c5xui4vmco",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705118409811,
  "history_end_time" : 1705789660781,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "87m131t6gy4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705072448184,
  "history_end_time" : 1705072448184,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "iqd3t5qqa35",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705032009663,
  "history_end_time" : 1705789659553,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l1m96mo6x96",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704979918389,
  "history_end_time" : 1704979918389,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xthzbwmyssi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704945609799,
  "history_end_time" : 1705789658823,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "85cubaian3w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704918977845,
  "history_end_time" : 1704918977845,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vphrcepxz5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704908919861,
  "history_end_time" : 1704908919861,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gvda3to0ozw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704859207739,
  "history_end_time" : 1705789668266,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "02c7qbn7brr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704775840764,
  "history_end_time" : 1704775840764,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mm1flgogz7w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704772806950,
  "history_end_time" : 1705789667296,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "v3ihpp9peqx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704726161288,
  "history_end_time" : 1704727049033,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rq3tt6g9fzs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704686408534,
  "history_end_time" : 1705789666653,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2r536z5pa1j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803741,
  "history_end_time" : 1704644803741,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lv7oq0gm64l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008374,
  "history_end_time" : 1705789665927,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "m5bqt7bsi3i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156638,
  "history_end_time" : 1704566156638,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kewwuoqnku0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587395,
  "history_end_time" : 1704565587395,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wiz9yrg7ajl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424171,
  "history_end_time" : 1704564424171,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fvlqugjbmv2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992192,
  "history_end_time" : 1704562992192,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cjnd4md706a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889810,
  "history_end_time" : 1704561889810,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ebyv209vkze",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861184,
  "history_end_time" : 1704561887042,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t2yss6khlpk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479220,
  "history_end_time" : 1704555479220,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a11ixjyr1t3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028211,
  "history_end_time" : 1704555028211,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q8rou6dwneg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241781,
  "history_end_time" : 1704553241781,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tnrbgohmgff",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254647,
  "history_end_time" : 1704552254647,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bnz1i11kkga",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607518,
  "history_end_time" : 1705789671136,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oibu0i49i0t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207700,
  "history_end_time" : 1705789671876,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nry1ow9ga77",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807716,
  "history_end_time" : 1705789673119,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gln4twkqwny",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109332,
  "history_end_time" : 1704330109332,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "877i59r5s3j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364881,
  "history_end_time" : 1704329364881,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hebnk1hmq50",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407691,
  "history_end_time" : 1705789675688,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q7szbrmz31m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947969,
  "history_end_time" : 1704208947969,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3dg3f9vsxmv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352031,
  "history_end_time" : 1704207352031,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2ut5bkgjmwy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859387,
  "history_end_time" : 1704205859387,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bdj0sk997jb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007547,
  "history_end_time" : 1705789676833,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nzsbg1qycem",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607644,
  "history_end_time" : 1705789677626,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vo0cabbhg48",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208589,
  "history_end_time" : 1705789678821,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "700yj1g4tvi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871417,
  "history_end_time" : 1703962871417,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "19dmnncqbc7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265455,
  "history_end_time" : 1703960265455,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ivhdw61nyev",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737859,
  "history_end_time" : 1703959737859,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1icc4baq731",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611603,
  "history_end_time" : 1703958611603,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i6ifjxqr4kt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838239,
  "history_end_time" : 1703955838239,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d3lomgi46c0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150392,
  "history_end_time" : 1703954150392,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x4s3kcrvoz5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768084,
  "history_end_time" : 1703915768084,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s9yn3e0i3tz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283500,
  "history_end_time" : 1703915283500,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dwx3r9tie4b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476656,
  "history_end_time" : 1703914476656,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "76o99a2pap1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302189,
  "history_end_time" : 1703912302189,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ophz8mizqb6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908807303,
  "history_end_time" : 1705789681239,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9gezv9jjwiz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215393,
  "history_end_time" : 1703906215393,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jnt1rm1u5sp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919157,
  "history_end_time" : 1703900919157,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0w09ownbu1q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837780,
  "history_end_time" : 1703899837780,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "69m11u5ss6k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422964,
  "history_end_time" : 1703897422964,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aiucs4oqpqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125597,
  "history_end_time" : 1703896125597,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6ytz5y7n5pk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890276004,
  "history_end_time" : 1703890276004,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3m7a96mtael",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800822,
  "history_end_time" : 1703886800822,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hfytcrd694j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997780,
  "history_end_time" : 1703885997780,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k0j88yunoq4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194729,
  "history_end_time" : 1703880194729,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2365iex6ph6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753047,
  "history_end_time" : 1703872753047,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xxw9w7f73xx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828248,
  "history_end_time" : 1703869828248,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ybpse4g1xj0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616945,
  "history_end_time" : 1703868616945,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5a5gprebfwh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114059,
  "history_end_time" : 1703867114059,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c12s6ja8g9j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885460,
  "history_end_time" : 1703864885460,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m3rooyxtoy6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637385,
  "history_end_time" : 1703862637385,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h3lkz5z5gab",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227364,
  "history_end_time" : 1703827227364,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5yr9990erd6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411634,
  "history_end_time" : 1703822411634,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4e9x5l7up4n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924646,
  "history_end_time" : 1703789718830,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "le9o64dkzkv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053500,
  "history_end_time" : 1703786917618,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "v2k0qm2biat",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395407,
  "history_end_time" : 1703778395407,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bqfzn484oxa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034598,
  "history_end_time" : 1703739034598,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ljzjpzb935g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754646,
  "history_end_time" : 1703792459281,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2syup0ttcjl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166921,
  "history_end_time" : 1703737316889,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lyy57vhpmvn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763594,
  "history_end_time" : 1703694763594,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8oh4frcbj87",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541212,
  "history_end_time" : 1703659541212,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qor229jelcx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144719,
  "history_end_time" : 1703658144719,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cr7wnaszncx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855785,
  "history_end_time" : 1703650855785,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ce6t0cxshbp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751547,
  "history_end_time" : 1703650812448,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h3dza0ky4px",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120906,
  "history_end_time" : 1703646749629,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "87u0aq2jkwb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988959,
  "history_end_time" : 1703642074637,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wb7dmfzck2n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665518,
  "history_end_time" : 1703629665518,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vdfvv94qwzb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687999,
  "history_end_time" : 1703627783061,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6yskqwj3gym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782113,
  "history_end_time" : 1703625782113,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zt9jgzpzrhn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624784036,
  "history_end_time" : 1703624784036,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fheyd5bx331",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592858,
  "history_end_time" : 1702875592858,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zedpy4ass15",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264387,
  "history_end_time" : 1702871264387,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x8jran7rqvd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996407,
  "history_end_time" : 1702867996407,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "74lyfd2rlfa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593405,
  "history_end_time" : 1702866593405,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "44bwcxuve09",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137650,
  "history_end_time" : 1702866137650,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2aw0s7enllk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305647,
  "history_end_time" : 1702657305647,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nax14w98478",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223058,
  "history_end_time" : 1702633223058,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wqbi7did1li",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156943,
  "history_end_time" : 1702633163905,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s7be3onkb8z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520895,
  "history_end_time" : 1702274520895,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z6u8e155n8x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109213,
  "history_end_time" : 1702257109213,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m7suc9ezeqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506556,
  "history_end_time" : 1702253506556,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7h9memn4uqd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800954,
  "history_end_time" : 1702047800954,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mzsuwygwnwp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671890,
  "history_end_time" : 1702047789490,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w9agnz8cadc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701838624115,
  "history_end_time" : 1701838624115,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "35u2b4w97nv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272631491,
  "history_end_time" : 1701272875117,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8melrj8yptg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272152713,
  "history_end_time" : 1701272363360,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3qzciinnypr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701269761359,
  "history_end_time" : 1701269761359,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "13az1vbl3ht",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701245472032,
  "history_end_time" : 1701245472032,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "abephke63bl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701234300629,
  "history_end_time" : 1701234300629,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "60k04lipta1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375292,
  "history_end_time" : 1701234158047,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s8o9mbwx08j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048679,
  "history_end_time" : 1701231048679,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1ychsgb92s4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933476,
  "history_end_time" : 1701230952354,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pgu5ncxq7hg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796354,
  "history_end_time" : 1701230932261,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3pdfcq0p6dk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230384900,
  "history_end_time" : 1701230384900,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v5izaprgy40",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701229983519,
  "history_end_time" : 1701229983519,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xro0cp069ui",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228899382,
  "history_end_time" : 1701228899382,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dq7em7olgpq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228374816,
  "history_end_time" : 1701228374816,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j0v4gsrdzgl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228236354,
  "history_end_time" : 1701228236354,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "545at6kbu70",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228118520,
  "history_end_time" : 1701228118520,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v39ppro72sz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228056543,
  "history_end_time" : 1701228056543,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rvhparozuu2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701227912544,
  "history_end_time" : 1701227912544,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ck94dmrzc9n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937422,
  "history_end_time" : 1701015920043,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "35efu0l4a7k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688739,
  "history_end_time" : 1700974688739,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3mz42m1nax3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700885116855,
  "history_end_time" : 1700885116855,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "whdk2tssq7m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700471590214,
  "history_end_time" : 1700471590214,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kcuq48z31ge",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700468936452,
  "history_end_time" : 1700468936452,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "isel32kxz82",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700461923054,
  "history_end_time" : 1700462913702,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t6so95z5m5g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700448500152,
  "history_end_time" : 1700448500152,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "acib6jnby1j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700447319858,
  "history_end_time" : 1700447319858,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9yyjny1o7ov",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700230067259,
  "history_end_time" : 1700230067259,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6qgg6tg2d0n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700229012382,
  "history_end_time" : 1700229012382,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "n5k3bogi133",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700210213825,
  "history_end_time" : 1700210213825,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mrfj0kb2zrs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209780177,
  "history_end_time" : 1700209780177,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8rq7v5r2gre",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700209729262,
  "history_end_time" : 1700209729262,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xfhkrog6ue8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700203478696,
  "history_end_time" : 1700204245690,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "k4d1jzmy6kn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700201828281,
  "history_end_time" : 1700201828281,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "stpobnkbel1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700200332871,
  "history_end_time" : 1700200332871,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "u26a78cpn3a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700145667882,
  "history_end_time" : 1700145667882,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1jw762hxi40",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700143295321,
  "history_end_time" : 1700143295321,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j3zh6r59yvh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700141615825,
  "history_end_time" : 1700141615825,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xv9bmugptrg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700134126853,
  "history_end_time" : 1700134126853,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t6rmk16gft0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700133783721,
  "history_end_time" : 1700133783721,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wh1ugwlghbn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699992839774,
  "history_end_time" : 1699992839774,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m5d9wts9edo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699982145472,
  "history_end_time" : 1699982145472,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8q1ij47k7uj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699941614816,
  "history_end_time" : 1699941614816,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fyjj1xhlgjn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699939440625,
  "history_end_time" : 1699939440625,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6cyvjblg8gj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699937910478,
  "history_end_time" : 1699937910478,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r0pj9zsk5kh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699805634711,
  "history_end_time" : 1699806085203,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kzlardyvkqt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154079,
  "history_end_time" : 1705789690248,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e4s8eoqshcv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071380,
  "history_end_time" : 1699681071380,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3lep75sik2e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762678712,
  "history_end_time" : 1698762678712,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7vaur4c8c6b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762637998,
  "history_end_time" : 1698762637998,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mbsVcfGD2N3S",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \n\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  print(f\"the snotel covered area: {snotel_covered_len}\")\n  old_train_df.drop([\"pmv\", \"SnowClass\", \"cumulative_fSCA\"], axis=1, inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  print(f\"len of no snow dataframe: {len(chosen_no_snow_row_df)}\")\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\nadd_no_snow_data_points()\n\n\n",
  "history_output" : "today date = 2023-10-27\ntest start date:  2018-01-01\ntest end date:  2023-10-27\n/home/ubuntu\nthe snotel covered area: 2740\n/home/ubuntu/gw-workspace/mbsVcfGD2N3S/data_merge_hackweek.py:257: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  final_test_df = pd.read_csv(final_testing_file_path)\nlen of no snow dataframe: 2740\nfinal training data is saved to /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n",
  "history_begin_time" : 1698378804430,
  "history_end_time" : 1698378805488,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kFTUwuDKTkzg",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  old_train_df.drop([\"pmv\", \"SnowClass\", \"cumulative_fSCA\"], axis=1, inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\nadd_no_snow_data_points()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only.csv_cum.csv\n/home/ubuntu/gw-workspace/kFTUwuDKTkzg/data_merge_hackweek.py:464: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  final_test_df = pd.read_csv(final_testing_file_path)\nfinal training data is saved to /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n",
  "history_begin_time" : 1698361903506,
  "history_end_time" : 1698361904596,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vnSMp4JheSwJ",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  old_train_df.drop([\"pmv\", \"SnowClass\"], axis=1, inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\nadd_no_snow_data_points()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only.csv_cum.csv\n/home/ubuntu/gw-workspace/vnSMp4JheSwJ/data_merge_hackweek.py:464: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  final_test_df = pd.read_csv(final_testing_file_path)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/vnSMp4JheSwJ/data_merge_hackweek.py\", line 484, in <module>\n    add_no_snow_data_points()\n  File \"/home/ubuntu/gw-workspace/vnSMp4JheSwJ/data_merge_hackweek.py\", line 470, in add_no_snow_data_points\n    chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n                            ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 3767, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 5876, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 5938, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['cumulative_fSCA'] not in index\"\n",
  "history_begin_time" : 1698361886868,
  "history_end_time" : 1698361887903,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7Mw0FNLMHRrH",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  old_train_df.drop([\"pmv\", \"SnowClass\"], axis=1, inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\nadd_no_snow_data_points()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only.csv_cum.csv\n/home/ubuntu/gw-workspace/7Mw0FNLMHRrH/data_merge_hackweek.py:464: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  final_test_df = pd.read_csv(final_testing_file_path)\nfinal training data is saved to /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only_with_no_snow.csv\n",
  "history_begin_time" : 1698361604470,
  "history_end_time" : 1698361605539,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TV4XOcRNU6i8",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  old_train_df.drop([\"pmv\", \"SnowClass\"], inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\nadd_no_snow_data_points()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only.csv_cum.csv\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/TV4XOcRNU6i8/data_merge_hackweek.py\", line 484, in <module>\n    add_no_snow_data_points()\n  File \"/home/ubuntu/gw-workspace/TV4XOcRNU6i8/data_merge_hackweek.py\", line 463, in add_no_snow_data_points\n    old_train_df.drop([\"pmv\", \"SnowClass\"], inplace=True)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['pmv', 'SnowClass'] not found in axis\"\n",
  "history_begin_time" : 1698361487153,
  "history_end_time" : 1698361487743,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "8Mb8MiSpvWXW",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\nadd_no_snow_data_points()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only.csv_cum.csv\n/home/ubuntu/gw-workspace/8Mb8MiSpvWXW/data_merge_hackweek.py:463: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  final_test_df = pd.read_csv(final_testing_file_path)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/8Mb8MiSpvWXW/data_merge_hackweek.py\", line 483, in <module>\n    add_no_snow_data_points()\n  File \"/home/ubuntu/gw-workspace/8Mb8MiSpvWXW/data_merge_hackweek.py\", line 469, in add_no_snow_data_points\n    chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n                            ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 3767, in __getitem__\n    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 5876, in _get_indexer_strict\n    self._raise_if_missing(keyarr, indexer, axis_name)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 5938, in _raise_if_missing\n    raise KeyError(f\"{not_found} not in index\")\nKeyError: \"['pmv', 'SnowClass'] not in index\"\n",
  "history_begin_time" : 1698361398918,
  "history_end_time" : 1698361399943,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "oRs6eCGsu7Km",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/\"\n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\nadd_no_snow_data_points()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_water_year_winter_month_only.csv_cum.csv\n",
  "history_begin_time" : 1698360818030,
  "history_end_time" : 1698360818723,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "rxN1tAqvNn1R",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\ncreate_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\nadd_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/rxN1tAqvNn1R/data_merge_hackweek.py:360: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n  current_df = pd.read_csv(csv_file_path)\n         date    lat      lon  etr  ...     aspect   curvature  northness  eastness\n0  2017-10-01  38.74 -119.744  4.7  ...  278.16498  -2075.4006   0.141080 -0.780304\n1  2017-10-01  38.74 -119.708  4.7  ...  261.02734   2939.2131  -0.154717 -0.779242\n2  2017-10-01  38.74 -119.672  4.5  ...  207.80370   8886.4110  -0.724214 -0.436444\n3  2017-10-01  38.74 -119.636  4.5  ...  180.53543  10687.0690  -0.785376 -0.009344\n4  2017-10-01  38.74 -119.600  4.5  ...   83.49026   5316.9870   0.112890  0.782164\n[5 rows x 21 columns]\n        date    lat      lon  ...  cumulative_vpd  cumulative_vs  cumulative_pr\n0 2017-10-01  38.74 -119.744  ...            0.80            3.2            0.0\n1 2017-10-01  38.74 -119.708  ...            0.80            3.2            0.0\n2 2017-10-01  38.74 -119.672  ...            0.73            3.3            0.0\n3 2017-10-01  38.74 -119.636  ...            0.76            3.2            0.0\n4 2017-10-01  38.74 -119.600  ...            0.75            3.1            0.0\n[5 rows x 29 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_testing_cum_water_year_winter_month_only.csv\n/home/ubuntu/gw-workspace/rxN1tAqvNn1R/data_merge_hackweek.py:380: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\nall data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_with_station_elevation.csv\n",
  "history_begin_time" : 1698356428309,
  "history_end_time" : 1698356435225,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "kbe9bTYjYuIl",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n784\n784\n/home/ubuntu/gw-workspace/kbe9bTYjYuIl/data_merge_hackweek.py:411: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n784\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n784\ncheck head:       lat      lon  etr        date   pr  ...   tmmx   vpd   vs  lc_code  fSCA\n0  38.74 -119.744  4.7  2017-10-01  0.0  ...  289.8  0.80  3.2    152.0   0.0\n1  38.74 -119.708  4.7  2017-10-01  0.0  ...  289.8  0.80  3.2    152.0   0.0\n2  38.74 -119.672  4.5  2017-10-01  0.0  ...  288.7  0.73  3.3    142.0   0.0\n3  38.74 -119.636  4.5  2017-10-01  0.0  ...  289.5  0.76  3.2    152.0   0.0\n4  38.74 -119.600  4.5  2017-10-01  0.0  ...  289.0  0.75  3.1    152.0   0.0\n[5 rows x 13 columns]\nIndex(['lat', 'lon', 'etr', 'date', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'fSCA', 'x', 'y', 'elevation', 'slope',\n       'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...    curvature  northness  eastness\n0       2017-10-01  38.740 -119.744  ...  -2075.40060   0.141080 -0.780304\n1       2017-10-01  38.740 -119.708  ...   2939.21310  -0.154717 -0.779242\n2       2017-10-01  38.740 -119.672  ...   8886.41100  -0.724214 -0.436444\n3       2017-10-01  38.740 -119.636  ...  10687.06900  -0.785376 -0.009344\n4       2017-10-01  38.740 -119.600  ...   5316.98700   0.112890  0.782164\n...            ...     ...      ...  ...          ...        ...       ...\n214811  2018-07-01  37.768 -118.916  ...   2246.17430   0.656027  0.568132\n214812  2018-07-01  37.768 -118.880  ...    493.22028  -0.700213  0.493939\n214813  2018-07-01  37.768 -118.844  ...  -1904.00550   0.242157  0.769663\n214814  2018-07-01  37.768 -118.808  ...  -2826.01150   0.739448 -0.389112\n214815  2018-07-01  37.768 -118.772  ...    462.71835   0.567474 -0.656508\n[214816 rows x 21 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing.csv\n",
  "history_begin_time" : 1698356410460,
  "history_end_time" : 1698356414563,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SDzS1lNm36Bt",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n784\n784\n/home/ubuntu/gw-workspace/SDzS1lNm36Bt/data_merge_hackweek.py:411: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n784\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n784\ncheck head:       lat      lon  etr        date   pr  ...   tmmx   vpd   vs  lc_code  fSCA\n0  38.74 -119.744  4.7  2017-10-01  0.0  ...  289.8  0.80  3.2    152.0   0.0\n1  38.74 -119.708  4.7  2017-10-01  0.0  ...  289.8  0.80  3.2    152.0   0.0\n2  38.74 -119.672  4.5  2017-10-01  0.0  ...  288.7  0.73  3.3    142.0   0.0\n3  38.74 -119.636  4.5  2017-10-01  0.0  ...  289.5  0.76  3.2    152.0   0.0\n4  38.74 -119.600  4.5  2017-10-01  0.0  ...  289.0  0.75  3.1    152.0   0.0\n[5 rows x 13 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/SDzS1lNm36Bt/data_merge_hackweek.py\", line 473, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/SDzS1lNm36Bt/data_merge_hackweek.py\", line 439, in merge_all_testing_data_together\n    merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Unnamed: 0'] not found in axis\"\n",
  "history_begin_time" : 1698356398298,
  "history_end_time" : 1698356401157,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "fZZgYsQdAKB4",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n784\n784\n/home/ubuntu/gw-workspace/fZZgYsQdAKB4/data_merge_hackweek.py:411: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n784\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n10\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/fZZgYsQdAKB4/data_merge_hackweek.py\", line 473, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/fZZgYsQdAKB4/data_merge_hackweek.py\", line 421, in merge_all_testing_data_together\n    data_sanity_checks(df4)\n  File \"/home/ubuntu/gw-workspace/fZZgYsQdAKB4/data_merge_hackweek.py\", line 396, in data_sanity_checks\n    raise ValueError(\"Number of unique locations is less than 700\")\nValueError: Number of unique locations is less than 700\n",
  "history_begin_time" : 1698356361277,
  "history_end_time" : 1698356363436,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "90J4kZAkzRKU",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/90J4kZAkzRKU/data_merge_hackweek.py:325: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n784\n               date     lat      lon    fSCA\n0        2016-01-01  38.740 -119.744  0.7804\n1        2016-01-01  38.740 -119.708  0.9766\n2        2016-01-01  38.740 -119.672   0.953\n3        2016-01-01  38.740 -119.636  0.7946\n4        2016-01-01  38.740 -119.600  0.8594\n...             ...     ...      ...     ...\n4867851  2018-12-31  37.768 -118.916  0.5584\n4867852  2018-12-31  37.768 -118.880  0.0098\n4867853  2018-12-31  37.768 -118.844     0.0\n4867854  2018-12-31  37.768 -118.808     0.0\n4867855  2018-12-31  37.768 -118.772  0.0924\n[4867856 rows x 4 columns]\nAll years of data are saved to /home/ubuntu/gridmet_test_run/fsca_testing_all_years.csv\n",
  "history_begin_time" : 1698356339349,
  "history_end_time" : 1698356346743,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fOwiBDBDgR2s",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\ncollect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\nprocessing 2017-10-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-07-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\n   Latitude  Longitude  etr        date   pr  ...  rmin   tmmn   tmmx   vpd   vs\n0     38.74   -119.744  4.7  2017-10-01  0.0  ...  19.8  273.1  289.8  0.80  3.2\n1     38.74   -119.708  4.7  2017-10-01  0.0  ...  19.8  273.1  289.8  0.80  3.2\n2     38.74   -119.672  4.5  2017-10-01  0.0  ...  20.8  272.1  288.7  0.73  3.3\n3     38.74   -119.636  4.5  2017-10-01  0.0  ...  20.2  272.5  289.5  0.76  3.2\n4     38.74   -119.600  4.5  2017-10-01  0.0  ...  20.9  272.6  289.0  0.75  3.1\n[5 rows x 11 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/fOwiBDBDgR2s/data_merge_hackweek.py\", line 469, in <module>\n    collect_gridmet_for_testing()\n  File \"/home/ubuntu/gw-workspace/fOwiBDBDgR2s/data_merge_hackweek.py\", line 149, in collect_gridmet_for_testing\n    all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nNameError: name 'gridmet_terrain_testing_data' is not defined. Did you mean: 'terrain_testing_data'?\n",
  "history_begin_time" : 1698356202414,
  "history_end_time" : 1698356206817,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "mGv9mmBf4XMM",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n784\n784\n/home/ubuntu/gw-workspace/mGv9mmBf4XMM/data_merge_hackweek.py:411: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n784\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n10\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/mGv9mmBf4XMM/data_merge_hackweek.py\", line 473, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/mGv9mmBf4XMM/data_merge_hackweek.py\", line 421, in merge_all_testing_data_together\n    data_sanity_checks(df4)\n  File \"/home/ubuntu/gw-workspace/mGv9mmBf4XMM/data_merge_hackweek.py\", line 396, in data_sanity_checks\n    raise ValueError(\"Number of unique locations is less than 700\")\nValueError: Number of unique locations is less than 700\n",
  "history_begin_time" : 1698356188781,
  "history_end_time" : 1698356190948,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dp0W3bGifkcf",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n784\n784\n/home/ubuntu/gw-workspace/dp0W3bGifkcf/data_merge_hackweek.py:409: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n784\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n10\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/dp0W3bGifkcf/data_merge_hackweek.py\", line 471, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/dp0W3bGifkcf/data_merge_hackweek.py\", line 419, in merge_all_testing_data_together\n    data_sanity_checks(df4)\n  File \"/home/ubuntu/gw-workspace/dp0W3bGifkcf/data_merge_hackweek.py\", line 394, in data_sanity_checks\n    raise ValueError(\"Number of unique locations is less than 700\")\nValueError: Number of unique locations is less than 700\n",
  "history_begin_time" : 1698356071731,
  "history_end_time" : 1698356073880,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "XFKPQVXHIffI",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/XFKPQVXHIffI/data_merge_hackweek.py:408: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/XFKPQVXHIffI/data_merge_hackweek.py\", line 470, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/XFKPQVXHIffI/data_merge_hackweek.py\", line 418, in merge_all_testing_data_together\n    data_sanity_checks(df4)\n  File \"/home/ubuntu/gw-workspace/XFKPQVXHIffI/data_merge_hackweek.py\", line 393, in data_sanity_checks\n    raise ValueError(\"Number of unique locations is less than 700\")\nValueError: Number of unique locations is less than 700\n",
  "history_begin_time" : 1698356040687,
  "history_end_time" : 1698356042858,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "a7bY4RuxMgr7",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/a7bY4RuxMgr7/data_merge_hackweek.py\", line 469, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/a7bY4RuxMgr7/data_merge_hackweek.py\", line 405, in merge_all_testing_data_together\n    data_sanity_checks(df2)\n  File \"/home/ubuntu/gw-workspace/a7bY4RuxMgr7/data_merge_hackweek.py\", line 393, in data_sanity_checks\n    raise ValueError(\"Number of unique locations is less than 700\")\nValueError: Number of unique locations is less than 700\n",
  "history_begin_time" : 1698355904771,
  "history_end_time" : 1698355905587,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "djde6H06GyFt",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \ndef data_sanity_checks(data_path):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = points_df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  print(unique_locations)\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\ndata_sanity_checks()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/djde6H06GyFt/data_merge_hackweek.py\", line 463, in <module>\n    data_sanity_checks()\nTypeError: data_sanity_checks() missing 1 required positional argument: 'data_path'\n",
  "history_begin_time" : 1698355726224,
  "history_end_time" : 1698355726708,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "CQTvnZBI8viO",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \ndef data_sanity_checks():\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  points_df = pd.read_csv(original_testing_points)\n  # Get unique locations\n  unique_locations = points_df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  print(unique_locations)\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\ndata_sanity_checks()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n        lat      lon\n0    38.740 -119.744\n1    38.740 -119.708\n2    38.740 -119.672\n3    38.740 -119.636\n4    38.740 -119.600\n..      ...      ...\n779  37.768 -118.916\n780  37.768 -118.880\n781  37.768 -118.844\n782  37.768 -118.808\n783  37.768 -118.772\n[784 rows x 2 columns]\n",
  "history_begin_time" : 1698355698542,
  "history_end_time" : 1698355699032,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "qwwEy7Fug82J",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \ndef data_sanity_checks():\n  all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  # Get unique locations\n  unique_locations = all_csv_df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  print(unique_locations)\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\ndata_sanity_checks()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n            lat         lon\n0     37.897480 -119.262434\n274   38.279274 -119.612776\n548   37.798171 -119.199551\n822   38.152231 -119.666675\n1096  38.504580 -119.621760\n1370  38.060340 -119.666675\n1644  37.833654 -119.451081\n1918  37.876210 -119.343283\n2192  38.039118 -119.307350\n2466  37.862028 -119.657692\n",
  "history_begin_time" : 1698355634385,
  "history_end_time" : 1698355634877,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "eahazTnH1aTu",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \ndef data_sanity_checks():\n  all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  # Get unique locations\n  unique_locations = df[['Latitude', 'Longitude']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  print(unique_locations)\n  \n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\ndata_sanity_checks()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/eahazTnH1aTu/data_merge_hackweek.py\", line 462, in <module>\n    data_sanity_checks()\n  File \"/home/ubuntu/gw-workspace/eahazTnH1aTu/data_merge_hackweek.py\", line 437, in data_sanity_checks\n    unique_locations = df[['Latitude', 'Longitude']].drop_duplicates()\n                       ^^\nNameError: name 'df' is not defined\n",
  "history_begin_time" : 1698355615255,
  "history_end_time" : 1698355615746,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AqA2RCd9t7Y2",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\nadd_elevation_in_feet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nall data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_with_station_elevation.csv\n",
  "history_begin_time" : 1698354591205,
  "history_end_time" : 1698354591737,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oiWOkzOOYMLt",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\nadd_elevation_in_feet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/oiWOkzOOYMLt/data_merge_hackweek.py:429: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing.csv\")\nall data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_with_station_elevation.csv\n",
  "history_begin_time" : 1698354481881,
  "history_end_time" : 1698354484028,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Ju0MZ50BRxbY",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\"\")\n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\nadd_elevation_in_feet()\n\n\n",
  "history_output" : "  File \"/home/ubuntu/gw-workspace/Ju0MZ50BRxbY/data_merge_hackweek.py\", line 432\n    print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\"\")\n                                                                                          ^\nSyntaxError: unterminated string literal (detected at line 432)\n",
  "history_begin_time" : 1698354473396,
  "history_end_time" : 1698354473412,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "VOcgKX6EqmJN",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\"\")\n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\nadd_elevation_in_feet()\n\n\n",
  "history_output" : "  File \"/home/ubuntu/gw-workspace/VOcgKX6EqmJN/data_merge_hackweek.py\", line 432\n    print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\"\")\n                                                                                          ^\nSyntaxError: unterminated string literal (detected at line 432)\n",
  "history_begin_time" : 1698354464376,
  "history_end_time" : 1698354464392,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ryJgo8cwUNKk",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\"\")\n  \n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\nadd_elevation_in_feet()\n\n\n",
  "history_output" : "  File \"/home/ubuntu/gw-workspace/ryJgo8cwUNKk/data_merge_hackweek.py\", line 432\n    print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\"\")\n                                                                                          ^\nSyntaxError: unterminated string literal (detected at line 432)\n",
  "history_begin_time" : 1698354455350,
  "history_end_time" : 1698354455367,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "rlLLfhCJZXwX",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_testing_cum_water_year_winter_month_only.csv\n",
  "history_begin_time" : 1698354205398,
  "history_end_time" : 1698354205976,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "yagB9XjJ3vZ3",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\ncreate_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/yagB9XjJ3vZ3/data_merge_hackweek.py:409: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  current_df = pd.read_csv(csv_file_path)\n         date    lat      lon  etr  ...     aspect   curvature  northness  eastness\n0  2017-10-01  38.74 -119.744  4.7  ...  278.16498  -2075.4006   0.141080 -0.780304\n1  2017-10-01  38.74 -119.708  4.7  ...  261.02734   2939.2131  -0.154717 -0.779242\n2  2017-10-01  38.74 -119.672  4.5  ...  207.80370   8886.4110  -0.724214 -0.436444\n3  2017-10-01  38.74 -119.636  4.5  ...  180.53543  10687.0690  -0.785376 -0.009344\n4  2017-10-01  38.74 -119.600  4.5  ...   83.49026   5316.9870   0.112890  0.782164\n[5 rows x 23 columns]\n        date    lat      lon  ...  cumulative_vpd  cumulative_vs  cumulative_pr\n0 2017-10-01  38.74 -119.744  ...            0.80            3.2            0.0\n1 2017-10-01  38.74 -119.708  ...            0.80            3.2            0.0\n2 2017-10-01  38.74 -119.672  ...            0.73            3.3            0.0\n3 2017-10-01  38.74 -119.636  ...            0.76            3.2            0.0\n4 2017-10-01  38.74 -119.600  ...            0.75            3.1            0.0\n[5 rows x 31 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_testing_cum_water_year_winter_month_only.csv\n",
  "history_begin_time" : 1698354178385,
  "history_end_time" : 1698354182824,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ULrY6QzQiJpL",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\ncreate_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/ULrY6QzQiJpL/data_merge_hackweek.py:409: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  current_df = pd.read_csv(csv_file_path)\n         date    lat      lon  etr  ...     aspect   curvature  northness  eastness\n0  2017-10-01  38.74 -119.744  4.7  ...  278.16498  -2075.4006   0.141080 -0.780304\n1  2017-10-01  38.74 -119.708  4.7  ...  261.02734   2939.2131  -0.154717 -0.779242\n2  2017-10-01  38.74 -119.672  4.5  ...  207.80370   8886.4110  -0.724214 -0.436444\n3  2017-10-01  38.74 -119.636  4.5  ...  180.53543  10687.0690  -0.785376 -0.009344\n4  2017-10-01  38.74 -119.600  4.5  ...   83.49026   5316.9870   0.112890  0.782164\n[5 rows x 23 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1353, in apply\n    result = self._python_apply_general(f, self._selected_obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1402, in _python_apply_general\n    values, mutated = self.grouper.apply(f, data, self.axis)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py\", line 767, in apply\n    res = f(group)\n          ^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/ULrY6QzQiJpL/data_merge_hackweek.py\", line 421, in <lambda>\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11472, in cumsum\n    return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11080, in cumsum\n    return self._accum_func(\"cumsum\", np.cumsum, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11065, in _accum_func\n    result = self._mgr.apply(block_accum_func)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 350, in apply\n    applied = b.apply(f, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 329, in apply\n    result = func(self.values, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11060, in block_accum_func\n    result = nanops.na_accum_func(values, func, skipna=skipna)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py\", line 1762, in na_accum_func\n    result = accum_func(vals, axis=0)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<__array_function__ internals>\", line 200, in cumsum\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 2597, in cumsum\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 66, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 43, in _wrapit\n    result = getattr(asarray(obj), method)(*args, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/ULrY6QzQiJpL/data_merge_hackweek.py\", line 444, in <module>\n    create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n  File \"/home/ubuntu/gw-workspace/ULrY6QzQiJpL/data_merge_hackweek.py\", line 421, in create_accumulative_columns\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1363, in apply\n    return self._python_apply_general(f, self._obj_with_exclusions)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1402, in _python_apply_general\n    values, mutated = self.grouper.apply(f, data, self.axis)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py\", line 767, in apply\n    res = f(group)\n          ^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/ULrY6QzQiJpL/data_merge_hackweek.py\", line 421, in <lambda>\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11472, in cumsum\n    return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11080, in cumsum\n    return self._accum_func(\"cumsum\", np.cumsum, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11065, in _accum_func\n    result = self._mgr.apply(block_accum_func)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 350, in apply\n    applied = b.apply(f, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 329, in apply\n    result = func(self.values, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11060, in block_accum_func\n    result = nanops.na_accum_func(values, func, skipna=skipna)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py\", line 1762, in na_accum_func\n    result = accum_func(vals, axis=0)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<__array_function__ internals>\", line 200, in cumsum\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 2597, in cumsum\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 66, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 43, in _wrapit\n    result = getattr(asarray(obj), method)(*args, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\n",
  "history_begin_time" : 1698354142307,
  "history_end_time" : 1698354144679,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "G429m9msC8Y1",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", ]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n\ncreate_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/G429m9msC8Y1/data_merge_hackweek.py:409: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  current_df = pd.read_csv(csv_file_path)\n         date    lat      lon  etr  ...     aspect   curvature  northness  eastness\n0  2017-10-01  38.74 -119.744  4.7  ...  278.16498  -2075.4006   0.141080 -0.780304\n1  2017-10-01  38.74 -119.708  4.7  ...  261.02734   2939.2131  -0.154717 -0.779242\n2  2017-10-01  38.74 -119.672  4.5  ...  207.80370   8886.4110  -0.724214 -0.436444\n3  2017-10-01  38.74 -119.636  4.5  ...  180.53543  10687.0690  -0.785376 -0.009344\n4  2017-10-01  38.74 -119.600  4.5  ...   83.49026   5316.9870   0.112890  0.782164\n[5 rows x 23 columns]\n        date    lat      lon  ...  cumulative_vpd  cumulative_vs  cumulative_pr\n0 2017-10-01  38.74 -119.744  ...            0.80            3.2            0.0\n1 2017-10-01  38.74 -119.708  ...            0.80            3.2            0.0\n2 2017-10-01  38.74 -119.672  ...            0.73            3.3            0.0\n3 2017-10-01  38.74 -119.636  ...            0.76            3.2            0.0\n4 2017-10-01  38.74 -119.600  ...            0.75            3.1            0.0\n[5 rows x 31 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_testing_cum_water_year_winter_month_only.csv\n",
  "history_begin_time" : 1698354073552,
  "history_end_time" : 1698354077956,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "1zhGIO2EOzNL",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\ncreate_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/1zhGIO2EOzNL/data_merge_hackweek.py:409: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n  current_df = pd.read_csv(csv_file_path)\n         date    lat      lon  etr  ...     aspect   curvature  northness  eastness\n0  2017-10-01  38.74 -119.744  4.7  ...  278.16498  -2075.4006   0.141080 -0.780304\n1  2017-10-01  38.74 -119.708  4.7  ...  261.02734   2939.2131  -0.154717 -0.779242\n2  2017-10-01  38.74 -119.672  4.5  ...  207.80370   8886.4110  -0.724214 -0.436444\n3  2017-10-01  38.74 -119.636  4.5  ...  180.53543  10687.0690  -0.785376 -0.009344\n4  2017-10-01  38.74 -119.600  4.5  ...   83.49026   5316.9870   0.112890  0.782164\n[5 rows x 23 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1353, in apply\n    result = self._python_apply_general(f, self._selected_obj)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1402, in _python_apply_general\n    values, mutated = self.grouper.apply(f, data, self.axis)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py\", line 767, in apply\n    res = f(group)\n          ^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/1zhGIO2EOzNL/data_merge_hackweek.py\", line 420, in <lambda>\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11472, in cumsum\n    return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11080, in cumsum\n    return self._accum_func(\"cumsum\", np.cumsum, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11065, in _accum_func\n    result = self._mgr.apply(block_accum_func)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 350, in apply\n    applied = b.apply(f, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 329, in apply\n    result = func(self.values, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11060, in block_accum_func\n    result = nanops.na_accum_func(values, func, skipna=skipna)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py\", line 1762, in na_accum_func\n    result = accum_func(vals, axis=0)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<__array_function__ internals>\", line 200, in cumsum\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 2597, in cumsum\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 66, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 43, in _wrapit\n    result = getattr(asarray(obj), method)(*args, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 57, in _wrapfunc\n    return bound(*args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/1zhGIO2EOzNL/data_merge_hackweek.py\", line 442, in <module>\n    create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n  File \"/home/ubuntu/gw-workspace/1zhGIO2EOzNL/data_merge_hackweek.py\", line 420, in create_accumulative_columns\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1363, in apply\n    return self._python_apply_general(f, self._obj_with_exclusions)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py\", line 1402, in _python_apply_general\n    values, mutated = self.grouper.apply(f, data, self.axis)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py\", line 767, in apply\n    res = f(group)\n          ^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/1zhGIO2EOzNL/data_merge_hackweek.py\", line 420, in <lambda>\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n                                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11472, in cumsum\n    return NDFrame.cumsum(self, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11080, in cumsum\n    return self._accum_func(\"cumsum\", np.cumsum, axis, skipna, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11065, in _accum_func\n    result = self._mgr.apply(block_accum_func)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py\", line 350, in apply\n    applied = b.apply(f, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py\", line 329, in apply\n    result = func(self.values, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 11060, in block_accum_func\n    result = nanops.na_accum_func(values, func, skipna=skipna)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py\", line 1762, in na_accum_func\n    result = accum_func(vals, axis=0)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"<__array_function__ internals>\", line 200, in cumsum\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 2597, in cumsum\n    return _wrapfunc(a, 'cumsum', axis=axis, dtype=dtype, out=out)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 66, in _wrapfunc\n    return _wrapit(obj, method, *args, **kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py\", line 43, in _wrapit\n    result = getattr(asarray(obj), method)(*args, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: can only concatenate str (not \"float\") to str\n",
  "history_begin_time" : 1698353943010,
  "history_end_time" : 1698353945373,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Ep6zW4XAXS6k",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nd2 types:  date     object\nlat     float64\nlon     float64\npmv     float64\ndtype: object\n/home/ubuntu/gw-workspace/Ep6zW4XAXS6k/data_merge_hackweek.py:347: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\ncheck head:       lat      lon  etr        date   pr  ...   vpd   vs  lc_code  pmv  fSCA\n0  38.74 -119.744  4.7  2017-10-01  0.0  ...  0.80  3.2    152.0  NaN   0.0\n1  38.74 -119.708  4.7  2017-10-01  0.0  ...  0.80  3.2    152.0  NaN   0.0\n2  38.74 -119.672  4.5  2017-10-01  0.0  ...  0.73  3.3    142.0  NaN   0.0\n3  38.74 -119.636  4.5  2017-10-01  0.0  ...  0.76  3.2    152.0  NaN   0.0\n4  38.74 -119.600  4.5  2017-10-01  0.0  ...  0.75  3.1    152.0  NaN   0.0\n[5 rows x 14 columns]\nIndex(['lat', 'lon', 'etr', 'date', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...    curvature  northness  eastness\n0       2017-10-01  38.740 -119.744  ...  -2075.40060   0.141080 -0.780304\n1       2017-10-01  38.740 -119.708  ...   2939.21310  -0.154717 -0.779242\n2       2017-10-01  38.740 -119.672  ...   8886.41100  -0.724214 -0.436444\n3       2017-10-01  38.740 -119.636  ...  10687.06900  -0.785376 -0.009344\n4       2017-10-01  38.740 -119.600  ...   5316.98700   0.112890  0.782164\n...            ...     ...      ...  ...          ...        ...       ...\n214811  2018-07-01  37.768 -118.916  ...   2246.17430   0.656027  0.568132\n214812  2018-07-01  37.768 -118.880  ...    493.22028  -0.700213  0.493939\n214813  2018-07-01  37.768 -118.844  ...  -1904.00550   0.242157  0.769663\n214814  2018-07-01  37.768 -118.808  ...  -2826.01150   0.739448 -0.389112\n214815  2018-07-01  37.768 -118.772  ...    462.71835   0.567474 -0.656508\n[214816 rows x 23 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing.csv\n",
  "history_begin_time" : 1698353330494,
  "history_end_time" : 1698353334528,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oNhVwcvFSFzC",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nd2 types:  date     object\nlat     float64\nlon     float64\npmv     float64\ndtype: object\n/home/ubuntu/gw-workspace/oNhVwcvFSFzC/data_merge_hackweek.py:347: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\ncheck head:       lat      lon  etr        date   pr  ...   vpd   vs  lc_code  pmv  fSCA\n0  38.74 -119.744  4.7  2017-10-01  0.0  ...  0.80  3.2    152.0  NaN   0.0\n1  38.74 -119.708  4.7  2017-10-01  0.0  ...  0.80  3.2    152.0  NaN   0.0\n2  38.74 -119.672  4.5  2017-10-01  0.0  ...  0.73  3.3    142.0  NaN   0.0\n3  38.74 -119.636  4.5  2017-10-01  0.0  ...  0.76  3.2    152.0  NaN   0.0\n4  38.74 -119.600  4.5  2017-10-01  0.0  ...  0.75  3.1    152.0  NaN   0.0\n[5 rows x 14 columns]\nIndex(['lat', 'lon', 'etr', 'date', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n       'vpd', 'vs', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...    curvature  northness  eastness\n0       2017-10-01  38.740 -119.744  ...  -2075.40060   0.141080 -0.780304\n1       2017-10-01  38.740 -119.708  ...   2939.21310  -0.154717 -0.779242\n2       2017-10-01  38.740 -119.672  ...   8886.41100  -0.724214 -0.436444\n3       2017-10-01  38.740 -119.636  ...  10687.06900  -0.785376 -0.009344\n4       2017-10-01  38.740 -119.600  ...   5316.98700   0.112890  0.782164\n...            ...     ...      ...  ...          ...        ...       ...\n214811  2018-07-01  37.768 -118.916  ...   2246.17430   0.656027  0.568132\n214812  2018-07-01  37.768 -118.880  ...    493.22028  -0.700213  0.493939\n214813  2018-07-01  37.768 -118.844  ...  -1904.00550   0.242157  0.769663\n214814  2018-07-01  37.768 -118.808  ...  -2826.01150   0.739448 -0.389112\n214815  2018-07-01  37.768 -118.772  ...    462.71835   0.567474 -0.656508\n[214816 rows x 23 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698353304322,
  "history_end_time" : 1698353308375,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5OZoTDGmc45I",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\ncollect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\nprocessing 2017-10-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-10-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-11-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2017-12-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-01-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-02-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-03-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-04-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-05-31\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-02\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-03\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-04\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-05\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-06\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-07\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-08\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-09\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-10\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-11\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-12\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-13\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-14\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-15\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-16\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-17\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-18\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-19\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-20\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-21\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-22\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-23\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-24\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-25\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-26\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-27\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-28\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-29\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-06-30\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\nprocessing 2018-07-01\netr\npr\nrmax\nrmin\ntmmn\ntmmx\nvpd\nvs\n   Latitude  Longitude  etr        date   pr  ...  rmin   tmmn   tmmx   vpd   vs\n0     38.74   -119.744  4.7  2017-10-01  0.0  ...  19.8  273.1  289.8  0.80  3.2\n1     38.74   -119.708  4.7  2017-10-01  0.0  ...  19.8  273.1  289.8  0.80  3.2\n2     38.74   -119.672  4.5  2017-10-01  0.0  ...  20.8  272.1  288.7  0.73  3.3\n3     38.74   -119.636  4.5  2017-10-01  0.0  ...  20.2  272.5  289.5  0.76  3.2\n4     38.74   -119.600  4.5  2017-10-01  0.0  ...  20.9  272.6  289.0  0.75  3.1\n[5 rows x 11 columns]\nOk, all gridmet data for testing days are saved to /home/ubuntu/gridmet_test_run/gridmet_testing_hackweek_subset.csv\n",
  "history_begin_time" : 1698353180073,
  "history_end_time" : 1698353185143,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "z4hxWyKWTwjl",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  \n\n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df == None:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df == None:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_terrain_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_terrain_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\ncollect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\nprocessing 2017-10-01\netr\npr\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/z4hxWyKWTwjl/data_merge_hackweek.py\", line 433, in <module>\n    collect_gridmet_for_testing()\n  File \"/home/ubuntu/gw-workspace/z4hxWyKWTwjl/data_merge_hackweek.py\", line 135, in collect_gridmet_for_testing\n    if single_day_df == None:\n       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1466, in __nonzero__\n    raise ValueError(\nValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
  "history_begin_time" : 1698353111686,
  "history_end_time" : 1698353112181,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nUNaEl8bGwvV",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.dtypes)\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nd2 types:  date     object\nlat     float64\nlon     float64\npmv     float64\ndtype: object\n/home/ubuntu/gw-workspace/nUNaEl8bGwvV/data_merge_hackweek.py:319: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\nUnnamed: 0      int64\nlat           float64\nlon           float64\nSnowClass       int64\ndate           object\ndtype: object\ncheck head:           date    lat      lon  lc_code  pmv    fSCA\n0  2017-01-01  38.74 -119.744      152  NaN     0.0\n1  2017-01-02  38.74 -119.744      152  NaN     0.0\n2  2017-01-03  38.74 -119.744      152  NaN  0.6802\n3  2017-01-04  38.74 -119.744      152  NaN  0.6274\n4  2017-01-05  38.74 -119.744      152  NaN  0.5212\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698352369266,
  "history_end_time" : 1698352375408,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XeIDx026CmN3",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(df4.dtypes)\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/XeIDx026CmN3/data_merge_hackweek.py:318: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndate     object\nlat     float64\nlon     float64\nfSCA     object\ndtype: object\nUnnamed: 0      int64\nlat           float64\nlon           float64\nSnowClass       int64\ndate           object\ndtype: object\ncheck head:           date    lat      lon  lc_code  pmv    fSCA\n0  2017-01-01  38.74 -119.744      152  NaN     0.0\n1  2017-01-02  38.74 -119.744      152  NaN     0.0\n2  2017-01-03  38.74 -119.744      152  NaN  0.6802\n3  2017-01-04  38.74 -119.744      152  NaN  0.6274\n4  2017-01-05  38.74 -119.744      152  NaN  0.5212\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698352335764,
  "history_end_time" : 1698352341954,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UxBuf82KsiWo",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  df3 = pd.read_csv(fsca_testing_data)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(\"df4: \", df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/UxBuf82KsiWo/data_merge_hackweek.py:318: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/UxBuf82KsiWo/data_merge_hackweek.py\", line 409, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/UxBuf82KsiWo/data_merge_hackweek.py\", line 326, in merge_all_testing_data_together_except_gridmet\n    df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n                  ^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/accessor.py\", line 224, in __get__\n    accessor_obj = self._accessor(obj)\n                   ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/accessors.py\", line 580, in __new__\n    raise AttributeError(\"Can only use .dt accessor with datetimelike values\")\nAttributeError: Can only use .dt accessor with datetimelike values. Did you mean: 'at'?\n",
  "history_begin_time" : 1698352298518,
  "history_end_time" : 1698352300421,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Ez8gihqLLcfy",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  df3 = pd.read_csv(fsca_testing_data)\n  df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(\"df4: \", df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/Ez8gihqLLcfy/data_merge_hackweek.py:318: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/Ez8gihqLLcfy/data_merge_hackweek.py\", line 409, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/Ez8gihqLLcfy/data_merge_hackweek.py\", line 319, in merge_all_testing_data_together_except_gridmet\n    df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n                  ^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/accessor.py\", line 224, in __get__\n    accessor_obj = self._accessor(obj)\n                   ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/accessors.py\", line 580, in __new__\n    raise AttributeError(\"Can only use .dt accessor with datetimelike values\")\nAttributeError: Can only use .dt accessor with datetimelike values. Did you mean: 'at'?\n",
  "history_begin_time" : 1698352280586,
  "history_end_time" : 1698352282520,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xaEJLaNp0Cx2",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  df3 = pd.read_csv(fsca_testing_data)\n  df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(\"df4: \", df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/xaEJLaNp0Cx2/data_merge_hackweek.py\", line 409, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/xaEJLaNp0Cx2/data_merge_hackweek.py\", line 317, in merge_all_testing_data_together_except_gridmet\n    df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n                  ^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/accessor.py\", line 224, in __get__\n    accessor_obj = self._accessor(obj)\n                   ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/accessors.py\", line 580, in __new__\n    raise AttributeError(\"Can only use .dt accessor with datetimelike values\")\nAttributeError: Can only use .dt accessor with datetimelike values. Did you mean: 'at'?\n",
  "history_begin_time" : 1698352243245,
  "history_end_time" : 1698352243930,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "x9ffb6kl7pWu",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  df2 = pd.read_csv(pmw_testing_new)\n  df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  df3 = pd.read_csv(fsca_testing_data)\n  df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  print(\"df4: \", df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/x9ffb6kl7pWu/data_merge_hackweek.py\", line 409, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/x9ffb6kl7pWu/data_merge_hackweek.py\", line 315, in merge_all_testing_data_together_except_gridmet\n    df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n                  ^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/accessor.py\", line 224, in __get__\n    accessor_obj = self._accessor(obj)\n                   ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/accessors.py\", line 580, in __new__\n    raise AttributeError(\"Can only use .dt accessor with datetimelike values\")\nAttributeError: Can only use .dt accessor with datetimelike values. Did you mean: 'at'?\n",
  "history_begin_time" : 1698352200079,
  "history_end_time" : 1698352200767,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "0M45Tp5JHXPy",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing_data)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  print(\"df4: \", df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/0M45Tp5JHXPy/data_merge_hackweek.py:316: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndf4:     Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\ncheck head:           date    lat      lon  lc_code  pmv    fSCA\n0  2017-01-01  38.74 -119.744      152  NaN     0.0\n1  2017-01-02  38.74 -119.744      152  NaN     0.0\n2  2017-01-03  38.74 -119.744      152  NaN  0.6802\n3  2017-01-04  38.74 -119.744      152  NaN  0.6274\n4  2017-01-05  38.74 -119.744      152  NaN  0.5212\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698351996657,
  "history_end_time" : 1698352002938,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "5eLsRU9wRARU",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/snowclass_hackweek_testing_correct_points.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing_data)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  print(\"df4: \", df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/5eLsRU9wRARU/data_merge_hackweek.py:316: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndf4:     Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698351882746,
  "history_end_time" : 1698351888980,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "osZHNGEE5Sdl",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/snowclass_hackweek_testing_correct_points.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing_data)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  print(\"df4: \", df4.head())\n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/osZHNGEE5Sdl/data_merge_hackweek.py:316: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\ndf4:     Unnamed: 0     lat     long  SnowClass        Date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698351868511,
  "history_end_time" : 1698351874958,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WBLWygEeAWoV",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/snowclass_hackweek_testing_correct_points.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the \n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing_data)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/WBLWygEeAWoV/data_merge_hackweek.py:316: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698351695910,
  "history_end_time" : 1698351702234,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gxobzy46mKj8",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing_data)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/gxobzy46mKj8/data_merge_hackweek.py:314: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing_data)\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698350127487,
  "history_end_time" : 1698350133716,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "T77JrQCcfaDy",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\n\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_fsca_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/T77JrQCcfaDy/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n               date     lat      lon    fSCA\n0        2016-01-01  38.740 -119.744  0.7804\n1        2016-01-01  38.740 -119.708  0.9766\n2        2016-01-01  38.740 -119.672   0.953\n3        2016-01-01  38.740 -119.636  0.7946\n4        2016-01-01  38.740 -119.600  0.8594\n...             ...     ...      ...     ...\n4867851  2018-12-31  37.768 -118.916  0.5584\n4867852  2018-12-31  37.768 -118.880  0.0098\n4867853  2018-12-31  37.768 -118.844     0.0\n4867854  2018-12-31  37.768 -118.808     0.0\n4867855  2018-12-31  37.768 -118.772  0.0924\n[4867856 rows x 4 columns]\nAll years of data are saved to /home/ubuntu/gridmet_test_run/fsca_testing_all_years.csv\n",
  "history_begin_time" : 1698350054243,
  "history_end_time" : 1698350061510,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UEaiRferr46u",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\n\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_fsca_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/UEaiRferr46u/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n               date     lat      lon    fSCA\n0        2016-01-01  38.740 -119.744  0.7804\n1        2016-01-01  38.740 -119.708  0.9766\n2        2016-01-01  38.740 -119.672   0.953\n3        2016-01-01  38.740 -119.636  0.7946\n4        2016-01-01  38.740 -119.600  0.8594\n...             ...     ...      ...     ...\n4867851  2018-12-31  37.768 -118.916  0.5584\n4867852  2018-12-31  37.768 -118.880  0.0098\n4867853  2018-12-31  37.768 -118.844     0.0\n4867854  2018-12-31  37.768 -118.808     0.0\n4867855  2018-12-31  37.768 -118.772  0.0924\n[4867856 rows x 4 columns]\n",
  "history_begin_time" : 1698350037262,
  "history_end_time" : 1698350044650,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "1P3C8yWQSXKx",
  "history_input" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\n\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  merged_df.to_csv(f'{work_dir}/fsca_testing_all_years.csv', index=False)\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_fsca_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n/home/ubuntu/gw-workspace/1P3C8yWQSXKx/data_merge_hackweek.py:296: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df = pd.read_csv(csv_file)\n               date     lat      lon    fSCA\n0        2016-01-01  38.740 -119.744  0.7804\n1        2016-01-01  38.740 -119.708  0.9766\n2        2016-01-01  38.740 -119.672   0.953\n3        2016-01-01  38.740 -119.636  0.7946\n4        2016-01-01  38.740 -119.600  0.8594\n...             ...     ...      ...     ...\n4867851  2018-12-31  37.768 -118.916  0.5584\n4867852  2018-12-31  37.768 -118.880  0.0098\n4867853  2018-12-31  37.768 -118.844     0.0\n4867854  2018-12-31  37.768 -118.808     0.0\n4867855  2018-12-31  37.768 -118.772  0.0924\n[4867856 rows x 4 columns]\n",
  "history_begin_time" : 1698350001141,
  "history_end_time" : 1698350008627,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fyrNdDFyjP8f",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\n\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n\n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  merged_df.to_csv(f'{work_dir}/fsca_testing_all_years.csv', index=False)\n  \n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  \n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_fsca_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/fyrNdDFyjP8f/data_merge_hackweek.py\", line 398, in <module>\n    merge_fsca_testing()\n  File \"/home/ubuntu/gw-workspace/fyrNdDFyjP8f/data_merge_hackweek.py\", line 289, in merge_fsca_testing\n    csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n                ^^^^\nNameError: name 'glob' is not defined\n",
  "history_begin_time" : 1698349988188,
  "history_end_time" : 1698349988679,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "SZYM3E4u6WyR",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/SZYM3E4u6WyR/data_merge_hackweek.py:290: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing)\n   Unnamed: 0     lat      lon  SnowClass        date\n0           1  37.912 -119.564          4  2017-10-01\n1           2  38.416 -119.276          5  2017-10-01\n2           3  38.632 -118.772          4  2017-10-01\n3           4  38.380 -118.988          4  2017-10-01\n4           5  38.308 -119.636          3  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass', 'x', 'y',\n       'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness'],\n      dtype='object')\n              date     lat      lon  ...   curvature  northness  eastness\n0       2017-01-01  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n1       2017-01-02  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n2       2017-01-03  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n3       2017-01-04  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n4       2017-01-05  38.740 -119.744  ... -2075.40060   0.141080 -0.780304\n...            ...     ...      ...  ...         ...        ...       ...\n858475  2019-12-27  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858476  2019-12-28  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858477  2019-12-29  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858478  2019-12-30  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n858479  2019-12-31  37.768 -118.772  ...   462.71835   0.567474 -0.656508\n[858480 rows x 15 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_testing_except_gridmet.csv\n",
  "history_begin_time" : 1698347307513,
  "history_end_time" : 1698347311861,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "I1KjrgPrILLp",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/I1KjrgPrILLp/data_merge_hackweek.py:290: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/I1KjrgPrILLp/data_merge_hackweek.py\", line 374, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/I1KjrgPrILLp/data_merge_hackweek.py\", line 301, in merge_all_testing_data_together_except_gridmet\n    merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 148, in merge\n    op = _MergeOperation(\n         ^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 737, in __init__\n    ) = self._get_merge_keys()\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 1203, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1778, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1698347080495,
  "history_end_time" : 1698347081513,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TU23StuVNRCi",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n         date     lat       lon        pmv\n0  2017-10-01  37.912  -119.564  256.08000\n1  2017-10-02  37.912  -119.564  252.03000\n2  2017-10-03  37.912  -119.564  253.20000\n3  2017-10-04  37.912  -119.564  250.89000\n4  2017-10-05  37.912  -119.564  254.06999\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698347021560,
  "history_end_time" : 1698347022133,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "e1w8Q0OJGI1a",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series([formatted_time_string, lat, lon,  new_row_data])\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series([formatted_time_string, lat, lon,  new_row_data])\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n         date     lat       lon                                        pmv\n0  2017-10-01  37.912  -119.564     [2017-10-01, 37.912, -119.564, 256.08]\n1  2017-10-02  37.912  -119.564     [2017-10-02, 37.912, -119.564, 252.03]\n2  2017-10-03  37.912  -119.564      [2017-10-03, 37.912, -119.564, 253.2]\n3  2017-10-04  37.912  -119.564     [2017-10-04, 37.912, -119.564, 250.89]\n4  2017-10-05  37.912  -119.564  [2017-10-05, 37.912, -119.564, 254.06999]\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698346958891,
  "history_end_time" : 1698346959478,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8tnRuzBzkgLC",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.DataFrame([formatted_time_string, lat, lon,  new_row_data])\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series([formatted_time_string, lat, lon,  new_row_data])\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n0                                            0\n0   ...\n1                                            0\n0   ...\n2                                           0\n0    ...\n3                                            0\n0   ...\n4                                               0\n0...\ndtype: object\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698346751447,
  "history_end_time" : 1698346752356,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "m8QxYSMPJUDi",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return [lat, lon, formatted_time_string, new_row_data]\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series([lat, lon, formatted_time_string, new_row_data])\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-01', '37.912', '-119.564', 256.08]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-02', '37.912', '-119.564', 252.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-03', '37.912', '-119.564', 253.2]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-04', '37.912', '-119.564', 250.89]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-05', '37.912', '-119.564', 254.06999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-07', '37.912', '-119.564', 255.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-08', '37.912', '-119.564', 256.74]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-09', '37.912', '-119.564', 255.89]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-10', '37.912', '-119.564', 254.98999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-11', '37.912', '-119.564', 255.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-12', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-13', '37.912', '-119.564', 253.83]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-15', '37.912', '-119.564', 256.63998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-16', '37.912', '-119.564', 256.57]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-17', '37.912', '-119.564', 258.91]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-18', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-19', '37.912', '-119.564', 256.13]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-20', '37.912', '-119.564', 255.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-22', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-23', '37.912', '-119.564', 260.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-24', '37.912', '-119.564', 261.21]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-25', '37.912', '-119.564', 258.46]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-26', '37.912', '-119.564', 259.13]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-27', '37.912', '-119.564', 259.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-28', '37.912', '-119.564', 260.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-30', '37.912', '-119.564', 256.61]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-31', '37.912', '-119.564', 254.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-01', '37.912', '-119.564', 255.97]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-02', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-03', '37.912', '-119.564', 254.37999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-04', '37.912', '-119.564', 254.92]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-07', '37.912', '-119.564', 249.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-08', '37.912', '-119.564', 253.68999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-09', '37.912', '-119.564', 255.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-10', '37.912', '-119.564', 254.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-11', '37.912', '-119.564', 251.42]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-12', '37.912', '-119.564', 256.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-14', '37.912', '-119.564', 250.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-15', '37.912', '-119.564', 254.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-16', '37.912', '-119.564', 254.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-17', '37.912', '-119.564', 250.43]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-18', '37.912', '-119.564', 247.20999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-19', '37.912', '-119.564', 248.51999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-20', '37.912', '-119.564', 255.15999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-22', '37.912', '-119.564', 247.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-23', '37.912', '-119.564', 247.25]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-24', '37.912', '-119.564', 247.76999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-25', '37.912', '-119.564', 249.68]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-26', '37.912', '-119.564', 250.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-27', '37.912', '-119.564', 248.29999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-29', '37.912', '-119.564', 240.9]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-30', '37.912', '-119.564', 239.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-01', '37.912', '-119.564', 242.18999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-02', '37.912', '-119.564', 245.89]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-03', '37.912', '-119.564', 248.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-04', '37.912', '-119.564', 240.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-05', '37.912', '-119.564', 244.68999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-07', '37.912', '-119.564', 241.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-08', '37.912', '-119.564', 242.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-09', '37.912', '-119.564', 242.59]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-10', '37.912', '-119.564', 244.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-11', '37.912', '-119.564', 249.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-12', '37.912', '-119.564', 248.76999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-15', '37.912', '-119.564', 245.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-16', '37.912', '-119.564', 243.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-17', '37.912', '-119.564', 245.20999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-18', '37.912', '-119.564', 246.76]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-19', '37.912', '-119.564', 247.01]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-20', '37.912', '-119.564', 249.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-22', '37.912', '-119.564', 241.18999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-23', '37.912', '-119.564', 245.59]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-24', '37.912', '-119.564', 247.09]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-25', '37.912', '-119.564', 250.45]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-26', '37.912', '-119.564', 248.43]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-27', '37.912', '-119.564', 249.36]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-30', '37.912', '-119.564', 244.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-31', '37.912', '-119.564', 245.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-01', '37.912', '-119.564', 247.15999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-02', '37.912', '-119.564', 249.04999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-03', '37.912', '-119.564', 251.29999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-04', '37.912', '-119.564', 254.06]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-06', '37.912', '-119.564', 246.45999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-07', '37.912', '-119.564', 242.39]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-08', '37.912', '-119.564', 248.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-09', '37.912', '-119.564', 250.9]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-10', '37.912', '-119.564', 251.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-11', '37.912', '-119.564', 245.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-12', '37.912', '-119.564', 243.75]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-14', '37.912', '-119.564', 239.68]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-15', '37.912', '-119.564', 240.06]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-16', '37.912', '-119.564', 242.31999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-17', '37.912', '-119.564', 244.67]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-18', '37.912', '-119.564', 245.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-19', '37.912', '-119.564', 248.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-21', '37.912', '-119.564', 232.51]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-22', '37.912', '-119.564', 240.09999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-23', '37.912', '-119.564', 237.33]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-24', '37.912', '-119.564', 239.20999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-25', '37.912', '-119.564', 238.43999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-26', '37.912', '-119.564', 236.81]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-27', '37.912', '-119.564', 244.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-29', '37.912', '-119.564', 239.37]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-30', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-31', '37.912', '-119.564', 242.06]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-01', '37.912', '-119.564', 243.67]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-02', '37.912', '-119.564', 246.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-03', '37.912', '-119.564', 246.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-04', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-06', '37.912', '-119.564', 235.23999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-07', '37.912', '-119.564', 237.81999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-08', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-09', '37.912', '-119.564', 240.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-10', '37.912', '-119.564', 241.45999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-11', '37.912', '-119.564', 239.79999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-12', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-13', '37.912', '-119.564', 229.59]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-14', '37.912', '-119.564', 228.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-15', '37.912', '-119.564', 228.47]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-16', '37.912', '-119.564', 237.17]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-17', '37.912', '-119.564', 239.47]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-18', '37.912', '-119.564', 241.51]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-19', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-21', '37.912', '-119.564', 226.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-22', '37.912', '-119.564', 232.2]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-23', '37.912', '-119.564', 228.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-24', '37.912', '-119.564', 233.34999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-25', '37.912', '-119.564', 235.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-26', '37.912', '-119.564', 238.79999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-27', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-28', '37.912', '-119.564', 230.47]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-01', '37.912', '-119.564', 240.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-02', '37.912', '-119.564', 235.54]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-03', '37.912', '-119.564', 237.15999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-04', '37.912', '-119.564', 235.36]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-05', '37.912', '-119.564', 233.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-06', '37.912', '-119.564', 235.09]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-07', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-08', '37.912', '-119.564', 234.87999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-09', '37.912', '-119.564', 233.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-10', '37.912', '-119.564', 235.51999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-11', '37.912', '-119.564', 235.79999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-12', '37.912', '-119.564', 236.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-13', '37.912', '-119.564', 252.40999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-15', '37.912', '-119.564', 235.87999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-16', '37.912', '-119.564', 237.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-17', '37.912', '-119.564', 234.65]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-18', '37.912', '-119.564', 233.65]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-19', '37.912', '-119.564', 235.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-20', '37.912', '-119.564', 240.54999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-21', '37.912', '-119.564', 251.01]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-22', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-23', '37.912', '-119.564', 240.11]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-24', '37.912', '-119.564', 232.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-25', '37.912', '-119.564', 232.73]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-26', '37.912', '-119.564', 230.23999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-27', '37.912', '-119.564', 235.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-28', '37.912', '-119.564', 238.34999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-30', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-31', '37.912', '-119.564', 231.40999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-01', '37.912', '-119.564', 229.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-02', '37.912', '-119.564', 231.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-03', '37.912', '-119.564', 231.98999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-04', '37.912', '-119.564', 236.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-05', '37.912', '-119.564', 246.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-07', '37.912', '-119.564', 248.48]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-08', '37.912', '-119.564', 225.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-09', '37.912', '-119.564', 230.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-10', '37.912', '-119.564', 243.33]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-11', '37.912', '-119.564', 240.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-12', '37.912', '-119.564', 232.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-13', '37.912', '-119.564', 221.37999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-15', '37.912', '-119.564', 229.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-16', '37.912', '-119.564', 231.29999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-17', '37.912', '-119.564', 226.97]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-18', '37.912', '-119.564', 231.87999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-19', '37.912', '-119.564', 229.33]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-20', '37.912', '-119.564', 234.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-22', '37.912', '-119.564', 237.68999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-23', '37.912', '-119.564', 234.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-24', '37.912', '-119.564', 242.54999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-25', '37.912', '-119.564', 239.67]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-26', '37.912', '-119.564', 248.83]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-27', '37.912', '-119.564', 254.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-28', '37.912', '-119.564', 242.26999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-30', '37.912', '-119.564', 230.56999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-01', '37.912', '-119.564', 238.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-02', '37.912', '-119.564', 238.65999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-03', '37.912', '-119.564', 238.45]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-04', '37.912', '-119.564', 246.65999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-05', '37.912', '-119.564', 253.45999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-07', '37.912', '-119.564', 250.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-08', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-09', '37.912', '-119.564', 256.44998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-10', '37.912', '-119.564', 252.61]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-11', '37.912', '-119.564', 253.01999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-12', '37.912', '-119.564', 249.01]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-13', '37.912', '-119.564', 255.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-15', '37.912', '-119.564', 254.09999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-16', '37.912', '-119.564', 248.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-17', '37.912', '-119.564', 245.37]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-18', '37.912', '-119.564', 254.17]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-19', '37.912', '-119.564', 256.08]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-20', '37.912', '-119.564', 255.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-22', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-23', '37.912', '-119.564', 257.66998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-24', '37.912', '-119.564', 256.36]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-25', '37.912', '-119.564', 255.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-26', '37.912', '-119.564', 251.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-27', '37.912', '-119.564', 256.02]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-28', '37.912', '-119.564', 260.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-30', '37.912', '-119.564', 259.49]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-31', '37.912', '-119.564', 257.44]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-01', '37.912', '-119.564', 255.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-02', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-03', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-04', '37.912', '-119.564', 263.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-07', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-08', '37.912', '-119.564', 261.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-09', '37.912', '-119.564', 260.54]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-10', '37.912', '-119.564', 259.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-11', '37.912', '-119.564', 263.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-12', '37.912', '-119.564', 266.94]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-14', '37.912', '-119.564', 262.46]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-15', '37.912', '-119.564', 262.9]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-16', '37.912', '-119.564', 263.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-17', '37.912', '-119.564', 258.25]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-18', '37.912', '-119.564', 262.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-19', '37.912', '-119.564', 263.25]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-20', '37.912', '-119.564', 265.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-22', '37.912', '-119.564', 264.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-23', '37.912', '-119.564', 265.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-24', '37.912', '-119.564', 267.66]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-25', '37.912', '-119.564', 266.8]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-26', '37.912', '-119.564', 268.43]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-27', '37.912', '-119.564', 268.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-29', '37.912', '-119.564', 261.28]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-30', '37.912', '-119.564', 265.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-01', '37.912', '-119.564', 266.38]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-02', '37.912', '-119.564', 268.19998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-03', '37.912', '-119.564', 265.77]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-04', '37.912', '-119.564', 265.28]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-07', '37.912', '-119.564', 267.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-08', '37.912', '-119.564', 268.63]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-09', '37.912', '-119.564', 266.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-10', '37.912', '-119.564', 268.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-11', '37.912', '-119.564', 268.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-12', '37.912', '-119.564', 268.3]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-15', '37.912', '-119.564', 269.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-16', '37.912', '-119.564', 270.19]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-17', '37.912', '-119.564', 268.28]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-18', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-19', '37.912', '-119.564', 269.44998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-20', '37.912', '-119.564', 270.12]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-22', '37.912', '-119.564', 267.94998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-23', '37.912', '-119.564', 268.35]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-24', '37.912', '-119.564', 270.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-25', '37.912', '-119.564', 268.24]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-26', '37.912', '-119.564', 269.50998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-27', '37.912', '-119.564', 269.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-30', '37.912', '-119.564', 269.00998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-31', '37.912', '-119.564', 270.05]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-01', '37.912', '-119.564', 269.6]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-02', '37.912', '-119.564', 269.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-03', '37.912', '-119.564', 265.11]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-04', '37.912', '-119.564', 268.00998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-06', '37.912', '-119.564', 264.84]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-07', '37.912', '-119.564', 266.65]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-08', '37.912', '-119.564', 266.61]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-09', '37.912', '-119.564', 266.74]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-10', '37.912', '-119.564', 268.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-11', '37.912', '-119.564', 267.96]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-12', '37.912', '-119.564', 267.12]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-14', '37.912', '-119.564', 267.3]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-15', '37.912', '-119.564', 266.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-16', '37.912', '-119.564', 266.8]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-17', '37.912', '-119.564', 266.55]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-18', '37.912', '-119.564', 266.68]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-19', '37.912', '-119.564', 268.16998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-21', '37.912', '-119.564', 262.38998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-22', '37.912', '-119.564', 262.44998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-23', '37.912', '-119.564', 262.99]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-24', '37.912', '-119.564', 263.49]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-25', '37.912', '-119.564', 262.09]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-26', '37.912', '-119.564', 262.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-27', '37.912', '-119.564', 263.25998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-29', '37.912', '-119.564', 263.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-30', '37.912', '-119.564', 261.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-31', '37.912', '-119.564', 259.52]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-01', '37.912', '-119.564', 262.66]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-02', '37.912', '-119.564', 264.91]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-03', '37.912', '-119.564', 265.11]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-04', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-06', '37.912', '-119.564', 264.63]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-07', '37.912', '-119.564', 264.63998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-08', '37.912', '-119.564', 263.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-09', '37.912', '-119.564', 261.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-10', '37.912', '-119.564', 261.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-11', '37.912', '-119.564', 261.82]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-12', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-13', '37.912', '-119.564', 258.83]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-14', '37.912', '-119.564', 258.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-15', '37.912', '-119.564', 260.46]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-16', '37.912', '-119.564', 257.99]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-17', '37.912', '-119.564', 257.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-18', '37.912', '-119.564', 257.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-19', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-21', '37.912', '-119.564', 260.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-22', '37.912', '-119.564', 260.1]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-23', '37.912', '-119.564', 259.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-24', '37.912', '-119.564', 259.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-25', '37.912', '-119.564', 260.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-26', '37.912', '-119.564', 261.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-27', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-28', '37.912', '-119.564', 260.3]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-29', '37.912', '-119.564', 260.0]\n0    [37.912, -119.564, 2017-10-01, [2017-10-01, 37...\n1    [37.912, -119.564, 2017-10-02, [2017-10-02, 37...\n2    [37.912, -119.564, 2017-10-03, [2017-10-03, 37...\n3    [37.912, -119.564, 2017-10-04, [2017-10-04, 37...\n4    [37.912, -119.564, 2017-10-05, [2017-10-05, 37...\ndtype: object\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698346641855,
  "history_end_time" : 1698346642425,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "aZsA61u9bDvE",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return [lat, lon, formatted_time_string, new_row_data]\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return [lat, lon, formatted_time_string, new_row_data]\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-01', '37.912', '-119.564', 256.08]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-02', '37.912', '-119.564', 252.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-03', '37.912', '-119.564', 253.2]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-04', '37.912', '-119.564', 250.89]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-05', '37.912', '-119.564', 254.06999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-07', '37.912', '-119.564', 255.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-08', '37.912', '-119.564', 256.74]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-09', '37.912', '-119.564', 255.89]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-10', '37.912', '-119.564', 254.98999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-11', '37.912', '-119.564', 255.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-12', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-13', '37.912', '-119.564', 253.83]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-15', '37.912', '-119.564', 256.63998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-16', '37.912', '-119.564', 256.57]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-17', '37.912', '-119.564', 258.91]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-18', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-19', '37.912', '-119.564', 256.13]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-20', '37.912', '-119.564', 255.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-22', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-23', '37.912', '-119.564', 260.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-24', '37.912', '-119.564', 261.21]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-25', '37.912', '-119.564', 258.46]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-26', '37.912', '-119.564', 259.13]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-27', '37.912', '-119.564', 259.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-28', '37.912', '-119.564', 260.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-30', '37.912', '-119.564', 256.61]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-31', '37.912', '-119.564', 254.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-01', '37.912', '-119.564', 255.97]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-02', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-03', '37.912', '-119.564', 254.37999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-04', '37.912', '-119.564', 254.92]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-07', '37.912', '-119.564', 249.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-08', '37.912', '-119.564', 253.68999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-09', '37.912', '-119.564', 255.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-10', '37.912', '-119.564', 254.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-11', '37.912', '-119.564', 251.42]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-12', '37.912', '-119.564', 256.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-14', '37.912', '-119.564', 250.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-15', '37.912', '-119.564', 254.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-16', '37.912', '-119.564', 254.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-17', '37.912', '-119.564', 250.43]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-18', '37.912', '-119.564', 247.20999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-19', '37.912', '-119.564', 248.51999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-20', '37.912', '-119.564', 255.15999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-22', '37.912', '-119.564', 247.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-23', '37.912', '-119.564', 247.25]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-24', '37.912', '-119.564', 247.76999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-25', '37.912', '-119.564', 249.68]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-26', '37.912', '-119.564', 250.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-27', '37.912', '-119.564', 248.29999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-29', '37.912', '-119.564', 240.9]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-11-30', '37.912', '-119.564', 239.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-01', '37.912', '-119.564', 242.18999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-02', '37.912', '-119.564', 245.89]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-03', '37.912', '-119.564', 248.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-04', '37.912', '-119.564', 240.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-05', '37.912', '-119.564', 244.68999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-07', '37.912', '-119.564', 241.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-08', '37.912', '-119.564', 242.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-09', '37.912', '-119.564', 242.59]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-10', '37.912', '-119.564', 244.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-11', '37.912', '-119.564', 249.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-12', '37.912', '-119.564', 248.76999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-15', '37.912', '-119.564', 245.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-16', '37.912', '-119.564', 243.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-17', '37.912', '-119.564', 245.20999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-18', '37.912', '-119.564', 246.76]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-19', '37.912', '-119.564', 247.01]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-20', '37.912', '-119.564', 249.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-22', '37.912', '-119.564', 241.18999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-23', '37.912', '-119.564', 245.59]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-24', '37.912', '-119.564', 247.09]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-25', '37.912', '-119.564', 250.45]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-26', '37.912', '-119.564', 248.43]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-27', '37.912', '-119.564', 249.36]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-30', '37.912', '-119.564', 244.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-12-31', '37.912', '-119.564', 245.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-01', '37.912', '-119.564', 247.15999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-02', '37.912', '-119.564', 249.04999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-03', '37.912', '-119.564', 251.29999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-04', '37.912', '-119.564', 254.06]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-06', '37.912', '-119.564', 246.45999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-07', '37.912', '-119.564', 242.39]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-08', '37.912', '-119.564', 248.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-09', '37.912', '-119.564', 250.9]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-10', '37.912', '-119.564', 251.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-11', '37.912', '-119.564', 245.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-12', '37.912', '-119.564', 243.75]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-14', '37.912', '-119.564', 239.68]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-15', '37.912', '-119.564', 240.06]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-16', '37.912', '-119.564', 242.31999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-17', '37.912', '-119.564', 244.67]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-18', '37.912', '-119.564', 245.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-19', '37.912', '-119.564', 248.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-21', '37.912', '-119.564', 232.51]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-22', '37.912', '-119.564', 240.09999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-23', '37.912', '-119.564', 237.33]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-24', '37.912', '-119.564', 239.20999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-25', '37.912', '-119.564', 238.43999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-26', '37.912', '-119.564', 236.81]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-27', '37.912', '-119.564', 244.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-29', '37.912', '-119.564', 239.37]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-30', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-01-31', '37.912', '-119.564', 242.06]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-01', '37.912', '-119.564', 243.67]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-02', '37.912', '-119.564', 246.12999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-03', '37.912', '-119.564', 246.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-04', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-06', '37.912', '-119.564', 235.23999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-07', '37.912', '-119.564', 237.81999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-08', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-09', '37.912', '-119.564', 240.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-10', '37.912', '-119.564', 241.45999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-11', '37.912', '-119.564', 239.79999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-12', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-13', '37.912', '-119.564', 229.59]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-14', '37.912', '-119.564', 228.62]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-15', '37.912', '-119.564', 228.47]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-16', '37.912', '-119.564', 237.17]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-17', '37.912', '-119.564', 239.47]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-18', '37.912', '-119.564', 241.51]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-19', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-21', '37.912', '-119.564', 226.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-22', '37.912', '-119.564', 232.2]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-23', '37.912', '-119.564', 228.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-24', '37.912', '-119.564', 233.34999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-25', '37.912', '-119.564', 235.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-26', '37.912', '-119.564', 238.79999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-27', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-02-28', '37.912', '-119.564', 230.47]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-01', '37.912', '-119.564', 240.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-02', '37.912', '-119.564', 235.54]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-03', '37.912', '-119.564', 237.15999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-04', '37.912', '-119.564', 235.36]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-05', '37.912', '-119.564', 233.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-06', '37.912', '-119.564', 235.09]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-07', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-08', '37.912', '-119.564', 234.87999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-09', '37.912', '-119.564', 233.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-10', '37.912', '-119.564', 235.51999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-11', '37.912', '-119.564', 235.79999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-12', '37.912', '-119.564', 236.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-13', '37.912', '-119.564', 252.40999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-15', '37.912', '-119.564', 235.87999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-16', '37.912', '-119.564', 237.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-17', '37.912', '-119.564', 234.65]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-18', '37.912', '-119.564', 233.65]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-19', '37.912', '-119.564', 235.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-20', '37.912', '-119.564', 240.54999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-21', '37.912', '-119.564', 251.01]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-22', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-23', '37.912', '-119.564', 240.11]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-24', '37.912', '-119.564', 232.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-25', '37.912', '-119.564', 232.73]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-26', '37.912', '-119.564', 230.23999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-27', '37.912', '-119.564', 235.26]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-28', '37.912', '-119.564', 238.34999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-30', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-03-31', '37.912', '-119.564', 231.40999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-01', '37.912', '-119.564', 229.79]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-02', '37.912', '-119.564', 231.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-03', '37.912', '-119.564', 231.98999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-04', '37.912', '-119.564', 236.53]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-05', '37.912', '-119.564', 246.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-07', '37.912', '-119.564', 248.48]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-08', '37.912', '-119.564', 225.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-09', '37.912', '-119.564', 230.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-10', '37.912', '-119.564', 243.33]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-11', '37.912', '-119.564', 240.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-12', '37.912', '-119.564', 232.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-13', '37.912', '-119.564', 221.37999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-15', '37.912', '-119.564', 229.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-16', '37.912', '-119.564', 231.29999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-17', '37.912', '-119.564', 226.97]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-18', '37.912', '-119.564', 231.87999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-19', '37.912', '-119.564', 229.33]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-20', '37.912', '-119.564', 234.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-22', '37.912', '-119.564', 237.68999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-23', '37.912', '-119.564', 234.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-24', '37.912', '-119.564', 242.54999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-25', '37.912', '-119.564', 239.67]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-26', '37.912', '-119.564', 248.83]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-27', '37.912', '-119.564', 254.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-28', '37.912', '-119.564', 242.26999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-04-30', '37.912', '-119.564', 230.56999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-01', '37.912', '-119.564', 238.95]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-02', '37.912', '-119.564', 238.65999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-03', '37.912', '-119.564', 238.45]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-04', '37.912', '-119.564', 246.65999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-05', '37.912', '-119.564', 253.45999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-07', '37.912', '-119.564', 250.23]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-08', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-09', '37.912', '-119.564', 256.44998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-10', '37.912', '-119.564', 252.61]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-11', '37.912', '-119.564', 253.01999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-12', '37.912', '-119.564', 249.01]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-13', '37.912', '-119.564', 255.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-15', '37.912', '-119.564', 254.09999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-16', '37.912', '-119.564', 248.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-17', '37.912', '-119.564', 245.37]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-18', '37.912', '-119.564', 254.17]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-19', '37.912', '-119.564', 256.08]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-20', '37.912', '-119.564', 255.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-22', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-23', '37.912', '-119.564', 257.66998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-24', '37.912', '-119.564', 256.36]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-25', '37.912', '-119.564', 255.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-26', '37.912', '-119.564', 251.15]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-27', '37.912', '-119.564', 256.02]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-28', '37.912', '-119.564', 260.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-30', '37.912', '-119.564', 259.49]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-05-31', '37.912', '-119.564', 257.44]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-01', '37.912', '-119.564', 255.73999]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-02', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-03', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-04', '37.912', '-119.564', 263.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-07', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-08', '37.912', '-119.564', 261.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-09', '37.912', '-119.564', 260.54]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-10', '37.912', '-119.564', 259.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-11', '37.912', '-119.564', 263.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-12', '37.912', '-119.564', 266.94]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-14', '37.912', '-119.564', 262.46]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-15', '37.912', '-119.564', 262.9]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-16', '37.912', '-119.564', 263.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-17', '37.912', '-119.564', 258.25]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-18', '37.912', '-119.564', 262.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-19', '37.912', '-119.564', 263.25]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-20', '37.912', '-119.564', 265.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-22', '37.912', '-119.564', 264.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-23', '37.912', '-119.564', 265.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-24', '37.912', '-119.564', 267.66]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-25', '37.912', '-119.564', 266.8]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-26', '37.912', '-119.564', 268.43]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-27', '37.912', '-119.564', 268.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-29', '37.912', '-119.564', 261.28]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-06-30', '37.912', '-119.564', 265.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-01', '37.912', '-119.564', 266.38]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-02', '37.912', '-119.564', 268.19998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-03', '37.912', '-119.564', 265.77]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-04', '37.912', '-119.564', 265.28]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-06', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-07', '37.912', '-119.564', 267.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-08', '37.912', '-119.564', 268.63]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-09', '37.912', '-119.564', 266.93]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-10', '37.912', '-119.564', 268.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-11', '37.912', '-119.564', 268.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-12', '37.912', '-119.564', 268.3]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-14', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-15', '37.912', '-119.564', 269.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-16', '37.912', '-119.564', 270.19]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-17', '37.912', '-119.564', 268.28]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-18', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-19', '37.912', '-119.564', 269.44998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-20', '37.912', '-119.564', 270.12]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-21', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-22', '37.912', '-119.564', 267.94998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-23', '37.912', '-119.564', 268.35]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-24', '37.912', '-119.564', 270.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-25', '37.912', '-119.564', 268.24]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-26', '37.912', '-119.564', 269.50998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-27', '37.912', '-119.564', 269.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-29', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-30', '37.912', '-119.564', 269.00998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-07-31', '37.912', '-119.564', 270.05]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-01', '37.912', '-119.564', 269.6]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-02', '37.912', '-119.564', 269.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-03', '37.912', '-119.564', 265.11]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-04', '37.912', '-119.564', 268.00998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-06', '37.912', '-119.564', 264.84]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-07', '37.912', '-119.564', 266.65]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-08', '37.912', '-119.564', 266.61]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-09', '37.912', '-119.564', 266.74]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-10', '37.912', '-119.564', 268.03]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-11', '37.912', '-119.564', 267.96]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-12', '37.912', '-119.564', 267.12]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-13', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-14', '37.912', '-119.564', 267.3]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-15', '37.912', '-119.564', 266.29]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-16', '37.912', '-119.564', 266.8]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-17', '37.912', '-119.564', 266.55]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-18', '37.912', '-119.564', 266.68]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-19', '37.912', '-119.564', 268.16998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-21', '37.912', '-119.564', 262.38998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-22', '37.912', '-119.564', 262.44998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-23', '37.912', '-119.564', 262.99]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-24', '37.912', '-119.564', 263.49]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-25', '37.912', '-119.564', 262.09]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-26', '37.912', '-119.564', 262.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-27', '37.912', '-119.564', 263.25998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-28', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-29', '37.912', '-119.564', 263.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-30', '37.912', '-119.564', 261.72]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-08-31', '37.912', '-119.564', 259.52]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-01', '37.912', '-119.564', 262.66]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-02', '37.912', '-119.564', 264.91]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-03', '37.912', '-119.564', 265.11]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-04', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-05', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-06', '37.912', '-119.564', 264.63]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-07', '37.912', '-119.564', 264.63998]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-08', '37.912', '-119.564', 263.22]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-09', '37.912', '-119.564', 261.31]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-10', '37.912', '-119.564', 261.5]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-11', '37.912', '-119.564', 261.82]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-12', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-13', '37.912', '-119.564', 258.83]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-14', '37.912', '-119.564', 258.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-15', '37.912', '-119.564', 260.46]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-16', '37.912', '-119.564', 257.99]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-17', '37.912', '-119.564', 257.78]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-18', '37.912', '-119.564', 257.71]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-19', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-20', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-21', '37.912', '-119.564', 260.18]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-22', '37.912', '-119.564', 260.1]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-23', '37.912', '-119.564', 259.04]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-24', '37.912', '-119.564', 259.4]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-25', '37.912', '-119.564', 260.86]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-26', '37.912', '-119.564', 261.0]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-27', '37.912', '-119.564', nan]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-28', '37.912', '-119.564', 260.3]\nLatitude: 37.912 Longitude: -119.564 pmw: ['2018-09-29', '37.912', '-119.564', 260.0]\n0    [37.912, -119.564, 2017-10-01, [2017-10-01, 37...\n1    [37.912, -119.564, 2017-10-02, [2017-10-02, 37...\n2    [37.912, -119.564, 2017-10-03, [2017-10-03, 37...\n3    [37.912, -119.564, 2017-10-04, [2017-10-04, 37...\n4    [37.912, -119.564, 2017-10-05, [2017-10-05, 37...\ndtype: object\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698346487537,
  "history_end_time" : 1698346488084,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "28CZi7a6c0qG",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nLatitude: 37.912 Longitude: -119.564 pmw: ['2017-10-01', '37.912', '-119.564', 256.08]\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/28CZi7a6c0qG/data_merge_hackweek.py\", line 358, in <module>\n    convert_pmv_to_right_format()\n  File \"/home/ubuntu/gw-workspace/28CZi7a6c0qG/data_merge_hackweek.py\", line 91, in convert_pmv_to_right_format\n    pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/28CZi7a6c0qG/data_merge_hackweek.py\", line 91, in <lambda>\n    pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/28CZi7a6c0qG/data_merge_hackweek.py\", line 73, in adjust_column_to_rows\n    pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n                            ^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'pmv_new_df' where it is not associated with a value\n",
  "history_begin_time" : 1698346334370,
  "history_end_time" : 1698346334877,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "OmhIAcm2Gq4R",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n\n  def adjust_column_to_rows(row, pmv_new_df):\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          print(\"Latitude:\", lat)\n          print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  pmv_old_df.apply(lambda row: adjust_column_to_rows(row, pmv_new_df), axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nLatitude: 37.912\nLongitude: -119.564\nLatitude: 38.416\nLongitude: -119.276\nLatitude: 38.632\nLongitude: -118.772\nLatitude: 38.38\nLongitude: -118.988\nLatitude: 38.308\nLongitude: -119.636\nLatitude: 38.056\nLongitude: -118.772\nLatitude: 37.804\nLongitude: -119.24\nLatitude: 38.488\nLongitude: -119.528\nLatitude: 37.984\nLongitude: -119.312\nLatitude: 37.876\nLongitude: -119.456\nEmpty DataFrame\nColumns: [date, lat, lon, pmv]\nIndex: []\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698346256882,
  "history_end_time" : 1698346261642,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "9N3Ubwiqn7qG",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n\n  def adjust_column_to_rows(row, pmv_new_df):\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  pmv_old_df.apply(lambda row: adjust_column_to_rows(row, pmv_new_df), axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nEmpty DataFrame\nColumns: [date, lat, lon, pmv]\nIndex: []\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698346050814,
  "history_end_time" : 1698346052215,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6pReZTRfR9hK",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/6pReZTRfR9hK/data_merge_hackweek.py\", line 358, in <module>\n    convert_pmv_to_right_format()\n  File \"/home/ubuntu/gw-workspace/6pReZTRfR9hK/data_merge_hackweek.py\", line 91, in convert_pmv_to_right_format\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/6pReZTRfR9hK/data_merge_hackweek.py\", line 73, in adjust_column_to_rows\n    pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n                            ^^^^^^^^^^\nNameError: name 'pmv_new_df' is not defined\n",
  "history_begin_time" : 1698345835467,
  "history_end_time" : 1698345835969,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pJxwITi9BaNL",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    #global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/pJxwITi9BaNL/data_merge_hackweek.py\", line 358, in <module>\n    convert_pmv_to_right_format()\n  File \"/home/ubuntu/gw-workspace/pJxwITi9BaNL/data_merge_hackweek.py\", line 91, in convert_pmv_to_right_format\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/pJxwITi9BaNL/data_merge_hackweek.py\", line 73, in adjust_column_to_rows\n    pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n                            ^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'pmv_new_df' where it is not associated with a value\n",
  "history_begin_time" : 1698345824164,
  "history_end_time" : 1698345824664,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "pNqcrnmgG1NU",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    #global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nEmpty DataFrame\nColumns: [date, lat, lon, pmv]\nIndex: []\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698345730222,
  "history_end_time" : 1698345730716,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "fvCCh9oB9qFc",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n    if 'pmv_new_df' not in locals():\n        pmv_new_df = pd.DataFrame(columns=[\"date\", \"lat\", \"lon\", \"pmv\"])\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nEmpty DataFrame\nColumns: [date, lat, lon, pmv]\nIndex: []\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698345674124,
  "history_end_time" : 1698345675603,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "VJqocF7BDVHn",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/VJqocF7BDVHn/data_merge_hackweek.py\", line 358, in <module>\n    convert_pmv_to_right_format()\n  File \"/home/ubuntu/gw-workspace/VJqocF7BDVHn/data_merge_hackweek.py\", line 91, in convert_pmv_to_right_format\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/VJqocF7BDVHn/data_merge_hackweek.py\", line 73, in adjust_column_to_rows\n    pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n                            ^^^^^^^^^^\nNameError: name 'pmv_new_df' is not defined\n",
  "history_begin_time" : 1698344943410,
  "history_end_time" : 1698344943920,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Q7tVKT39qp30",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)') # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/Q7tVKT39qp30/data_merge_hackweek.py\", line 358, in <module>\n    convert_pmv_to_right_format()\n  File \"/home/ubuntu/gw-workspace/Q7tVKT39qp30/data_merge_hackweek.py\", line 91, in convert_pmv_to_right_format\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/Q7tVKT39qp30/data_merge_hackweek.py\", line 54, in adjust_column_to_rows\n    match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)') # for testing data\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: search() missing 1 required positional argument: 'string'\n",
  "history_begin_time" : 1698344926142,
  "history_end_time" : 1698344926654,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "wLqHOhMe8I2q",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_testing)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\nconvert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/wLqHOhMe8I2q/data_merge_hackweek.py\", line 357, in <module>\n    convert_pmv_to_right_format()\n  File \"/home/ubuntu/gw-workspace/wLqHOhMe8I2q/data_merge_hackweek.py\", line 90, in convert_pmv_to_right_format\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/wLqHOhMe8I2q/data_merge_hackweek.py\", line 76, in adjust_column_to_rows\n    lat = match.group(1)\n          ^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'group'\n",
  "history_begin_time" : 1698344851984,
  "history_end_time" : 1698344852485,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "1DL6yKZ8n1U9",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/1DL6yKZ8n1U9/data_merge_hackweek.py:283: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/1DL6yKZ8n1U9/data_merge_hackweek.py\", line 367, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/1DL6yKZ8n1U9/data_merge_hackweek.py\", line 292, in merge_all_testing_data_together_except_gridmet\n    merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 148, in merge\n    op = _MergeOperation(\n         ^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 737, in __init__\n    ) = self._get_merge_keys()\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 1203, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1778, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1698344692704,
  "history_end_time" : 1698344693487,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9ATQGa3BTp1f",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(gridmet_terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/9ATQGa3BTp1f/data_merge_hackweek.py:283: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/9ATQGa3BTp1f/data_merge_hackweek.py\", line 367, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/9ATQGa3BTp1f/data_merge_hackweek.py\", line 288, in merge_all_testing_data_together_except_gridmet\n    df6 = pd.read_csv(gridmet_terrain_testing_data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n    self.handles = get_handle(\n                   ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/common.py\", line 859, in get_handle\n    handle = open(\n             ^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/gridmet_test_run/gridmet_testing_hackweek_subset.csv'\n",
  "history_begin_time" : 1698344645861,
  "history_end_time" : 1698344646638,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "k9GTNIcUvicY",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassifcation_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(gridmet_terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n/home/ubuntu/gw-workspace/k9GTNIcUvicY/data_merge_hackweek.py:283: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(fsca_testing)\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/k9GTNIcUvicY/data_merge_hackweek.py\", line 367, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/k9GTNIcUvicY/data_merge_hackweek.py\", line 284, in merge_all_testing_data_together_except_gridmet\n    df4 = pd.read_csv(snowclassification_testing_data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n    self.handles = get_handle(\n                   ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/common.py\", line 859, in get_handle\n    handle = open(\n             ^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/gridmet_test_run/snowclassifcation_hackweek_testing.csv'\n",
  "history_begin_time" : 1698344600058,
  "history_end_time" : 1698344600871,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "azOPlGrUUhDL",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\ngridmet_terrain_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together_except_gridmet():\n  #df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  \n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n  #df5 = df5.rename(columns={'Date': 'date', \n  #                         'long': 'lon'})\n  df6 = pd.read_csv(gridmet_terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing_except_gridmet.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\nmerge_all_testing_data_together_except_gridmet()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/azOPlGrUUhDL/data_merge_hackweek.py\", line 365, in <module>\n    merge_all_testing_data_together_except_gridmet()\n  File \"/home/ubuntu/gw-workspace/azOPlGrUUhDL/data_merge_hackweek.py\", line 281, in merge_all_testing_data_together_except_gridmet\n    df2 = pd.read_csv(pmw_testing_new)\n                      ^^^^^^^^^^^^^^^\nNameError: name 'pmw_testing_new' is not defined. Did you mean: 'pmw_training_new'?\n",
  "history_begin_time" : 1698344339788,
  "history_end_time" : 1698344340483,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2IZzxcK6Tg0s",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\ncollect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\n2017-10-01\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-01_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-01_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-01_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-01_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-01_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-01_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-01_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-01_hackweek_subset.csv\n2017-10-02\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-02_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-02_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-02_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-02_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-02_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-02_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-02_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-02_hackweek_subset.csv\n2017-10-03\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-03_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-03_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-03_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-03_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-03_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-03_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-03_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-03_hackweek_subset.csv\n2017-10-04\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-04_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-04_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-04_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-04_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-04_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-04_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-04_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-04_hackweek_subset.csv\n2017-10-05\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-05_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-05_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-05_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-05_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-05_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-05_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-05_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-05_hackweek_subset.csv\n2017-10-06\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-06_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-06_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-06_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-06_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-06_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-06_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-06_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-06_hackweek_subset.csv\n2017-10-07\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-07_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-07_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-07_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-07_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-07_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-07_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-07_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-07_hackweek_subset.csv\n2017-10-08\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-08_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-08_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-08_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-08_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-08_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-08_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-08_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-08_hackweek_subset.csv\n2017-10-09\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-09_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-09_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-09_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-09_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-09_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-09_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-09_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-09_hackweek_subset.csv\n2017-10-10\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-10_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-10_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-10_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-10_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-10_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-10_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-10_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-10_hackweek_subset.csv\n2017-10-11\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-11_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-11_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-11_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-11_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-11_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-11_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-11_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-11_hackweek_subset.csv\n2017-10-12\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-12_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-12_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-12_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-12_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-12_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-12_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-12_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-12_hackweek_subset.csv\n2017-10-13\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-13_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-13_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-13_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-13_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-13_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-13_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-13_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-13_hackweek_subset.csv\n2017-10-14\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-14_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-14_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-14_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-14_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-14_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-14_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-14_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-14_hackweek_subset.csv\n2017-10-15\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-15_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-15_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-15_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-15_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-15_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-15_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-15_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-15_hackweek_subset.csv\n2017-10-16\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-16_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-16_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-16_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-16_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-16_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-16_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-16_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-16_hackweek_subset.csv\n2017-10-17\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-17_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-17_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-17_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-17_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-17_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-17_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-17_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-17_hackweek_subset.csv\n2017-10-18\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-18_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-18_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-18_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-18_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-18_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-18_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-18_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-18_hackweek_subset.csv\n2017-10-19\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-19_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-19_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-19_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-19_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-19_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-19_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-19_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-19_hackweek_subset.csv\n2017-10-20\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-20_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-20_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-20_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-20_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-20_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-20_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-20_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-20_hackweek_subset.csv\n2017-10-21\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-21_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-21_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-21_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-21_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-21_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-21_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-21_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-21_hackweek_subset.csv\n2017-10-22\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-22_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-22_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-22_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-22_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-22_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-22_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-22_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-22_hackweek_subset.csv\n2017-10-23\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-23_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-23_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-23_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-23_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-23_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-23_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-23_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-23_hackweek_subset.csv\n2017-10-24\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-24_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-24_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-24_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-24_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-24_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-24_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-24_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-24_hackweek_subset.csv\n2017-10-25\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-25_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-25_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-25_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-25_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-25_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-25_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-25_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-25_hackweek_subset.csv\n2017-10-26\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-26_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-26_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-26_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-26_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-26_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-26_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-26_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-26_hackweek_subset.csv\n2017-10-27\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-27_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-27_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-27_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-27_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-27_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-27_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-27_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-27_hackweek_subset.csv\n2017-10-28\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-28_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-28_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-28_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-28_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-28_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-28_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-28_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-28_hackweek_subset.csv\n2017-10-29\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-29_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-29_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-29_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-29_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-29_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-29_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-29_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-29_hackweek_subset.csv\n2017-10-30\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-30_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-30_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-30_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-30_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-30_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-30_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-30_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-30_hackweek_subset.csv\n2017-10-31\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-31_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-31_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-31_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-31_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-31_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-31_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-31_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-31_hackweek_subset.csv\n2017-11-01\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-01_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-01_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-01_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-01_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-01_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-01_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-01_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-01_hackweek_subset.csv\n2017-11-02\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-02_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-02_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-02_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-02_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-02_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-02_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-02_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-02_hackweek_subset.csv\n2017-11-03\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-03_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-03_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-03_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-03_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-03_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-03_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-03_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-03_hackweek_subset.csv\n2017-11-04\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-04_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-04_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-04_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-04_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-04_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-04_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-04_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-04_hackweek_subset.csv\n2017-11-05\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-05_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-05_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-05_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-05_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-05_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-05_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-05_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-05_hackweek_subset.csv\n2017-11-06\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-06_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-06_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-06_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-06_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-06_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-06_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-06_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-06_hackweek_subset.csv\n2017-11-07\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-07_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-07_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-07_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-07_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-07_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-07_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-07_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-07_hackweek_subset.csv\n2017-11-08\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-08_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-08_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-08_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-08_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-08_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-08_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-08_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-08_hackweek_subset.csv\n2017-11-09\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-09_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-09_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-09_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-09_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-09_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-09_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-09_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-09_hackweek_subset.csv\n2017-11-10\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-10_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-10_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-10_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-10_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-10_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-10_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-10_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-10_hackweek_subset.csv\n2017-11-11\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-11_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-11_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-11_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-11_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-11_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-11_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-11_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-11_hackweek_subset.csv\n2017-11-12\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-12_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-12_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-12_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-12_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-12_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-12_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-12_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-12_hackweek_subset.csv\n2017-11-13\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-13_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-13_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-13_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-13_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-13_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-13_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-13_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-13_hackweek_subset.csv\n2017-11-14\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-14_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-14_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-14_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-14_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-14_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-14_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-14_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-14_hackweek_subset.csv\n2017-11-15\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-15_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-15_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-15_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-15_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-15_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-15_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-15_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-15_hackweek_subset.csv\n2017-11-16\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-16_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-16_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-16_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-16_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-16_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-16_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-16_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-16_hackweek_subset.csv\n2017-11-17\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-17_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-17_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-17_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-17_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-17_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-17_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-17_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-17_hackweek_subset.csv\n2017-11-18\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-18_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-18_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-18_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-18_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-18_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-18_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-18_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-18_hackweek_subset.csv\n2017-11-19\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-19_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-19_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-19_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-19_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-19_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-19_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-19_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-19_hackweek_subset.csv\n2017-11-20\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-20_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-20_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-20_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-20_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-20_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-20_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-20_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-20_hackweek_subset.csv\n2017-11-21\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-21_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-21_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-21_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-21_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-21_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-21_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-21_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-21_hackweek_subset.csv\n2017-11-22\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-22_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-22_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-22_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-22_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-22_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-22_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-22_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-22_hackweek_subset.csv\n2017-11-23\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-23_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-23_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-23_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-23_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-23_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-23_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-23_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-23_hackweek_subset.csv\n2017-11-24\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-24_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-24_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-24_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-24_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-24_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-24_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-24_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-24_hackweek_subset.csv\n2017-11-25\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-25_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-25_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-25_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-25_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-25_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-25_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-25_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-25_hackweek_subset.csv\n2017-11-26\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-26_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-26_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-26_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-26_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-26_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-26_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-26_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-26_hackweek_subset.csv\n2017-11-27\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-27_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-27_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-27_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-27_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-27_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-27_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-27_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-27_hackweek_subset.csv\n2017-11-28\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-28_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-28_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-28_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-28_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-28_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-28_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-28_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-28_hackweek_subset.csv\n2017-11-29\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-29_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-29_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-29_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-29_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-29_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-29_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-29_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-29_hackweek_subset.csv\n2017-11-30\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-30_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-30_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-30_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-30_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-30_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-30_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-30_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-30_hackweek_subset.csv\n2017-12-01\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-01_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-01_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-01_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-01_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-01_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-01_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-01_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-01_hackweek_subset.csv\n2017-12-02\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-02_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-02_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-02_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-02_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-02_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-02_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-02_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-02_hackweek_subset.csv\n2017-12-03\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-03_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-03_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-03_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-03_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-03_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-03_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-03_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-03_hackweek_subset.csv\n2017-12-04\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-04_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-04_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-04_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-04_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-04_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-04_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-04_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-04_hackweek_subset.csv\n2017-12-05\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-05_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-05_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-05_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-05_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-05_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-05_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-05_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-05_hackweek_subset.csv\n2017-12-06\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-06_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-06_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-06_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-06_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-06_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-06_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-06_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-06_hackweek_subset.csv\n2017-12-07\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-07_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-07_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-07_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-07_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-07_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-07_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-07_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-07_hackweek_subset.csv\n2017-12-08\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-08_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-08_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-08_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-08_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-08_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-08_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-08_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-08_hackweek_subset.csv\n2017-12-09\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-09_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-09_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-09_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-09_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-09_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-09_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-09_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-09_hackweek_subset.csv\n2017-12-10\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-10_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-10_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-10_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-10_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-10_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-10_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-10_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-10_hackweek_subset.csv\n2017-12-11\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-11_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-11_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-11_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-11_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-11_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-11_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-11_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-11_hackweek_subset.csv\n2017-12-12\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-12_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-12_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-12_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-12_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-12_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-12_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-12_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-12_hackweek_subset.csv\n2017-12-13\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-13_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-13_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-13_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-13_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-13_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-13_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-13_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-13_hackweek_subset.csv\n2017-12-14\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-14_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-14_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-14_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-14_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-14_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-14_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-14_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-14_hackweek_subset.csv\n2017-12-15\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-15_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-15_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-15_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-15_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-15_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-15_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-15_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-15_hackweek_subset.csv\n2017-12-16\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-16_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-16_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-16_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-16_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-16_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-16_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-16_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-16_hackweek_subset.csv\n2017-12-17\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-17_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-17_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-17_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-17_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-17_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-17_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-17_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-17_hackweek_subset.csv\n2017-12-18\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-18_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-18_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-18_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-18_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-18_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-18_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-18_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-18_hackweek_subset.csv\n2017-12-19\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-19_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-19_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-19_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-19_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-19_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-19_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-19_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-19_hackweek_subset.csv\n2017-12-20\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-20_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-20_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-20_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-20_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-20_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-20_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-20_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-20_hackweek_subset.csv\n2017-12-21\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-21_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-21_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-21_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-21_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-21_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-21_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-21_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-21_hackweek_subset.csv\n2017-12-22\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-22_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-22_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-22_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-22_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-22_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-22_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-22_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-22_hackweek_subset.csv\n2017-12-23\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-23_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-23_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-23_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-23_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-23_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-23_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-23_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-23_hackweek_subset.csv\n2017-12-24\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-24_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-24_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-24_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-24_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-24_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-24_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-24_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-24_hackweek_subset.csv\n2017-12-25\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-25_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-25_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-25_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-25_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-25_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-25_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-25_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-25_hackweek_subset.csv\n2017-12-26\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-26_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-26_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-26_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-26_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-26_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-26_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-26_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-26_hackweek_subset.csv\n2017-12-27\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-27_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-27_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-27_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-27_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-27_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-27_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-27_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-27_hackweek_subset.csv\n2017-12-28\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-28_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-28_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-28_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-28_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-28_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-28_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-28_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-28_hackweek_subset.csv\n2017-12-29\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-29_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-29_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-29_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-29_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-29_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-29_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-29_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-29_hackweek_subset.csv\n2017-12-30\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-30_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-30_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-30_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-30_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-30_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-30_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-30_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-30_hackweek_subset.csv\n2017-12-31\nChecking file: etr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-12-31_hackweek_subset.csv\nChecking file: rmin_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-12-31_hackweek_subset.csv\nChecking file: tmmn_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-12-31_hackweek_subset.csv\nChecking file: vs_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-12-31_hackweek_subset.csv\nChecking file: rmax_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-12-31_hackweek_subset.csv\nChecking file: pr_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-12-31_hackweek_subset.csv\nChecking file: vpd_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-12-31_hackweek_subset.csv\nChecking file: tmmx_2017.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-12-31_hackweek_subset.csv\n2018-01-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-01_hackweek_subset.csv\n2018-01-02\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-02_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-02_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-02_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-02_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-02_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-02_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-02_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-02_hackweek_subset.csv\n2018-01-03\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-03_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-03_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-03_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-03_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-03_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-03_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-03_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-03_hackweek_subset.csv\n2018-01-04\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-04_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-04_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-04_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-04_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-04_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-04_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-04_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-04_hackweek_subset.csv\n2018-01-05\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-05_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-05_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-05_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-05_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-05_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-05_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-05_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-05_hackweek_subset.csv\n2018-01-06\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-06_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-06_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-06_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-06_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-06_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-06_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-06_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-06_hackweek_subset.csv\n2018-01-07\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-07_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-07_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-07_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-07_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-07_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-07_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-07_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-07_hackweek_subset.csv\n2018-01-08\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-08_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-08_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-08_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-08_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-08_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-08_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-08_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-08_hackweek_subset.csv\n2018-01-09\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-09_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-09_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-09_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-09_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-09_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-09_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-09_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-09_hackweek_subset.csv\n2018-01-10\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-10_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-10_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-10_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-10_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-10_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-10_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-10_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-10_hackweek_subset.csv\n2018-01-11\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-11_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-11_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-11_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-11_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-11_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-11_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-11_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-11_hackweek_subset.csv\n2018-01-12\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-12_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-12_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-12_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-12_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-12_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-12_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-12_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-12_hackweek_subset.csv\n2018-01-13\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-13_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-13_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-13_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-13_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-13_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-13_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-13_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-13_hackweek_subset.csv\n2018-01-14\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-14_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-14_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-14_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-14_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-14_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-14_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-14_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-14_hackweek_subset.csv\n2018-01-15\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-15_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-15_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-15_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-15_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-15_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-15_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-15_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-15_hackweek_subset.csv\n2018-01-16\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-16_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-16_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-16_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-16_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-16_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-16_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-16_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-16_hackweek_subset.csv\n2018-01-17\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-17_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-17_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-17_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-17_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-17_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-17_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-17_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-17_hackweek_subset.csv\n2018-01-18\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-18_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-18_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-18_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-18_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-18_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-18_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-18_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-18_hackweek_subset.csv\n2018-01-19\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-19_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-19_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-19_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-19_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-19_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-19_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-19_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-19_hackweek_subset.csv\n2018-01-20\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-20_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-20_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-20_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-20_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-20_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-20_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-20_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-20_hackweek_subset.csv\n2018-01-21\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-21_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-21_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-21_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-21_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-21_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-21_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-21_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-21_hackweek_subset.csv\n2018-01-22\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-22_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-22_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-22_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-22_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-22_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-22_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-22_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-22_hackweek_subset.csv\n2018-01-23\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-23_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-23_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-23_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-23_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-23_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-23_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-23_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-23_hackweek_subset.csv\n2018-01-24\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-24_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-24_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-24_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-24_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-24_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-24_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-24_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-24_hackweek_subset.csv\n2018-01-25\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-25_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-25_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-25_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-25_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-25_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-25_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-25_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-25_hackweek_subset.csv\n2018-01-26\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-26_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-26_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-26_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-26_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-26_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-26_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-26_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-26_hackweek_subset.csv\n2018-01-27\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-27_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-27_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-27_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-27_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-27_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-27_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-27_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-27_hackweek_subset.csv\n2018-01-28\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-28_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-28_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-28_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-28_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-28_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-28_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-28_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-28_hackweek_subset.csv\n2018-01-29\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-29_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-29_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-29_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-29_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-29_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-29_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-29_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-29_hackweek_subset.csv\n2018-01-30\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-30_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-30_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-30_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-30_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-30_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-30_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-30_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-30_hackweek_subset.csv\n2018-01-31\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-01-31_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-01-31_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-01-31_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-01-31_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-01-31_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-01-31_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-01-31_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-01-31_hackweek_subset.csv\n2018-02-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-01_hackweek_subset.csv\n2018-02-02\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-02_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-02_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-02_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-02_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-02_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-02_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-02_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-02_hackweek_subset.csv\n2018-02-03\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-03_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-03_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-03_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-03_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-03_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-03_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-03_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-03_hackweek_subset.csv\n2018-02-04\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-04_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-04_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-04_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-04_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-04_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-04_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-04_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-04_hackweek_subset.csv\n2018-02-05\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-05_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-05_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-05_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-05_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-05_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-05_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-05_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-05_hackweek_subset.csv\n2018-02-06\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-06_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-06_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-06_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-06_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-06_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-06_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-06_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-06_hackweek_subset.csv\n2018-02-07\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-07_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-07_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-07_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-07_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-07_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-07_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-07_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-07_hackweek_subset.csv\n2018-02-08\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-08_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-08_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-08_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-08_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-08_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-08_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-08_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-08_hackweek_subset.csv\n2018-02-09\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-09_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-09_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-09_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-09_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-09_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-09_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-09_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-09_hackweek_subset.csv\n2018-02-10\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-10_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-10_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-10_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-10_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-10_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-10_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-10_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-10_hackweek_subset.csv\n2018-02-11\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-11_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-11_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-11_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-11_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-11_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-11_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-11_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-11_hackweek_subset.csv\n2018-02-12\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-12_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-12_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-12_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-12_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-12_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-12_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-12_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-12_hackweek_subset.csv\n2018-02-13\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-13_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-13_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-13_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-13_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-13_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-13_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-13_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-13_hackweek_subset.csv\n2018-02-14\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-14_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-14_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-14_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-14_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-14_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-14_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-14_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-14_hackweek_subset.csv\n2018-02-15\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-15_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-15_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-15_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-15_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-15_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-15_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-15_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-15_hackweek_subset.csv\n2018-02-16\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-16_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-16_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-16_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-16_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-16_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-16_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-16_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-16_hackweek_subset.csv\n2018-02-17\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-17_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-17_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-17_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-17_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-17_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-17_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-17_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-17_hackweek_subset.csv\n2018-02-18\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-18_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-18_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-18_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-18_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-18_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-18_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-18_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-18_hackweek_subset.csv\n2018-02-19\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-19_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-19_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-19_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-19_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-19_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-19_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-19_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-19_hackweek_subset.csv\n2018-02-20\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-20_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-20_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-20_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-20_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-20_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-20_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-20_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-20_hackweek_subset.csv\n2018-02-21\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-21_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-21_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-21_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-21_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-21_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-21_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-21_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-21_hackweek_subset.csv\n2018-02-22\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-22_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-22_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-22_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-22_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-22_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-22_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-22_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-22_hackweek_subset.csv\n2018-02-23\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-23_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-23_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-23_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-23_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-23_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-23_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-23_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-23_hackweek_subset.csv\n2018-02-24\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-24_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-24_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-24_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-24_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-24_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-24_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-24_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-24_hackweek_subset.csv\n2018-02-25\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-25_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-25_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-25_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-25_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-25_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-25_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-25_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-25_hackweek_subset.csv\n2018-02-26\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-26_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-26_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-26_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-26_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-26_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-26_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-26_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-26_hackweek_subset.csv\n2018-02-27\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-27_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-27_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-27_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-27_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-27_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-27_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-27_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-27_hackweek_subset.csv\n2018-02-28\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-02-28_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-02-28_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-02-28_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-02-28_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-02-28_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-02-28_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-02-28_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-02-28_hackweek_subset.csv\n2018-03-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-01_hackweek_subset.csv\n2018-03-02\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-02_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-02_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-02_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-02_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-02_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-02_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-02_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-02_hackweek_subset.csv\n2018-03-03\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-03_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-03_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-03_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-03_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-03_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-03_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-03_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-03_hackweek_subset.csv\n2018-03-04\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-04_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-04_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-04_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-04_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-04_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-04_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-04_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-04_hackweek_subset.csv\n2018-03-05\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-05_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-05_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-05_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-05_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-05_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-05_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-05_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-05_hackweek_subset.csv\n2018-03-06\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-06_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-06_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-06_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-06_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-06_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-06_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-06_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-06_hackweek_subset.csv\n2018-03-07\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-07_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-07_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-07_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-07_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-07_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-07_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-07_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-07_hackweek_subset.csv\n2018-03-08\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-08_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-08_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-08_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-08_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-08_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-08_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-08_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-08_hackweek_subset.csv\n2018-03-09\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-09_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-09_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-09_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-09_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-09_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-09_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-09_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-09_hackweek_subset.csv\n2018-03-10\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-10_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-10_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-10_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-10_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-10_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-10_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-10_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-10_hackweek_subset.csv\n2018-03-11\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-11_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-11_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-11_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-11_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-11_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-11_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-11_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-11_hackweek_subset.csv\n2018-03-12\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-12_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-12_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-12_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-12_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-12_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-12_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-12_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-12_hackweek_subset.csv\n2018-03-13\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-13_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-13_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-13_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-13_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-13_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-13_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-13_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-13_hackweek_subset.csv\n2018-03-14\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-14_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-14_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-14_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-14_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-14_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-14_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-14_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-14_hackweek_subset.csv\n2018-03-15\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-15_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-15_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-15_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-15_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-15_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-15_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-15_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-15_hackweek_subset.csv\n2018-03-16\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-16_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-16_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-16_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-16_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-16_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-16_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-16_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-16_hackweek_subset.csv\n2018-03-17\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-17_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-17_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-17_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-17_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-17_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-17_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-17_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-17_hackweek_subset.csv\n2018-03-18\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-18_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-18_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-18_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-18_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-18_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-18_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-18_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-18_hackweek_subset.csv\n2018-03-19\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-19_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-19_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-19_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-19_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-19_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-19_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-19_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-19_hackweek_subset.csv\n2018-03-20\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-20_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-20_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-20_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-20_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-20_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-20_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-20_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-20_hackweek_subset.csv\n2018-03-21\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-21_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-21_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-21_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-21_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-21_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-21_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-21_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-21_hackweek_subset.csv\n2018-03-22\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-22_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-22_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-22_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-22_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-22_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-22_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-22_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-22_hackweek_subset.csv\n2018-03-23\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-23_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-23_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-23_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-23_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-23_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-23_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-23_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-23_hackweek_subset.csv\n2018-03-24\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-24_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-24_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-24_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-24_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-24_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-24_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-24_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-24_hackweek_subset.csv\n2018-03-25\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-25_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-25_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-25_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-25_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-25_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-25_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-25_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-25_hackweek_subset.csv\n2018-03-26\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-26_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-26_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-26_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-26_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-26_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-26_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-26_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-26_hackweek_subset.csv\n2018-03-27\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-27_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-27_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-27_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-27_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-27_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-27_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-27_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-27_hackweek_subset.csv\n2018-03-28\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-28_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-28_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-28_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-28_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-28_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-28_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-28_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-28_hackweek_subset.csv\n2018-03-29\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-29_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-29_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-29_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-29_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-29_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-29_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-29_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-29_hackweek_subset.csv\n2018-03-30\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-30_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-30_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-30_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-30_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-30_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-30_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-30_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-30_hackweek_subset.csv\n2018-03-31\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-03-31_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-03-31_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-03-31_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-03-31_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-03-31_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-03-31_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-03-31_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-03-31_hackweek_subset.csv\n2018-04-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-01_hackweek_subset.csv\n2018-04-02\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-02_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-02_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-02_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-02_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-02_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-02_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-02_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-02_hackweek_subset.csv\n2018-04-03\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-03_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-03_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-03_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-03_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-03_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-03_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-03_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-03_hackweek_subset.csv\n2018-04-04\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-04_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-04_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-04_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-04_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-04_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-04_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-04_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-04_hackweek_subset.csv\n2018-04-05\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-05_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-05_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-05_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-05_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-05_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-05_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-05_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-05_hackweek_subset.csv\n2018-04-06\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-06_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-06_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-06_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-06_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-06_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-06_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-06_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-06_hackweek_subset.csv\n2018-04-07\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-07_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-07_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-07_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-07_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-07_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-07_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-07_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-07_hackweek_subset.csv\n2018-04-08\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-08_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-08_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-08_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-08_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-08_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-08_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-08_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-08_hackweek_subset.csv\n2018-04-09\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-09_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-09_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-09_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-09_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-09_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-09_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-09_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-09_hackweek_subset.csv\n2018-04-10\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-10_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-10_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-10_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-10_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-10_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-10_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-10_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-10_hackweek_subset.csv\n2018-04-11\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-11_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-11_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-11_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-11_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-11_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-11_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-11_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-11_hackweek_subset.csv\n2018-04-12\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-12_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-12_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-12_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-12_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-12_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-12_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-12_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-12_hackweek_subset.csv\n2018-04-13\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-13_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-13_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-13_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-13_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-13_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-13_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-13_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-13_hackweek_subset.csv\n2018-04-14\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-14_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-14_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-14_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-14_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-14_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-14_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-14_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-14_hackweek_subset.csv\n2018-04-15\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-15_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-15_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-15_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-15_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-15_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-15_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-15_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-15_hackweek_subset.csv\n2018-04-16\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-16_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-16_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-16_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-16_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-16_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-16_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-16_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-16_hackweek_subset.csv\n2018-04-17\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-17_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-17_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-17_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-17_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-17_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-17_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-17_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-17_hackweek_subset.csv\n2018-04-18\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-18_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-18_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-18_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-18_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-18_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-18_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-18_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-18_hackweek_subset.csv\n2018-04-19\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-19_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-19_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-19_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-19_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-19_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-19_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-19_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-19_hackweek_subset.csv\n2018-04-20\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-20_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-20_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-20_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-20_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-20_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-20_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-20_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-20_hackweek_subset.csv\n2018-04-21\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-21_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-21_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-21_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-21_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-21_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-21_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-21_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-21_hackweek_subset.csv\n2018-04-22\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-22_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-22_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-22_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-22_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-22_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-22_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-22_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-22_hackweek_subset.csv\n2018-04-23\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-23_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-23_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-23_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-23_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-23_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-23_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-23_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-23_hackweek_subset.csv\n2018-04-24\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-24_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-24_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-24_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-24_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-24_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-24_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-24_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-24_hackweek_subset.csv\n2018-04-25\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-25_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-25_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-25_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-25_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-25_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-25_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-25_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-25_hackweek_subset.csv\n2018-04-26\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-26_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-26_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-26_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-26_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-26_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-26_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-26_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-26_hackweek_subset.csv\n2018-04-27\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-27_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-27_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-27_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-27_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-27_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-27_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-27_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-27_hackweek_subset.csv\n2018-04-28\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-28_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-28_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-28_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-28_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-28_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-28_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-28_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-28_hackweek_subset.csv\n2018-04-29\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-29_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-29_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-29_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-29_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-29_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-29_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-29_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-29_hackweek_subset.csv\n2018-04-30\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-04-30_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-04-30_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-04-30_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-04-30_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-04-30_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-04-30_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-04-30_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-04-30_hackweek_subset.csv\n2018-05-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-01_hackweek_subset.csv\n2018-05-02\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-02_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-02_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-02_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-02_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-02_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-02_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-02_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-02_hackweek_subset.csv\n2018-05-03\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-03_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-03_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-03_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-03_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-03_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-03_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-03_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-03_hackweek_subset.csv\n2018-05-04\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-04_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-04_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-04_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-04_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-04_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-04_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-04_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-04_hackweek_subset.csv\n2018-05-05\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-05_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-05_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-05_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-05_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-05_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-05_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-05_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-05_hackweek_subset.csv\n2018-05-06\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-06_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-06_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-06_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-06_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-06_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-06_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-06_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-06_hackweek_subset.csv\n2018-05-07\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-07_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-07_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-07_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-07_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-07_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-07_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-07_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-07_hackweek_subset.csv\n2018-05-08\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-08_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-08_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-08_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-08_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-08_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-08_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-08_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-08_hackweek_subset.csv\n2018-05-09\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-09_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-09_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-09_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-09_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-09_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-09_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-09_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-09_hackweek_subset.csv\n2018-05-10\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-10_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-10_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-10_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-10_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-10_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-10_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-10_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-10_hackweek_subset.csv\n2018-05-11\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-11_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-11_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-11_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-11_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-11_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-11_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-11_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-11_hackweek_subset.csv\n2018-05-12\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-12_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-12_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-12_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-12_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-12_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-12_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-12_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-12_hackweek_subset.csv\n2018-05-13\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-13_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-13_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-13_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-13_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-13_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-13_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-13_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-13_hackweek_subset.csv\n2018-05-14\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-14_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-14_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-14_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-14_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-14_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-14_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-14_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-14_hackweek_subset.csv\n2018-05-15\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-15_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-15_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-15_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-15_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-15_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-15_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-15_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-15_hackweek_subset.csv\n2018-05-16\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-16_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-16_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-16_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-16_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-16_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-16_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-16_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-16_hackweek_subset.csv\n2018-05-17\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-17_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-17_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-17_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-17_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-17_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-17_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-17_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-17_hackweek_subset.csv\n2018-05-18\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-18_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-18_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-18_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-18_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-18_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-18_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-18_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-18_hackweek_subset.csv\n2018-05-19\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-19_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-19_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-19_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-19_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-19_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-19_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-19_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-19_hackweek_subset.csv\n2018-05-20\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-20_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-20_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-20_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-20_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-20_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-20_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-20_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-20_hackweek_subset.csv\n2018-05-21\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-21_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-21_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-21_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-21_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-21_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-21_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-21_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-21_hackweek_subset.csv\n2018-05-22\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-22_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-22_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-22_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-22_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-22_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-22_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-22_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-22_hackweek_subset.csv\n2018-05-23\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-23_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-23_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-23_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-23_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-23_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-23_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-23_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-23_hackweek_subset.csv\n2018-05-24\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-24_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-24_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-24_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-24_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-24_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-24_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-24_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-24_hackweek_subset.csv\n2018-05-25\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-25_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-25_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-25_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-25_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-25_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-25_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-25_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-25_hackweek_subset.csv\n2018-05-26\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-26_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-26_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-26_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-26_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-26_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-26_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-26_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-26_hackweek_subset.csv\n2018-05-27\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-27_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-27_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-27_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-27_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-27_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-27_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-27_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-27_hackweek_subset.csv\n2018-05-28\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-28_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-28_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-28_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-28_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-28_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-28_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-28_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-28_hackweek_subset.csv\n2018-05-29\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-29_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-29_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-29_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-29_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-29_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-29_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-29_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-29_hackweek_subset.csv\n2018-05-30\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-30_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-30_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-30_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-30_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-30_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-30_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-30_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-30_hackweek_subset.csv\n2018-05-31\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-05-31_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-05-31_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-05-31_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-05-31_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-05-31_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-05-31_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-05-31_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-05-31_hackweek_subset.csv\n2018-06-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-01_hackweek_subset.csv\n2018-06-02\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-02_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-02_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-02_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-02_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-02_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-02_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-02_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-02_hackweek_subset.csv\n2018-06-03\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-03_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-03_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-03_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-03_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-03_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-03_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-03_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-03_hackweek_subset.csv\n2018-06-04\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-04_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-04_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-04_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-04_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-04_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-04_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-04_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-04_hackweek_subset.csv\n2018-06-05\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-05_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-05_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-05_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-05_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-05_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-05_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-05_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-05_hackweek_subset.csv\n2018-06-06\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-06_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-06_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-06_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-06_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-06_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-06_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-06_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-06_hackweek_subset.csv\n2018-06-07\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-07_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-07_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-07_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-07_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-07_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-07_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-07_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-07_hackweek_subset.csv\n2018-06-08\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-08_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-08_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-08_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-08_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-08_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-08_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-08_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-08_hackweek_subset.csv\n2018-06-09\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-09_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-09_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-09_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-09_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-09_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-09_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-09_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-09_hackweek_subset.csv\n2018-06-10\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-10_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-10_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-10_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-10_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-10_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-10_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-10_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-10_hackweek_subset.csv\n2018-06-11\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-11_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-11_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-11_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-11_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-11_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-11_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-11_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-11_hackweek_subset.csv\n2018-06-12\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-12_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-12_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-12_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-12_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-12_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-12_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-12_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-12_hackweek_subset.csv\n2018-06-13\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-13_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-13_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-13_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-13_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-13_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-13_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-13_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-13_hackweek_subset.csv\n2018-06-14\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-14_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-14_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-14_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-14_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-14_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-14_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-14_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-14_hackweek_subset.csv\n2018-06-15\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-15_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-15_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-15_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-15_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-15_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-15_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-15_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-15_hackweek_subset.csv\n2018-06-16\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-16_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-16_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-16_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-16_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-16_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-16_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-16_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-16_hackweek_subset.csv\n2018-06-17\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-17_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-17_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-17_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-17_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-17_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-17_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-17_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-17_hackweek_subset.csv\n2018-06-18\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-18_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-18_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-18_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-18_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-18_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-18_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-18_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-18_hackweek_subset.csv\n2018-06-19\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-19_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-19_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-19_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-19_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-19_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-19_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-19_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-19_hackweek_subset.csv\n2018-06-20\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-20_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-20_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-20_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-20_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-20_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-20_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-20_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-20_hackweek_subset.csv\n2018-06-21\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-21_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-21_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-21_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-21_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-21_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-21_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-21_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-21_hackweek_subset.csv\n2018-06-22\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-22_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-22_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-22_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-22_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-22_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-22_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-22_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-22_hackweek_subset.csv\n2018-06-23\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-23_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-23_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-23_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-23_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-23_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-23_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-23_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-23_hackweek_subset.csv\n2018-06-24\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-24_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-24_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-24_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-24_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-24_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-24_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-24_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-24_hackweek_subset.csv\n2018-06-25\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-25_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-25_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-25_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-25_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-25_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-25_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-25_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-25_hackweek_subset.csv\n2018-06-26\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-26_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-26_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-26_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-26_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-26_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-26_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-26_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-26_hackweek_subset.csv\n2018-06-27\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-27_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-27_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-27_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-27_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-27_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-27_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-27_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-27_hackweek_subset.csv\n2018-06-28\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-28_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-28_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-28_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-28_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-28_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-28_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-28_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-28_hackweek_subset.csv\n2018-06-29\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-29_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-29_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-29_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-29_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-29_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-29_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-29_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-29_hackweek_subset.csv\n2018-06-30\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-06-30_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-06-30_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-06-30_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-06-30_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-06-30_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-06-30_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-06-30_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-06-30_hackweek_subset.csv\n2018-07-01\nChecking file: vs_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vs_2018-07-01_hackweek_subset.csv\nChecking file: etr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_etr_2018-07-01_hackweek_subset.csv\nChecking file: vpd_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_vpd_2018-07-01_hackweek_subset.csv\nChecking file: tmmn_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmn_2018-07-01_hackweek_subset.csv\nChecking file: rmin_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmin_2018-07-01_hackweek_subset.csv\nChecking file: rmax_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_rmax_2018-07-01_hackweek_subset.csv\nChecking file: pr_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_pr_2018-07-01_hackweek_subset.csv\nChecking file: tmmx_2018.nc\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2018_tmmx_2018-07-01_hackweek_subset.csv\n",
  "history_begin_time" : 1698342826437,
  "history_end_time" : 1698352110876,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "2xMjfBOSab3h",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncreate_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nThe subset of the rows is saved to /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper_hackweek_subset.csv\nDone\n",
  "history_begin_time" : 1698342740207,
  "history_end_time" : 1698342740906,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Dszwl9lEPdDp",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \n  \n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#collect_gridmet_for_testing()\ncollect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\n   Latitude  Longitude  x  y  ...  Aspect  Curvature  Northness  Eastness\n0      49.0   -125.000  0  0  ...     NaN        NaN        NaN       NaN\n1      49.0   -124.964  1  0  ...     NaN        NaN        NaN       NaN\n2      49.0   -124.928  2  0  ...     NaN        NaN        NaN       NaN\n3      49.0   -124.892  3  0  ...     NaN        NaN        NaN       NaN\n4      49.0   -124.856  4  0  ...     NaN        NaN        NaN       NaN\n[5 rows x 10 columns]\nFiltered Data:\n          lat      lon    x    y  ...     aspect   curvature  northness  eastness\n197936  38.74 -119.744  146  285  ...  278.16498  -2075.4006   0.141080 -0.780304\n197937  38.74 -119.708  147  285  ...  261.02734   2939.2131  -0.154717 -0.779242\n197938  38.74 -119.672  148  285  ...  207.80370   8886.4110  -0.724214 -0.436444\n197939  38.74 -119.636  149  285  ...  180.53543  10687.0690  -0.785376 -0.009344\n197940  38.74 -119.600  150  285  ...   83.49026   5316.9870   0.112890  0.782164\n[5 rows x 10 columns]\nThe subset of the rows is saved to /home/ubuntu/gridmet_test_run/dem_all.csv_hackweek_subset_testing.csv\nDone\n",
  "history_begin_time" : 1698342290027,
  "history_end_time" : 1698342290744,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "PcqQs0Bd3Vn0",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \n  \n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\n#collect_gridmet_for_testing()\ncollect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\n   Latitude  Longitude  x  y  ...  Aspect  Curvature  Northness  Eastness\n0      49.0   -125.000  0  0  ...     NaN        NaN        NaN       NaN\n1      49.0   -124.964  1  0  ...     NaN        NaN        NaN       NaN\n2      49.0   -124.928  2  0  ...     NaN        NaN        NaN       NaN\n3      49.0   -124.892  3  0  ...     NaN        NaN        NaN       NaN\n4      49.0   -124.856  4  0  ...     NaN        NaN        NaN       NaN\n[5 rows x 10 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3652, in get_loc\n    return self._engine.get_loc(casted_key)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'lat'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/PcqQs0Bd3Vn0/data_merge_hackweek.py\", line 326, in <module>\n    collect_terrain_for_testing()\n  File \"/home/ubuntu/gw-workspace/PcqQs0Bd3Vn0/data_merge_hackweek.py\", line 128, in collect_terrain_for_testing\n    filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                              ~~~~~~~~~~^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 3761, in __getitem__\n    indexer = self.columns.get_loc(key)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3654, in get_loc\n    raise KeyError(key) from err\nKeyError: 'lat'\n",
  "history_begin_time" : 1698342176238,
  "history_end_time" : 1698342176945,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hkvA5T9JrCFO",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \n  \n\ndef collect_terrain_for_testing():\n  pass\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncollect_gridmet_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\n2017-10-01\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       155\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-01.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       850\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-01.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       375\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-01.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       129\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-01.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       785\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-01.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       482.0\ntop            0.0\nfreq      182073.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-01.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       386\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-01.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 273\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       432\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-01.csv\n2017-10-02\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       141\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-02.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       957\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-02.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       394\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-02.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       125\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-02.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       769\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-02.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       744.0\ntop            0.0\nfreq      195058.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-02.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       314\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-02.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 274\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       421\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-02.csv\n2017-10-03\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       144\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-03.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       965\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-03.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       406\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-03.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        93\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-03.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       772\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-03.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       820.0\ntop            0.0\nfreq      223820.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-03.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       330\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-03.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 275\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       407\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-03.csv\n2017-10-04\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       110\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-04.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       877\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-04.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       380\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-04.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       103\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-04.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       769\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-04.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       724.0\ntop            0.0\nfreq      220390.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-04.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       345\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-04.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 276\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       406\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-04.csv\n2017-10-05\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       121\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-05.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       858\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-05.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       349\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-05.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        80\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-05.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       821\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-05.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       794.0\ntop            0.0\nfreq      249503.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-05.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       385\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-05.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 277\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       379\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-05.csv\n2017-10-06\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       130\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-06.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       855\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-06.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       339\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-06.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       131\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-06.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       880\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-06.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       397.0\ntop            0.0\nfreq      254051.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-06.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       407\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-06.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 278\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       382\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-06.csv\n2017-10-07\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       155\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-07.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       988\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-07.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       326\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-07.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       146\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-07.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       868\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-07.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       476.0\ntop            0.0\nfreq      258582.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-07.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       406\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-07.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 279\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       408\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-07.csv\n2017-10-08\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       165\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-08.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       858\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-08.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       381\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-08.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       134\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-08.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       831\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-08.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       242.0\ntop            0.0\nfreq      234071.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-08.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       400\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-08.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 280\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       458\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-08.csv\n2017-10-09\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       161\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-09.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       986\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-09.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       367\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-09.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       100\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-09.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       836\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-09.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       264.0\ntop            0.0\nfreq      262558.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-09.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       304\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-09.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 281\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       459\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-09.csv\n2017-10-10\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       119\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-10.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       931\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-10.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       363\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-10.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        84\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-10.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       812\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-10.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       281.0\ntop            0.0\nfreq      279417.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-10.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       348\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-10.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 282\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       358\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-10.csv\n2017-10-11\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       123\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-11.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       979\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-11.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       343\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-11.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       145\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-11.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       817\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-11.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       297.0\ntop            0.0\nfreq      265106.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-11.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       359\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-11.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 283\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       413\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-11.csv\n2017-10-12\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       134\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-12.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       976\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-12.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       375\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-12.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       117\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-12.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       837\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-12.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       626.0\ntop            0.0\nfreq      255097.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-12.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       339\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-12.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 284\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       425\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-12.csv\n2017-10-13\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       129\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-13.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       991\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-13.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       371\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-13.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       111\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-13.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       829\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-13.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       378.0\ntop            0.0\nfreq      218769.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-13.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       342\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-13.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 285\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       448\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-13.csv\n2017-10-14\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       155\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-14.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       827\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-14.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       379\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-14.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       121\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-14.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       849\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-14.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       270.0\ntop            0.0\nfreq      262056.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-14.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       357\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-14.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 286\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       437\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-14.csv\n2017-10-15\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       127\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-15.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       781\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-15.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       391\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-15.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       122\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-15.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       870\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-15.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       290.0\ntop            0.0\nfreq      288774.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-15.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       378\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-15.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 287\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       364\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-15.csv\n2017-10-16\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       113\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-16.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       887\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-16.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       377\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-16.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       129\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-16.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       885\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-16.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       262.0\ntop            0.0\nfreq      298377.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-16.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       418\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-16.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 288\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       342\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-16.csv\n2017-10-17\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       137\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-17.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       938\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-17.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       354\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-17.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       162\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-17.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       896\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-17.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       508.0\ntop            0.0\nfreq      290820.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-17.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       403\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-17.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 289\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       362\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-17.csv\n2017-10-18\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       126\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-18.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       981\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-18.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       340\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-18.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       132\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-18.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       884\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-18.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1508.0\ntop            0.0\nfreq      275623.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-18.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       396\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-18.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 290\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       381\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-18.csv\n2017-10-19\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       131\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-19.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       977\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-19.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       354\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-19.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       132\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-19.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       821\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-19.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1022.0\ntop            0.0\nfreq      211411.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-19.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       394\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-19.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 291\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       396\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-19.csv\n2017-10-20\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       134\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-20.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       944\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-20.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       349\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-20.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       110\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-20.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       705\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-20.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       629.0\ntop            0.0\nfreq      188675.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-20.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       269\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-20.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 292\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       393\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-20.csv\n2017-10-21\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       123\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-21.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       938\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-21.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       354\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-21.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       180\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-21.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       752\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-21.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1695.0\ntop            0.0\nfreq      214672.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-21.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       273\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-21.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 293\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       442\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-21.csv\n2017-10-22\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       152\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-22.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       979\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-22.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       344\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-22.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       184\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-22.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       802\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-22.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       730.0\ntop            0.0\nfreq      232979.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-22.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       344\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-22.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 294\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       389\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-22.csv\n2017-10-23\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       164\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-23.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       782\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-23.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       396\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-23.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       144\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-23.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       832\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-23.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       138.0\ntop            0.0\nfreq      302683.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-23.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       460\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-23.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 295\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       441\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-23.csv\n2017-10-24\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       162\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-24.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       683\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-24.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       392\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-24.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       119\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-24.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       812\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-24.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique        73.0\ntop            0.0\nfreq      301987.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-24.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       462\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-24.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 296\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       388\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-24.csv\n2017-10-25\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       138\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-25.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       887\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-25.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       359\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-25.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       146\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-25.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       806\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-25.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       215.0\ntop            0.0\nfreq      269106.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-25.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       410\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-25.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 297\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       344\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-25.csv\n2017-10-26\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       134\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-26.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       844\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-26.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       408\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-26.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       122\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-26.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       800\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-26.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique        81.0\ntop            0.0\nfreq      294360.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-26.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       388\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-26.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 298\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       432\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-26.csv\n2017-10-27\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       113\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-27.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       721\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-27.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       397\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-27.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       104\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-27.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       818\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-27.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique        50.0\ntop            0.0\nfreq      305832.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-27.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       379\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-27.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 299\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       407\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-27.csv\n2017-10-28\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       121\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-28.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       808\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-28.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       344\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-28.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       108\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-28.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       804\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-28.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique        43.0\ntop            0.0\nfreq      300175.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-28.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       372\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-28.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 300\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       415\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-28.csv\n2017-10-29\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       115\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-29.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       916\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-29.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       377\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-29.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       109\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-29.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       766\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-29.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique        64.0\ntop            0.0\nfreq      291363.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-29.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       356\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-29.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 301\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       363\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-29.csv\n2017-10-30\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       127\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-30.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       756\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-30.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       415\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-30.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       101\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-30.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       741\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-30.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       119.0\ntop            0.0\nfreq      276030.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-30.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       319\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-30.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 302\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       447\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-30.csv\n2017-10-31\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        90\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-10-31.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       660\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-10-31.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       386\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-10-31.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       131\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-10-31.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       704\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-10-31.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       311.0\ntop            0.0\nfreq      234229.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-10-31.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       239\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-10-31.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 303\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       352\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-10-31.csv\n2017-11-01\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       118\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-01.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       925\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-01.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       365\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-01.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       176\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-01.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       706\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-01.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       419.0\ntop            0.0\nfreq      218571.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-01.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       222\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-01.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 304\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       392\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-01.csv\n2017-11-02\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       123\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-02.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       914\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-02.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       355\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-02.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       141\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-02.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       624\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-02.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       481.0\ntop            0.0\nfreq      198352.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-02.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       239\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-02.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 305\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       437\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-02.csv\n2017-11-03\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       100\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-03.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       895\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-03.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       458\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-03.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       120\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-03.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       603\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-03.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       643.0\ntop            0.0\nfreq      173414.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-03.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       249\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-03.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 306\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       485\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-03.csv\n2017-11-04\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       118\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-04.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       911\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-04.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       444\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-04.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       147\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-04.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       692\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-04.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       462.0\ntop            0.0\nfreq      173460.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-04.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       264\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-04.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 307\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       468\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-04.csv\n2017-11-05\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       110\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-05.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       891\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-05.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       412\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-05.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       133\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-05.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       653\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-05.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       408.0\ntop            0.0\nfreq      185908.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-05.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       284\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-05.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 308\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       484\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-05.csv\n2017-11-06\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       117\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-06.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       916\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-06.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       442\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-06.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       108\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-06.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       654\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-06.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       191.0\ntop            0.0\nfreq      223467.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-06.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       296\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-06.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 309\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       482\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-06.csv\n2017-11-07\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        85\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-07.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       952\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-07.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       480\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-07.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        97\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-07.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       721\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-07.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       230.0\ntop            0.0\nfreq      231477.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-07.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       267\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-07.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 310\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       454\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-07.csv\n2017-11-08\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        74\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-08.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       989\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-08.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       432\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-08.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       154\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-08.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       770\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-08.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       890.0\ntop            0.0\nfreq      204923.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-08.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       192\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-08.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 311\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       410\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-08.csv\n2017-11-09\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        95\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-09.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       941\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-09.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       420\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-09.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       127\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-09.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       726\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-09.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       610.0\ntop            0.0\nfreq      220619.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-09.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       220\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-09.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 312\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       452\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-09.csv\n2017-11-10\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        88\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-10.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       972\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-10.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       317\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-10.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       105\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-10.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       719\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-10.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       392.0\ntop            0.0\nfreq      223463.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-10.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       249\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-10.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 313\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       385\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-10.csv\n2017-11-11\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        81\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-11.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       979\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-11.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       361\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-11.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        88\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-11.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       741\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-11.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       672.0\ntop            0.0\nfreq      264199.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-11.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       266\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-11.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 314\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       405\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-11.csv\n2017-11-12\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        76\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-12.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       976\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-12.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       363\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-12.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       117\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-12.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       788\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-12.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       946.0\ntop            0.0\nfreq      267578.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-12.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       278\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-12.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 315\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       411\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-12.csv\n2017-11-13\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        89\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-13.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       968\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-13.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       348\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-13.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       188\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-13.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       802\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-13.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       745.0\ntop            0.0\nfreq      225986.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-13.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       321\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-13.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 316\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       386\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-13.csv\n2017-11-14\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       111\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-14.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       977\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-14.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       372\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-14.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       140\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-14.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       794\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-14.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1120.0\ntop            0.0\nfreq      252698.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-14.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       322\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-14.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 317\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       410\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-14.csv\n2017-11-15\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       111\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-15.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       972\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-15.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       392\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-15.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       131\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-15.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       759\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-15.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1177.0\ntop            0.0\nfreq      210647.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-15.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       294\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-15.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 318\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       445\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-15.csv\n2017-11-16\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       133\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-16.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       961\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-16.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       401\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-16.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       165\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-16.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       745\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-16.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1192.0\ntop            0.0\nfreq      172644.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-16.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       299\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-16.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 319\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       412\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-16.csv\n2017-11-17\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       134\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-17.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       913\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-17.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       404\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-17.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       152\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-17.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       699\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-17.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       494.0\ntop            0.0\nfreq      170607.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-17.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       284\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-17.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 320\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       443\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-17.csv\n2017-11-18\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       120\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-18.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       891\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-18.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       381\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-18.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       127\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-18.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       755\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-18.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       305.0\ntop            0.0\nfreq      287259.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-18.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       227\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-18.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 321\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       430\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-18.csv\n2017-11-19\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        91\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-19.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       986\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-19.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       348\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-19.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       148\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-19.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       870\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-19.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1066.0\ntop            0.0\nfreq      234054.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-19.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       214\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-19.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 322\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       353\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-19.csv\n2017-11-20\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       105\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-20.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       992\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-20.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       374\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-20.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       145\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-20.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       873\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-20.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       914.0\ntop            0.0\nfreq      188448.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-20.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       234\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-20.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 323\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       346\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-20.csv\n2017-11-21\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       125\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-21.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       955\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-21.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       439\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-21.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       138\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-21.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       873\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-21.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1438.0\ntop            0.0\nfreq      232803.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-21.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       297\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-21.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 324\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       446\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-21.csv\n2017-11-22\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       132\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-22.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       992\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-22.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       392\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-22.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       141\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-22.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       779\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-22.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique      1018.0\ntop            0.0\nfreq      251562.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-22.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       347\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-22.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 325\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       408\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-22.csv\n2017-11-23\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       126\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-23.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       889\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-23.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       334\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-23.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       162\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-23.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       752\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-23.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       817.0\ntop            0.0\nfreq      209619.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-23.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       335\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-23.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 326\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       355\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-23.csv\n2017-11-24\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       128\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-24.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       953\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-24.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       332\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-24.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       137\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-24.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       744\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-24.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       293.0\ntop            0.0\nfreq      261733.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-24.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       314\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-24.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 327\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       389\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-24.csv\n2017-11-25\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        93\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-25.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       845\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-25.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       314\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-25.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       113\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-25.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       765\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-25.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       540.0\ntop            0.0\nfreq      261934.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-25.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       299\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-25.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 328\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       373\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-25.csv\n2017-11-26\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       118\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-26.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       969\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-26.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       291\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-26.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       155\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-26.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       776\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-26.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       754.0\ntop            0.0\nfreq      211525.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-26.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       328\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-26.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 329\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       387\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-26.csv\n2017-11-27\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       119\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-27.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       943\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-27.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       310\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-27.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       135\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-27.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       696\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-27.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       251.0\ntop            0.0\nfreq      217210.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-27.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       285\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-27.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 330\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       395\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-27.csv\n2017-11-28\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       111\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-28.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       958\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-28.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       335\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-28.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       112\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-28.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       715\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-28.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       451.0\ntop            0.0\nfreq      246204.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-28.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       200\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-28.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 331\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       376\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-28.csv\n2017-11-29\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        73\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-29.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       931\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-29.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       370\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-29.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       152\ntop           --\nfreq      151634\nName: vs, dtype: object\n   Latitude  Longitude  vs\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vs_2017-11-29.csv\nChecking file: rmax_2017.nc\nVariable name: rmax\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       720\ntop           --\nfreq      151634\nName: rmax, dtype: object\n   Latitude  Longitude rmax\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmax_2017-11-29.csv\nChecking file: pr_2017.nc\nVariable name: pr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204.0\nunique       200.0\ntop            0.0\nfreq      275784.0\nName: pr, dtype: float64\n   Latitude  Longitude  pr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_pr_2017-11-29.csv\nChecking file: vpd_2017.nc\nVariable name: vpd\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       218\ntop           --\nfreq      151634\nName: vpd, dtype: object\n   Latitude  Longitude vpd\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_vpd_2017-11-29.csv\nChecking file: tmmx_2017.nc\nVariable name: tmmx\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 332\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       356\ntop           --\nfreq      151634\nName: tmmx, dtype: object\n   Latitude  Longitude tmmx\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmx_2017-11-29.csv\n2017-11-30\nChecking file: etr_2017.nc\nVariable name: etr\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 333\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique        75\ntop           --\nfreq      151634\nName: etr, dtype: object\n   Latitude  Longitude etr\n0      49.0   -125.000  --\n1      49.0   -124.964  --\n2      49.0   -124.928  --\n3      49.0   -124.892  --\n4      49.0   -124.856  --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_etr_2017-11-30.csv\nChecking file: rmin_2017.nc\nVariable name: rmin\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 333\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       941\ntop           --\nfreq      151634\nName: rmin, dtype: object\n   Latitude  Longitude rmin\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_rmin_2017-11-30.csv\nChecking file: tmmn_2017.nc\nVariable name: tmmn\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\nval_col.shape:  (365, 585, 1386)\nday_index: 333\nIndex(['dem_lat', 'dem_lon', 'x', 'y', 'Elevation', 'gridmet_lat',\n       'gridmet_lon', 'gridmet_lat_idx', 'gridmet_lon_idx'],\n      dtype='object')\n   dem_lat  dem_lon  x  ...  gridmet_lon  gridmet_lat_idx  gridmet_lon_idx\n0     49.0 -125.000  0  ...  -124.766667             10.0              0.0\n1     49.0 -124.964  1  ...  -124.766667             10.0              0.0\n2     49.0 -124.928  2  ...  -124.766667             10.0              0.0\n3     49.0 -124.892  3  ...  -124.766667             10.0              0.0\n4     49.0 -124.856  4  ...  -124.766667             10.0              0.0\n[5 rows x 9 columns]\nmapper_df[var_name]:  count     462204\nunique       409\ntop           --\nfreq      151634\nName: tmmn, dtype: object\n   Latitude  Longitude tmmn\n0      49.0   -125.000   --\n1      49.0   -124.964   --\n2      49.0   -124.928   --\n3      49.0   -124.892   --\n4      49.0   -124.856   --\ngridmet var saved:  /home/ubuntu/gridmet_test_run/testing_output/2017_tmmn_2017-11-30.csv\nChecking file: vs_2017.nc\nVariable name: vs\nProcessing file: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_to_dem_mapper.csv already exists, skipping..\n",
  "history_begin_time" : 1698338011882,
  "history_end_time" : 1698341758860,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "M7xksZRlLGCu",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date <= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \n  \n\ndef collect_terrain_for_testing():\n  pass\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncollect_gridmet_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\n2017-10-01\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/M7xksZRlLGCu/data_merge_hackweek.py\", line 283, in <module>\n    collect_gridmet_for_testing()\n  File \"/home/ubuntu/gw-workspace/M7xksZRlLGCu/data_merge_hackweek.py\", line 109, in collect_gridmet_for_testing\n    turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n    ^^^^^^^^^^^^^^^^^^^^^^\nNameError: name 'turn_gridmet_nc_to_csv' is not defined\n",
  "history_begin_time" : 1698337999005,
  "history_end_time" : 1698337999497,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "9tvMgLMCTQGM",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date >= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \n  \n\ndef collect_terrain_for_testing():\n  pass\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncollect_gridmet_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\n",
  "history_begin_time" : 1698337976730,
  "history_end_time" : 1698337977215,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "u0fyWVMRkQ2g",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n\n  # Traverse the days using a loop\n  while current_date >= end_date:\n      print(current_date.strftime('%Y-%m-%d'))\n      turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      current_date += step\n  \n  \n\ndef collect_terrain_for_testing():\n  pass\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncollect_gridmet_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc exists\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/u0fyWVMRkQ2g/data_merge_hackweek.py\", line 282, in <module>\n    collect_gridmet_for_testing()\n  File \"/home/ubuntu/gw-workspace/u0fyWVMRkQ2g/data_merge_hackweek.py\", line 100, in collect_gridmet_for_testing\n    step = timedelta(days=1)\n           ^^^^^^^^^\nNameError: name 'timedelta' is not defined\n",
  "history_begin_time" : 1698337958250,
  "history_end_time" : 1698337958740,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "5HOfcZKdgV04",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # \n  \n  pass\n\ndef collect_terrain_for_testing():\n  pass\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncollect_gridmet_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\nDownloading http://www.northwestknowledge.net/metdata/data/tmmn_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmn_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/tmmx_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/tmmx_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/pr_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/pr_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/vpd_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vpd_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/etr_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/etr_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/rmax_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmax_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/rmin_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/rmin_2018.nc exists\nDownloading http://www.northwestknowledge.net/metdata/data/vs_2017.nc\nFile downloaded successfully and saved as: /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2017.nc\nFile /home/ubuntu/gridmet_test_run/gridmet_climatology/vs_2018.nc exists\n",
  "history_begin_time" : 1698337434075,
  "history_end_time" : 1698337475790,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "39YB4TH3ZXx2",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\namsr_gridmet_terrain_testing_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years_testing.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  \n  pass\n\ndef collect_terrain_for_testing():\n  pass\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# this is section for preparing training data\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n# this is section for preparing testing data\ncollect_gridmet_for_testing()\n#merge_all_testing_data_together()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n     lat      lon\n0  38.74 -119.744\n1  38.74 -119.708\n2  38.74 -119.672\n3  38.74 -119.636\n4  38.74 -119.600\n",
  "history_begin_time" : 1698336763794,
  "history_end_time" : 1698336764258,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HywA8yQWKWwR",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n  df1 = pd.read_csv(landcover_testing)\n  df2 = pd.read_csv(pmw_testing_new)\n  df3 = pd.read_csv(fsca_testing)\n  df4 = pd.read_csv(snowclassification_testing_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\nmerge_all_testing_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/HywA8yQWKWwR/data_merge_hackweek.py\", line 241, in <module>\n    merge_all_testing_data_together()\n  File \"/home/ubuntu/gw-workspace/HywA8yQWKWwR/data_merge_hackweek.py\", line 170, in merge_all_testing_data_together\n    df5 = pd.read_csv(amsr_gridmet_terrain_testing_data)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nNameError: name 'amsr_gridmet_terrain_testing_data' is not defined. Did you mean: 'amsr_gridmet_terrain_training_data'?\n",
  "history_begin_time" : 1698335837473,
  "history_end_time" : 1698335837934,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vHJjZmwmol9y",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\", \"fSCA\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-26\ntest start date:  2018-01-01\ntest end date:  2023-10-26\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_pr  cumulative_fSCA\n0         273 2017-10-01  ...            0.0           0.3508\n1         274 2017-10-02  ...            0.0           0.5146\n2         275 2017-10-03  ...            2.0           0.5188\n3         276 2017-10-04  ...            2.0           0.5812\n4         277 2017-10-05  ...            2.0           0.6210\n[5 rows x 33 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n",
  "history_begin_time" : 1698333262604,
  "history_end_time" : 1698333263156,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "0rd7z38uetp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698276496970,
  "history_end_time" : 1698276496970,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aEIoHzuNnqxQ",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\n#filter_water_year_winter_months_only()\ncreate_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        date       lat  ...        pmv    fSCA  SnowClass\n0         273  2017-10-01  37.89748  ...  247.56000  0.3508        6.0\n1         274  2017-10-02  37.89748  ...  243.37999  0.1638        6.0\n2         275  2017-10-03  37.89748  ...  242.95000  0.0042        6.0\n3         276  2017-10-04  37.89748  ...  239.87000  0.0624        6.0\n4         277  2017-10-05  37.89748  ...  241.40999  0.0398        6.0\n[5 rows x 24 columns]\n   Unnamed: 0       date  ...  cumulative_vs  cumulative_pr\n0         273 2017-10-01  ...            4.2            0.0\n1         274 2017-10-02  ...            8.3            0.0\n2         275 2017-10-03  ...           10.9            2.0\n3         276 2017-10-04  ...           13.2            2.0\n4         277 2017-10-05  ...           15.7            2.0\n[5 rows x 32 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_cum_water_year_winter_month_only.csv\n",
  "history_begin_time" : 1698273321970,
  "history_end_time" : 1698273322576,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "vbI1qPKITOSK",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\n\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\nfilter_water_year_winter_months_only()\n#create_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\n          date       lat         lon  ...        pmv    fSCA  SnowClass\n273 2017-10-01  37.89748 -119.262434  ...  247.56000  0.3508        6.0\n274 2017-10-02  37.89748 -119.262434  ...  243.37999  0.1638        6.0\n275 2017-10-03  37.89748 -119.262434  ...  242.95000  0.0042        6.0\n276 2017-10-04  37.89748 -119.262434  ...  239.87000  0.0624        6.0\n277 2017-10-05  37.89748 -119.262434  ...  241.40999  0.0398        6.0\n[5 rows x 23 columns]\n",
  "history_begin_time" : 1698273256059,
  "history_end_time" : 1698273256631,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6A9rQbAQDv75",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {training_hackweek_cum_csv}\")\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\ncreate_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\n        date       lat  ...  cumulative_vs  cumulative_pr\n0 2017-01-01  37.89748  ...            6.9            0.0\n1 2017-01-02  37.89748  ...           18.7            1.7\n2 2017-01-03  37.89748  ...           29.3           34.7\n3 2017-01-04  37.89748  ...           42.7          105.3\n4 2017-01-05  37.89748  ...           51.7          112.8\n[5 rows x 31 columns]\nAll the cumulative variables are added successfully! /home/ubuntu/gridmet_test_run/all_merged_training_cum.csv\n",
  "history_begin_time" : 1698269090763,
  "history_end_time" : 1698269091449,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "6WMyQEaNjfgG",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\ncreate_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\n        date       lat  ...  cumulative_vs  cumulative_pr\n0 2017-01-01  37.89748  ...            6.9            0.0\n1 2017-01-02  37.89748  ...           18.7            1.7\n2 2017-01-03  37.89748  ...           29.3           34.7\n3 2017-01-04  37.89748  ...           42.7          105.3\n4 2017-01-05  37.89748  ...           51.7          112.8\n[5 rows x 31 columns]\n",
  "history_begin_time" : 1698269018374,
  "history_end_time" : 1698269019055,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "r75LHvG1xV8J",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'])\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\ncreate_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 11610, in _reindex_for_setitem\n    reindexed_value = value.reindex(index)._values\n                      ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/series.py\", line 4918, in reindex\n    return super().reindex(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5360, in reindex\n    return self._reindex_axes(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5375, in _reindex_axes\n    new_index, indexer = ax.reindex(\n                         ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 4278, in reindex\n    target = self._wrap_reindex_result(target, indexer, preserve_names)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/multi.py\", line 2490, in _wrap_reindex_result\n    target = MultiIndex.from_tuples(target)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/multi.py\", line 211, in new_meth\n    return meth(self_or_cls, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/multi.py\", line 590, in from_tuples\n    arrays = list(lib.tuples_to_object_array(tuples).T)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/lib.pyx\", line 2894, in pandas._libs.lib.tuples_to_object_array\nValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/r75LHvG1xV8J/data_merge_hackweek.py\", line 193, in <module>\n    create_accumulative_columns()\n  File \"/home/ubuntu/gw-workspace/r75LHvG1xV8J/data_merge_hackweek.py\", line 184, in create_accumulative_columns\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 3950, in __setitem__\n    self._set_item(key, value)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 4143, in _set_item\n    value = self._sanitize_column(value)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 4867, in _sanitize_column\n    return _reindex_for_setitem(Series(value), self.index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 11617, in _reindex_for_setitem\n    raise TypeError(\nTypeError: incompatible index of inserted column with frame index\n",
  "history_begin_time" : 1698265487383,
  "history_end_time" : 1698265487976,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "tXQPmZ2MXJur",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\ndef create_accumulative_columns():\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  current_df = pd.read_csv(training_hackweek_csv)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'])\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\",]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  training_hackweek_cum_csv = f\"{work_dir}/all_merged_training_cum.csv\"\n  current_df.to_csv(training_hackweek_cum_csv, index=False)\n\n# convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_data_together()\ncreate_accumulative_columns()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\n         date       lat         lon  swe_value  ...  lc_code  pmv    fSCA  SnowClass\n0  2017-01-01  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n1  2017-01-02  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n2  2017-01-03  37.89748 -119.262434       18.0  ...    142.0  NaN  0.9954        NaN\n3  2017-01-04  37.89748 -119.262434       23.0  ...    142.0  NaN  0.9954        NaN\n4  2017-01-05  37.89748 -119.262434       43.0  ...    142.0  NaN  0.9954        NaN\n[5 rows x 23 columns]\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 11610, in _reindex_for_setitem\n    reindexed_value = value.reindex(index)._values\n                      ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/series.py\", line 4918, in reindex\n    return super().reindex(\n           ^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5360, in reindex\n    return self._reindex_axes(\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5375, in _reindex_axes\n    new_index, indexer = ax.reindex(\n                         ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 4278, in reindex\n    target = self._wrap_reindex_result(target, indexer, preserve_names)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/multi.py\", line 2490, in _wrap_reindex_result\n    target = MultiIndex.from_tuples(target)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/multi.py\", line 211, in new_meth\n    return meth(self_or_cls, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/multi.py\", line 590, in from_tuples\n    arrays = list(lib.tuples_to_object_array(tuples).T)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pandas/_libs/lib.pyx\", line 2894, in pandas._libs.lib.tuples_to_object_array\nValueError: Buffer dtype mismatch, expected 'Python object' but got 'long'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/tXQPmZ2MXJur/data_merge_hackweek.py\", line 193, in <module>\n    create_accumulative_columns()\n  File \"/home/ubuntu/gw-workspace/tXQPmZ2MXJur/data_merge_hackweek.py\", line 184, in create_accumulative_columns\n    current_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 3950, in __setitem__\n    self._set_item(key, value)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 4143, in _set_item\n    value = self._sanitize_column(value)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 4867, in _sanitize_column\n    return _reindex_for_setitem(Series(value), self.index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 11617, in _reindex_for_setitem\n    raise TypeError(\nTypeError: incompatible index of inserted column with frame index\n",
  "history_begin_time" : 1698259040187,
  "history_end_time" : 1698259040747,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "x2cwm8s162w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698252277371,
  "history_end_time" : 1698252277371,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3ylmeghmq1k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698251392477,
  "history_end_time" : 1698251392477,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rn7n4b5pbqe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698227897150,
  "history_end_time" : 1698227897150,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lNeAeB45K1Nq",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\nmerge_all_data_together()\n# filter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['swe_value', 'station_elevation', 'lon', 'lat', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'date', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\n            date        lat         lon  ...  pmv    fSCA  SnowClass\n0     2017-01-01  37.897480 -119.262434  ...  NaN  0.9954        NaN\n1     2017-01-02  37.897480 -119.262434  ...  NaN  0.9954        NaN\n2     2017-01-03  37.897480 -119.262434  ...  NaN  0.9954        NaN\n3     2017-01-04  37.897480 -119.262434  ...  NaN  0.9954        NaN\n4     2017-01-05  37.897480 -119.262434  ...  NaN  0.9954        NaN\n...          ...        ...         ...  ...  ...     ...        ...\n7295  2018-12-27  37.862028 -119.657692  ...  NaN  1.0000        NaN\n7296  2018-12-28  37.862028 -119.657692  ...  NaN  1.0000        NaN\n7297  2018-12-29  37.862028 -119.657692  ...  NaN  1.0000        NaN\n7298  2018-12-30  37.862028 -119.657692  ...  NaN  1.0000        NaN\n7299  2018-12-31  37.862028 -119.657692  ...  NaN  1.0000        NaN\n[7300 rows x 23 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_training.csv\n",
  "history_begin_time" : 1698206230994,
  "history_end_time" : 1698206231789,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "F29cggBgCxJW",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  \n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\nmerge_all_data_together()\n# filter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['swe_value', 'station_elevation', 'lon', 'lat', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'date', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', 'lc_code', 'pmv', 'fSCA',\n       'SnowClass'],\n      dtype='object')\n            date        lat         lon  ...        pmv    fSCA  SnowClass\n0     2017-10-01  37.897480 -119.262434  ...  247.56000  0.3508          6\n1     2017-10-02  37.897480 -119.262434  ...  243.37999  0.1638          6\n2     2017-10-03  37.897480 -119.262434  ...  242.95000  0.0042          6\n3     2017-10-04  37.897480 -119.262434  ...  239.87000  0.0624          6\n4     2017-10-05  37.897480 -119.262434  ...  241.40999  0.0398          6\n...          ...        ...         ...  ...        ...     ...        ...\n2179  2018-09-25  37.862028 -119.657692  ...  263.61000  0.0000          4\n2180  2018-09-26  37.862028 -119.657692  ...  265.22000  0.0000          4\n2181  2018-09-27  37.862028 -119.657692  ...        NaN  0.0000          4\n2182  2018-09-28  37.862028 -119.657692  ...  262.97998  0.0000          4\n2183  2018-09-29  37.862028 -119.657692  ...  263.10000  0.0000          4\n[2184 rows x 23 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_training.csv\n",
  "history_begin_time" : 1698206150621,
  "history_end_time" : 1698206151351,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "SrXiIy55Usih",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\nmerge_all_data_together()\n# filter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'swe_value', 'station_elevation', 'elevation', 'aspect', 'curvature',\n       'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn',\n       'tmmx', 'vpd', 'vs'],\n      dtype='object')\n            date        lat         lon  lc_code  ...   tmmn   tmmx   vpd   vs\n0     2017-10-01  38.279274 -119.612776      142  ...  269.2  283.9  0.64  3.2\n1     2017-10-02  38.279274 -119.612776      142  ...  265.5  280.2  0.47  2.1\n2     2017-10-03  38.279274 -119.612776      142  ...  264.7  271.9  0.20  3.8\n3     2017-10-04  38.279274 -119.612776      142  ...  265.3  279.6  0.39  1.5\n4     2017-10-05  38.279274 -119.612776      142  ...  270.3  282.9  0.64  1.8\n...          ...        ...         ...      ...  ...    ...    ...   ...  ...\n2179  2018-09-25  37.833654 -119.451081      152  ...  276.7  290.0  1.05  2.3\n2180  2018-09-26  37.833654 -119.451081      152  ...  278.5  292.8  1.33  2.4\n2181  2018-09-27  37.833654 -119.451081      152  ...  279.9  293.3  1.43  2.1\n2182  2018-09-28  37.833654 -119.451081      152  ...  280.8  291.6  1.30  3.9\n2183  2018-09-29  37.833654 -119.451081      152  ...  277.7  286.3  0.74  5.5\n[2184 rows x 23 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_training.csv\n",
  "history_begin_time" : 1698205969494,
  "history_end_time" : 1698205970210,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AxyFpZNXO5Zj",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\nmerge_all_data_together()\n# filter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'swe_value', 'station_elevation', 'elevation', 'aspect', 'curvature',\n       'slope', 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn',\n       'tmmx', 'vpd', 'vs'],\n      dtype='object')\n             date        lat         lon  lc_code  ...   tmmn   tmmx   vpd   vs\n0      2017-10-01  38.279274 -119.612776      142  ...  269.2  283.9  0.64  3.2\n1      2017-10-01  38.279274 -119.612776      142  ...  269.2  283.9  0.64  3.2\n2      2017-10-01  38.279274 -119.612776      142  ...  269.2  283.9  0.64  3.2\n3      2017-10-01  38.279274 -119.612776      142  ...  269.2  283.9  0.64  3.2\n4      2017-10-01  38.279274 -119.612776      142  ...  269.2  283.9  0.64  3.2\n...           ...        ...         ...      ...  ...    ...    ...   ...  ...\n64787  2018-09-27  37.833654 -119.451081      152  ...  279.9  293.3  1.43  2.1\n64788  2018-09-28  37.833654 -119.451081      152  ...  280.8  291.6  1.30  3.9\n64789  2018-09-28  37.833654 -119.451081      152  ...  280.8  291.6  1.30  3.9\n64790  2018-09-29  37.833654 -119.451081      152  ...  277.7  286.3  0.74  5.5\n64791  2018-09-29  37.833654 -119.451081      152  ...  277.7  286.3  0.74  5.5\n[64792 rows x 23 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_training.csv\n",
  "history_begin_time" : 1698205661248,
  "history_end_time" : 1698205662404,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "b27a77KB9Esc",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_training_data)\n  df6 = pd.read_csv(amsr_gridmet_terrain_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df6, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\nmerge_all_data_together()\n# filter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/b27a77KB9Esc/data_merge_hackweek.py\", line 165, in <module>\n    merge_all_data_together()\n  File \"/home/ubuntu/gw-workspace/b27a77KB9Esc/data_merge_hackweek.py\", line 145, in merge_all_data_together\n    df5 = pd.read_csv(amsr_training_data)\n                      ^^^^^^^^^^^^^^^^^^\nNameError: name 'amsr_training_data' is not defined\n",
  "history_begin_time" : 1698205646710,
  "history_end_time" : 1698205647206,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "bTRPU2zWLzOI",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\n# merge_all_data_together()\nfilter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nIndex(['swe_value', 'station_elevation', 'lon', 'lat', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'date', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'],\n      dtype='object')\n/home/ubuntu/gw-workspace/bTRPU2zWLzOI/data_merge_hackweek.py:108: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\nFiltered Data:\n          swe_value  station_elevation         lon       lat  elevation  \\\n10634494       18.0             9400.0 -119.262434  37.89748   3057.726   \n10634495       18.0             9400.0 -119.262434  37.89748   3057.726   \n10634496       18.0             9400.0 -119.262434  37.89748   3057.726   \n10634497       18.0             9400.0 -119.262434  37.89748   3057.726   \n10634498       18.0             9400.0 -119.262434  37.89748   3057.726   \n              aspect  curvature      slope  eastness  northness       date  \\\n10634494  114.622729  -0.012519  12.720217 -0.416641   0.909071 2017-01-01   \n10634495  114.622729  -0.012519  12.720217 -0.416641   0.909071 2017-01-01   \n10634496  114.622729  -0.012519  12.720217 -0.416641   0.909071 2017-01-02   \n10634497  114.622729  -0.012519  12.720217 -0.416641   0.909071 2017-01-02   \n10634498  114.622729  -0.012519  12.720217 -0.416641   0.909071 2017-01-03   \n          etr    pr  rmax  rmin   tmmn   tmmx   vpd    vs  \n10634494  2.5   0.0  62.2  22.5  260.8  273.0  0.27   6.9  \n10634495  2.5   0.0  62.2  22.5  260.8  273.0  0.27   6.9  \n10634496  1.4   1.7  71.4  50.8  260.7  265.2  0.11  11.8  \n10634497  1.4   1.7  71.4  50.8  260.7  265.2  0.11  11.8  \n10634498  1.5  33.0  90.4  49.4  262.7  270.9  0.14  10.6  \nThe subset of the rows is saved to /home/ubuntu/gridmet_test_run/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\nDone\n",
  "history_begin_time" : 1698205327224,
  "history_end_time" : 1698205376209,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "CGC7yavuDigR",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(lat_lon_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  lat_lon_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\n# merge_all_data_together()\nfilter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nIndex(['swe_value', 'station_elevation', 'lon', 'lat', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'date', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'],\n      dtype='object')\n/home/ubuntu/gw-workspace/CGC7yavuDigR/data_merge_hackweek.py:108: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\nFiltered Data:\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/CGC7yavuDigR/data_merge_hackweek.py\", line 163, in <module>\n    filter_2years_from_20_years()\n  File \"/home/ubuntu/gw-workspace/CGC7yavuDigR/data_merge_hackweek.py\", line 119, in filter_2years_from_20_years\n    print(lat_lon_df.head())\n          ^^^^^^^^^^\nNameError: name 'lat_lon_df' is not defined\n",
  "history_begin_time" : 1698205233917,
  "history_end_time" : 1698205281591,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "0jAKJqMhaHNE",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(lat_lon_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years.csv'\n  lat_lon_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\n# merge_all_data_together()\nfilter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nIndex(['swe_value', 'station_elevation', 'lon', 'lat', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'date', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'],\n      dtype='object')\nFiltered Data:\n               lat         lon\n10623546  37.89748 -119.262434\n10623547  37.89748 -119.262434\n10623548  37.89748 -119.262434\n10623549  37.89748 -119.262434\n10623550  37.89748 -119.262434\nThe subset of the rows is saved to /home/ubuntu/gridmet_test_run/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\nDone\n",
  "history_begin_time" : 1698204935514,
  "history_end_time" : 1698205003117,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "JdryIGZCqvWg",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(lat_lon_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{current_ready_csv_path}_hackweek_subset_all_years.csv'\n  lat_lon_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\n# merge_all_data_together()\nfilter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nIndex(['swe_value', 'station_elevation', 'lon', 'lat', 'elevation', 'aspect',\n       'curvature', 'slope', 'eastness', 'northness', 'date', 'etr', 'pr',\n       'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs'],\n      dtype='object')\nFiltered Data:\n               lat         lon\n10623546  37.89748 -119.262434\n10623547  37.89748 -119.262434\n10623548  37.89748 -119.262434\n10623549  37.89748 -119.262434\n10623550  37.89748 -119.262434\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/JdryIGZCqvWg/data_merge_hackweek.py\", line 156, in <module>\n    filter_2years_from_20_years()\n  File \"/home/ubuntu/gw-workspace/JdryIGZCqvWg/data_merge_hackweek.py\", line 115, in filter_2years_from_20_years\n    subset_csv_path = f'{current_ready_csv_path}_hackweek_subset_all_years.csv'\n                         ^^^^^^^^^^^^^^^^^^^^^^\nNameError: name 'current_ready_csv_path' is not defined. Did you mean: 'all_ready_csv_path'?\n",
  "history_begin_time" : 1698204828627,
  "history_end_time" : 1698204881385,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "RHpJzW62vJto",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(lat_lon_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{current_ready_csv_path}_hackweek_subset_all_years.csv'\n  lat_lon_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n\n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_data_together():\n\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n  df5 = pd.read_csv(amsr_training_data)\n\n  merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n  merged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n# convert_pmv_to_right_format()\n# merge_all_data_together()\nfilter_2years_from_20_years()\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/RHpJzW62vJto/data_merge_hackweek.py\", line 156, in <module>\n    filter_2years_from_20_years()\n  File \"/home/ubuntu/gw-workspace/RHpJzW62vJto/data_merge_hackweek.py\", line 94, in filter_2years_from_20_years\n    df = pd.read_csv(file_path)\n         ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n    self.handles = get_handle(\n                   ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/common.py\", line 859, in get_handle\n    handle = open(\n             ^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/gridmet_test_run/training_data_20_years_cleaned.csv'\n",
  "history_begin_time" : 1698202836399,
  "history_end_time" : 1698202836858,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eBBnKL7Ega5i",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\nprint(merged_df.columns)\n\n# # Print the merged DataFrame\nprint(merged_df)\ntraining_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\nmerged_df.to_csv(training_hackweek_csv, index=False)\nprint(f\"Training data is saved to {training_hackweek_csv}\")\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'amsr_swe'],\n      dtype='object')\n            date        lat         lon  ...    fSCA  SnowClass  amsr_swe\n0     2017-10-01  38.152231 -119.666675  ...  0.8030          3       0.0\n1     2017-10-02  38.152231 -119.666675  ...  0.2660          3       NaN\n2     2017-10-03  38.152231 -119.666675  ...  0.0000          3       0.0\n3     2017-10-04  38.152231 -119.666675  ...  0.0000          3       0.0\n4     2017-10-05  38.152231 -119.666675  ...  0.0000          3       0.0\n...          ...        ...         ...  ...     ...        ...       ...\n3635  2018-09-25  38.039118 -119.307350  ...  0.0088          5       0.0\n3636  2018-09-26  38.039118 -119.307350  ...  0.0086          5       NaN\n3637  2018-09-27  38.039118 -119.307350  ...  0.0082          5       0.0\n3638  2018-09-28  38.039118 -119.307350  ...  0.0080          5       0.0\n3639  2018-09-29  38.039118 -119.307350  ...  0.0078          5       0.0\n[3640 rows x 8 columns]\nTraining data is saved to /home/ubuntu/gridmet_test_run/all_merged_training.csv\n",
  "history_begin_time" : 1698202215666,
  "history_end_time" : 1698202216188,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tuTYYvNnhHE6",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\nprint(merged_df.columns)\n\n# # Print the merged DataFrame\nprint(merged_df)\ntraining_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\nmerged_df.to_csv(training_hackweek_csv, index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'amsr_swe'],\n      dtype='object')\n            date        lat         lon  ...    fSCA  SnowClass  amsr_swe\n0     2017-10-01  38.152231 -119.666675  ...  0.8030          3       0.0\n1     2017-10-02  38.152231 -119.666675  ...  0.2660          3       NaN\n2     2017-10-03  38.152231 -119.666675  ...  0.0000          3       0.0\n3     2017-10-04  38.152231 -119.666675  ...  0.0000          3       0.0\n4     2017-10-05  38.152231 -119.666675  ...  0.0000          3       0.0\n...          ...        ...         ...  ...     ...        ...       ...\n3635  2018-09-25  38.039118 -119.307350  ...  0.0088          5       0.0\n3636  2018-09-26  38.039118 -119.307350  ...  0.0086          5       NaN\n3637  2018-09-27  38.039118 -119.307350  ...  0.0082          5       0.0\n3638  2018-09-28  38.039118 -119.307350  ...  0.0080          5       0.0\n3639  2018-09-29  38.039118 -119.307350  ...  0.0078          5       0.0\n[3640 rows x 8 columns]\n",
  "history_begin_time" : 1698202193926,
  "history_end_time" : 1698202194449,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "r5LUIKXMrftZ",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\nprint(merged_df.columns)\n\n# # Print the merged DataFrame\nprint(merged_df)\nmerged_df.to_csv(f\"{work_dir}/all_merged_training.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'amsr_swe'],\n      dtype='object')\n            date        lat         lon  ...    fSCA  SnowClass  amsr_swe\n0     2017-10-01  38.152231 -119.666675  ...  0.8030          3       0.0\n1     2017-10-02  38.152231 -119.666675  ...  0.2660          3       NaN\n2     2017-10-03  38.152231 -119.666675  ...  0.0000          3       0.0\n3     2017-10-04  38.152231 -119.666675  ...  0.0000          3       0.0\n4     2017-10-05  38.152231 -119.666675  ...  0.0000          3       0.0\n...          ...        ...         ...  ...     ...        ...       ...\n3635  2018-09-25  38.039118 -119.307350  ...  0.0088          5       0.0\n3636  2018-09-26  38.039118 -119.307350  ...  0.0086          5       NaN\n3637  2018-09-27  38.039118 -119.307350  ...  0.0082          5       0.0\n3638  2018-09-28  38.039118 -119.307350  ...  0.0080          5       0.0\n3639  2018-09-29  38.039118 -119.307350  ...  0.0078          5       0.0\n[3640 rows x 8 columns]\n",
  "history_begin_time" : 1698202174425,
  "history_end_time" : 1698202174943,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "hKXFxs76naY8",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\nprint(merged_df.columns)\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'SnowClass',\n       'amsr_swe'],\n      dtype='object')\n",
  "history_begin_time" : 1698201872161,
  "history_end_time" : 1698201872673,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "h6G1eOK2rxno",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", inplace=True)\nprint(merged_df.columns)\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/h6G1eOK2rxno/data_merge_hackweek.py\", line 98, in <module>\n    merged_df.drop(\"Unnamed: 0\", inplace=True)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Unnamed: 0'] not found in axis\"\n",
  "history_begin_time" : 1698201854463,
  "history_end_time" : 1698201854976,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "A7TOXqZyBp3c",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nprint(merged_df.columns)\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'Unnamed: 0',\n       'SnowClass', 'amsr_swe'],\n      dtype='object')\n",
  "history_begin_time" : 1698201810361,
  "history_end_time" : 1698201810875,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "s2i8sKVMhqk1",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\namsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\" # 2019-2022\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\" # any current testing data\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv(amsr_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df5, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", inplace=True)\nprint(merged_df.columns)\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/s2i8sKVMhqk1/data_merge_hackweek.py\", line 98, in <module>\n    merged_df.drop(\"Unnamed: 0\", inplace=True)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 5258, in drop\n    return super().drop(\n           ^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4549, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 4591, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 6696, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['Unnamed: 0'] not found in axis\"\n",
  "history_begin_time" : 1698201796441,
  "history_end_time" : 1698201796950,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "giE85sc0VgOi",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\namsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\ngridmet_amsr_terrain_training_data = f\"{work_dir}/hackweek_subset_training.csv\" # 2019-2022\ngridmet_amsr_terrain_testing_data = f\"{work_dir}/hackweek_subset_testing.csv\"\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\ndf5 = pd.read_csv()\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nmerged_df.drop(\"Unnamed: 0\", inplace=True)\nprint(merged_df.columns)\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/giE85sc0VgOi/data_merge_hackweek.py\", line 91, in <module>\n    df5 = pd.read_csv()\n          ^^^^^^^^^^^^^\nTypeError: read_csv() missing 1 required positional argument: 'filepath_or_buffer'\n",
  "history_begin_time" : 1698201440343,
  "history_end_time" : 1698201440827,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "dSn4hFQAB4wK",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nprint(merged_df.columns)\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nIndex(['date', 'lat', 'lon', 'lc_code', 'pmv', 'fSCA', 'Unnamed: 0',\n       'SnowClass'],\n      dtype='object')\n",
  "history_begin_time" : 1698200213435,
  "history_end_time" : 1698200213982,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "EqzEoTA2HDEQ",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\ndf4 = df4.rename(columns={'Date': 'date', \n                         'long': 'lon'})\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nprint(merged_df)\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat         lon  SnowClass        date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\n            date        lat         lon  ...    fSCA  Unnamed: 0  SnowClass\n0     2017-10-01  38.152231 -119.666675  ...  0.8030           1          3\n1     2017-10-02  38.152231 -119.666675  ...  0.2660          11          3\n2     2017-10-03  38.152231 -119.666675  ...  0.0000          21          3\n3     2017-10-04  38.152231 -119.666675  ...  0.0000          31          3\n4     2017-10-05  38.152231 -119.666675  ...  0.0000          41          3\n...          ...        ...         ...  ...     ...         ...        ...\n3635  2018-09-25  38.039118 -119.307350  ...  0.0088        3600          5\n3636  2018-09-26  38.039118 -119.307350  ...  0.0086        3610          5\n3637  2018-09-27  38.039118 -119.307350  ...  0.0082        3620          5\n3638  2018-09-28  38.039118 -119.307350  ...  0.0080        3630          5\n3639  2018-09-29  38.039118 -119.307350  ...  0.0078        3640          5\n[3640 rows x 8 columns]\n",
  "history_begin_time" : 1698200201225,
  "history_end_time" : 1698200201762,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "OYjTIycmiv4s",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nprint(df4.head())\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nprint(merged_df)\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\n   Unnamed: 0        lat        long  SnowClass        Date\n0           1  38.152231 -119.666675          3  2017-10-01\n1           2  38.279274 -119.612776          3  2017-10-01\n2           3  38.504580 -119.621760          4  2017-10-01\n3           4  37.862028 -119.657692          4  2017-10-01\n4           5  37.897480 -119.262434          6  2017-10-01\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/OYjTIycmiv4s/data_merge_hackweek.py\", line 90, in <module>\n    merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 148, in merge\n    op = _MergeOperation(\n         ^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 737, in __init__\n    ) = self._get_merge_keys()\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 1203, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1778, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1698200118579,
  "history_end_time" : 1698200119132,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "TiRuZoRMX9gg",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  pmv_new_df = pd.DataFrame(columns=column_names)\n\n  def adjust_column_to_rows(row):\n    global pmv_new_df\n\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n\n\n  pmv_old_df = pd.read_csv(pmw_training)\n  pmv_old_df.apply(adjust_column_to_rows, axis=1)\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_training_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n\ndf1 = pd.read_csv(landcover_training)\ndf2 = pd.read_csv(pmw_training_new)\ndf3 = pd.read_csv(fsca_training)\ndf4 = pd.read_csv(snowclassification_training_data)\n\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='inner')\nmerged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\nprint(merged_df)\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/TiRuZoRMX9gg/data_merge_hackweek.py\", line 89, in <module>\n    merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='inner')\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 148, in merge\n    op = _MergeOperation(\n         ^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 737, in __init__\n    ) = self._get_merge_keys()\n        ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/reshape/merge.py\", line 1203, in _get_merge_keys\n    right_keys.append(right._get_label_or_level_values(rk))\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 1778, in _get_label_or_level_values\n    raise KeyError(key)\nKeyError: 'date'\n",
  "history_begin_time" : 1698200066556,
  "history_end_time" : 1698200067093,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "EvcV3sYJnvrh",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  \n  for property_name, value in row.items():\n    #print(\"property_name: \", property_name)\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n    \n    # Convert the input time string to a datetime object\n    datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n    # Convert the datetime object to the desired format\n    formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [formatted_time_string, lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [formatted_time_string, lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\npmv_new_df.to_csv(pmw_training_new, index=False)\nprint(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\n         date        lat           lon     pmv\n0  2017-10-01  38.152231  -119.6666755  256.03\n1  2017-10-01  38.279274  -119.6127765  257.35\n2  2017-10-01   38.50458    -119.62176  259.46\n3  2017-10-01  37.862028  -119.6576925  259.87\n4  2017-10-01   37.89748  -119.2624335  247.56\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698188887517,
  "history_end_time" : 1698188888974,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "iSJaGtVAdODK",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  print(row)\n  \n  for property_name, value in row.items():\n    #print(\"property_name: \", property_name)\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n    \n    # Convert the input time string to a datetime object\n    datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n    # Convert the datetime object to the desired format\n    formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [formatted_time_string, lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [formatted_time_string, lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\npmv_new_df.to_csv(pmw_training_new, index=False)\nprint(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nTime                            10/2/2017 6:00\nA (38.152231, -119.6666755)             249.79\nB (38.279274, -119.6127765)             250.26\nC (38.50458, -119.62176)                253.73\nD (37.862028, -119.6576925)          256.16998\nE (37.89748, -119.2624335)           243.37999\nF (37.8762105, -119.3432825)            247.51\nG (37.833654, -119.4510805)              251.0\nH (38.0603395, -119.6666755)             251.2\nI (37.798171, -119.19955150             242.98\nJ (38.0391175, -119.3073495)         246.26999\nName: 1, dtype: object\nTime                            10/3/2017 6:00\nA (38.152231, -119.6666755)             249.61\nB (38.279274, -119.6127765)             248.98\nC (38.50458, -119.62176)                252.51\nD (37.862028, -119.6576925)          255.79999\nE (37.89748, -119.2624335)              242.95\nF (37.8762105, -119.3432825)         244.23999\nG (37.833654, -119.4510805)             248.87\nH (38.0603395, -119.6666755)            250.31\nI (37.798171, -119.19955150             242.78\nJ (38.0391175, -119.3073495)             243.7\nName: 2, dtype: object\nTime                            10/4/2017 6:00\nA (38.152231, -119.6666755)             249.28\nB (38.279274, -119.6127765)          249.40999\nC (38.50458, -119.62176)                252.68\nD (37.862028, -119.6576925)              255.2\nE (37.89748, -119.2624335)              239.87\nF (37.8762105, -119.3432825)            242.56\nG (37.833654, -119.4510805)             249.95\nH (38.0603395, -119.6666755)         249.95999\nI (37.798171, -119.19955150          242.20999\nJ (38.0391175, -119.3073495)         242.09999\nName: 3, dtype: object\nTime                            10/5/2017 6:00\nA (38.152231, -119.6666755)             250.11\nB (38.279274, -119.6127765)          251.12999\nC (38.50458, -119.62176)             254.76999\nD (37.862028, -119.6576925)             258.03\nE (37.89748, -119.2624335)           241.40999\nF (37.8762105, -119.3432825)         244.70999\nG (37.833654, -119.4510805)             252.42\nH (38.0603395, -119.6666755)         251.29999\nI (37.798171, -119.19955150             245.09\nJ (38.0391175, -119.3073495)         243.06999\nName: 4, dtype: object\nTime                            10/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 5, dtype: object\nTime                            10/7/2017 6:00\nA (38.152231, -119.6666755)          256.16998\nB (38.279274, -119.6127765)              258.3\nC (38.50458, -119.62176)                261.61\nD (37.862028, -119.6576925)             258.69\nE (37.89748, -119.2624335)              252.11\nF (37.8762105, -119.3432825)            252.48\nG (37.833654, -119.4510805)             254.45\nH (38.0603395, -119.6666755)         255.95999\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)             252.4\nName: 6, dtype: object\nTime                            10/8/2017 6:00\nA (38.152231, -119.6666755)             255.84\nB (38.279274, -119.6127765)             257.32\nC (38.50458, -119.62176)                260.16\nD (37.862028, -119.6576925)          259.66998\nE (37.89748, -119.2624335)              248.25\nF (37.8762105, -119.3432825)            250.84\nG (37.833654, -119.4510805)          255.84999\nH (38.0603395, -119.6666755)            255.72\nI (37.798171, -119.19955150             250.14\nJ (38.0391175, -119.3073495)            248.34\nName: 7, dtype: object\nTime                            10/9/2017 6:00\nA (38.152231, -119.6666755)          253.45999\nB (38.279274, -119.6127765)          252.51999\nC (38.50458, -119.62176)                253.48\nD (37.862028, -119.6576925)             260.43\nE (37.89748, -119.2624335)              245.37\nF (37.8762105, -119.3432825)         248.04999\nG (37.833654, -119.4510805)             252.81\nH (38.0603395, -119.6666755)            254.29\nI (37.798171, -119.19955150          247.12999\nJ (38.0391175, -119.3073495)             245.4\nName: 8, dtype: object\nTime                            10/10/2017 6:00\nA (38.152231, -119.6666755)              252.25\nB (38.279274, -119.6127765)              253.53\nC (38.50458, -119.62176)                 255.42\nD (37.862028, -119.6576925)              257.82\nE (37.89748, -119.2624335)               241.45\nF (37.8762105, -119.3432825)          245.81999\nG (37.833654, -119.4510805)           254.76999\nH (38.0603395, -119.6666755)             253.28\nI (37.798171, -119.19955150              243.92\nJ (38.0391175, -119.3073495)             243.51\nName: 9, dtype: object\nTime                            10/11/2017 6:00\nA (38.152231, -119.6666755)              252.04\nB (38.279274, -119.6127765)              253.58\nC (38.50458, -119.62176)              257.69998\nD (37.862028, -119.6576925)              258.77\nE (37.89748, -119.2624335)            244.37999\nF (37.8762105, -119.3432825)             248.17\nG (37.833654, -119.4510805)           252.37999\nH (38.0603395, -119.6666755)             253.06\nI (37.798171, -119.19955150              245.86\nJ (38.0391175, -119.3073495)             247.12\nName: 10, dtype: object\nTime                            10/12/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 11, dtype: object\nTime                            10/13/2017 6:00\nA (38.152231, -119.6666755)              251.18\nB (38.279274, -119.6127765)           251.81999\nC (38.50458, -119.62176)              254.79999\nD (37.862028, -119.6576925)           257.22998\nE (37.89748, -119.2624335)               241.64\nF (37.8762105, -119.3432825)          244.65999\nG (37.833654, -119.4510805)           252.01999\nH (38.0603395, -119.6666755)          251.84999\nI (37.798171, -119.19955150           244.51999\nJ (38.0391175, -119.3073495)             242.09\nName: 12, dtype: object\nTime                            10/14/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 13, dtype: object\nTime                            10/15/2017 6:00\nA (38.152231, -119.6666755)              255.17\nB (38.279274, -119.6127765)           256.94998\nC (38.50458, -119.62176)                 258.22\nD (37.862028, -119.6576925)              259.59\nE (37.89748, -119.2624335)               245.95\nF (37.8762105, -119.3432825)          250.01999\nG (37.833654, -119.4510805)           255.37999\nH (38.0603395, -119.6666755)             255.48\nI (37.798171, -119.19955150              248.29\nJ (38.0391175, -119.3073495)          244.98999\nName: 14, dtype: object\nTime                            10/16/2017 6:00\nA (38.152231, -119.6666755)           255.45999\nB (38.279274, -119.6127765)              256.08\nC (38.50458, -119.62176)                 257.69\nD (37.862028, -119.6576925)              260.29\nE (37.89748, -119.2624335)               246.09\nF (37.8762105, -119.3432825)              249.4\nG (37.833654, -119.4510805)              254.62\nH (38.0603395, -119.6666755)             256.03\nI (37.798171, -119.19955150              248.39\nJ (38.0391175, -119.3073495)          246.37999\nName: 15, dtype: object\nTime                            10/17/2017 6:00\nA (38.152231, -119.6666755)           257.16998\nB (38.279274, -119.6127765)           257.63998\nC (38.50458, -119.62176)              261.00998\nD (37.862028, -119.6576925)           262.44998\nE (37.89748, -119.2624335)               249.87\nF (37.8762105, -119.3432825)          252.06999\nG (37.833654, -119.4510805)              254.84\nH (38.0603395, -119.6666755)             257.38\nI (37.798171, -119.19955150              252.18\nJ (38.0391175, -119.3073495)          250.01999\nName: 16, dtype: object\nTime                            10/18/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 17, dtype: object\nTime                            10/19/2017 6:00\nA (38.152231, -119.6666755)              252.36\nB (38.279274, -119.6127765)           253.29999\nC (38.50458, -119.62176)                 257.31\nD (37.862028, -119.6576925)              258.85\nE (37.89748, -119.2624335)            244.59999\nF (37.8762105, -119.3432825)             247.59\nG (37.833654, -119.4510805)           254.54999\nH (38.0603395, -119.6666755)             253.62\nI (37.798171, -119.19955150              246.47\nJ (38.0391175, -119.3073495)          246.01999\nName: 18, dtype: object\nTime                            10/20/2017 6:00\nA (38.152231, -119.6666755)               250.5\nB (38.279274, -119.6127765)              252.06\nC (38.50458, -119.62176)                  254.4\nD (37.862028, -119.6576925)              256.29\nE (37.89748, -119.2624335)               247.29\nF (37.8762105, -119.3432825)             250.47\nG (37.833654, -119.4510805)              254.95\nH (38.0603395, -119.6666755)             251.06\nI (37.798171, -119.19955150               250.5\nJ (38.0391175, -119.3073495)          248.15999\nName: 19, dtype: object\nTime                            10/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 20, dtype: object\nTime                            10/22/2017 6:00\nA (38.152231, -119.6666755)           255.43999\nB (38.279274, -119.6127765)              254.42\nC (38.50458, -119.62176)                 259.25\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)             255.12\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 21, dtype: object\nTime                            10/23/2017 6:00\nA (38.152231, -119.6666755)           257.66998\nB (38.279274, -119.6127765)              258.59\nC (38.50458, -119.62176)                 260.68\nD (37.862028, -119.6576925)              262.99\nE (37.89748, -119.2624335)               249.03\nF (37.8762105, -119.3432825)             252.68\nG (37.833654, -119.4510805)              258.36\nH (38.0603395, -119.6666755)          257.63998\nI (37.798171, -119.19955150              251.12\nJ (38.0391175, -119.3073495)              249.0\nName: 22, dtype: object\nTime                            10/24/2017 6:00\nA (38.152231, -119.6666755)              256.99\nB (38.279274, -119.6127765)              257.54\nC (38.50458, -119.62176)                 258.43\nD (37.862028, -119.6576925)              263.55\nE (37.89748, -119.2624335)            247.76999\nF (37.8762105, -119.3432825)             251.18\nG (37.833654, -119.4510805)               257.1\nH (38.0603395, -119.6666755)             257.79\nI (37.798171, -119.19955150              251.25\nJ (38.0391175, -119.3073495)             248.36\nName: 23, dtype: object\nTime                            10/25/2017 6:00\nA (38.152231, -119.6666755)              255.22\nB (38.279274, -119.6127765)              255.72\nC (38.50458, -119.62176)              259.00998\nD (37.862028, -119.6576925)              262.69\nE (37.89748, -119.2624335)               247.92\nF (37.8762105, -119.3432825)             251.83\nG (37.833654, -119.4510805)              256.34\nH (38.0603395, -119.6666755)          256.41998\nI (37.798171, -119.19955150           248.01999\nJ (38.0391175, -119.3073495)          250.65999\nName: 24, dtype: object\nTime                            10/26/2017 6:00\nA (38.152231, -119.6666755)              255.36\nB (38.279274, -119.6127765)              254.75\nC (38.50458, -119.62176)              258.91998\nD (37.862028, -119.6576925)              262.52\nE (37.89748, -119.2624335)            247.87999\nF (37.8762105, -119.3432825)             251.34\nG (37.833654, -119.4510805)              254.89\nH (38.0603395, -119.6666755)          256.22998\nI (37.798171, -119.19955150           247.95999\nJ (38.0391175, -119.3073495)          249.79999\nName: 25, dtype: object\nTime                            10/27/2017 6:00\nA (38.152231, -119.6666755)           254.68999\nB (38.279274, -119.6127765)              255.09\nC (38.50458, -119.62176)                 258.69\nD (37.862028, -119.6576925)              263.72\nE (37.89748, -119.2624335)               245.62\nF (37.8762105, -119.3432825)             250.15\nG (37.833654, -119.4510805)              257.02\nH (38.0603395, -119.6666755)             256.41\nI (37.798171, -119.19955150              248.17\nJ (38.0391175, -119.3073495)          248.31999\nName: 26, dtype: object\nTime                            10/28/2017 6:00\nA (38.152231, -119.6666755)              256.13\nB (38.279274, -119.6127765)              256.35\nC (38.50458, -119.62176)                 259.44\nD (37.862028, -119.6576925)              263.86\nE (37.89748, -119.2624335)            247.18999\nF (37.8762105, -119.3432825)             251.08\nG (37.833654, -119.4510805)              258.74\nH (38.0603395, -119.6666755)             256.93\nI (37.798171, -119.19955150           249.90999\nJ (38.0391175, -119.3073495)             247.28\nName: 27, dtype: object\nTime                            10/29/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 28, dtype: object\nTime                            10/30/2017 6:00\nA (38.152231, -119.6666755)              256.11\nB (38.279274, -119.6127765)              257.93\nC (38.50458, -119.62176)              261.38998\nD (37.862028, -119.6576925)              259.72\nE (37.89748, -119.2624335)            247.43999\nF (37.8762105, -119.3432825)             251.28\nG (37.833654, -119.4510805)              255.79\nH (38.0603395, -119.6666755)          255.81999\nI (37.798171, -119.19955150              249.97\nJ (38.0391175, -119.3073495)             246.33\nName: 29, dtype: object\nTime                            10/31/2017 6:00\nA (38.152231, -119.6666755)              252.62\nB (38.279274, -119.6127765)           253.31999\nC (38.50458, -119.62176)                 255.43\nD (37.862028, -119.6576925)              256.88\nE (37.89748, -119.2624335)            245.12999\nF (37.8762105, -119.3432825)             247.95\nG (37.833654, -119.4510805)               252.7\nH (38.0603395, -119.6666755)             252.93\nI (37.798171, -119.19955150           246.70999\nJ (38.0391175, -119.3073495)             246.01\nName: 30, dtype: object\nTime                            11/1/2017 6:00\nA (38.152231, -119.6666755)          252.76999\nB (38.279274, -119.6127765)             253.83\nC (38.50458, -119.62176)                256.04\nD (37.862028, -119.6576925)          257.88998\nE (37.89748, -119.2624335)              242.54\nF (37.8762105, -119.3432825)            244.75\nG (37.833654, -119.4510805)             252.54\nH (38.0603395, -119.6666755)            253.72\nI (37.798171, -119.19955150          247.48999\nJ (38.0391175, -119.3073495)         242.26999\nName: 31, dtype: object\nTime                            11/2/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 32, dtype: object\nTime                            11/3/2017 6:00\nA (38.152231, -119.6666755)             251.12\nB (38.279274, -119.6127765)             251.48\nC (38.50458, -119.62176)             255.37999\nD (37.862028, -119.6576925)             257.25\nE (37.89748, -119.2624335)              243.33\nF (37.8762105, -119.3432825)            247.25\nG (37.833654, -119.4510805)             252.22\nH (38.0603395, -119.6666755)         252.04999\nI (37.798171, -119.19955150             245.11\nJ (38.0391175, -119.3073495)         246.51999\nName: 33, dtype: object\nTime                            11/4/2017 6:00\nA (38.152231, -119.6666755)          252.26999\nB (38.279274, -119.6127765)             253.08\nC (38.50458, -119.62176)             256.19998\nD (37.862028, -119.6576925)             257.35\nE (37.89748, -119.2624335)              243.34\nF (37.8762105, -119.3432825)         246.37999\nG (37.833654, -119.4510805)             252.48\nH (38.0603395, -119.6666755)            252.83\nI (37.798171, -119.19955150          246.84999\nJ (38.0391175, -119.3073495)         245.20999\nName: 34, dtype: object\nTime                            11/5/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              237.04\nF (37.8762105, -119.3432825)            238.01\nG (37.833654, -119.4510805)             240.61\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             245.48\nJ (38.0391175, -119.3073495)            233.28\nName: 35, dtype: object\nTime                            11/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 36, dtype: object\nTime                            11/7/2017 6:00\nA (38.152231, -119.6666755)             247.76\nB (38.279274, -119.6127765)             248.95\nC (38.50458, -119.62176)                 250.9\nD (37.862028, -119.6576925)             252.73\nE (37.89748, -119.2624335)              240.93\nF (37.8762105, -119.3432825)             244.2\nG (37.833654, -119.4510805)             248.58\nH (38.0603395, -119.6666755)            247.93\nI (37.798171, -119.19955150          243.37999\nJ (38.0391175, -119.3073495)            239.97\nName: 37, dtype: object\nTime                            11/8/2017 6:00\nA (38.152231, -119.6666755)             250.84\nB (38.279274, -119.6127765)          251.68999\nC (38.50458, -119.62176)                254.92\nD (37.862028, -119.6576925)          255.98999\nE (37.89748, -119.2624335)              242.11\nF (37.8762105, -119.3432825)         245.79999\nG (37.833654, -119.4510805)             252.47\nH (38.0603395, -119.6666755)            250.92\nI (37.798171, -119.19955150              244.7\nJ (38.0391175, -119.3073495)            242.86\nName: 38, dtype: object\nTime                            11/9/2017 6:00\nA (38.152231, -119.6666755)          253.34999\nB (38.279274, -119.6127765)             254.17\nC (38.50458, -119.62176)             257.19998\nD (37.862028, -119.6576925)             258.83\nE (37.89748, -119.2624335)           247.93999\nF (37.8762105, -119.3432825)            248.22\nG (37.833654, -119.4510805)             251.83\nH (38.0603395, -119.6666755)             253.9\nI (37.798171, -119.19955150             249.59\nJ (38.0391175, -119.3073495)             247.7\nName: 39, dtype: object\nTime                            11/10/2017 6:00\nA (38.152231, -119.6666755)              250.61\nB (38.279274, -119.6127765)              251.12\nC (38.50458, -119.62176)                 254.92\nD (37.862028, -119.6576925)              256.31\nE (37.89748, -119.2624335)               242.43\nF (37.8762105, -119.3432825)              242.9\nG (37.833654, -119.4510805)           250.15999\nH (38.0603395, -119.6666755)             251.79\nI (37.798171, -119.19955150              246.26\nJ (38.0391175, -119.3073495)             243.03\nName: 40, dtype: object\nTime                            11/11/2017 6:00\nA (38.152231, -119.6666755)               246.5\nB (38.279274, -119.6127765)              246.33\nC (38.50458, -119.62176)              250.06999\nD (37.862028, -119.6576925)           255.31999\nE (37.89748, -119.2624335)            238.62999\nF (37.8762105, -119.3432825)          242.12999\nG (37.833654, -119.4510805)           249.81999\nH (38.0603395, -119.6666755)          247.59999\nI (37.798171, -119.19955150               241.7\nJ (38.0391175, -119.3073495)             240.06\nName: 41, dtype: object\nTime                            11/12/2017 6:00\nA (38.152231, -119.6666755)              250.92\nB (38.279274, -119.6127765)              252.59\nC (38.50458, -119.62176)              255.45999\nD (37.862028, -119.6576925)              258.66\nE (37.89748, -119.2624335)               244.75\nF (37.8762105, -119.3432825)             247.93\nG (37.833654, -119.4510805)              254.73\nH (38.0603395, -119.6666755)             251.79\nI (37.798171, -119.19955150              247.64\nJ (38.0391175, -119.3073495)             244.81\nName: 42, dtype: object\nTime                            11/13/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 43, dtype: object\nTime                            11/14/2017 6:00\nA (38.152231, -119.6666755)              250.61\nB (38.279274, -119.6127765)           253.26999\nC (38.50458, -119.62176)                  256.1\nD (37.862028, -119.6576925)           253.62999\nE (37.89748, -119.2624335)               245.37\nF (37.8762105, -119.3432825)          247.59999\nG (37.833654, -119.4510805)           251.12999\nH (38.0603395, -119.6666755)             250.08\nI (37.798171, -119.19955150              246.73\nJ (38.0391175, -119.3073495)             244.75\nName: 44, dtype: object\nTime                            11/15/2017 6:00\nA (38.152231, -119.6666755)           251.73999\nB (38.279274, -119.6127765)           253.40999\nC (38.50458, -119.62176)              256.00998\nD (37.862028, -119.6576925)              257.55\nE (37.89748, -119.2624335)            247.68999\nF (37.8762105, -119.3432825)             249.64\nG (37.833654, -119.4510805)              253.65\nH (38.0603395, -119.6666755)          252.73999\nI (37.798171, -119.19955150           249.73999\nJ (38.0391175, -119.3073495)             247.09\nName: 45, dtype: object\nTime                            11/16/2017 6:00\nA (38.152231, -119.6666755)              250.28\nB (38.279274, -119.6127765)              250.43\nC (38.50458, -119.62176)                 253.34\nD (37.862028, -119.6576925)           255.84999\nE (37.89748, -119.2624335)               247.28\nF (37.8762105, -119.3432825)          249.65999\nG (37.833654, -119.4510805)              253.95\nH (38.0603395, -119.6666755)             251.54\nI (37.798171, -119.19955150              250.08\nJ (38.0391175, -119.3073495)             247.51\nName: 46, dtype: object\nTime                            11/17/2017 6:00\nA (38.152231, -119.6666755)              246.45\nB (38.279274, -119.6127765)           246.51999\nC (38.50458, -119.62176)              250.09999\nD (37.862028, -119.6576925)              254.43\nE (37.89748, -119.2624335)               243.64\nF (37.8762105, -119.3432825)              247.9\nG (37.833654, -119.4510805)              249.95\nH (38.0603395, -119.6666755)             248.22\nI (37.798171, -119.19955150           245.43999\nJ (38.0391175, -119.3073495)          244.15999\nName: 47, dtype: object\nTime                            11/18/2017 6:00\nA (38.152231, -119.6666755)               240.2\nB (38.279274, -119.6127765)           238.37999\nC (38.50458, -119.62176)                 245.33\nD (37.862028, -119.6576925)              252.06\nE (37.89748, -119.2624335)            236.51999\nF (37.8762105, -119.3432825)             240.48\nG (37.833654, -119.4510805)              244.54\nH (38.0603395, -119.6666755)             241.78\nI (37.798171, -119.19955150           238.76999\nJ (38.0391175, -119.3073495)             237.09\nName: 48, dtype: object\nTime                            11/19/2017 6:00\nA (38.152231, -119.6666755)              239.37\nB (38.279274, -119.6127765)              239.86\nC (38.50458, -119.62176)                 247.62\nD (37.862028, -119.6576925)              252.37\nE (37.89748, -119.2624335)            237.01999\nF (37.8762105, -119.3432825)             239.89\nG (37.833654, -119.4510805)              245.39\nH (38.0603395, -119.6666755)             242.42\nI (37.798171, -119.19955150              240.23\nJ (38.0391175, -119.3073495)             235.48\nName: 49, dtype: object\nTime                            11/20/2017 6:00\nA (38.152231, -119.6666755)              245.98\nB (38.279274, -119.6127765)           247.18999\nC (38.50458, -119.62176)                 253.28\nD (37.862028, -119.6576925)               258.8\nE (37.89748, -119.2624335)                244.9\nF (37.8762105, -119.3432825)          248.59999\nG (37.833654, -119.4510805)           254.18999\nH (38.0603395, -119.6666755)             248.26\nI (37.798171, -119.19955150              248.01\nJ (38.0391175, -119.3073495)             242.86\nName: 50, dtype: object\nTime                            11/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 51, dtype: object\nTime                            11/22/2017 6:00\nA (38.152231, -119.6666755)              244.84\nB (38.279274, -119.6127765)              246.54\nC (38.50458, -119.62176)                 255.64\nD (37.862028, -119.6576925)              255.28\nE (37.89748, -119.2624335)               237.67\nF (37.8762105, -119.3432825)             239.58\nG (37.833654, -119.4510805)              244.31\nH (38.0603395, -119.6666755)             246.15\nI (37.798171, -119.19955150           241.59999\nJ (38.0391175, -119.3073495)          237.56999\nName: 52, dtype: object\nTime                            11/23/2017 6:00\nA (38.152231, -119.6666755)           242.45999\nB (38.279274, -119.6127765)               243.9\nC (38.50458, -119.62176)              254.59999\nD (37.862028, -119.6576925)              254.93\nE (37.89748, -119.2624335)            234.18999\nF (37.8762105, -119.3432825)             237.89\nG (37.833654, -119.4510805)              244.11\nH (38.0603395, -119.6666755)          244.62999\nI (37.798171, -119.19955150           238.29999\nJ (38.0391175, -119.3073495)             236.76\nName: 53, dtype: object\nTime                            11/24/2017 6:00\nA (38.152231, -119.6666755)           242.51999\nB (38.279274, -119.6127765)              243.61\nC (38.50458, -119.62176)                 251.39\nD (37.862028, -119.6576925)           254.15999\nE (37.89748, -119.2624335)            233.37999\nF (37.8762105, -119.3432825)             235.84\nG (37.833654, -119.4510805)              244.61\nH (38.0603395, -119.6666755)          244.48999\nI (37.798171, -119.19955150              238.12\nJ (38.0391175, -119.3073495)             234.15\nName: 54, dtype: object\nTime                            11/25/2017 6:00\nA (38.152231, -119.6666755)              243.26\nB (38.279274, -119.6127765)              242.62\nC (38.50458, -119.62176)                 250.72\nD (37.862028, -119.6576925)              257.53\nE (37.89748, -119.2624335)            233.23999\nF (37.8762105, -119.3432825)              238.2\nG (37.833654, -119.4510805)           249.34999\nH (38.0603395, -119.6666755)             246.98\nI (37.798171, -119.19955150              237.34\nJ (38.0391175, -119.3073495)             234.65\nName: 55, dtype: object\nTime                            11/26/2017 6:00\nA (38.152231, -119.6666755)              244.26\nB (38.279274, -119.6127765)              244.31\nC (38.50458, -119.62176)                 254.39\nD (37.862028, -119.6576925)           258.91998\nE (37.89748, -119.2624335)               236.93\nF (37.8762105, -119.3432825)              238.0\nG (37.833654, -119.4510805)              247.14\nH (38.0603395, -119.6666755)          247.06999\nI (37.798171, -119.19955150              241.98\nJ (38.0391175, -119.3073495)             238.17\nName: 56, dtype: object\nTime                            11/27/2017 6:00\nA (38.152231, -119.6666755)           244.31999\nB (38.279274, -119.6127765)           244.51999\nC (38.50458, -119.62176)                 251.09\nD (37.862028, -119.6576925)              252.11\nE (37.89748, -119.2624335)            238.26999\nF (37.8762105, -119.3432825)          241.15999\nG (37.833654, -119.4510805)           246.65999\nH (38.0603395, -119.6666755)          245.70999\nI (37.798171, -119.19955150              241.47\nJ (38.0391175, -119.3073495)             238.61\nName: 57, dtype: object\nTime                            11/28/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 58, dtype: object\nTime                            11/29/2017 6:00\nA (38.152231, -119.6666755)              236.54\nB (38.279274, -119.6127765)              238.01\nC (38.50458, -119.62176)                 246.93\nD (37.862028, -119.6576925)           246.56999\nE (37.89748, -119.2624335)               229.22\nF (37.8762105, -119.3432825)          232.59999\nG (37.833654, -119.4510805)           239.90999\nH (38.0603395, -119.6666755)          237.37999\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)             231.08\nName: 59, dtype: object\nTime                            11/30/2017 6:00\nA (38.152231, -119.6666755)              235.97\nB (38.279274, -119.6127765)              237.15\nC (38.50458, -119.62176)              245.81999\nD (37.862028, -119.6576925)              247.42\nE (37.89748, -119.2624335)            232.06999\nF (37.8762105, -119.3432825)          233.15999\nG (37.833654, -119.4510805)              239.03\nH (38.0603395, -119.6666755)          237.84999\nI (37.798171, -119.19955150              235.79\nJ (38.0391175, -119.3073495)          231.84999\nName: 60, dtype: object\nTime                            12/1/2017 6:00\nA (38.152231, -119.6666755)             237.48\nB (38.279274, -119.6127765)          234.90999\nC (38.50458, -119.62176)                245.12\nD (37.862028, -119.6576925)          250.20999\nE (37.89748, -119.2624335)              233.17\nF (37.8762105, -119.3432825)            236.31\nG (37.833654, -119.4510805)          241.76999\nH (38.0603395, -119.6666755)            238.89\nI (37.798171, -119.19955150          234.29999\nJ (38.0391175, -119.3073495)         234.54999\nName: 61, dtype: object\nTime                            12/2/2017 6:00\nA (38.152231, -119.6666755)             238.76\nB (38.279274, -119.6127765)          237.48999\nC (38.50458, -119.62176)             247.90999\nD (37.862028, -119.6576925)          252.51999\nE (37.89748, -119.2624335)           233.76999\nF (37.8762105, -119.3432825)         234.68999\nG (37.833654, -119.4510805)          239.70999\nH (38.0603395, -119.6666755)         240.79999\nI (37.798171, -119.19955150          238.18999\nJ (38.0391175, -119.3073495)            234.33\nName: 62, dtype: object\nTime                            12/3/2017 6:00\nA (38.152231, -119.6666755)             241.45\nB (38.279274, -119.6127765)          239.65999\nC (38.50458, -119.62176)             246.93999\nD (37.862028, -119.6576925)          252.40999\nE (37.89748, -119.2624335)           232.81999\nF (37.8762105, -119.3432825)         233.93999\nG (37.833654, -119.4510805)             243.89\nH (38.0603395, -119.6666755)         242.87999\nI (37.798171, -119.19955150          238.37999\nJ (38.0391175, -119.3073495)            234.45\nName: 63, dtype: object\nTime                            12/4/2017 6:00\nA (38.152231, -119.6666755)             231.39\nB (38.279274, -119.6127765)          230.09999\nC (38.50458, -119.62176)                 239.4\nD (37.862028, -119.6576925)             246.08\nE (37.89748, -119.2624335)              226.76\nF (37.8762105, -119.3432825)         229.40999\nG (37.833654, -119.4510805)          236.37999\nH (38.0603395, -119.6666755)            233.78\nI (37.798171, -119.19955150             227.89\nJ (38.0391175, -119.3073495)            226.75\nName: 64, dtype: object\nTime                            12/5/2017 6:00\nA (38.152231, -119.6666755)          236.18999\nB (38.279274, -119.6127765)             234.72\nC (38.50458, -119.62176)                241.17\nD (37.862028, -119.6576925)          251.48999\nE (37.89748, -119.2624335)           229.84999\nF (37.8762105, -119.3432825)            233.87\nG (37.833654, -119.4510805)          242.59999\nH (38.0603395, -119.6666755)         238.26999\nI (37.798171, -119.19955150             231.08\nJ (38.0391175, -119.3073495)            227.93\nName: 65, dtype: object\nTime                            12/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 66, dtype: object\nTime                            12/7/2017 6:00\nA (38.152231, -119.6666755)          235.18999\nB (38.279274, -119.6127765)          236.59999\nC (38.50458, -119.62176)                 244.7\nD (37.862028, -119.6576925)             248.34\nE (37.89748, -119.2624335)              231.23\nF (37.8762105, -119.3432825)            234.18\nG (37.833654, -119.4510805)          239.40999\nH (38.0603395, -119.6666755)            236.51\nI (37.798171, -119.19955150             234.36\nJ (38.0391175, -119.3073495)            228.76\nName: 67, dtype: object\nTime                            12/8/2017 6:00\nA (38.152231, -119.6666755)             236.06\nB (38.279274, -119.6127765)             236.08\nC (38.50458, -119.62176)             244.43999\nD (37.862028, -119.6576925)             250.37\nE (37.89748, -119.2624335)           231.34999\nF (37.8762105, -119.3432825)         233.56999\nG (37.833654, -119.4510805)             240.86\nH (38.0603395, -119.6666755)            238.65\nI (37.798171, -119.19955150             234.59\nJ (38.0391175, -119.3073495)         230.87999\nName: 68, dtype: object\nTime                            12/9/2017 6:00\nA (38.152231, -119.6666755)             236.18\nB (38.279274, -119.6127765)             233.83\nC (38.50458, -119.62176)                241.39\nD (37.862028, -119.6576925)          250.01999\nE (37.89748, -119.2624335)           229.26999\nF (37.8762105, -119.3432825)            234.53\nG (37.833654, -119.4510805)          241.15999\nH (38.0603395, -119.6666755)         238.59999\nI (37.798171, -119.19955150             231.98\nJ (38.0391175, -119.3073495)            230.83\nName: 69, dtype: object\nTime                            12/10/2017 6:00\nA (38.152231, -119.6666755)           236.37999\nB (38.279274, -119.6127765)           234.18999\nC (38.50458, -119.62176)              241.70999\nD (37.862028, -119.6576925)           253.20999\nE (37.89748, -119.2624335)               231.69\nF (37.8762105, -119.3432825)             236.18\nG (37.833654, -119.4510805)           241.62999\nH (38.0603395, -119.6666755)             239.34\nI (37.798171, -119.19955150              233.12\nJ (38.0391175, -119.3073495)          231.65999\nName: 70, dtype: object\nTime                            12/11/2017 6:00\nA (38.152231, -119.6666755)           239.68999\nB (38.279274, -119.6127765)              236.76\nC (38.50458, -119.62176)                 244.56\nD (37.862028, -119.6576925)              253.65\nE (37.89748, -119.2624335)               231.12\nF (37.8762105, -119.3432825)          235.23999\nG (37.833654, -119.4510805)           243.59999\nH (38.0603395, -119.6666755)          241.93999\nI (37.798171, -119.19955150              236.26\nJ (38.0391175, -119.3073495)          232.79999\nName: 71, dtype: object\nTime                            12/12/2017 6:00\nA (38.152231, -119.6666755)              238.65\nB (38.279274, -119.6127765)               238.5\nC (38.50458, -119.62176)                 247.04\nD (37.862028, -119.6576925)              255.15\nE (37.89748, -119.2624335)               233.26\nF (37.8762105, -119.3432825)             236.97\nG (37.833654, -119.4510805)           245.18999\nH (38.0603395, -119.6666755)          241.87999\nI (37.798171, -119.19955150           235.93999\nJ (38.0391175, -119.3073495)             233.28\nName: 72, dtype: object\nTime                            12/13/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 73, dtype: object\nTime                            12/14/2017 6:00\nA (38.152231, -119.6666755)              247.68\nB (38.279274, -119.6127765)           241.56999\nC (38.50458, -119.62176)              248.95999\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 74, dtype: object\nTime                            12/15/2017 6:00\nA (38.152231, -119.6666755)           238.90999\nB (38.279274, -119.6127765)              240.84\nC (38.50458, -119.62176)                 249.36\nD (37.862028, -119.6576925)              252.59\nE (37.89748, -119.2624335)               235.58\nF (37.8762105, -119.3432825)          238.43999\nG (37.833654, -119.4510805)              243.92\nH (38.0603395, -119.6666755)             241.22\nI (37.798171, -119.19955150           239.43999\nJ (38.0391175, -119.3073495)          234.90999\nName: 75, dtype: object\nTime                            12/16/2017 6:00\nA (38.152231, -119.6666755)              239.14\nB (38.279274, -119.6127765)              238.95\nC (38.50458, -119.62176)              248.51999\nD (37.862028, -119.6576925)           250.23999\nE (37.89748, -119.2624335)               238.84\nF (37.8762105, -119.3432825)             239.56\nG (37.833654, -119.4510805)              241.98\nH (38.0603395, -119.6666755)          241.40999\nI (37.798171, -119.19955150              240.45\nJ (38.0391175, -119.3073495)             239.31\nName: 76, dtype: object\nTime                            12/17/2017 6:00\nA (38.152231, -119.6666755)           238.56999\nB (38.279274, -119.6127765)              237.15\nC (38.50458, -119.62176)              241.87999\nD (37.862028, -119.6576925)              250.59\nE (37.89748, -119.2624335)            229.54999\nF (37.8762105, -119.3432825)             232.39\nG (37.833654, -119.4510805)           242.40999\nH (38.0603395, -119.6666755)             241.01\nI (37.798171, -119.19955150              233.39\nJ (38.0391175, -119.3073495)             228.44\nName: 77, dtype: object\nTime                            12/18/2017 6:00\nA (38.152231, -119.6666755)              240.45\nB (38.279274, -119.6127765)              238.34\nC (38.50458, -119.62176)                 244.81\nD (37.862028, -119.6576925)              253.04\nE (37.89748, -119.2624335)            231.37999\nF (37.8762105, -119.3432825)          236.59999\nG (37.833654, -119.4510805)               246.5\nH (38.0603395, -119.6666755)          242.56999\nI (37.798171, -119.19955150           234.76999\nJ (38.0391175, -119.3073495)          233.65999\nName: 78, dtype: object\nTime                            12/19/2017 6:00\nA (38.152231, -119.6666755)              238.34\nB (38.279274, -119.6127765)              239.31\nC (38.50458, -119.62176)                 248.06\nD (37.862028, -119.6576925)              251.93\nE (37.89748, -119.2624335)               234.04\nF (37.8762105, -119.3432825)             236.58\nG (37.833654, -119.4510805)           243.76999\nH (38.0603395, -119.6666755)             241.93\nI (37.798171, -119.19955150              236.81\nJ (38.0391175, -119.3073495)             234.39\nName: 79, dtype: object\nTime                            12/20/2017 6:00\nA (38.152231, -119.6666755)              241.03\nB (38.279274, -119.6127765)           240.70999\nC (38.50458, -119.62176)              249.09999\nD (37.862028, -119.6576925)              254.59\nE (37.89748, -119.2624335)               237.83\nF (37.8762105, -119.3432825)             240.75\nG (37.833654, -119.4510805)              247.23\nH (38.0603395, -119.6666755)          243.29999\nI (37.798171, -119.19955150              240.37\nJ (38.0391175, -119.3073495)          237.09999\nName: 80, dtype: object\nTime                            12/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 81, dtype: object\nTime                            12/22/2017 6:00\nA (38.152231, -119.6666755)           237.79999\nB (38.279274, -119.6127765)              239.42\nC (38.50458, -119.62176)              248.01999\nD (37.862028, -119.6576925)           247.68999\nE (37.89748, -119.2624335)               232.62\nF (37.8762105, -119.3432825)             235.31\nG (37.833654, -119.4510805)           240.09999\nH (38.0603395, -119.6666755)             239.87\nI (37.798171, -119.19955150              236.33\nJ (38.0391175, -119.3073495)          231.06999\nName: 82, dtype: object\nTime                            12/23/2017 6:00\nA (38.152231, -119.6666755)              240.22\nB (38.279274, -119.6127765)           242.06999\nC (38.50458, -119.62176)                 250.67\nD (37.862028, -119.6576925)              251.15\nE (37.89748, -119.2624335)                236.4\nF (37.8762105, -119.3432825)             237.93\nG (37.833654, -119.4510805)              243.29\nH (38.0603395, -119.6666755)             241.29\nI (37.798171, -119.19955150              238.95\nJ (38.0391175, -119.3073495)          237.29999\nName: 83, dtype: object\nTime                            12/24/2017 6:00\nA (38.152231, -119.6666755)              241.61\nB (38.279274, -119.6127765)           241.34999\nC (38.50458, -119.62176)              249.98999\nD (37.862028, -119.6576925)              254.78\nE (37.89748, -119.2624335)               239.25\nF (37.8762105, -119.3432825)          242.20999\nG (37.833654, -119.4510805)              245.76\nH (38.0603395, -119.6666755)          243.98999\nI (37.798171, -119.19955150              240.37\nJ (38.0391175, -119.3073495)          239.40999\nName: 84, dtype: object\nTime                            12/25/2017 6:00\nA (38.152231, -119.6666755)           244.09999\nB (38.279274, -119.6127765)              242.81\nC (38.50458, -119.62176)                  249.7\nD (37.862028, -119.6576925)           255.79999\nE (37.89748, -119.2624335)               239.15\nF (37.8762105, -119.3432825)             240.26\nG (37.833654, -119.4510805)              244.58\nH (38.0603395, -119.6666755)          246.56999\nI (37.798171, -119.19955150              242.08\nJ (38.0391175, -119.3073495)             240.22\nName: 85, dtype: object\nTime                            12/26/2017 6:00\nA (38.152231, -119.6666755)              240.29\nB (38.279274, -119.6127765)              238.42\nC (38.50458, -119.62176)                 246.92\nD (37.862028, -119.6576925)              253.67\nE (37.89748, -119.2624335)               233.79\nF (37.8762105, -119.3432825)             237.84\nG (37.833654, -119.4510805)              246.54\nH (38.0603395, -119.6666755)          243.06999\nI (37.798171, -119.19955150           236.93999\nJ (38.0391175, -119.3073495)          235.12999\nName: 86, dtype: object\nTime                            12/27/2017 6:00\nA (38.152231, -119.6666755)           239.87999\nB (38.279274, -119.6127765)              240.62\nC (38.50458, -119.62176)                 249.68\nD (37.862028, -119.6576925)              253.92\nE (37.89748, -119.2624335)            234.20999\nF (37.8762105, -119.3432825)             238.47\nG (37.833654, -119.4510805)           245.31999\nH (38.0603395, -119.6666755)          242.31999\nI (37.798171, -119.19955150           237.45999\nJ (38.0391175, -119.3073495)          234.98999\nName: 87, dtype: object\nTime                            12/28/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)               232.04\nF (37.8762105, -119.3432825)             234.56\nG (37.833654, -119.4510805)           238.93999\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150               238.9\nJ (38.0391175, -119.3073495)          229.79999\nName: 88, dtype: object\nTime                            12/29/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 89, dtype: object\nTime                            12/30/2017 6:00\nA (38.152231, -119.6666755)              240.98\nB (38.279274, -119.6127765)              242.12\nC (38.50458, -119.62176)                 251.34\nD (37.862028, -119.6576925)           252.70999\nE (37.89748, -119.2624335)               237.45\nF (37.8762105, -119.3432825)             239.18\nG (37.833654, -119.4510805)              243.86\nH (38.0603395, -119.6666755)             241.92\nI (37.798171, -119.19955150              240.47\nJ (38.0391175, -119.3073495)          236.48999\nName: 90, dtype: object\nTime                            12/31/2017 6:00\nA (38.152231, -119.6666755)              240.37\nB (38.279274, -119.6127765)              241.25\nC (38.50458, -119.62176)                 248.87\nD (37.862028, -119.6576925)              253.73\nE (37.89748, -119.2624335)            237.15999\nF (37.8762105, -119.3432825)          239.20999\nG (37.833654, -119.4510805)           242.56999\nH (38.0603395, -119.6666755)             243.17\nI (37.798171, -119.19955150              238.65\nJ (38.0391175, -119.3073495)          237.06999\nName: 91, dtype: object\nTime                            1/1/2018 6:00\nA (38.152231, -119.6666755)         241.26999\nB (38.279274, -119.6127765)            242.09\nC (38.50458, -119.62176)            248.93999\nD (37.862028, -119.6576925)         253.48999\nE (37.89748, -119.2624335)             236.67\nF (37.8762105, -119.3432825)        239.70999\nG (37.833654, -119.4510805)         247.09999\nH (38.0603395, -119.6666755)        244.20999\nI (37.798171, -119.19955150         237.48999\nJ (38.0391175, -119.3073495)        238.23999\nName: 92, dtype: object\nTime                            1/2/2018 6:00\nA (38.152231, -119.6666755)            241.31\nB (38.279274, -119.6127765)            239.89\nC (38.50458, -119.62176)               249.37\nD (37.862028, -119.6576925)            257.44\nE (37.89748, -119.2624335)              237.5\nF (37.8762105, -119.3432825)           240.73\nG (37.833654, -119.4510805)             243.2\nH (38.0603395, -119.6666755)        242.90999\nI (37.798171, -119.19955150            239.93\nJ (38.0391175, -119.3073495)        239.04999\nName: 93, dtype: object\nTime                            1/3/2018 6:00\nA (38.152231, -119.6666755)            242.48\nB (38.279274, -119.6127765)            242.58\nC (38.50458, -119.62176)                251.0\nD (37.862028, -119.6576925)            256.81\nE (37.89748, -119.2624335)          236.54999\nF (37.8762105, -119.3432825)           238.98\nG (37.833654, -119.4510805)         247.15999\nH (38.0603395, -119.6666755)        245.65999\nI (37.798171, -119.19955150            238.78\nJ (38.0391175, -119.3073495)           237.25\nName: 94, dtype: object\nTime                            1/4/2018 6:00\nA (38.152231, -119.6666755)            247.78\nB (38.279274, -119.6127765)         248.68999\nC (38.50458, -119.62176)               255.93\nD (37.862028, -119.6576925)            258.78\nE (37.89748, -119.2624335)          241.54999\nF (37.8762105, -119.3432825)           244.53\nG (37.833654, -119.4510805)            252.26\nH (38.0603395, -119.6666755)        249.68999\nI (37.798171, -119.19955150         244.65999\nJ (38.0391175, -119.3073495)        242.18999\nName: 95, dtype: object\nTime                            1/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 96, dtype: object\nTime                            1/6/2018 6:00\nA (38.152231, -119.6666755)            247.47\nB (38.279274, -119.6127765)            250.92\nC (38.50458, -119.62176)               256.43\nD (37.862028, -119.6576925)            250.73\nE (37.89748, -119.2624335)          239.68999\nF (37.8762105, -119.3432825)           241.11\nG (37.833654, -119.4510805)         244.73999\nH (38.0603395, -119.6666755)           247.39\nI (37.798171, -119.19955150            240.73\nJ (38.0391175, -119.3073495)        241.18999\nName: 97, dtype: object\nTime                            1/7/2018 6:00\nA (38.152231, -119.6666755)            239.73\nB (38.279274, -119.6127765)         240.87999\nC (38.50458, -119.62176)            247.40999\nD (37.862028, -119.6576925)            248.98\nE (37.89748, -119.2624335)             234.15\nF (37.8762105, -119.3432825)           236.15\nG (37.833654, -119.4510805)         240.37999\nH (38.0603395, -119.6666755)           240.18\nI (37.798171, -119.19955150            236.97\nJ (38.0391175, -119.3073495)        234.09999\nName: 98, dtype: object\nTime                            1/8/2018 6:00\nA (38.152231, -119.6666755)            246.51\nB (38.279274, -119.6127765)            248.01\nC (38.50458, -119.62176)               254.12\nD (37.862028, -119.6576925)            253.79\nE (37.89748, -119.2624335)             241.42\nF (37.8762105, -119.3432825)           241.93\nG (37.833654, -119.4510805)            245.14\nH (38.0603395, -119.6666755)           247.37\nI (37.798171, -119.19955150            243.14\nJ (38.0391175, -119.3073495)           242.97\nName: 99, dtype: object\nTime                            1/9/2018 6:00\nA (38.152231, -119.6666755)            248.72\nB (38.279274, -119.6127765)         251.09999\nC (38.50458, -119.62176)            255.51999\nD (37.862028, -119.6576925)            254.31\nE (37.89748, -119.2624335)          243.20999\nF (37.8762105, -119.3432825)        244.12999\nG (37.833654, -119.4510805)         249.45999\nH (38.0603395, -119.6666755)           249.79\nI (37.798171, -119.19955150            244.58\nJ (38.0391175, -119.3073495)           245.47\nName: 100, dtype: object\nTime                            1/10/2018 6:00\nA (38.152231, -119.6666755)             247.75\nB (38.279274, -119.6127765)             247.62\nC (38.50458, -119.62176)                 253.7\nD (37.862028, -119.6576925)             255.04\nE (37.89748, -119.2624335)           238.59999\nF (37.8762105, -119.3432825)             241.7\nG (37.833654, -119.4510805)             246.45\nH (38.0603395, -119.6666755)            249.67\nI (37.798171, -119.19955150             238.87\nJ (38.0391175, -119.3073495)             241.4\nName: 101, dtype: object\nTime                            1/11/2018 6:00\nA (38.152231, -119.6666755)          240.79999\nB (38.279274, -119.6127765)             241.93\nC (38.50458, -119.62176)             249.23999\nD (37.862028, -119.6576925)             251.67\nE (37.89748, -119.2624335)           234.84999\nF (37.8762105, -119.3432825)            236.62\nG (37.833654, -119.4510805)             243.39\nH (38.0603395, -119.6666755)         241.98999\nI (37.798171, -119.19955150          236.95999\nJ (38.0391175, -119.3073495)            236.26\nName: 102, dtype: object\nTime                            1/12/2018 6:00\nA (38.152231, -119.6666755)             234.83\nB (38.279274, -119.6127765)             239.97\nC (38.50458, -119.62176)                250.06\nD (37.862028, -119.6576925)          249.09999\nE (37.89748, -119.2624335)              235.42\nF (37.8762105, -119.3432825)            238.17\nG (37.833654, -119.4510805)             244.48\nH (38.0603395, -119.6666755)            235.92\nI (37.798171, -119.19955150             236.98\nJ (38.0391175, -119.3073495)         234.18999\nName: 103, dtype: object\nTime                            1/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 104, dtype: object\nTime                            1/14/2018 6:00\nA (38.152231, -119.6666755)             235.58\nB (38.279274, -119.6127765)             239.45\nC (38.50458, -119.62176)                248.81\nD (37.862028, -119.6576925)             248.23\nE (37.89748, -119.2624335)              234.03\nF (37.8762105, -119.3432825)            234.51\nG (37.833654, -119.4510805)             237.83\nH (38.0603395, -119.6666755)         236.84999\nI (37.798171, -119.19955150             236.81\nJ (38.0391175, -119.3073495)            234.01\nName: 105, dtype: object\nTime                            1/15/2018 6:00\nA (38.152231, -119.6666755)             237.06\nB (38.279274, -119.6127765)             239.97\nC (38.50458, -119.62176)                 249.9\nD (37.862028, -119.6576925)             247.93\nE (37.89748, -119.2624335)              234.28\nF (37.8762105, -119.3432825)            235.51\nG (37.833654, -119.4510805)             237.37\nH (38.0603395, -119.6666755)            237.48\nI (37.798171, -119.19955150             236.12\nJ (38.0391175, -119.3073495)            235.72\nName: 106, dtype: object\nTime                            1/16/2018 6:00\nA (38.152231, -119.6666755)             237.03\nB (38.279274, -119.6127765)          238.76999\nC (38.50458, -119.62176)                249.17\nD (37.862028, -119.6576925)          249.90999\nE (37.89748, -119.2624335)               235.9\nF (37.8762105, -119.3432825)            236.78\nG (37.833654, -119.4510805)          237.87999\nH (38.0603395, -119.6666755)            238.79\nI (37.798171, -119.19955150          238.18999\nJ (38.0391175, -119.3073495)         238.45999\nName: 107, dtype: object\nTime                            1/17/2018 6:00\nA (38.152231, -119.6666755)             239.29\nB (38.279274, -119.6127765)          239.76999\nC (38.50458, -119.62176)             248.48999\nD (37.862028, -119.6576925)             249.75\nE (37.89748, -119.2624335)               232.9\nF (37.8762105, -119.3432825)            232.23\nG (37.833654, -119.4510805)             241.15\nH (38.0603395, -119.6666755)         240.48999\nI (37.798171, -119.19955150              236.7\nJ (38.0391175, -119.3073495)            236.36\nName: 108, dtype: object\nTime                            1/18/2018 6:00\nA (38.152231, -119.6666755)             239.78\nB (38.279274, -119.6127765)          242.09999\nC (38.50458, -119.62176)                253.11\nD (37.862028, -119.6576925)             252.86\nE (37.89748, -119.2624335)              234.56\nF (37.8762105, -119.3432825)            235.73\nG (37.833654, -119.4510805)             243.09\nH (38.0603395, -119.6666755)            240.62\nI (37.798171, -119.19955150             237.65\nJ (38.0391175, -119.3073495)             238.4\nName: 109, dtype: object\nTime                            1/19/2018 6:00\nA (38.152231, -119.6666755)             242.67\nB (38.279274, -119.6127765)          242.51999\nC (38.50458, -119.62176)             249.45999\nD (37.862028, -119.6576925)             251.72\nE (37.89748, -119.2624335)           239.04999\nF (37.8762105, -119.3432825)            240.81\nG (37.833654, -119.4510805)             246.59\nH (38.0603395, -119.6666755)         244.45999\nI (37.798171, -119.19955150             243.08\nJ (38.0391175, -119.3073495)         238.45999\nName: 110, dtype: object\nTime                            1/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 111, dtype: object\nTime                            1/21/2018 6:00\nA (38.152231, -119.6666755)              230.2\nB (38.279274, -119.6127765)             231.93\nC (38.50458, -119.62176)                239.48\nD (37.862028, -119.6576925)             239.28\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)         230.95999\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 112, dtype: object\nTime                            1/22/2018 6:00\nA (38.152231, -119.6666755)             235.59\nB (38.279274, -119.6127765)          236.65999\nC (38.50458, -119.62176)                245.26\nD (37.862028, -119.6576925)             246.98\nE (37.89748, -119.2624335)           233.73999\nF (37.8762105, -119.3432825)         234.65999\nG (37.833654, -119.4510805)             238.58\nH (38.0603395, -119.6666755)            237.95\nI (37.798171, -119.19955150          235.93999\nJ (38.0391175, -119.3073495)            233.26\nName: 113, dtype: object\nTime                            1/23/2018 6:00\nA (38.152231, -119.6666755)             231.51\nB (38.279274, -119.6127765)             233.67\nC (38.50458, -119.62176)                240.72\nD (37.862028, -119.6576925)              243.9\nE (37.89748, -119.2624335)              228.98\nF (37.8762105, -119.3432825)            228.81\nG (37.833654, -119.4510805)          233.40999\nH (38.0603395, -119.6666755)            233.54\nI (37.798171, -119.19955150             232.22\nJ (38.0391175, -119.3073495)            228.87\nName: 114, dtype: object\nTime                            1/24/2018 6:00\nA (38.152231, -119.6666755)             231.06\nB (38.279274, -119.6127765)             232.25\nC (38.50458, -119.62176)             243.37999\nD (37.862028, -119.6576925)             247.26\nE (37.89748, -119.2624335)           229.65999\nF (37.8762105, -119.3432825)            232.51\nG (37.833654, -119.4510805)             235.81\nH (38.0603395, -119.6666755)            234.75\nI (37.798171, -119.19955150             231.75\nJ (38.0391175, -119.3073495)            231.87\nName: 115, dtype: object\nTime                            1/25/2018 6:00\nA (38.152231, -119.6666755)             233.64\nB (38.279274, -119.6127765)          233.12999\nC (38.50458, -119.62176)             241.81999\nD (37.862028, -119.6576925)          243.29999\nE (37.89748, -119.2624335)              229.83\nF (37.8762105, -119.3432825)         230.40999\nG (37.833654, -119.4510805)          236.20999\nH (38.0603395, -119.6666755)             234.5\nI (37.798171, -119.19955150          233.93999\nJ (38.0391175, -119.3073495)            232.23\nName: 116, dtype: object\nTime                            1/26/2018 6:00\nA (38.152231, -119.6666755)          229.81999\nB (38.279274, -119.6127765)          231.29999\nC (38.50458, -119.62176)             238.73999\nD (37.862028, -119.6576925)          242.43999\nE (37.89748, -119.2624335)              225.72\nF (37.8762105, -119.3432825)            227.15\nG (37.833654, -119.4510805)             234.15\nH (38.0603395, -119.6666755)         232.81999\nI (37.798171, -119.19955150          227.26999\nJ (38.0391175, -119.3073495)            226.89\nName: 117, dtype: object\nTime                            1/27/2018 6:00\nA (38.152231, -119.6666755)             238.43\nB (38.279274, -119.6127765)             240.39\nC (38.50458, -119.62176)             247.06999\nD (37.862028, -119.6576925)          249.79999\nE (37.89748, -119.2624335)           232.90999\nF (37.8762105, -119.3432825)             236.2\nG (37.833654, -119.4510805)             242.67\nH (38.0603395, -119.6666755)            239.58\nI (37.798171, -119.19955150          234.98999\nJ (38.0391175, -119.3073495)             232.2\nName: 118, dtype: object\nTime                            1/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 119, dtype: object\nTime                            1/29/2018 6:00\nA (38.152231, -119.6666755)             236.37\nB (38.279274, -119.6127765)          240.20999\nC (38.50458, -119.62176)                248.53\nD (37.862028, -119.6576925)             246.93\nE (37.89748, -119.2624335)              233.98\nF (37.8762105, -119.3432825)            235.39\nG (37.833654, -119.4510805)             238.75\nH (38.0603395, -119.6666755)         236.87999\nI (37.798171, -119.19955150          236.98999\nJ (38.0391175, -119.3073495)            233.53\nName: 120, dtype: object\nTime                            1/30/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 121, dtype: object\nTime                            1/31/2018 6:00\nA (38.152231, -119.6666755)             235.68\nB (38.279274, -119.6127765)             237.89\nC (38.50458, -119.62176)                246.29\nD (37.862028, -119.6576925)             247.03\nE (37.89748, -119.2624335)              231.09\nF (37.8762105, -119.3432825)         231.01999\nG (37.833654, -119.4510805)          237.84999\nH (38.0603395, -119.6666755)         236.48999\nI (37.798171, -119.19955150             235.65\nJ (38.0391175, -119.3073495)            231.93\nName: 122, dtype: object\nTime                            2/1/2018 6:00\nA (38.152231, -119.6666755)            237.62\nB (38.279274, -119.6127765)            238.87\nC (38.50458, -119.62176)               248.01\nD (37.862028, -119.6576925)         250.90999\nE (37.89748, -119.2624335)             233.12\nF (37.8762105, -119.3432825)        235.62999\nG (37.833654, -119.4510805)            244.11\nH (38.0603395, -119.6666755)        240.62999\nI (37.798171, -119.19955150             234.0\nJ (38.0391175, -119.3073495)        234.70999\nName: 123, dtype: object\nTime                            2/2/2018 6:00\nA (38.152231, -119.6666755)            238.78\nB (38.279274, -119.6127765)         239.04999\nC (38.50458, -119.62176)               247.61\nD (37.862028, -119.6576925)         251.81999\nE (37.89748, -119.2624335)              231.5\nF (37.8762105, -119.3432825)        234.90999\nG (37.833654, -119.4510805)         241.87999\nH (38.0603395, -119.6666755)           240.68\nI (37.798171, -119.19955150            235.81\nJ (38.0391175, -119.3073495)           235.76\nName: 124, dtype: object\nTime                            2/3/2018 6:00\nA (38.152231, -119.6666755)            237.86\nB (38.279274, -119.6127765)            238.79\nC (38.50458, -119.62176)               247.51\nD (37.862028, -119.6576925)            253.01\nE (37.89748, -119.2624335)             232.53\nF (37.8762105, -119.3432825)           235.53\nG (37.833654, -119.4510805)            242.25\nH (38.0603395, -119.6666755)           239.93\nI (37.798171, -119.19955150            233.65\nJ (38.0391175, -119.3073495)           233.72\nName: 125, dtype: object\nTime                            2/4/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 126, dtype: object\nTime                            2/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 127, dtype: object\nTime                            2/6/2018 6:00\nA (38.152231, -119.6666755)            232.68\nB (38.279274, -119.6127765)         236.23999\nC (38.50458, -119.62176)            246.48999\nD (37.862028, -119.6576925)            244.03\nE (37.89748, -119.2624335)          231.01999\nF (37.8762105, -119.3432825)           231.86\nG (37.833654, -119.4510805)         234.29999\nH (38.0603395, -119.6666755)        232.93999\nI (37.798171, -119.19955150            234.51\nJ (38.0391175, -119.3073495)           231.18\nName: 128, dtype: object\nTime                            2/7/2018 6:00\nA (38.152231, -119.6666755)            231.73\nB (38.279274, -119.6127765)            235.86\nC (38.50458, -119.62176)               245.28\nD (37.862028, -119.6576925)            243.75\nE (37.89748, -119.2624335)          227.87999\nF (37.8762105, -119.3432825)        229.76999\nG (37.833654, -119.4510805)            235.78\nH (38.0603395, -119.6666755)           232.39\nI (37.798171, -119.19955150         230.95999\nJ (38.0391175, -119.3073495)           230.68\nName: 129, dtype: object\nTime                            2/8/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 130, dtype: object\nTime                            2/9/2018 6:00\nA (38.152231, -119.6666755)            233.42\nB (38.279274, -119.6127765)            236.01\nC (38.50458, -119.62176)               246.73\nD (37.862028, -119.6576925)            247.43\nE (37.89748, -119.2624335)          226.65999\nF (37.8762105, -119.3432825)           229.53\nG (37.833654, -119.4510805)            238.31\nH (38.0603395, -119.6666755)           236.54\nI (37.798171, -119.19955150         228.65999\nJ (38.0391175, -119.3073495)           231.01\nName: 131, dtype: object\nTime                            2/10/2018 6:00\nA (38.152231, -119.6666755)             235.56\nB (38.279274, -119.6127765)          238.87999\nC (38.50458, -119.62176)             249.09999\nD (37.862028, -119.6576925)          247.37999\nE (37.89748, -119.2624335)              231.11\nF (37.8762105, -119.3432825)         232.43999\nG (37.833654, -119.4510805)          239.20999\nH (38.0603395, -119.6666755)            237.36\nI (37.798171, -119.19955150          234.04999\nJ (38.0391175, -119.3073495)         234.26999\nName: 132, dtype: object\nTime                            2/11/2018 6:00\nA (38.152231, -119.6666755)             232.76\nB (38.279274, -119.6127765)          235.20999\nC (38.50458, -119.62176)                 244.2\nD (37.862028, -119.6576925)          246.93999\nE (37.89748, -119.2624335)           226.12999\nF (37.8762105, -119.3432825)            229.06\nG (37.833654, -119.4510805)          236.43999\nH (38.0603395, -119.6666755)         234.43999\nI (37.798171, -119.19955150             228.92\nJ (38.0391175, -119.3073495)            227.53\nName: 133, dtype: object\nTime                            2/12/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 134, dtype: object\nTime                            2/13/2018 6:00\nA (38.152231, -119.6666755)             227.14\nB (38.279274, -119.6127765)             233.76\nC (38.50458, -119.62176)             243.29999\nD (37.862028, -119.6576925)             235.81\nE (37.89748, -119.2624335)              228.39\nF (37.8762105, -119.3432825)            227.92\nG (37.833654, -119.4510805)             228.92\nH (38.0603395, -119.6666755)            227.92\nI (37.798171, -119.19955150             230.56\nJ (38.0391175, -119.3073495)            229.33\nName: 135, dtype: object\nTime                            2/14/2018 6:00\nA (38.152231, -119.6666755)             225.73\nB (38.279274, -119.6127765)             231.87\nC (38.50458, -119.62176)             242.09999\nD (37.862028, -119.6576925)             237.48\nE (37.89748, -119.2624335)           225.01999\nF (37.8762105, -119.3432825)            224.58\nG (37.833654, -119.4510805)             226.92\nH (38.0603395, -119.6666755)            227.72\nI (37.798171, -119.19955150             226.73\nJ (38.0391175, -119.3073495)            227.68\nName: 136, dtype: object\nTime                            2/15/2018 6:00\nA (38.152231, -119.6666755)          225.09999\nB (38.279274, -119.6127765)             227.73\nC (38.50458, -119.62176)                238.58\nD (37.862028, -119.6576925)             237.54\nE (37.89748, -119.2624335)              221.03\nF (37.8762105, -119.3432825)            223.18\nG (37.833654, -119.4510805)          226.70999\nH (38.0603395, -119.6666755)         226.09999\nI (37.798171, -119.19955150              221.2\nJ (38.0391175, -119.3073495)            225.25\nName: 137, dtype: object\nTime                            2/16/2018 6:00\nA (38.152231, -119.6666755)             232.31\nB (38.279274, -119.6127765)          233.73999\nC (38.50458, -119.62176)                243.48\nD (37.862028, -119.6576925)          244.43999\nE (37.89748, -119.2624335)              228.28\nF (37.8762105, -119.3432825)            228.12\nG (37.833654, -119.4510805)             231.08\nH (38.0603395, -119.6666755)            232.89\nI (37.798171, -119.19955150             228.75\nJ (38.0391175, -119.3073495)         231.31999\nName: 138, dtype: object\nTime                            2/17/2018 6:00\nA (38.152231, -119.6666755)             232.31\nB (38.279274, -119.6127765)          233.59999\nC (38.50458, -119.62176)                244.37\nD (37.862028, -119.6576925)             245.54\nE (37.89748, -119.2624335)              226.17\nF (37.8762105, -119.3432825)            228.53\nG (37.833654, -119.4510805)             235.08\nH (38.0603395, -119.6666755)         233.81999\nI (37.798171, -119.19955150              227.5\nJ (38.0391175, -119.3073495)            228.19\nName: 139, dtype: object\nTime                            2/18/2018 6:00\nA (38.152231, -119.6666755)          233.90999\nB (38.279274, -119.6127765)             237.45\nC (38.50458, -119.62176)             247.43999\nD (37.862028, -119.6576925)             247.36\nE (37.89748, -119.2624335)           229.20999\nF (37.8762105, -119.3432825)            231.34\nG (37.833654, -119.4510805)          238.06999\nH (38.0603395, -119.6666755)            235.81\nI (37.798171, -119.19955150             231.98\nJ (38.0391175, -119.3073495)             231.7\nName: 140, dtype: object\nTime                            2/19/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)           221.15999\nF (37.8762105, -119.3432825)            221.89\nG (37.833654, -119.4510805)          224.04999\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             226.79\nJ (38.0391175, -119.3073495)            221.31\nName: 141, dtype: object\nTime                            2/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 142, dtype: object\nTime                            2/21/2018 6:00\nA (38.152231, -119.6666755)          224.59999\nB (38.279274, -119.6127765)          227.65999\nC (38.50458, -119.62176)                237.43\nD (37.862028, -119.6576925)             235.23\nE (37.89748, -119.2624335)              223.83\nF (37.8762105, -119.3432825)            223.29\nG (37.833654, -119.4510805)             225.86\nH (38.0603395, -119.6666755)             225.7\nI (37.798171, -119.19955150             225.94\nJ (38.0391175, -119.3073495)         223.95999\nName: 143, dtype: object\nTime                            2/22/2018 6:00\nA (38.152231, -119.6666755)             229.58\nB (38.279274, -119.6127765)          230.51999\nC (38.50458, -119.62176)             238.73999\nD (37.862028, -119.6576925)          239.59999\nE (37.89748, -119.2624335)              223.61\nF (37.8762105, -119.3432825)         224.98999\nG (37.833654, -119.4510805)             229.01\nH (38.0603395, -119.6666755)         231.12999\nI (37.798171, -119.19955150          224.84999\nJ (38.0391175, -119.3073495)         226.09999\nName: 144, dtype: object\nTime                            2/23/2018 6:00\nA (38.152231, -119.6666755)          225.15999\nB (38.279274, -119.6127765)             225.28\nC (38.50458, -119.62176)                 236.5\nD (37.862028, -119.6576925)          235.59999\nE (37.89748, -119.2624335)              220.37\nF (37.8762105, -119.3432825)            222.22\nG (37.833654, -119.4510805)          226.40999\nH (38.0603395, -119.6666755)            226.67\nI (37.798171, -119.19955150             220.84\nJ (38.0391175, -119.3073495)             222.7\nName: 145, dtype: object\nTime                            2/24/2018 6:00\nA (38.152231, -119.6666755)             226.34\nB (38.279274, -119.6127765)             228.04\nC (38.50458, -119.62176)                236.84\nD (37.862028, -119.6576925)              239.4\nE (37.89748, -119.2624335)              222.26\nF (37.8762105, -119.3432825)         225.15999\nG (37.833654, -119.4510805)             227.01\nH (38.0603395, -119.6666755)         228.34999\nI (37.798171, -119.19955150             224.04\nJ (38.0391175, -119.3073495)             225.2\nName: 146, dtype: object\nTime                            2/25/2018 6:00\nA (38.152231, -119.6666755)             230.12\nB (38.279274, -119.6127765)             229.53\nC (38.50458, -119.62176)                238.26\nD (37.862028, -119.6576925)             240.72\nE (37.89748, -119.2624335)           222.09999\nF (37.8762105, -119.3432825)         224.15999\nG (37.833654, -119.4510805)          231.98999\nH (38.0603395, -119.6666755)            231.65\nI (37.798171, -119.19955150             222.86\nJ (38.0391175, -119.3073495)             224.5\nName: 147, dtype: object\nTime                            2/26/2018 6:00\nA (38.152231, -119.6666755)             231.47\nB (38.279274, -119.6127765)             232.17\nC (38.50458, -119.62176)             240.90999\nD (37.862028, -119.6576925)             245.97\nE (37.89748, -119.2624335)              226.19\nF (37.8762105, -119.3432825)            229.44\nG (37.833654, -119.4510805)          235.54999\nH (38.0603395, -119.6666755)         233.37999\nI (37.798171, -119.19955150             229.18\nJ (38.0391175, -119.3073495)             228.2\nName: 148, dtype: object\nTime                            2/27/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 149, dtype: object\nTime                            2/28/2018 6:00\nA (38.152231, -119.6666755)          228.73999\nB (38.279274, -119.6127765)             232.53\nC (38.50458, -119.62176)                 239.2\nD (37.862028, -119.6576925)             237.22\nE (37.89748, -119.2624335)              225.04\nF (37.8762105, -119.3432825)            225.75\nG (37.833654, -119.4510805)             229.79\nH (38.0603395, -119.6666755)         229.04999\nI (37.798171, -119.19955150             225.29\nJ (38.0391175, -119.3073495)         227.23999\nName: 150, dtype: object\nTime                            3/1/2018 6:00\nA (38.152231, -119.6666755)         237.06999\nB (38.279274, -119.6127765)            236.45\nC (38.50458, -119.62176)               243.03\nD (37.862028, -119.6576925)            246.83\nE (37.89748, -119.2624335)             230.15\nF (37.8762105, -119.3432825)           231.33\nG (37.833654, -119.4510805)             236.5\nH (38.0603395, -119.6666755)        238.81999\nI (37.798171, -119.19955150            232.15\nJ (38.0391175, -119.3073495)           231.42\nName: 151, dtype: object\nTime                            3/2/2018 6:00\nA (38.152231, -119.6666755)            232.53\nB (38.279274, -119.6127765)            232.28\nC (38.50458, -119.62176)            238.37999\nD (37.862028, -119.6576925)            243.09\nE (37.89748, -119.2624335)             228.87\nF (37.8762105, -119.3432825)        229.29999\nG (37.833654, -119.4510805)         232.12999\nH (38.0603395, -119.6666755)           234.01\nI (37.798171, -119.19955150            230.83\nJ (38.0391175, -119.3073495)        229.79999\nName: 152, dtype: object\nTime                            3/3/2018 6:00\nA (38.152231, -119.6666755)         232.18999\nB (38.279274, -119.6127765)         231.81999\nC (38.50458, -119.62176)               238.03\nD (37.862028, -119.6576925)             242.4\nE (37.89748, -119.2624335)             224.98\nF (37.8762105, -119.3432825)           227.56\nG (37.833654, -119.4510805)            236.78\nH (38.0603395, -119.6666755)        233.48999\nI (37.798171, -119.19955150         227.79999\nJ (38.0391175, -119.3073495)        226.20999\nName: 153, dtype: object\nTime                            3/4/2018 6:00\nA (38.152231, -119.6666755)            228.56\nB (38.279274, -119.6127765)            228.17\nC (38.50458, -119.62176)            234.20999\nD (37.862028, -119.6576925)         240.15999\nE (37.89748, -119.2624335)             221.44\nF (37.8762105, -119.3432825)        224.95999\nG (37.833654, -119.4510805)            230.62\nH (38.0603395, -119.6666755)           230.51\nI (37.798171, -119.19955150            223.04\nJ (38.0391175, -119.3073495)           224.62\nName: 154, dtype: object\nTime                            3/5/2018 6:00\nA (38.152231, -119.6666755)             227.7\nB (38.279274, -119.6127765)            228.84\nC (38.50458, -119.62176)               232.43\nD (37.862028, -119.6576925)            237.45\nE (37.89748, -119.2624335)             222.61\nF (37.8762105, -119.3432825)        226.01999\nG (37.833654, -119.4510805)            231.12\nH (38.0603395, -119.6666755)           228.28\nI (37.798171, -119.19955150            224.22\nJ (38.0391175, -119.3073495)        224.51999\nName: 155, dtype: object\nTime                            3/6/2018 6:00\nA (38.152231, -119.6666755)            228.04\nB (38.279274, -119.6127765)            231.09\nC (38.50458, -119.62176)               234.26\nD (37.862028, -119.6576925)         238.98999\nE (37.89748, -119.2624335)             224.83\nF (37.8762105, -119.3432825)           227.97\nG (37.833654, -119.4510805)            234.87\nH (38.0603395, -119.6666755)        229.31999\nI (37.798171, -119.19955150             227.4\nJ (38.0391175, -119.3073495)           223.76\nName: 156, dtype: object\nTime                            3/7/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 157, dtype: object\nTime                            3/8/2018 6:00\nA (38.152231, -119.6666755)            232.84\nB (38.279274, -119.6127765)         234.59999\nC (38.50458, -119.62176)               241.28\nD (37.862028, -119.6576925)            239.58\nE (37.89748, -119.2624335)          229.26999\nF (37.8762105, -119.3432825)        230.76999\nG (37.833654, -119.4510805)         234.04999\nH (38.0603395, -119.6666755)           232.93\nI (37.798171, -119.19955150         230.95999\nJ (38.0391175, -119.3073495)        228.34999\nName: 158, dtype: object\nTime                            3/9/2018 6:00\nA (38.152231, -119.6666755)            230.18\nB (38.279274, -119.6127765)         231.98999\nC (38.50458, -119.62176)            237.65999\nD (37.862028, -119.6576925)            238.84\nE (37.89748, -119.2624335)             226.69\nF (37.8762105, -119.3432825)        227.40999\nG (37.833654, -119.4510805)         230.90999\nH (38.0603395, -119.6666755)        230.37999\nI (37.798171, -119.19955150         228.76999\nJ (38.0391175, -119.3073495)           227.09\nName: 159, dtype: object\nTime                            3/10/2018 6:00\nA (38.152231, -119.6666755)             231.33\nB (38.279274, -119.6127765)          231.06999\nC (38.50458, -119.62176)                239.06\nD (37.862028, -119.6576925)             241.42\nE (37.89748, -119.2624335)              229.86\nF (37.8762105, -119.3432825)         229.20999\nG (37.833654, -119.4510805)          232.51999\nH (38.0603395, -119.6666755)         232.90999\nI (37.798171, -119.19955150             231.53\nJ (38.0391175, -119.3073495)            230.42\nName: 160, dtype: object\nTime                            3/11/2018 6:00\nA (38.152231, -119.6666755)             228.84\nB (38.279274, -119.6127765)             229.06\nC (38.50458, -119.62176)                237.39\nD (37.862028, -119.6576925)             238.97\nE (37.89748, -119.2624335)              226.72\nF (37.8762105, -119.3432825)             227.0\nG (37.833654, -119.4510805)              232.7\nH (38.0603395, -119.6666755)            231.48\nI (37.798171, -119.19955150          228.70999\nJ (38.0391175, -119.3073495)         227.06999\nName: 161, dtype: object\nTime                            3/12/2018 6:00\nA (38.152231, -119.6666755)             229.94\nB (38.279274, -119.6127765)             228.79\nC (38.50458, -119.62176)             238.18999\nD (37.862028, -119.6576925)          244.31999\nE (37.89748, -119.2624335)           225.73999\nF (37.8762105, -119.3432825)            227.43\nG (37.833654, -119.4510805)          233.81999\nH (38.0603395, -119.6666755)         232.23999\nI (37.798171, -119.19955150             228.03\nJ (38.0391175, -119.3073495)            227.95\nName: 162, dtype: object\nTime                            3/13/2018 6:00\nA (38.152231, -119.6666755)             248.33\nB (38.279274, -119.6127765)          246.48999\nC (38.50458, -119.62176)                251.12\nD (37.862028, -119.6576925)             256.91\nE (37.89748, -119.2624335)              235.56\nF (37.8762105, -119.3432825)         240.04999\nG (37.833654, -119.4510805)          246.65999\nH (38.0603395, -119.6666755)            249.65\nI (37.798171, -119.19955150             239.22\nJ (38.0391175, -119.3073495)         238.34999\nName: 163, dtype: object\nTime                            3/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 164, dtype: object\nTime                            3/15/2018 6:00\nA (38.152231, -119.6666755)          234.81999\nB (38.279274, -119.6127765)             234.17\nC (38.50458, -119.62176)                240.09\nD (37.862028, -119.6576925)             241.26\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)          233.20999\nH (38.0603395, -119.6666755)            235.31\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)         227.09999\nName: 165, dtype: object\nTime                            3/16/2018 6:00\nA (38.152231, -119.6666755)          234.93999\nB (38.279274, -119.6127765)             232.54\nC (38.50458, -119.62176)                238.65\nD (37.862028, -119.6576925)             244.47\nE (37.89748, -119.2624335)              228.42\nF (37.8762105, -119.3432825)            230.14\nG (37.833654, -119.4510805)          234.73999\nH (38.0603395, -119.6666755)             236.5\nI (37.798171, -119.19955150             230.25\nJ (38.0391175, -119.3073495)             229.0\nName: 166, dtype: object\nTime                            3/17/2018 6:00\nA (38.152231, -119.6666755)             231.09\nB (38.279274, -119.6127765)             229.72\nC (38.50458, -119.62176)                233.65\nD (37.862028, -119.6576925)          239.79999\nE (37.89748, -119.2624335)              223.92\nF (37.8762105, -119.3432825)            225.97\nG (37.833654, -119.4510805)             231.97\nH (38.0603395, -119.6666755)            232.28\nI (37.798171, -119.19955150          225.29999\nJ (38.0391175, -119.3073495)            225.37\nName: 167, dtype: object\nTime                            3/18/2018 6:00\nA (38.152231, -119.6666755)             227.95\nB (38.279274, -119.6127765)              225.5\nC (38.50458, -119.62176)                230.95\nD (37.862028, -119.6576925)          238.98999\nE (37.89748, -119.2624335)              223.25\nF (37.8762105, -119.3432825)         225.54999\nG (37.833654, -119.4510805)              227.4\nH (38.0603395, -119.6666755)            229.89\nI (37.798171, -119.19955150             223.58\nJ (38.0391175, -119.3073495)         225.23999\nName: 168, dtype: object\nTime                            3/19/2018 6:00\nA (38.152231, -119.6666755)          228.34999\nB (38.279274, -119.6127765)             226.84\nC (38.50458, -119.62176)                234.04\nD (37.862028, -119.6576925)             238.76\nE (37.89748, -119.2624335)           224.29999\nF (37.8762105, -119.3432825)         225.59999\nG (37.833654, -119.4510805)             230.26\nH (38.0603395, -119.6666755)            230.15\nI (37.798171, -119.19955150             225.22\nJ (38.0391175, -119.3073495)            226.45\nName: 169, dtype: object\nTime                            3/20/2018 6:00\nA (38.152231, -119.6666755)          233.76999\nB (38.279274, -119.6127765)          230.26999\nC (38.50458, -119.62176)                238.31\nD (37.862028, -119.6576925)             246.42\nE (37.89748, -119.2624335)           228.65999\nF (37.8762105, -119.3432825)         231.51999\nG (37.833654, -119.4510805)          237.12999\nH (38.0603395, -119.6666755)            235.33\nI (37.798171, -119.19955150             229.31\nJ (38.0391175, -119.3073495)         229.87999\nName: 170, dtype: object\nTime                            3/21/2018 6:00\nA (38.152231, -119.6666755)          245.95999\nB (38.279274, -119.6127765)             243.37\nC (38.50458, -119.62176)             249.37999\nD (37.862028, -119.6576925)          256.50998\nE (37.89748, -119.2624335)              237.45\nF (37.8762105, -119.3432825)            240.28\nG (37.833654, -119.4510805)             247.58\nH (38.0603395, -119.6666755)         248.37999\nI (37.798171, -119.19955150             239.01\nJ (38.0391175, -119.3073495)            237.87\nName: 171, dtype: object\nTime                            3/22/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 172, dtype: object\nTime                            3/23/2018 6:00\nA (38.152231, -119.6666755)             238.58\nB (38.279274, -119.6127765)             237.51\nC (38.50458, -119.62176)                242.33\nD (37.862028, -119.6576925)             244.98\nE (37.89748, -119.2624335)              230.76\nF (37.8762105, -119.3432825)            233.18\nG (37.833654, -119.4510805)          238.06999\nH (38.0603395, -119.6666755)            239.31\nI (37.798171, -119.19955150             233.25\nJ (38.0391175, -119.3073495)            229.51\nName: 173, dtype: object\nTime                            3/24/2018 6:00\nA (38.152231, -119.6666755)             230.14\nB (38.279274, -119.6127765)             230.51\nC (38.50458, -119.62176)                236.68\nD (37.862028, -119.6576925)          237.37999\nE (37.89748, -119.2624335)              225.09\nF (37.8762105, -119.3432825)            226.83\nG (37.833654, -119.4510805)          231.09999\nH (38.0603395, -119.6666755)            231.01\nI (37.798171, -119.19955150             227.51\nJ (38.0391175, -119.3073495)            224.47\nName: 174, dtype: object\nTime                            3/25/2018 6:00\nA (38.152231, -119.6666755)             228.15\nB (38.279274, -119.6127765)             228.03\nC (38.50458, -119.62176)                232.56\nD (37.862028, -119.6576925)          236.20999\nE (37.89748, -119.2624335)              223.47\nF (37.8762105, -119.3432825)            226.53\nG (37.833654, -119.4510805)             232.86\nH (38.0603395, -119.6666755)            230.09\nI (37.798171, -119.19955150             225.62\nJ (38.0391175, -119.3073495)            222.89\nName: 175, dtype: object\nTime                            3/26/2018 6:00\nA (38.152231, -119.6666755)             225.06\nB (38.279274, -119.6127765)          224.56999\nC (38.50458, -119.62176)                230.26\nD (37.862028, -119.6576925)             234.98\nE (37.89748, -119.2624335)              219.72\nF (37.8762105, -119.3432825)         223.23999\nG (37.833654, -119.4510805)             229.72\nH (38.0603395, -119.6666755)         227.31999\nI (37.798171, -119.19955150             221.34\nJ (38.0391175, -119.3073495)            220.87\nName: 176, dtype: object\nTime                            3/27/2018 6:00\nA (38.152231, -119.6666755)          228.34999\nB (38.279274, -119.6127765)          225.76999\nC (38.50458, -119.62176)                233.84\nD (37.862028, -119.6576925)             242.09\nE (37.89748, -119.2624335)           225.23999\nF (37.8762105, -119.3432825)            228.29\nG (37.833654, -119.4510805)             233.97\nH (38.0603395, -119.6666755)            230.58\nI (37.798171, -119.19955150          226.01999\nJ (38.0391175, -119.3073495)            224.81\nName: 177, dtype: object\nTime                            3/28/2018 6:00\nA (38.152231, -119.6666755)             229.09\nB (38.279274, -119.6127765)             228.59\nC (38.50458, -119.62176)             237.87999\nD (37.862028, -119.6576925)          245.54999\nE (37.89748, -119.2624335)           226.26999\nF (37.8762105, -119.3432825)             228.9\nG (37.833654, -119.4510805)          236.76999\nH (38.0603395, -119.6666755)            232.45\nI (37.798171, -119.19955150             227.56\nJ (38.0391175, -119.3073495)            225.39\nName: 178, dtype: object\nTime                            3/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 179, dtype: object\nTime                            3/30/2018 6:00\nA (38.152231, -119.6666755)          232.87999\nB (38.279274, -119.6127765)          231.37999\nC (38.50458, -119.62176)             242.15999\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 180, dtype: object\nTime                            3/31/2018 6:00\nA (38.152231, -119.6666755)             226.98\nB (38.279274, -119.6127765)             227.94\nC (38.50458, -119.62176)                242.09\nD (37.862028, -119.6576925)             240.53\nE (37.89748, -119.2624335)           226.62999\nF (37.8762105, -119.3432825)            227.34\nG (37.833654, -119.4510805)             230.06\nH (38.0603395, -119.6666755)            228.68\nI (37.798171, -119.19955150             229.78\nJ (38.0391175, -119.3073495)            226.62\nName: 181, dtype: object\nTime                            4/1/2018 6:00\nA (38.152231, -119.6666755)             225.2\nB (38.279274, -119.6127765)         224.31999\nC (38.50458, -119.62176)               238.03\nD (37.862028, -119.6576925)         239.15999\nE (37.89748, -119.2624335)          225.48999\nF (37.8762105, -119.3432825)           226.58\nG (37.833654, -119.4510805)            228.45\nH (38.0603395, -119.6666755)           227.61\nI (37.798171, -119.19955150         226.84999\nJ (38.0391175, -119.3073495)           226.56\nName: 182, dtype: object\nTime                            4/2/2018 6:00\nA (38.152231, -119.6666755)         224.62999\nB (38.279274, -119.6127765)            226.37\nC (38.50458, -119.62176)               239.83\nD (37.862028, -119.6576925)         238.04999\nE (37.89748, -119.2624335)             221.03\nF (37.8762105, -119.3432825)        221.73999\nG (37.833654, -119.4510805)         228.15999\nH (38.0603395, -119.6666755)           227.26\nI (37.798171, -119.19955150            224.23\nJ (38.0391175, -119.3073495)        223.01999\nName: 183, dtype: object\nTime                            4/3/2018 6:00\nA (38.152231, -119.6666755)            222.17\nB (38.279274, -119.6127765)            222.23\nC (38.50458, -119.62176)            233.45999\nD (37.862028, -119.6576925)            241.15\nE (37.89748, -119.2624335)             217.44\nF (37.8762105, -119.3432825)        220.54999\nG (37.833654, -119.4510805)            230.08\nH (38.0603395, -119.6666755)            227.2\nI (37.798171, -119.19955150            219.73\nJ (38.0391175, -119.3073495)           222.26\nName: 184, dtype: object\nTime                            4/4/2018 6:00\nA (38.152231, -119.6666755)            225.51\nB (38.279274, -119.6127765)            223.95\nC (38.50458, -119.62176)            239.98999\nD (37.862028, -119.6576925)            243.48\nE (37.89748, -119.2624335)          223.54999\nF (37.8762105, -119.3432825)           224.79\nG (37.833654, -119.4510805)            230.36\nH (38.0603395, -119.6666755)           228.43\nI (37.798171, -119.19955150             223.9\nJ (38.0391175, -119.3073495)           225.81\nName: 185, dtype: object\nTime                            4/5/2018 6:00\nA (38.152231, -119.6666755)            239.76\nB (38.279274, -119.6127765)         240.04999\nC (38.50458, -119.62176)               251.81\nD (37.862028, -119.6576925)            253.42\nE (37.89748, -119.2624335)          233.20999\nF (37.8762105, -119.3432825)        235.20999\nG (37.833654, -119.4510805)         242.93999\nH (38.0603395, -119.6666755)           241.51\nI (37.798171, -119.19955150         234.26999\nJ (38.0391175, -119.3073495)        233.45999\nName: 186, dtype: object\nTime                            4/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 187, dtype: object\nTime                            4/7/2018 6:00\nA (38.152231, -119.6666755)            248.17\nB (38.279274, -119.6127765)            250.08\nC (38.50458, -119.62176)            254.23999\nD (37.862028, -119.6576925)         249.23999\nE (37.89748, -119.2624335)             246.68\nF (37.8762105, -119.3432825)           248.58\nG (37.833654, -119.4510805)         250.09999\nH (38.0603395, -119.6666755)        247.93999\nI (37.798171, -119.19955150         250.20999\nJ (38.0391175, -119.3073495)           245.76\nName: 188, dtype: object\nTime                            4/8/2018 6:00\nA (38.152231, -119.6666755)         221.34999\nB (38.279274, -119.6127765)            227.53\nC (38.50458, -119.62176)               242.95\nD (37.862028, -119.6576925)         235.31999\nE (37.89748, -119.2624335)             222.34\nF (37.8762105, -119.3432825)        220.76999\nG (37.833654, -119.4510805)            223.79\nH (38.0603395, -119.6666755)           222.47\nI (37.798171, -119.19955150            225.39\nJ (38.0391175, -119.3073495)           224.53\nName: 189, dtype: object\nTime                            4/9/2018 6:00\nA (38.152231, -119.6666755)         224.37999\nB (38.279274, -119.6127765)         227.95999\nC (38.50458, -119.62176)               243.48\nD (37.862028, -119.6576925)            244.22\nE (37.89748, -119.2624335)             224.11\nF (37.8762105, -119.3432825)        223.62999\nG (37.833654, -119.4510805)         226.29999\nH (38.0603395, -119.6666755)           228.09\nI (37.798171, -119.19955150            227.92\nJ (38.0391175, -119.3073495)           226.98\nName: 190, dtype: object\nTime                            4/10/2018 6:00\nA (38.152231, -119.6666755)             234.47\nB (38.279274, -119.6127765)             235.39\nC (38.50458, -119.62176)             249.62999\nD (37.862028, -119.6576925)              249.7\nE (37.89748, -119.2624335)              227.84\nF (37.8762105, -119.3432825)            226.65\nG (37.833654, -119.4510805)             234.78\nH (38.0603395, -119.6666755)         235.93999\nI (37.798171, -119.19955150             232.75\nJ (38.0391175, -119.3073495)            231.51\nName: 191, dtype: object\nTime                            4/11/2018 6:00\nA (38.152231, -119.6666755)             236.25\nB (38.279274, -119.6127765)             237.64\nC (38.50458, -119.62176)                248.76\nD (37.862028, -119.6576925)             249.26\nE (37.89748, -119.2624335)              225.03\nF (37.8762105, -119.3432825)            226.01\nG (37.833654, -119.4510805)             235.65\nH (38.0603395, -119.6666755)            237.93\nI (37.798171, -119.19955150              227.0\nJ (38.0391175, -119.3073495)            230.33\nName: 192, dtype: object\nTime                            4/12/2018 6:00\nA (38.152231, -119.6666755)             224.87\nB (38.279274, -119.6127765)             227.34\nC (38.50458, -119.62176)                240.48\nD (37.862028, -119.6576925)          241.45999\nE (37.89748, -119.2624335)              220.45\nF (37.8762105, -119.3432825)            221.19\nG (37.833654, -119.4510805)          228.29999\nH (38.0603395, -119.6666755)            228.36\nI (37.798171, -119.19955150          222.48999\nJ (38.0391175, -119.3073495)            221.89\nName: 193, dtype: object\nTime                            4/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              219.03\nF (37.8762105, -119.3432825)         219.34999\nG (37.833654, -119.4510805)             220.44\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             223.98\nJ (38.0391175, -119.3073495)         218.06999\nName: 194, dtype: object\nTime                            4/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 195, dtype: object\nTime                            4/15/2018 6:00\nA (38.152231, -119.6666755)          225.48999\nB (38.279274, -119.6127765)          233.62999\nC (38.50458, -119.62176)                 249.7\nD (37.862028, -119.6576925)             241.51\nE (37.89748, -119.2624335)              224.23\nF (37.8762105, -119.3432825)         222.48999\nG (37.833654, -119.4510805)             225.75\nH (38.0603395, -119.6666755)            227.03\nI (37.798171, -119.19955150             227.72\nJ (38.0391175, -119.3073495)            226.23\nName: 196, dtype: object\nTime                            4/16/2018 6:00\nA (38.152231, -119.6666755)          231.01999\nB (38.279274, -119.6127765)          234.68999\nC (38.50458, -119.62176)                244.65\nD (37.862028, -119.6576925)             241.62\nE (37.89748, -119.2624335)              224.76\nF (37.8762105, -119.3432825)            223.17\nG (37.833654, -119.4510805)          226.65999\nH (38.0603395, -119.6666755)         231.54999\nI (37.798171, -119.19955150             227.83\nJ (38.0391175, -119.3073495)         226.76999\nName: 197, dtype: object\nTime                            4/17/2018 6:00\nA (38.152231, -119.6666755)             224.06\nB (38.279274, -119.6127765)             227.29\nC (38.50458, -119.62176)                239.97\nD (37.862028, -119.6576925)          236.06999\nE (37.89748, -119.2624335)              220.19\nF (37.8762105, -119.3432825)            220.51\nG (37.833654, -119.4510805)             222.67\nH (38.0603395, -119.6666755)            227.09\nI (37.798171, -119.19955150             221.54\nJ (38.0391175, -119.3073495)            223.11\nName: 198, dtype: object\nTime                            4/18/2018 6:00\nA (38.152231, -119.6666755)             223.65\nB (38.279274, -119.6127765)             227.26\nC (38.50458, -119.62176)             242.84999\nD (37.862028, -119.6576925)             242.29\nE (37.89748, -119.2624335)              220.67\nF (37.8762105, -119.3432825)            219.86\nG (37.833654, -119.4510805)          221.56999\nH (38.0603395, -119.6666755)         225.26999\nI (37.798171, -119.19955150          225.09999\nJ (38.0391175, -119.3073495)            224.45\nName: 199, dtype: object\nTime                            4/19/2018 6:00\nA (38.152231, -119.6666755)             221.58\nB (38.279274, -119.6127765)          224.37999\nC (38.50458, -119.62176)                239.04\nD (37.862028, -119.6576925)             236.93\nE (37.89748, -119.2624335)              218.76\nF (37.8762105, -119.3432825)            220.09\nG (37.833654, -119.4510805)             225.51\nH (38.0603395, -119.6666755)            224.61\nI (37.798171, -119.19955150          219.76999\nJ (38.0391175, -119.3073495)            221.17\nName: 200, dtype: object\nTime                            4/20/2018 6:00\nA (38.152231, -119.6666755)          226.12999\nB (38.279274, -119.6127765)          227.65999\nC (38.50458, -119.62176)                 243.0\nD (37.862028, -119.6576925)             246.47\nE (37.89748, -119.2624335)              221.28\nF (37.8762105, -119.3432825)            223.83\nG (37.833654, -119.4510805)              231.4\nH (38.0603395, -119.6666755)            228.61\nI (37.798171, -119.19955150          225.12999\nJ (38.0391175, -119.3073495)            222.84\nName: 201, dtype: object\nTime                            4/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 202, dtype: object\nTime                            4/22/2018 6:00\nA (38.152231, -119.6666755)             237.83\nB (38.279274, -119.6127765)              246.0\nC (38.50458, -119.62176)                258.57\nD (37.862028, -119.6576925)             247.36\nE (37.89748, -119.2624335)              234.22\nF (37.8762105, -119.3432825)            233.65\nG (37.833654, -119.4510805)          236.59999\nH (38.0603395, -119.6666755)         238.20999\nI (37.798171, -119.19955150             233.89\nJ (38.0391175, -119.3073495)            238.56\nName: 203, dtype: object\nTime                            4/23/2018 6:00\nA (38.152231, -119.6666755)             232.67\nB (38.279274, -119.6127765)          238.54999\nC (38.50458, -119.62176)                252.28\nD (37.862028, -119.6576925)          244.34999\nE (37.89748, -119.2624335)              230.83\nF (37.8762105, -119.3432825)            229.51\nG (37.833654, -119.4510805)          231.56999\nH (38.0603395, -119.6666755)         233.34999\nI (37.798171, -119.19955150          233.06999\nJ (38.0391175, -119.3073495)            233.31\nName: 204, dtype: object\nTime                            4/24/2018 6:00\nA (38.152231, -119.6666755)          234.93999\nB (38.279274, -119.6127765)             238.22\nC (38.50458, -119.62176)                251.97\nD (37.862028, -119.6576925)          250.01999\nE (37.89748, -119.2624335)           230.12999\nF (37.8762105, -119.3432825)            227.86\nG (37.833654, -119.4510805)             236.09\nH (38.0603395, -119.6666755)            236.39\nI (37.798171, -119.19955150             234.65\nJ (38.0391175, -119.3073495)            232.18\nName: 205, dtype: object\nTime                            4/25/2018 6:00\nA (38.152231, -119.6666755)             236.01\nB (38.279274, -119.6127765)          238.87999\nC (38.50458, -119.62176)                252.61\nD (37.862028, -119.6576925)             248.48\nE (37.89748, -119.2624335)              229.04\nF (37.8762105, -119.3432825)         231.87999\nG (37.833654, -119.4510805)             239.14\nH (38.0603395, -119.6666755)         237.73999\nI (37.798171, -119.19955150          229.81999\nJ (38.0391175, -119.3073495)            233.53\nName: 206, dtype: object\nTime                            4/26/2018 6:00\nA (38.152231, -119.6666755)          240.68999\nB (38.279274, -119.6127765)             242.65\nC (38.50458, -119.62176)                255.68\nD (37.862028, -119.6576925)             256.43\nE (37.89748, -119.2624335)              234.56\nF (37.8762105, -119.3432825)            235.64\nG (37.833654, -119.4510805)             241.03\nH (38.0603395, -119.6666755)            242.62\nI (37.798171, -119.19955150             236.12\nJ (38.0391175, -119.3073495)            238.79\nName: 207, dtype: object\nTime                            4/27/2018 6:00\nA (38.152231, -119.6666755)             253.51\nB (38.279274, -119.6127765)              254.4\nC (38.50458, -119.62176)                260.62\nD (37.862028, -119.6576925)              258.8\nE (37.89748, -119.2624335)              238.92\nF (37.8762105, -119.3432825)             240.9\nG (37.833654, -119.4510805)             247.62\nH (38.0603395, -119.6666755)            253.83\nI (37.798171, -119.19955150             241.78\nJ (38.0391175, -119.3073495)            242.68\nName: 208, dtype: object\nTime                            4/28/2018 6:00\nA (38.152231, -119.6666755)             230.42\nB (38.279274, -119.6127765)             238.12\nC (38.50458, -119.62176)             252.59999\nD (37.862028, -119.6576925)             249.79\nE (37.89748, -119.2624335)              230.03\nF (37.8762105, -119.3432825)            233.61\nG (37.833654, -119.4510805)             240.01\nH (38.0603395, -119.6666755)            230.86\nI (37.798171, -119.19955150             229.81\nJ (38.0391175, -119.3073495)         230.15999\nName: 209, dtype: object\nTime                            4/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 210, dtype: object\nTime                            4/30/2018 6:00\nA (38.152231, -119.6666755)          227.81999\nB (38.279274, -119.6127765)             235.65\nC (38.50458, -119.62176)                251.43\nD (37.862028, -119.6576925)             240.84\nE (37.89748, -119.2624335)              227.23\nF (37.8762105, -119.3432825)         224.95999\nG (37.833654, -119.4510805)             228.86\nH (38.0603395, -119.6666755)            227.81\nI (37.798171, -119.19955150          229.34999\nJ (38.0391175, -119.3073495)         229.98999\nName: 211, dtype: object\nTime                            5/1/2018 6:00\nA (38.152231, -119.6666755)            231.59\nB (38.279274, -119.6127765)            236.11\nC (38.50458, -119.62176)            251.26999\nD (37.862028, -119.6576925)            245.75\nE (37.89748, -119.2624335)             231.93\nF (37.8762105, -119.3432825)        230.76999\nG (37.833654, -119.4510805)         234.76999\nH (38.0603395, -119.6666755)           233.23\nI (37.798171, -119.19955150            234.97\nJ (38.0391175, -119.3073495)           233.76\nName: 212, dtype: object\nTime                            5/2/2018 6:00\nA (38.152231, -119.6666755)            230.48\nB (38.279274, -119.6127765)            236.33\nC (38.50458, -119.62176)            251.43999\nD (37.862028, -119.6576925)            245.26\nE (37.89748, -119.2624335)             225.62\nF (37.8762105, -119.3432825)            225.0\nG (37.833654, -119.4510805)            230.93\nH (38.0603395, -119.6666755)           233.42\nI (37.798171, -119.19955150         230.37999\nJ (38.0391175, -119.3073495)           230.97\nName: 213, dtype: object\nTime                            5/3/2018 6:00\nA (38.152231, -119.6666755)            234.28\nB (38.279274, -119.6127765)            238.58\nC (38.50458, -119.62176)            250.68999\nD (37.862028, -119.6576925)         247.98999\nE (37.89748, -119.2624335)          225.79999\nF (37.8762105, -119.3432825)           226.97\nG (37.833654, -119.4510805)         238.15999\nH (38.0603395, -119.6666755)        235.79999\nI (37.798171, -119.19955150         227.56999\nJ (38.0391175, -119.3073495)        231.95999\nName: 214, dtype: object\nTime                            5/4/2018 6:00\nA (38.152231, -119.6666755)            239.76\nB (38.279274, -119.6127765)         242.68999\nC (38.50458, -119.62176)            254.76999\nD (37.862028, -119.6576925)         255.93999\nE (37.89748, -119.2624335)             230.29\nF (37.8762105, -119.3432825)           234.14\nG (37.833654, -119.4510805)            242.42\nH (38.0603395, -119.6666755)           242.26\nI (37.798171, -119.19955150         234.95999\nJ (38.0391175, -119.3073495)        236.01999\nName: 215, dtype: object\nTime                            5/5/2018 6:00\nA (38.152231, -119.6666755)         248.56999\nB (38.279274, -119.6127765)         252.01999\nC (38.50458, -119.62176)               261.15\nD (37.862028, -119.6576925)            260.66\nE (37.89748, -119.2624335)          236.84999\nF (37.8762105, -119.3432825)           238.22\nG (37.833654, -119.4510805)         249.43999\nH (38.0603395, -119.6666755)        249.37999\nI (37.798171, -119.19955150            237.53\nJ (38.0391175, -119.3073495)        241.09999\nName: 216, dtype: object\nTime                            5/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 217, dtype: object\nTime                            5/7/2018 6:00\nA (38.152231, -119.6666755)         250.54999\nB (38.279274, -119.6127765)         255.48999\nC (38.50458, -119.62176)               264.72\nD (37.862028, -119.6576925)            256.54\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)            247.42\nH (38.0603395, -119.6666755)        250.51999\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)        241.06999\nName: 218, dtype: object\nTime                            5/8/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 219, dtype: object\nTime                            5/9/2018 6:00\nA (38.152231, -119.6666755)            255.47\nB (38.279274, -119.6127765)            258.32\nC (38.50458, -119.62176)            264.66998\nD (37.862028, -119.6576925)         261.16998\nE (37.89748, -119.2624335)             247.01\nF (37.8762105, -119.3432825)           249.72\nG (37.833654, -119.4510805)         254.59999\nH (38.0603395, -119.6666755)           255.75\nI (37.798171, -119.19955150            249.17\nJ (38.0391175, -119.3073495)        250.34999\nName: 220, dtype: object\nTime                            5/10/2018 6:00\nA (38.152231, -119.6666755)             251.29\nB (38.279274, -119.6127765)             253.65\nC (38.50458, -119.62176)                261.72\nD (37.862028, -119.6576925)             259.37\nE (37.89748, -119.2624335)              243.01\nF (37.8762105, -119.3432825)            243.31\nG (37.833654, -119.4510805)             246.61\nH (38.0603395, -119.6666755)            250.73\nI (37.798171, -119.19955150             245.87\nJ (38.0391175, -119.3073495)         246.79999\nName: 221, dtype: object\nTime                            5/11/2018 6:00\nA (38.152231, -119.6666755)             250.37\nB (38.279274, -119.6127765)          252.73999\nC (38.50458, -119.62176)             259.00998\nD (37.862028, -119.6576925)             257.33\nE (37.89748, -119.2624335)              237.67\nF (37.8762105, -119.3432825)         240.45999\nG (37.833654, -119.4510805)             249.54\nH (38.0603395, -119.6666755)         251.43999\nI (37.798171, -119.19955150             240.59\nJ (38.0391175, -119.3073495)         244.29999\nName: 222, dtype: object\nTime                            5/12/2018 6:00\nA (38.152231, -119.6666755)          241.68999\nB (38.279274, -119.6127765)              245.2\nC (38.50458, -119.62176)                 254.9\nD (37.862028, -119.6576925)             255.58\nE (37.89748, -119.2624335)           233.98999\nF (37.8762105, -119.3432825)         236.01999\nG (37.833654, -119.4510805)          244.81999\nH (38.0603395, -119.6666755)            244.03\nI (37.798171, -119.19955150             236.51\nJ (38.0391175, -119.3073495)            238.61\nName: 223, dtype: object\nTime                            5/13/2018 6:00\nA (38.152231, -119.6666755)          252.37999\nB (38.279274, -119.6127765)             253.62\nC (38.50458, -119.62176)                259.59\nD (37.862028, -119.6576925)             260.04\nE (37.89748, -119.2624335)              244.84\nF (37.8762105, -119.3432825)         246.81999\nG (37.833654, -119.4510805)          252.59999\nH (38.0603395, -119.6666755)         252.59999\nI (37.798171, -119.19955150             244.95\nJ (38.0391175, -119.3073495)         246.26999\nName: 224, dtype: object\nTime                            5/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 225, dtype: object\nTime                            5/15/2018 6:00\nA (38.152231, -119.6666755)             252.76\nB (38.279274, -119.6127765)             258.03\nC (38.50458, -119.62176)                265.36\nD (37.862028, -119.6576925)             257.66\nE (37.89748, -119.2624335)               249.4\nF (37.8762105, -119.3432825)            251.03\nG (37.833654, -119.4510805)             253.65\nH (38.0603395, -119.6666755)         252.76999\nI (37.798171, -119.19955150             251.18\nJ (38.0391175, -119.3073495)            249.86\nName: 226, dtype: object\nTime                            5/16/2018 6:00\nA (38.152231, -119.6666755)             248.62\nB (38.279274, -119.6127765)             252.79\nC (38.50458, -119.62176)                261.82\nD (37.862028, -119.6576925)             254.04\nE (37.89748, -119.2624335)              241.79\nF (37.8762105, -119.3432825)            240.84\nG (37.833654, -119.4510805)          244.54999\nH (38.0603395, -119.6666755)            248.22\nI (37.798171, -119.19955150             243.68\nJ (38.0391175, -119.3073495)         245.37999\nName: 227, dtype: object\nTime                            5/17/2018 6:00\nA (38.152231, -119.6666755)             244.01\nB (38.279274, -119.6127765)             248.25\nC (38.50458, -119.62176)             257.88998\nD (37.862028, -119.6576925)          251.06999\nE (37.89748, -119.2624335)           233.43999\nF (37.8762105, -119.3432825)         236.20999\nG (37.833654, -119.4510805)              243.2\nH (38.0603395, -119.6666755)         243.54999\nI (37.798171, -119.19955150             234.59\nJ (38.0391175, -119.3073495)            239.04\nName: 228, dtype: object\nTime                            5/18/2018 6:00\nA (38.152231, -119.6666755)              252.2\nB (38.279274, -119.6127765)          255.51999\nC (38.50458, -119.62176)                262.84\nD (37.862028, -119.6576925)          260.44998\nE (37.89748, -119.2624335)              245.22\nF (37.8762105, -119.3432825)            246.33\nG (37.833654, -119.4510805)             246.86\nH (38.0603395, -119.6666755)            252.43\nI (37.798171, -119.19955150             246.14\nJ (38.0391175, -119.3073495)         250.54999\nName: 229, dtype: object\nTime                            5/19/2018 6:00\nA (38.152231, -119.6666755)          248.51999\nB (38.279274, -119.6127765)          252.68999\nC (38.50458, -119.62176)             261.91998\nD (37.862028, -119.6576925)          259.75998\nE (37.89748, -119.2624335)           247.04999\nF (37.8762105, -119.3432825)            248.78\nG (37.833654, -119.4510805)          253.81999\nH (38.0603395, -119.6666755)            250.03\nI (37.798171, -119.19955150             248.59\nJ (38.0391175, -119.3073495)         249.79999\nName: 230, dtype: object\nTime                            5/20/2018 6:00\nA (38.152231, -119.6666755)             251.01\nB (38.279274, -119.6127765)             254.31\nC (38.50458, -119.62176)             262.25998\nD (37.862028, -119.6576925)              260.1\nE (37.89748, -119.2624335)              240.47\nF (37.8762105, -119.3432825)            242.29\nG (37.833654, -119.4510805)             250.26\nH (38.0603395, -119.6666755)         251.87999\nI (37.798171, -119.19955150             241.78\nJ (38.0391175, -119.3073495)             244.9\nName: 231, dtype: object\nTime                            5/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 232, dtype: object\nTime                            5/22/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                266.34\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 233, dtype: object\nTime                            5/23/2018 6:00\nA (38.152231, -119.6666755)          257.97998\nB (38.279274, -119.6127765)          261.16998\nC (38.50458, -119.62176)                265.72\nD (37.862028, -119.6576925)          259.94998\nE (37.89748, -119.2624335)              252.18\nF (37.8762105, -119.3432825)            253.92\nG (37.833654, -119.4510805)             256.55\nH (38.0603395, -119.6666755)            257.83\nI (37.798171, -119.19955150             253.31\nJ (38.0391175, -119.3073495)         252.98999\nName: 234, dtype: object\nTime                            5/24/2018 6:00\nA (38.152231, -119.6666755)             258.07\nB (38.279274, -119.6127765)          261.13998\nC (38.50458, -119.62176)                266.81\nD (37.862028, -119.6576925)             260.74\nE (37.89748, -119.2624335)           250.26999\nF (37.8762105, -119.3432825)             252.0\nG (37.833654, -119.4510805)          254.15999\nH (38.0603395, -119.6666755)            257.33\nI (37.798171, -119.19955150             249.92\nJ (38.0391175, -119.3073495)            253.39\nName: 235, dtype: object\nTime                            5/25/2018 6:00\nA (38.152231, -119.6666755)             255.76\nB (38.279274, -119.6127765)              258.1\nC (38.50458, -119.62176)             260.19998\nD (37.862028, -119.6576925)             258.08\nE (37.89748, -119.2624335)           244.65999\nF (37.8762105, -119.3432825)         247.29999\nG (37.833654, -119.4510805)          254.23999\nH (38.0603395, -119.6666755)            255.83\nI (37.798171, -119.19955150             246.37\nJ (38.0391175, -119.3073495)         248.51999\nName: 236, dtype: object\nTime                            5/26/2018 6:00\nA (38.152231, -119.6666755)             248.84\nB (38.279274, -119.6127765)          252.26999\nC (38.50458, -119.62176)                260.07\nD (37.862028, -119.6576925)             256.56\nE (37.89748, -119.2624335)              241.03\nF (37.8762105, -119.3432825)         242.56999\nG (37.833654, -119.4510805)          244.51999\nH (38.0603395, -119.6666755)            248.61\nI (37.798171, -119.19955150             242.28\nJ (38.0391175, -119.3073495)             246.5\nName: 237, dtype: object\nTime                            5/27/2018 6:00\nA (38.152231, -119.6666755)          253.04999\nB (38.279274, -119.6127765)          255.81999\nC (38.50458, -119.62176)                262.16\nD (37.862028, -119.6576925)             260.59\nE (37.89748, -119.2624335)           242.56999\nF (37.8762105, -119.3432825)            245.68\nG (37.833654, -119.4510805)             251.79\nH (38.0603395, -119.6666755)            253.25\nI (37.798171, -119.19955150             245.34\nJ (38.0391175, -119.3073495)         248.93999\nName: 238, dtype: object\nTime                            5/28/2018 6:00\nA (38.152231, -119.6666755)          256.72998\nB (38.279274, -119.6127765)             259.86\nC (38.50458, -119.62176)                265.53\nD (37.862028, -119.6576925)             264.37\nE (37.89748, -119.2624335)           247.23999\nF (37.8762105, -119.3432825)            250.39\nG (37.833654, -119.4510805)             256.58\nH (38.0603395, -119.6666755)            258.13\nI (37.798171, -119.19955150          250.09999\nJ (38.0391175, -119.3073495)         250.01999\nName: 239, dtype: object\nTime                            5/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 240, dtype: object\nTime                            5/30/2018 6:00\nA (38.152231, -119.6666755)          260.50998\nB (38.279274, -119.6127765)             265.27\nC (38.50458, -119.62176)                270.56\nD (37.862028, -119.6576925)             263.03\nE (37.89748, -119.2624335)              252.84\nF (37.8762105, -119.3432825)            254.68\nG (37.833654, -119.4510805)             257.88\nH (38.0603395, -119.6666755)            259.75\nI (37.798171, -119.19955150          255.37999\nJ (38.0391175, -119.3073495)            254.28\nName: 241, dtype: object\nTime                            5/31/2018 6:00\nA (38.152231, -119.6666755)             256.66\nB (38.279274, -119.6127765)             260.12\nC (38.50458, -119.62176)                265.31\nD (37.862028, -119.6576925)             260.43\nE (37.89748, -119.2624335)              249.04\nF (37.8762105, -119.3432825)            250.86\nG (37.833654, -119.4510805)             254.79\nH (38.0603395, -119.6666755)            256.15\nI (37.798171, -119.19955150              250.2\nJ (38.0391175, -119.3073495)            251.43\nName: 242, dtype: object\nTime                            6/1/2018 6:00\nA (38.152231, -119.6666755)            256.37\nB (38.279274, -119.6127765)         259.16998\nC (38.50458, -119.62176)               263.81\nD (37.862028, -119.6576925)            261.25\nE (37.89748, -119.2624335)             248.95\nF (37.8762105, -119.3432825)        250.31999\nG (37.833654, -119.4510805)         251.98999\nH (38.0603395, -119.6666755)        256.41998\nI (37.798171, -119.19955150            249.64\nJ (38.0391175, -119.3073495)           252.45\nName: 243, dtype: object\nTime                            6/2/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 244, dtype: object\nTime                            6/3/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 245, dtype: object\nTime                            6/4/2018 6:00\nA (38.152231, -119.6666755)            261.05\nB (38.279274, -119.6127765)            263.11\nC (38.50458, -119.62176)                269.1\nD (37.862028, -119.6576925)         267.69998\nE (37.89748, -119.2624335)             250.37\nF (37.8762105, -119.3432825)           253.81\nG (37.833654, -119.4510805)            261.02\nH (38.0603395, -119.6666755)           261.41\nI (37.798171, -119.19955150         253.84999\nJ (38.0391175, -119.3073495)        253.29999\nName: 246, dtype: object\nTime                            6/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)             246.86\nF (37.8762105, -119.3432825)           239.28\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150            255.86\nJ (38.0391175, -119.3073495)              NaN\nName: 247, dtype: object\nTime                            6/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 248, dtype: object\nTime                            6/7/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 249, dtype: object\nTime                            6/8/2018 6:00\nA (38.152231, -119.6666755)         260.41998\nB (38.279274, -119.6127765)         263.97998\nC (38.50458, -119.62176)               267.22\nD (37.862028, -119.6576925)            264.74\nE (37.89748, -119.2624335)             254.65\nF (37.8762105, -119.3432825)        256.38998\nG (37.833654, -119.4510805)            259.33\nH (38.0603395, -119.6666755)            260.1\nI (37.798171, -119.19955150            256.08\nJ (38.0391175, -119.3073495)            256.1\nName: 250, dtype: object\nTime                            6/9/2018 6:00\nA (38.152231, -119.6666755)            260.56\nB (38.279274, -119.6127765)            263.04\nC (38.50458, -119.62176)                267.9\nD (37.862028, -119.6576925)            264.85\nE (37.89748, -119.2624335)              253.7\nF (37.8762105, -119.3432825)           256.32\nG (37.833654, -119.4510805)            257.24\nH (38.0603395, -119.6666755)           260.82\nI (37.798171, -119.19955150            254.29\nJ (38.0391175, -119.3073495)        258.38998\nName: 251, dtype: object\nTime                            6/10/2018 6:00\nA (38.152231, -119.6666755)          256.63998\nB (38.279274, -119.6127765)             259.22\nC (38.50458, -119.62176)                263.21\nD (37.862028, -119.6576925)              261.3\nE (37.89748, -119.2624335)              247.08\nF (37.8762105, -119.3432825)             248.5\nG (37.833654, -119.4510805)             255.31\nH (38.0603395, -119.6666755)         257.50998\nI (37.798171, -119.19955150             250.04\nJ (38.0391175, -119.3073495)         249.76999\nName: 252, dtype: object\nTime                            6/11/2018 6:00\nA (38.152231, -119.6666755)              259.6\nB (38.279274, -119.6127765)             263.05\nC (38.50458, -119.62176)                267.85\nD (37.862028, -119.6576925)              265.8\nE (37.89748, -119.2624335)              251.65\nF (37.8762105, -119.3432825)            254.22\nG (37.833654, -119.4510805)             259.56\nH (38.0603395, -119.6666755)            259.71\nI (37.798171, -119.19955150              253.9\nJ (38.0391175, -119.3073495)            255.29\nName: 253, dtype: object\nTime                            6/12/2018 6:00\nA (38.152231, -119.6666755)             262.65\nB (38.279274, -119.6127765)             265.66\nC (38.50458, -119.62176)                269.19\nD (37.862028, -119.6576925)          269.94998\nE (37.89748, -119.2624335)              255.06\nF (37.8762105, -119.3432825)            259.15\nG (37.833654, -119.4510805)          265.13998\nH (38.0603395, -119.6666755)             263.0\nI (37.798171, -119.19955150             258.43\nJ (38.0391175, -119.3073495)            257.18\nName: 254, dtype: object\nTime                            6/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 255, dtype: object\nTime                            6/14/2018 6:00\nA (38.152231, -119.6666755)             264.57\nB (38.279274, -119.6127765)             268.58\nC (38.50458, -119.62176)             272.50998\nD (37.862028, -119.6576925)             265.86\nE (37.89748, -119.2624335)              257.03\nF (37.8762105, -119.3432825)            258.53\nG (37.833654, -119.4510805)             261.86\nH (38.0603395, -119.6666755)            262.99\nI (37.798171, -119.19955150             257.43\nJ (38.0391175, -119.3073495)         258.41998\nName: 256, dtype: object\nTime                            6/15/2018 6:00\nA (38.152231, -119.6666755)             263.24\nB (38.279274, -119.6127765)             266.54\nC (38.50458, -119.62176)                270.35\nD (37.862028, -119.6576925)              265.3\nE (37.89748, -119.2624335)              255.68\nF (37.8762105, -119.3432825)            258.16\nG (37.833654, -119.4510805)             261.54\nH (38.0603395, -119.6666755)            262.61\nI (37.798171, -119.19955150          255.84999\nJ (38.0391175, -119.3073495)            258.31\nName: 257, dtype: object\nTime                            6/16/2018 6:00\nA (38.152231, -119.6666755)          261.50998\nB (38.279274, -119.6127765)             264.72\nC (38.50458, -119.62176)             267.66998\nD (37.862028, -119.6576925)          264.91998\nE (37.89748, -119.2624335)              251.62\nF (37.8762105, -119.3432825)            254.26\nG (37.833654, -119.4510805)             261.29\nH (38.0603395, -119.6666755)            260.97\nI (37.798171, -119.19955150             254.78\nJ (38.0391175, -119.3073495)            254.67\nName: 258, dtype: object\nTime                            6/17/2018 6:00\nA (38.152231, -119.6666755)             257.65\nB (38.279274, -119.6127765)             260.25\nC (38.50458, -119.62176)                 265.0\nD (37.862028, -119.6576925)             261.53\nE (37.89748, -119.2624335)              250.17\nF (37.8762105, -119.3432825)         253.37999\nG (37.833654, -119.4510805)             256.22\nH (38.0603395, -119.6666755)            257.44\nI (37.798171, -119.19955150             250.18\nJ (38.0391175, -119.3073495)         255.31999\nName: 259, dtype: object\nTime                            6/18/2018 6:00\nA (38.152231, -119.6666755)             259.36\nB (38.279274, -119.6127765)             260.61\nC (38.50458, -119.62176)             264.97998\nD (37.862028, -119.6576925)             264.54\nE (37.89748, -119.2624335)           251.43999\nF (37.8762105, -119.3432825)            253.08\nG (37.833654, -119.4510805)             258.83\nH (38.0603395, -119.6666755)            259.87\nI (37.798171, -119.19955150             254.25\nJ (38.0391175, -119.3073495)            253.64\nName: 260, dtype: object\nTime                            6/19/2018 6:00\nA (38.152231, -119.6666755)          261.22998\nB (38.279274, -119.6127765)             263.57\nC (38.50458, -119.62176)                267.21\nD (37.862028, -119.6576925)              266.4\nE (37.89748, -119.2624335)           252.62999\nF (37.8762105, -119.3432825)            256.25\nG (37.833654, -119.4510805)          261.72998\nH (38.0603395, -119.6666755)            261.44\nI (37.798171, -119.19955150          254.31999\nJ (38.0391175, -119.3073495)         255.90999\nName: 261, dtype: object\nTime                            6/20/2018 6:00\nA (38.152231, -119.6666755)             263.28\nB (38.279274, -119.6127765)             266.75\nC (38.50458, -119.62176)                 271.0\nD (37.862028, -119.6576925)          268.19998\nE (37.89748, -119.2624335)              256.68\nF (37.8762105, -119.3432825)            259.43\nG (37.833654, -119.4510805)             264.29\nH (38.0603395, -119.6666755)         264.63998\nI (37.798171, -119.19955150             260.77\nJ (38.0391175, -119.3073495)         256.69998\nName: 262, dtype: object\nTime                            6/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 263, dtype: object\nTime                            6/22/2018 6:00\nA (38.152231, -119.6666755)             264.88\nB (38.279274, -119.6127765)             268.63\nC (38.50458, -119.62176)             271.50998\nD (37.862028, -119.6576925)             266.97\nE (37.89748, -119.2624335)              255.64\nF (37.8762105, -119.3432825)            259.43\nG (37.833654, -119.4510805)          263.25998\nH (38.0603395, -119.6666755)            264.11\nI (37.798171, -119.19955150          257.94998\nJ (38.0391175, -119.3073495)            257.53\nName: 264, dtype: object\nTime                            6/23/2018 6:00\nA (38.152231, -119.6666755)             264.47\nB (38.279274, -119.6127765)             267.34\nC (38.50458, -119.62176)                270.83\nD (37.862028, -119.6576925)             267.19\nE (37.89748, -119.2624335)              257.21\nF (37.8762105, -119.3432825)            258.99\nG (37.833654, -119.4510805)             263.87\nH (38.0603395, -119.6666755)             263.9\nI (37.798171, -119.19955150          258.38998\nJ (38.0391175, -119.3073495)            258.21\nName: 265, dtype: object\nTime                            6/24/2018 6:00\nA (38.152231, -119.6666755)             265.84\nB (38.279274, -119.6127765)             267.49\nC (38.50458, -119.62176)                269.36\nD (37.862028, -119.6576925)              269.0\nE (37.89748, -119.2624335)           254.06999\nF (37.8762105, -119.3432825)            257.28\nG (37.833654, -119.4510805)             265.71\nH (38.0603395, -119.6666755)         265.63998\nI (37.798171, -119.19955150             257.61\nJ (38.0391175, -119.3073495)         257.38998\nName: 266, dtype: object\nTime                            6/25/2018 6:00\nA (38.152231, -119.6666755)              264.6\nB (38.279274, -119.6127765)             267.71\nC (38.50458, -119.62176)             271.63998\nD (37.862028, -119.6576925)             269.12\nE (37.89748, -119.2624335)           255.06999\nF (37.8762105, -119.3432825)            259.44\nG (37.833654, -119.4510805)             265.41\nH (38.0603395, -119.6666755)            264.86\nI (37.798171, -119.19955150              256.0\nJ (38.0391175, -119.3073495)         260.50998\nName: 267, dtype: object\nTime                            6/26/2018 6:00\nA (38.152231, -119.6666755)             264.69\nB (38.279274, -119.6127765)             267.16\nC (38.50458, -119.62176)             271.13998\nD (37.862028, -119.6576925)             270.53\nE (37.89748, -119.2624335)              256.38\nF (37.8762105, -119.3432825)            259.05\nG (37.833654, -119.4510805)             264.36\nH (38.0603395, -119.6666755)            264.91\nI (37.798171, -119.19955150             258.88\nJ (38.0391175, -119.3073495)            259.37\nName: 268, dtype: object\nTime                            6/27/2018 6:00\nA (38.152231, -119.6666755)             264.94\nB (38.279274, -119.6127765)             267.91\nC (38.50458, -119.62176)                271.27\nD (37.862028, -119.6576925)          270.47998\nE (37.89748, -119.2624335)              256.71\nF (37.8762105, -119.3432825)         259.94998\nG (37.833654, -119.4510805)             265.71\nH (38.0603395, -119.6666755)            265.02\nI (37.798171, -119.19955150             260.55\nJ (38.0391175, -119.3073495)            259.79\nName: 269, dtype: object\nTime                            6/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 270, dtype: object\nTime                            6/29/2018 6:00\nA (38.152231, -119.6666755)             263.55\nB (38.279274, -119.6127765)             266.18\nC (38.50458, -119.62176)                269.28\nD (37.862028, -119.6576925)             264.57\nE (37.89748, -119.2624335)           259.00998\nF (37.8762105, -119.3432825)            258.88\nG (37.833654, -119.4510805)             260.75\nH (38.0603395, -119.6666755)         262.47998\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)            261.37\nName: 271, dtype: object\nTime                            6/30/2018 6:00\nA (38.152231, -119.6666755)          264.88998\nB (38.279274, -119.6127765)             267.49\nC (38.50458, -119.62176)                270.27\nD (37.862028, -119.6576925)          268.13998\nE (37.89748, -119.2624335)           258.00998\nF (37.8762105, -119.3432825)            260.41\nG (37.833654, -119.4510805)          264.16998\nH (38.0603395, -119.6666755)            264.71\nI (37.798171, -119.19955150          259.19998\nJ (38.0391175, -119.3073495)            258.22\nName: 272, dtype: object\nTime                            7/1/2018 6:00\nA (38.152231, -119.6666755)            266.59\nB (38.279274, -119.6127765)            268.25\nC (38.50458, -119.62176)                271.1\nD (37.862028, -119.6576925)            269.52\nE (37.89748, -119.2624335)          258.91998\nF (37.8762105, -119.3432825)            261.9\nG (37.833654, -119.4510805)            265.09\nH (38.0603395, -119.6666755)           266.19\nI (37.798171, -119.19955150            256.85\nJ (38.0391175, -119.3073495)           261.33\nName: 273, dtype: object\nTime                            7/2/2018 6:00\nA (38.152231, -119.6666755)            267.18\nB (38.279274, -119.6127765)            269.34\nC (38.50458, -119.62176)               272.33\nD (37.862028, -119.6576925)            269.72\nE (37.89748, -119.2624335)          256.66998\nF (37.8762105, -119.3432825)        257.22998\nG (37.833654, -119.4510805)            263.91\nH (38.0603395, -119.6666755)           267.19\nI (37.798171, -119.19955150            260.63\nJ (38.0391175, -119.3073495)           258.21\nName: 274, dtype: object\nTime                            7/3/2018 6:00\nA (38.152231, -119.6666755)            263.86\nB (38.279274, -119.6127765)            266.62\nC (38.50458, -119.62176)               269.68\nD (37.862028, -119.6576925)            268.09\nE (37.89748, -119.2624335)             252.22\nF (37.8762105, -119.3432825)           256.05\nG (37.833654, -119.4510805)            264.19\nH (38.0603395, -119.6666755)        264.19998\nI (37.798171, -119.19955150            254.58\nJ (38.0391175, -119.3073495)           257.33\nName: 275, dtype: object\nTime                            7/4/2018 6:00\nA (38.152231, -119.6666755)            263.06\nB (38.279274, -119.6127765)            265.71\nC (38.50458, -119.62176)               269.27\nD (37.862028, -119.6576925)         268.22998\nE (37.89748, -119.2624335)             253.09\nF (37.8762105, -119.3432825)           256.97\nG (37.833654, -119.4510805)            263.79\nH (38.0603395, -119.6666755)           263.04\nI (37.798171, -119.19955150            255.79\nJ (38.0391175, -119.3073495)           257.41\nName: 276, dtype: object\nTime                            7/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 277, dtype: object\nTime                            7/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 278, dtype: object\nTime                            7/7/2018 6:00\nA (38.152231, -119.6666755)            268.91\nB (38.279274, -119.6127765)            272.56\nC (38.50458, -119.62176)            275.50998\nD (37.862028, -119.6576925)         270.44998\nE (37.89748, -119.2624335)              260.6\nF (37.8762105, -119.3432825)           263.27\nG (37.833654, -119.4510805)            266.94\nH (38.0603395, -119.6666755)           267.94\nI (37.798171, -119.19955150         262.13998\nJ (38.0391175, -119.3073495)           261.57\nName: 279, dtype: object\nTime                            7/8/2018 6:00\nA (38.152231, -119.6666755)         269.00998\nB (38.279274, -119.6127765)            271.57\nC (38.50458, -119.62176)               274.34\nD (37.862028, -119.6576925)            271.24\nE (37.89748, -119.2624335)          259.94998\nF (37.8762105, -119.3432825)           262.75\nG (37.833654, -119.4510805)            267.33\nH (38.0603395, -119.6666755)           268.65\nI (37.798171, -119.19955150            260.93\nJ (38.0391175, -119.3073495)           261.79\nName: 280, dtype: object\nTime                            7/9/2018 6:00\nA (38.152231, -119.6666755)            267.61\nB (38.279274, -119.6127765)             269.8\nC (38.50458, -119.62176)               273.05\nD (37.862028, -119.6576925)            270.15\nE (37.89748, -119.2624335)          260.47998\nF (37.8762105, -119.3432825)        263.22998\nG (37.833654, -119.4510805)            265.57\nH (38.0603395, -119.6666755)           267.52\nI (37.798171, -119.19955150         259.47998\nJ (38.0391175, -119.3073495)           264.96\nName: 281, dtype: object\nTime                            7/10/2018 6:00\nA (38.152231, -119.6666755)             267.04\nB (38.279274, -119.6127765)             269.25\nC (38.50458, -119.62176)                272.11\nD (37.862028, -119.6576925)             271.02\nE (37.89748, -119.2624335)              259.72\nF (37.8762105, -119.3432825)            261.12\nG (37.833654, -119.4510805)             264.28\nH (38.0603395, -119.6666755)         266.97998\nI (37.798171, -119.19955150             260.99\nJ (38.0391175, -119.3073495)         262.50998\nName: 282, dtype: object\nTime                            7/11/2018 6:00\nA (38.152231, -119.6666755)             266.49\nB (38.279274, -119.6127765)              269.0\nC (38.50458, -119.62176)                272.53\nD (37.862028, -119.6576925)             270.22\nE (37.89748, -119.2624335)               257.4\nF (37.8762105, -119.3432825)         260.38998\nG (37.833654, -119.4510805)             265.86\nH (38.0603395, -119.6666755)            266.29\nI (37.798171, -119.19955150             259.78\nJ (38.0391175, -119.3073495)            261.56\nName: 283, dtype: object\nTime                            7/12/2018 6:00\nA (38.152231, -119.6666755)             267.37\nB (38.279274, -119.6127765)             270.11\nC (38.50458, -119.62176)             273.63998\nD (37.862028, -119.6576925)             272.06\nE (37.89748, -119.2624335)           259.16998\nF (37.8762105, -119.3432825)         262.00998\nG (37.833654, -119.4510805)             268.38\nH (38.0603395, -119.6666755)            267.53\nI (37.798171, -119.19955150             261.29\nJ (38.0391175, -119.3073495)            262.27\nName: 284, dtype: object\nTime                            7/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 285, dtype: object\nTime                            7/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 286, dtype: object\nTime                            7/15/2018 6:00\nA (38.152231, -119.6666755)          269.69998\nB (38.279274, -119.6127765)             273.16\nC (38.50458, -119.62176)                276.56\nD (37.862028, -119.6576925)             271.16\nE (37.89748, -119.2624335)              262.83\nF (37.8762105, -119.3432825)         265.19998\nG (37.833654, -119.4510805)             267.97\nH (38.0603395, -119.6666755)         269.16998\nI (37.798171, -119.19955150             264.08\nJ (38.0391175, -119.3073495)         264.13998\nName: 287, dtype: object\nTime                            7/16/2018 6:00\nA (38.152231, -119.6666755)              269.4\nB (38.279274, -119.6127765)             272.29\nC (38.50458, -119.62176)                273.69\nD (37.862028, -119.6576925)             271.43\nE (37.89748, -119.2624335)               261.5\nF (37.8762105, -119.3432825)            262.78\nG (37.833654, -119.4510805)             266.71\nH (38.0603395, -119.6666755)             268.9\nI (37.798171, -119.19955150             263.65\nJ (38.0391175, -119.3073495)            263.36\nName: 288, dtype: object\nTime                            7/17/2018 6:00\nA (38.152231, -119.6666755)             268.27\nB (38.279274, -119.6127765)          270.63998\nC (38.50458, -119.62176)                 273.1\nD (37.862028, -119.6576925)             269.97\nE (37.89748, -119.2624335)              259.81\nF (37.8762105, -119.3432825)             264.1\nG (37.833654, -119.4510805)             268.29\nH (38.0603395, -119.6666755)            268.22\nI (37.798171, -119.19955150          258.66998\nJ (38.0391175, -119.3073495)            263.91\nName: 289, dtype: object\nTime                            7/18/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 290, dtype: object\nTime                            7/19/2018 6:00\nA (38.152231, -119.6666755)             267.47\nB (38.279274, -119.6127765)          269.50998\nC (38.50458, -119.62176)                272.71\nD (37.862028, -119.6576925)             271.87\nE (37.89748, -119.2624335)               258.3\nF (37.8762105, -119.3432825)            260.99\nG (37.833654, -119.4510805)             269.24\nH (38.0603395, -119.6666755)         267.50998\nI (37.798171, -119.19955150             260.93\nJ (38.0391175, -119.3073495)            263.03\nName: 291, dtype: object\nTime                            7/20/2018 6:00\nA (38.152231, -119.6666755)             268.16\nB (38.279274, -119.6127765)             270.85\nC (38.50458, -119.62176)                273.77\nD (37.862028, -119.6576925)             273.07\nE (37.89748, -119.2624335)           260.50998\nF (37.8762105, -119.3432825)            263.21\nG (37.833654, -119.4510805)             269.27\nH (38.0603395, -119.6666755)            268.37\nI (37.798171, -119.19955150             263.49\nJ (38.0391175, -119.3073495)            261.58\nName: 292, dtype: object\nTime                            7/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 293, dtype: object\nTime                            7/22/2018 6:00\nA (38.152231, -119.6666755)             269.41\nB (38.279274, -119.6127765)             271.07\nC (38.50458, -119.62176)                274.18\nD (37.862028, -119.6576925)          270.13998\nE (37.89748, -119.2624335)              259.54\nF (37.8762105, -119.3432825)            262.52\nG (37.833654, -119.4510805)          266.63998\nH (38.0603395, -119.6666755)            269.08\nI (37.798171, -119.19955150             261.71\nJ (38.0391175, -119.3073495)             260.4\nName: 294, dtype: object\nTime                            7/23/2018 6:00\nA (38.152231, -119.6666755)             267.78\nB (38.279274, -119.6127765)          269.50998\nC (38.50458, -119.62176)                272.66\nD (37.862028, -119.6576925)              271.1\nE (37.89748, -119.2624335)           259.50998\nF (37.8762105, -119.3432825)            262.52\nG (37.833654, -119.4510805)             266.38\nH (38.0603395, -119.6666755)            267.94\nI (37.798171, -119.19955150          260.72998\nJ (38.0391175, -119.3073495)         261.63998\nName: 295, dtype: object\nTime                            7/24/2018 6:00\nA (38.152231, -119.6666755)             268.83\nB (38.279274, -119.6127765)             270.61\nC (38.50458, -119.62176)                272.94\nD (37.862028, -119.6576925)          271.72998\nE (37.89748, -119.2624335)              258.71\nF (37.8762105, -119.3432825)            260.25\nG (37.833654, -119.4510805)          266.50998\nH (38.0603395, -119.6666755)            268.78\nI (37.798171, -119.19955150             261.99\nJ (38.0391175, -119.3073495)            260.49\nName: 296, dtype: object\nTime                            7/25/2018 6:00\nA (38.152231, -119.6666755)             267.12\nB (38.279274, -119.6127765)             269.63\nC (38.50458, -119.62176)                271.63\nD (37.862028, -119.6576925)          269.72998\nE (37.89748, -119.2624335)           256.72998\nF (37.8762105, -119.3432825)             259.4\nG (37.833654, -119.4510805)             266.66\nH (38.0603395, -119.6666755)         266.88998\nI (37.798171, -119.19955150             259.55\nJ (38.0391175, -119.3073495)             260.0\nName: 297, dtype: object\nTime                            7/26/2018 6:00\nA (38.152231, -119.6666755)             268.12\nB (38.279274, -119.6127765)          270.16998\nC (38.50458, -119.62176)                273.02\nD (37.862028, -119.6576925)             271.58\nE (37.89748, -119.2624335)              259.06\nF (37.8762105, -119.3432825)            262.29\nG (37.833654, -119.4510805)             268.53\nH (38.0603395, -119.6666755)            267.59\nI (37.798171, -119.19955150          260.00998\nJ (38.0391175, -119.3073495)         263.41998\nName: 298, dtype: object\nTime                            7/27/2018 6:00\nA (38.152231, -119.6666755)             267.37\nB (38.279274, -119.6127765)             269.35\nC (38.50458, -119.62176)                272.85\nD (37.862028, -119.6576925)          271.97998\nE (37.89748, -119.2624335)              258.22\nF (37.8762105, -119.3432825)            260.96\nG (37.833654, -119.4510805)             267.09\nH (38.0603395, -119.6666755)            267.11\nI (37.798171, -119.19955150             260.77\nJ (38.0391175, -119.3073495)            261.31\nName: 299, dtype: object\nTime                            7/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              252.92\nF (37.8762105, -119.3432825)            255.17\nG (37.833654, -119.4510805)             262.03\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             261.72\nJ (38.0391175, -119.3073495)            253.37\nName: 300, dtype: object\nTime                            7/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 301, dtype: object\nTime                            7/30/2018 6:00\nA (38.152231, -119.6666755)             269.59\nB (38.279274, -119.6127765)             272.61\nC (38.50458, -119.62176)                274.62\nD (37.862028, -119.6576925)             270.93\nE (37.89748, -119.2624335)           262.22998\nF (37.8762105, -119.3432825)            264.88\nG (37.833654, -119.4510805)              268.3\nH (38.0603395, -119.6666755)            268.71\nI (37.798171, -119.19955150             263.41\nJ (38.0391175, -119.3073495)            262.79\nName: 302, dtype: object\nTime                            7/31/2018 6:00\nA (38.152231, -119.6666755)             269.21\nB (38.279274, -119.6127765)             271.83\nC (38.50458, -119.62176)             274.72998\nD (37.862028, -119.6576925)             271.43\nE (37.89748, -119.2624335)               260.1\nF (37.8762105, -119.3432825)             263.5\nG (37.833654, -119.4510805)             269.52\nH (38.0603395, -119.6666755)         269.19998\nI (37.798171, -119.19955150             260.94\nJ (38.0391175, -119.3073495)            262.87\nName: 303, dtype: object\nTime                            8/1/2018 6:00\nA (38.152231, -119.6666755)            269.43\nB (38.279274, -119.6127765)            271.53\nC (38.50458, -119.62176)               273.65\nD (37.862028, -119.6576925)            271.55\nE (37.89748, -119.2624335)             262.85\nF (37.8762105, -119.3432825)           262.63\nG (37.833654, -119.4510805)            265.66\nH (38.0603395, -119.6666755)           268.99\nI (37.798171, -119.19955150            263.09\nJ (38.0391175, -119.3073495)           263.34\nName: 304, dtype: object\nTime                            8/2/2018 6:00\nA (38.152231, -119.6666755)            267.19\nB (38.279274, -119.6127765)            269.16\nC (38.50458, -119.62176)            271.75998\nD (37.862028, -119.6576925)            270.15\nE (37.89748, -119.2624335)          259.75998\nF (37.8762105, -119.3432825)            259.5\nG (37.833654, -119.4510805)            264.93\nH (38.0603395, -119.6666755)           266.59\nI (37.798171, -119.19955150            262.12\nJ (38.0391175, -119.3073495)           261.03\nName: 305, dtype: object\nTime                            8/3/2018 6:00\nA (38.152231, -119.6666755)            264.15\nB (38.279274, -119.6127765)            266.85\nC (38.50458, -119.62176)                269.9\nD (37.862028, -119.6576925)            268.77\nE (37.89748, -119.2624335)             255.65\nF (37.8762105, -119.3432825)           259.18\nG (37.833654, -119.4510805)             264.6\nH (38.0603395, -119.6666755)           264.08\nI (37.798171, -119.19955150            257.03\nJ (38.0391175, -119.3073495)           259.53\nName: 306, dtype: object\nTime                            8/4/2018 6:00\nA (38.152231, -119.6666755)         264.41998\nB (38.279274, -119.6127765)            267.24\nC (38.50458, -119.62176)               270.15\nD (37.862028, -119.6576925)             269.9\nE (37.89748, -119.2624335)          256.44998\nF (37.8762105, -119.3432825)           259.44\nG (37.833654, -119.4510805)            265.79\nH (38.0603395, -119.6666755)        264.38998\nI (37.798171, -119.19955150            258.57\nJ (38.0391175, -119.3073495)           259.13\nName: 307, dtype: object\nTime                            8/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 308, dtype: object\nTime                            8/6/2018 6:00\nA (38.152231, -119.6666755)            265.74\nB (38.279274, -119.6127765)         267.75998\nC (38.50458, -119.62176)               270.44\nD (37.862028, -119.6576925)            266.91\nE (37.89748, -119.2624335)             258.11\nF (37.8762105, -119.3432825)           260.31\nG (37.833654, -119.4510805)            263.75\nH (38.0603395, -119.6666755)           265.18\nI (37.798171, -119.19955150             259.3\nJ (38.0391175, -119.3073495)            257.4\nName: 309, dtype: object\nTime                            8/7/2018 6:00\nA (38.152231, -119.6666755)             266.5\nB (38.279274, -119.6127765)            268.03\nC (38.50458, -119.62176)               270.85\nD (37.862028, -119.6576925)            269.09\nE (37.89748, -119.2624335)          257.91998\nF (37.8762105, -119.3432825)           260.61\nG (37.833654, -119.4510805)            265.77\nH (38.0603395, -119.6666755)        265.97998\nI (37.798171, -119.19955150            259.12\nJ (38.0391175, -119.3073495)        259.69998\nName: 310, dtype: object\nTime                            8/8/2018 6:00\nA (38.152231, -119.6666755)            265.62\nB (38.279274, -119.6127765)            268.28\nC (38.50458, -119.62176)               271.11\nD (37.862028, -119.6576925)            268.63\nE (37.89748, -119.2624335)             257.36\nF (37.8762105, -119.3432825)           262.03\nG (37.833654, -119.4510805)         267.13998\nH (38.0603395, -119.6666755)           264.94\nI (37.798171, -119.19955150            256.85\nJ (38.0391175, -119.3073495)        259.66998\nName: 311, dtype: object\nTime                            8/9/2018 6:00\nA (38.152231, -119.6666755)            266.46\nB (38.279274, -119.6127765)         268.19998\nC (38.50458, -119.62176)                270.5\nD (37.862028, -119.6576925)            269.65\nE (37.89748, -119.2624335)             259.54\nF (37.8762105, -119.3432825)           263.49\nG (37.833654, -119.4510805)            265.05\nH (38.0603395, -119.6666755)           266.49\nI (37.798171, -119.19955150            257.63\nJ (38.0391175, -119.3073495)           264.19\nName: 312, dtype: object\nTime                            8/10/2018 6:00\nA (38.152231, -119.6666755)          266.47998\nB (38.279274, -119.6127765)          268.47998\nC (38.50458, -119.62176)             271.13998\nD (37.862028, -119.6576925)             269.54\nE (37.89748, -119.2624335)              258.33\nF (37.8762105, -119.3432825)            260.88\nG (37.833654, -119.4510805)             264.52\nH (38.0603395, -119.6666755)            266.22\nI (37.798171, -119.19955150             259.09\nJ (38.0391175, -119.3073495)            261.79\nName: 313, dtype: object\nTime                            8/11/2018 6:00\nA (38.152231, -119.6666755)          265.16998\nB (38.279274, -119.6127765)             266.97\nC (38.50458, -119.62176)                270.63\nD (37.862028, -119.6576925)             269.71\nE (37.89748, -119.2624335)              257.22\nF (37.8762105, -119.3432825)            259.36\nG (37.833654, -119.4510805)              265.3\nH (38.0603395, -119.6666755)             265.6\nI (37.798171, -119.19955150             258.88\nJ (38.0391175, -119.3073495)         259.13998\nName: 314, dtype: object\nTime                            8/12/2018 6:00\nA (38.152231, -119.6666755)             265.32\nB (38.279274, -119.6127765)             267.96\nC (38.50458, -119.62176)                270.59\nD (37.862028, -119.6576925)             269.74\nE (37.89748, -119.2624335)              258.62\nF (37.8762105, -119.3432825)            261.21\nG (37.833654, -119.4510805)              266.3\nH (38.0603395, -119.6666755)         265.25998\nI (37.798171, -119.19955150             261.09\nJ (38.0391175, -119.3073495)         259.41998\nName: 315, dtype: object\nTime                            8/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 316, dtype: object\nTime                            8/14/2018 6:00\nA (38.152231, -119.6666755)             267.68\nB (38.279274, -119.6127765)          269.91998\nC (38.50458, -119.62176)                273.07\nD (37.862028, -119.6576925)             269.37\nE (37.89748, -119.2624335)              260.82\nF (37.8762105, -119.3432825)            263.15\nG (37.833654, -119.4510805)             266.33\nH (38.0603395, -119.6666755)         267.13998\nI (37.798171, -119.19955150             262.04\nJ (38.0391175, -119.3073495)            260.53\nName: 317, dtype: object\nTime                            8/15/2018 6:00\nA (38.152231, -119.6666755)          266.66998\nB (38.279274, -119.6127765)             268.53\nC (38.50458, -119.62176)                271.56\nD (37.862028, -119.6576925)             268.34\nE (37.89748, -119.2624335)              258.94\nF (37.8762105, -119.3432825)            262.47\nG (37.833654, -119.4510805)             265.55\nH (38.0603395, -119.6666755)            266.08\nI (37.798171, -119.19955150             259.53\nJ (38.0391175, -119.3073495)            260.74\nName: 318, dtype: object\nTime                            8/16/2018 6:00\nA (38.152231, -119.6666755)          266.00998\nB (38.279274, -119.6127765)          267.97998\nC (38.50458, -119.62176)                269.54\nD (37.862028, -119.6576925)          267.88998\nE (37.89748, -119.2624335)           254.20999\nF (37.8762105, -119.3432825)            258.46\nG (37.833654, -119.4510805)             265.27\nH (38.0603395, -119.6666755)            265.58\nI (37.798171, -119.19955150             257.53\nJ (38.0391175, -119.3073495)            257.61\nName: 319, dtype: object\nTime                            8/17/2018 6:00\nA (38.152231, -119.6666755)             264.69\nB (38.279274, -119.6127765)          266.72998\nC (38.50458, -119.62176)                269.15\nD (37.862028, -119.6576925)             268.46\nE (37.89748, -119.2624335)              254.64\nF (37.8762105, -119.3432825)            258.24\nG (37.833654, -119.4510805)             266.94\nH (38.0603395, -119.6666755)            265.15\nI (37.798171, -119.19955150             256.19\nJ (38.0391175, -119.3073495)            258.82\nName: 320, dtype: object\nTime                            8/18/2018 6:00\nA (38.152231, -119.6666755)             264.12\nB (38.279274, -119.6127765)             266.18\nC (38.50458, -119.62176)                 269.1\nD (37.862028, -119.6576925)              269.0\nE (37.89748, -119.2624335)           256.19998\nF (37.8762105, -119.3432825)            259.37\nG (37.833654, -119.4510805)             263.79\nH (38.0603395, -119.6666755)         263.88998\nI (37.798171, -119.19955150             257.11\nJ (38.0391175, -119.3073495)            260.09\nName: 321, dtype: object\nTime                            8/19/2018 6:00\nA (38.152231, -119.6666755)             265.27\nB (38.279274, -119.6127765)             267.13\nC (38.50458, -119.62176)                270.33\nD (37.862028, -119.6576925)             270.24\nE (37.89748, -119.2624335)           256.66998\nF (37.8762105, -119.3432825)             259.5\nG (37.833654, -119.4510805)          266.38998\nH (38.0603395, -119.6666755)         265.44998\nI (37.798171, -119.19955150          259.00998\nJ (38.0391175, -119.3073495)            258.61\nName: 322, dtype: object\nTime                            8/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 323, dtype: object\nTime                            8/21/2018 6:00\nA (38.152231, -119.6666755)             263.71\nB (38.279274, -119.6127765)             266.06\nC (38.50458, -119.62176)                270.08\nD (37.862028, -119.6576925)             265.63\nE (37.89748, -119.2624335)              259.08\nF (37.8762105, -119.3432825)             260.0\nG (37.833654, -119.4510805)             261.66\nH (38.0603395, -119.6666755)            263.18\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)            260.28\nName: 324, dtype: object\nTime                            8/22/2018 6:00\nA (38.152231, -119.6666755)             262.04\nB (38.279274, -119.6127765)          264.94998\nC (38.50458, -119.62176)             267.50998\nD (37.862028, -119.6576925)             264.81\nE (37.89748, -119.2624335)              255.15\nF (37.8762105, -119.3432825)            258.03\nG (37.833654, -119.4510805)             262.15\nH (38.0603395, -119.6666755)         261.47998\nI (37.798171, -119.19955150             257.04\nJ (38.0391175, -119.3073495)            256.31\nName: 325, dtype: object\nTime                            8/23/2018 6:00\nA (38.152231, -119.6666755)             263.07\nB (38.279274, -119.6127765)             264.34\nC (38.50458, -119.62176)                267.63\nD (37.862028, -119.6576925)              265.8\nE (37.89748, -119.2624335)              256.02\nF (37.8762105, -119.3432825)            258.91\nG (37.833654, -119.4510805)          261.94998\nH (38.0603395, -119.6666755)            262.82\nI (37.798171, -119.19955150             254.39\nJ (38.0391175, -119.3073495)            257.94\nName: 326, dtype: object\nTime                            8/24/2018 6:00\nA (38.152231, -119.6666755)             261.86\nB (38.279274, -119.6127765)             263.79\nC (38.50458, -119.62176)                266.56\nD (37.862028, -119.6576925)             265.02\nE (37.89748, -119.2624335)              254.76\nF (37.8762105, -119.3432825)            255.64\nG (37.833654, -119.4510805)             259.77\nH (38.0603395, -119.6666755)         262.19998\nI (37.798171, -119.19955150             256.29\nJ (38.0391175, -119.3073495)         255.26999\nName: 327, dtype: object\nTime                            8/25/2018 6:00\nA (38.152231, -119.6666755)             260.05\nB (38.279274, -119.6127765)             261.81\nC (38.50458, -119.62176)                264.41\nD (37.862028, -119.6576925)             264.15\nE (37.89748, -119.2624335)              248.97\nF (37.8762105, -119.3432825)            252.22\nG (37.833654, -119.4510805)          259.94998\nH (38.0603395, -119.6666755)            260.41\nI (37.798171, -119.19955150          252.29999\nJ (38.0391175, -119.3073495)            252.92\nName: 328, dtype: object\nTime                            8/26/2018 6:00\nA (38.152231, -119.6666755)             260.16\nB (38.279274, -119.6127765)          262.50998\nC (38.50458, -119.62176)                264.87\nD (37.862028, -119.6576925)             264.96\nE (37.89748, -119.2624335)           251.18999\nF (37.8762105, -119.3432825)         254.18999\nG (37.833654, -119.4510805)             260.93\nH (38.0603395, -119.6666755)             260.6\nI (37.798171, -119.19955150              254.5\nJ (38.0391175, -119.3073495)         253.81999\nName: 329, dtype: object\nTime                            8/27/2018 6:00\nA (38.152231, -119.6666755)             261.08\nB (38.279274, -119.6127765)             263.61\nC (38.50458, -119.62176)                266.33\nD (37.862028, -119.6576925)             265.82\nE (37.89748, -119.2624335)              253.04\nF (37.8762105, -119.3432825)            256.78\nG (37.833654, -119.4510805)          261.44998\nH (38.0603395, -119.6666755)            261.46\nI (37.798171, -119.19955150             256.13\nJ (38.0391175, -119.3073495)         254.68999\nName: 330, dtype: object\nTime                            8/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 331, dtype: object\nTime                            8/29/2018 6:00\nA (38.152231, -119.6666755)          263.69998\nB (38.279274, -119.6127765)          266.47998\nC (38.50458, -119.62176)                270.09\nD (37.862028, -119.6576925)              265.8\nE (37.89748, -119.2624335)              255.97\nF (37.8762105, -119.3432825)             259.4\nG (37.833654, -119.4510805)          262.91998\nH (38.0603395, -119.6666755)         263.41998\nI (37.798171, -119.19955150             258.86\nJ (38.0391175, -119.3073495)            255.72\nName: 332, dtype: object\nTime                            8/30/2018 6:00\nA (38.152231, -119.6666755)             261.27\nB (38.279274, -119.6127765)             263.41\nC (38.50458, -119.62176)             265.63998\nD (37.862028, -119.6576925)             264.33\nE (37.89748, -119.2624335)              252.76\nF (37.8762105, -119.3432825)            255.31\nG (37.833654, -119.4510805)             260.29\nH (38.0603395, -119.6666755)         261.38998\nI (37.798171, -119.19955150          253.68999\nJ (38.0391175, -119.3073495)            254.31\nName: 333, dtype: object\nTime                            8/31/2018 6:00\nA (38.152231, -119.6666755)          258.97998\nB (38.279274, -119.6127765)             260.03\nC (38.50458, -119.62176)                 263.4\nD (37.862028, -119.6576925)             262.35\nE (37.89748, -119.2624335)           252.01999\nF (37.8762105, -119.3432825)            255.34\nG (37.833654, -119.4510805)          258.63998\nH (38.0603395, -119.6666755)            258.72\nI (37.798171, -119.19955150          251.48999\nJ (38.0391175, -119.3073495)            253.98\nName: 334, dtype: object\nTime                            9/1/2018 6:00\nA (38.152231, -119.6666755)            260.99\nB (38.279274, -119.6127765)         261.63998\nC (38.50458, -119.62176)               264.71\nD (37.862028, -119.6576925)            265.94\nE (37.89748, -119.2624335)             254.37\nF (37.8762105, -119.3432825)        257.19998\nG (37.833654, -119.4510805)            259.68\nH (38.0603395, -119.6666755)           261.04\nI (37.798171, -119.19955150            254.89\nJ (38.0391175, -119.3073495)        257.44998\nName: 335, dtype: object\nTime                            9/2/2018 6:00\nA (38.152231, -119.6666755)            261.21\nB (38.279274, -119.6127765)         262.63998\nC (38.50458, -119.62176)            266.16998\nD (37.862028, -119.6576925)            267.07\nE (37.89748, -119.2624335)             252.37\nF (37.8762105, -119.3432825)           255.68\nG (37.833654, -119.4510805)            262.77\nH (38.0603395, -119.6666755)        261.50998\nI (37.798171, -119.19955150         255.51999\nJ (38.0391175, -119.3073495)            255.2\nName: 336, dtype: object\nTime                            9/3/2018 6:00\nA (38.152231, -119.6666755)            262.31\nB (38.279274, -119.6127765)            263.02\nC (38.50458, -119.62176)                266.9\nD (37.862028, -119.6576925)            268.18\nE (37.89748, -119.2624335)             255.53\nF (37.8762105, -119.3432825)           258.54\nG (37.833654, -119.4510805)            265.25\nH (38.0603395, -119.6666755)           263.19\nI (37.798171, -119.19955150            256.47\nJ (38.0391175, -119.3073495)        256.25998\nName: 337, dtype: object\nTime                            9/4/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 338, dtype: object\nTime                            9/5/2018 6:00\nA (38.152231, -119.6666755)         264.94998\nB (38.279274, -119.6127765)            265.06\nC (38.50458, -119.62176)               270.52\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)           265.31\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 339, dtype: object\nTime                            9/6/2018 6:00\nA (38.152231, -119.6666755)            263.37\nB (38.279274, -119.6127765)            265.86\nC (38.50458, -119.62176)                269.0\nD (37.862028, -119.6576925)            268.19\nE (37.89748, -119.2624335)             255.65\nF (37.8762105, -119.3432825)           259.02\nG (37.833654, -119.4510805)            262.87\nH (38.0603395, -119.6666755)           263.59\nI (37.798171, -119.19955150         257.44998\nJ (38.0391175, -119.3073495)           256.27\nName: 340, dtype: object\nTime                            9/7/2018 6:00\nA (38.152231, -119.6666755)            264.18\nB (38.279274, -119.6127765)            266.06\nC (38.50458, -119.62176)               268.15\nD (37.862028, -119.6576925)         267.66998\nE (37.89748, -119.2624335)             256.56\nF (37.8762105, -119.3432825)            259.0\nG (37.833654, -119.4510805)            262.16\nH (38.0603395, -119.6666755)           264.15\nI (37.798171, -119.19955150             258.0\nJ (38.0391175, -119.3073495)           258.13\nName: 341, dtype: object\nTime                            9/8/2018 6:00\nA (38.152231, -119.6666755)            262.49\nB (38.279274, -119.6127765)            264.18\nC (38.50458, -119.62176)               267.18\nD (37.862028, -119.6576925)             265.5\nE (37.89748, -119.2624335)          251.48999\nF (37.8762105, -119.3432825)           256.28\nG (37.833654, -119.4510805)            262.91\nH (38.0603395, -119.6666755)           262.54\nI (37.798171, -119.19955150            253.45\nJ (38.0391175, -119.3073495)        254.87999\nName: 342, dtype: object\nTime                            9/9/2018 6:00\nA (38.152231, -119.6666755)            259.31\nB (38.279274, -119.6127765)         260.25998\nC (38.50458, -119.62176)                263.4\nD (37.862028, -119.6576925)            264.49\nE (37.89748, -119.2624335)          250.37999\nF (37.8762105, -119.3432825)           254.26\nG (37.833654, -119.4510805)            258.61\nH (38.0603395, -119.6666755)           258.94\nI (37.798171, -119.19955150         250.76999\nJ (38.0391175, -119.3073495)        253.23999\nName: 343, dtype: object\nTime                            9/10/2018 6:00\nA (38.152231, -119.6666755)             259.43\nB (38.279274, -119.6127765)              261.1\nC (38.50458, -119.62176)                 264.1\nD (37.862028, -119.6576925)             264.37\nE (37.89748, -119.2624335)           248.93999\nF (37.8762105, -119.3432825)             252.7\nG (37.833654, -119.4510805)             260.68\nH (38.0603395, -119.6666755)            259.71\nI (37.798171, -119.19955150             252.06\nJ (38.0391175, -119.3073495)            252.39\nName: 344, dtype: object\nTime                            9/11/2018 6:00\nA (38.152231, -119.6666755)             258.78\nB (38.279274, -119.6127765)             260.62\nC (38.50458, -119.62176)                263.69\nD (37.862028, -119.6576925)             264.65\nE (37.89748, -119.2624335)              250.47\nF (37.8762105, -119.3432825)            253.98\nG (37.833654, -119.4510805)             260.47\nH (38.0603395, -119.6666755)         259.25998\nI (37.798171, -119.19955150             253.39\nJ (38.0391175, -119.3073495)            251.22\nName: 345, dtype: object\nTime                            9/12/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 346, dtype: object\nTime                            9/13/2018 6:00\nA (38.152231, -119.6666755)             258.91\nB (38.279274, -119.6127765)             261.19\nC (38.50458, -119.62176)                 264.4\nD (37.862028, -119.6576925)          261.75998\nE (37.89748, -119.2624335)           250.95999\nF (37.8762105, -119.3432825)            253.95\nG (37.833654, -119.4510805)             257.53\nH (38.0603395, -119.6666755)         258.38998\nI (37.798171, -119.19955150             253.09\nJ (38.0391175, -119.3073495)            250.33\nName: 347, dtype: object\nTime                            9/14/2018 6:00\nA (38.152231, -119.6666755)             257.82\nB (38.279274, -119.6127765)             259.83\nC (38.50458, -119.62176)                262.27\nD (37.862028, -119.6576925)             261.16\nE (37.89748, -119.2624335)              250.75\nF (37.8762105, -119.3432825)            252.73\nG (37.833654, -119.4510805)             257.56\nH (38.0603395, -119.6666755)         257.66998\nI (37.798171, -119.19955150             251.84\nJ (38.0391175, -119.3073495)         251.09999\nName: 348, dtype: object\nTime                            9/15/2018 6:00\nA (38.152231, -119.6666755)              259.8\nB (38.279274, -119.6127765)             261.11\nC (38.50458, -119.62176)                263.65\nD (37.862028, -119.6576925)             263.16\nE (37.89748, -119.2624335)              253.62\nF (37.8762105, -119.3432825)             254.4\nG (37.833654, -119.4510805)              257.0\nH (38.0603395, -119.6666755)         259.88998\nI (37.798171, -119.19955150             254.58\nJ (38.0391175, -119.3073495)            253.98\nName: 349, dtype: object\nTime                            9/16/2018 6:00\nA (38.152231, -119.6666755)             256.24\nB (38.279274, -119.6127765)             257.91\nC (38.50458, -119.62176)                260.12\nD (37.862028, -119.6576925)          259.91998\nE (37.89748, -119.2624335)              244.48\nF (37.8762105, -119.3432825)         247.65999\nG (37.833654, -119.4510805)             255.78\nH (38.0603395, -119.6666755)            256.25\nI (37.798171, -119.19955150          249.34999\nJ (38.0391175, -119.3073495)            246.68\nName: 350, dtype: object\nTime                            9/17/2018 6:00\nA (38.152231, -119.6666755)              255.9\nB (38.279274, -119.6127765)             256.93\nC (38.50458, -119.62176)                260.41\nD (37.862028, -119.6576925)             260.75\nE (37.89748, -119.2624335)           246.23999\nF (37.8762105, -119.3432825)            250.06\nG (37.833654, -119.4510805)             256.38\nH (38.0603395, -119.6666755)            256.04\nI (37.798171, -119.19955150             248.61\nJ (38.0391175, -119.3073495)         249.87999\nName: 351, dtype: object\nTime                            9/18/2018 6:00\nA (38.152231, -119.6666755)          255.87999\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                260.35\nD (37.862028, -119.6576925)          261.19998\nE (37.89748, -119.2624335)              246.86\nF (37.8762105, -119.3432825)         250.48999\nG (37.833654, -119.4510805)          257.44998\nH (38.0603395, -119.6666755)            256.03\nI (37.798171, -119.19955150              249.0\nJ (38.0391175, -119.3073495)            249.83\nName: 352, dtype: object\nTime                            9/19/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)           236.01999\nF (37.8762105, -119.3432825)         244.06999\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             250.83\nJ (38.0391175, -119.3073495)            232.73\nName: 353, dtype: object\nTime                            9/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 354, dtype: object\nTime                            9/21/2018 6:00\nA (38.152231, -119.6666755)             260.37\nB (38.279274, -119.6127765)             262.32\nC (38.50458, -119.62176)                264.22\nD (37.862028, -119.6576925)             262.87\nE (37.89748, -119.2624335)               251.5\nF (37.8762105, -119.3432825)         254.76999\nG (37.833654, -119.4510805)             258.78\nH (38.0603395, -119.6666755)            259.83\nI (37.798171, -119.19955150          253.70999\nJ (38.0391175, -119.3073495)         251.70999\nName: 355, dtype: object\nTime                            9/22/2018 6:00\nA (38.152231, -119.6666755)             258.46\nB (38.279274, -119.6127765)          260.97998\nC (38.50458, -119.62176)                262.99\nD (37.862028, -119.6576925)             262.28\nE (37.89748, -119.2624335)           250.23999\nF (37.8762105, -119.3432825)            253.28\nG (37.833654, -119.4510805)             257.22\nH (38.0603395, -119.6666755)            258.43\nI (37.798171, -119.19955150             252.01\nJ (38.0391175, -119.3073495)         252.15999\nName: 356, dtype: object\nTime                            9/23/2018 6:00\nA (38.152231, -119.6666755)          258.72998\nB (38.279274, -119.6127765)             259.35\nC (38.50458, -119.62176)                262.74\nD (37.862028, -119.6576925)             263.06\nE (37.89748, -119.2624335)           251.70999\nF (37.8762105, -119.3432825)            255.06\nG (37.833654, -119.4510805)             256.08\nH (38.0603395, -119.6666755)            259.15\nI (37.798171, -119.19955150          251.48999\nJ (38.0391175, -119.3073495)            255.45\nName: 357, dtype: object\nTime                            9/24/2018 6:00\nA (38.152231, -119.6666755)          257.13998\nB (38.279274, -119.6127765)             257.56\nC (38.50458, -119.62176)                260.66\nD (37.862028, -119.6576925)          261.22998\nE (37.89748, -119.2624335)              247.48\nF (37.8762105, -119.3432825)            248.53\nG (37.833654, -119.4510805)             255.36\nH (38.0603395, -119.6666755)            257.46\nI (37.798171, -119.19955150          249.54999\nJ (38.0391175, -119.3073495)         248.45999\nName: 358, dtype: object\nTime                            9/25/2018 6:00\nA (38.152231, -119.6666755)             256.36\nB (38.279274, -119.6127765)          257.00998\nC (38.50458, -119.62176)                259.55\nD (37.862028, -119.6576925)             263.61\nE (37.89748, -119.2624335)              247.53\nF (37.8762105, -119.3432825)            252.42\nG (37.833654, -119.4510805)             257.21\nH (38.0603395, -119.6666755)            257.47\nI (37.798171, -119.19955150             248.78\nJ (38.0391175, -119.3073495)            250.34\nName: 359, dtype: object\nTime                            9/26/2018 6:00\nA (38.152231, -119.6666755)             257.65\nB (38.279274, -119.6127765)          258.88998\nC (38.50458, -119.62176)                260.77\nD (37.862028, -119.6576925)             265.22\nE (37.89748, -119.2624335)              248.58\nF (37.8762105, -119.3432825)         252.26999\nG (37.833654, -119.4510805)          258.50998\nH (38.0603395, -119.6666755)            258.22\nI (37.798171, -119.19955150             251.23\nJ (38.0391175, -119.3073495)            249.79\nName: 360, dtype: object\nTime                            9/27/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 361, dtype: object\nTime                            9/28/2018 6:00\nA (38.152231, -119.6666755)             260.49\nB (38.279274, -119.6127765)             262.97\nC (38.50458, -119.62176)                266.09\nD (37.862028, -119.6576925)          262.97998\nE (37.89748, -119.2624335)              253.01\nF (37.8762105, -119.3432825)            255.48\nG (37.833654, -119.4510805)             259.61\nH (38.0603395, -119.6666755)            260.33\nI (37.798171, -119.19955150          254.70999\nJ (38.0391175, -119.3073495)         252.51999\nName: 362, dtype: object\nTime                            9/29/2018 6:00\nA (38.152231, -119.6666755)             259.52\nB (38.279274, -119.6127765)             261.13\nC (38.50458, -119.62176)                263.65\nD (37.862028, -119.6576925)              263.1\nE (37.89748, -119.2624335)              252.01\nF (37.8762105, -119.3432825)            254.34\nG (37.833654, -119.4510805)             258.03\nH (38.0603395, -119.6666755)            259.63\nI (37.798171, -119.19955150          253.34999\nJ (38.0391175, -119.3073495)         252.98999\nName: 363, dtype: object\n         date        lat           lon     pmv\n0  2017-10-01  38.152231  -119.6666755  256.03\n1  2017-10-01  38.279274  -119.6127765  257.35\n2  2017-10-01   38.50458    -119.62176  259.46\n3  2017-10-01  37.862028  -119.6576925  259.87\n4  2017-10-01   37.89748  -119.2624335  247.56\nNew PMV file is saved!!! /home/ubuntu/gridmet_test_run/PMW_training_new.csv\n",
  "history_begin_time" : 1698188817445,
  "history_end_time" : 1698188820776,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "XUJ2UixAnpWt",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  print(row)\n  \n  for property_name, value in row.items():\n    #print(\"property_name: \", property_name)\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n    \n    # Convert the input time string to a datetime object\n    datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n    # Convert the datetime object to the desired format\n    formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [formatted_time_string, lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [formatted_time_string, lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\npmv_new_df.to_csv(pmw_training_new, index=False)\nprint(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/XUJ2UixAnpWt/data_merge_hackweek.py\", line 74, in <module>\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/XUJ2UixAnpWt/data_merge_hackweek.py\", line 40, in adjust_column_to_rows\n    datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n                   ^^^^^^^^\nNameError: name 'datetime' is not defined\n",
  "history_begin_time" : 1698188802556,
  "history_end_time" : 1698188803052,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "tD6x9eyL4KSi",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  print(row)\n  \n  for property_name, value in row.items():\n    #print(\"property_name: \", property_name)\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n        lat = match.group(1)\n        lon = match.group(2)\n        #print(\"Latitude:\", lat)\n        #print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\nprint(pmv_new_df.shape())\npmv_new_df.to_csv(pmw_training_new, index=False)\nprint(f\"New PMV file is saved!!! {pmw_training_new}\")\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nTime                            10/2/2017 6:00\nA (38.152231, -119.6666755)             249.79\nB (38.279274, -119.6127765)             250.26\nC (38.50458, -119.62176)                253.73\nD (37.862028, -119.6576925)          256.16998\nE (37.89748, -119.2624335)           243.37999\nF (37.8762105, -119.3432825)            247.51\nG (37.833654, -119.4510805)              251.0\nH (38.0603395, -119.6666755)             251.2\nI (37.798171, -119.19955150             242.98\nJ (38.0391175, -119.3073495)         246.26999\nName: 1, dtype: object\nTime                            10/3/2017 6:00\nA (38.152231, -119.6666755)             249.61\nB (38.279274, -119.6127765)             248.98\nC (38.50458, -119.62176)                252.51\nD (37.862028, -119.6576925)          255.79999\nE (37.89748, -119.2624335)              242.95\nF (37.8762105, -119.3432825)         244.23999\nG (37.833654, -119.4510805)             248.87\nH (38.0603395, -119.6666755)            250.31\nI (37.798171, -119.19955150             242.78\nJ (38.0391175, -119.3073495)             243.7\nName: 2, dtype: object\nTime                            10/4/2017 6:00\nA (38.152231, -119.6666755)             249.28\nB (38.279274, -119.6127765)          249.40999\nC (38.50458, -119.62176)                252.68\nD (37.862028, -119.6576925)              255.2\nE (37.89748, -119.2624335)              239.87\nF (37.8762105, -119.3432825)            242.56\nG (37.833654, -119.4510805)             249.95\nH (38.0603395, -119.6666755)         249.95999\nI (37.798171, -119.19955150          242.20999\nJ (38.0391175, -119.3073495)         242.09999\nName: 3, dtype: object\nTime                            10/5/2017 6:00\nA (38.152231, -119.6666755)             250.11\nB (38.279274, -119.6127765)          251.12999\nC (38.50458, -119.62176)             254.76999\nD (37.862028, -119.6576925)             258.03\nE (37.89748, -119.2624335)           241.40999\nF (37.8762105, -119.3432825)         244.70999\nG (37.833654, -119.4510805)             252.42\nH (38.0603395, -119.6666755)         251.29999\nI (37.798171, -119.19955150             245.09\nJ (38.0391175, -119.3073495)         243.06999\nName: 4, dtype: object\nTime                            10/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 5, dtype: object\nTime                            10/7/2017 6:00\nA (38.152231, -119.6666755)          256.16998\nB (38.279274, -119.6127765)              258.3\nC (38.50458, -119.62176)                261.61\nD (37.862028, -119.6576925)             258.69\nE (37.89748, -119.2624335)              252.11\nF (37.8762105, -119.3432825)            252.48\nG (37.833654, -119.4510805)             254.45\nH (38.0603395, -119.6666755)         255.95999\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)             252.4\nName: 6, dtype: object\nTime                            10/8/2017 6:00\nA (38.152231, -119.6666755)             255.84\nB (38.279274, -119.6127765)             257.32\nC (38.50458, -119.62176)                260.16\nD (37.862028, -119.6576925)          259.66998\nE (37.89748, -119.2624335)              248.25\nF (37.8762105, -119.3432825)            250.84\nG (37.833654, -119.4510805)          255.84999\nH (38.0603395, -119.6666755)            255.72\nI (37.798171, -119.19955150             250.14\nJ (38.0391175, -119.3073495)            248.34\nName: 7, dtype: object\nTime                            10/9/2017 6:00\nA (38.152231, -119.6666755)          253.45999\nB (38.279274, -119.6127765)          252.51999\nC (38.50458, -119.62176)                253.48\nD (37.862028, -119.6576925)             260.43\nE (37.89748, -119.2624335)              245.37\nF (37.8762105, -119.3432825)         248.04999\nG (37.833654, -119.4510805)             252.81\nH (38.0603395, -119.6666755)            254.29\nI (37.798171, -119.19955150          247.12999\nJ (38.0391175, -119.3073495)             245.4\nName: 8, dtype: object\nTime                            10/10/2017 6:00\nA (38.152231, -119.6666755)              252.25\nB (38.279274, -119.6127765)              253.53\nC (38.50458, -119.62176)                 255.42\nD (37.862028, -119.6576925)              257.82\nE (37.89748, -119.2624335)               241.45\nF (37.8762105, -119.3432825)          245.81999\nG (37.833654, -119.4510805)           254.76999\nH (38.0603395, -119.6666755)             253.28\nI (37.798171, -119.19955150              243.92\nJ (38.0391175, -119.3073495)             243.51\nName: 9, dtype: object\nTime                            10/11/2017 6:00\nA (38.152231, -119.6666755)              252.04\nB (38.279274, -119.6127765)              253.58\nC (38.50458, -119.62176)              257.69998\nD (37.862028, -119.6576925)              258.77\nE (37.89748, -119.2624335)            244.37999\nF (37.8762105, -119.3432825)             248.17\nG (37.833654, -119.4510805)           252.37999\nH (38.0603395, -119.6666755)             253.06\nI (37.798171, -119.19955150              245.86\nJ (38.0391175, -119.3073495)             247.12\nName: 10, dtype: object\nTime                            10/12/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 11, dtype: object\nTime                            10/13/2017 6:00\nA (38.152231, -119.6666755)              251.18\nB (38.279274, -119.6127765)           251.81999\nC (38.50458, -119.62176)              254.79999\nD (37.862028, -119.6576925)           257.22998\nE (37.89748, -119.2624335)               241.64\nF (37.8762105, -119.3432825)          244.65999\nG (37.833654, -119.4510805)           252.01999\nH (38.0603395, -119.6666755)          251.84999\nI (37.798171, -119.19955150           244.51999\nJ (38.0391175, -119.3073495)             242.09\nName: 12, dtype: object\nTime                            10/14/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 13, dtype: object\nTime                            10/15/2017 6:00\nA (38.152231, -119.6666755)              255.17\nB (38.279274, -119.6127765)           256.94998\nC (38.50458, -119.62176)                 258.22\nD (37.862028, -119.6576925)              259.59\nE (37.89748, -119.2624335)               245.95\nF (37.8762105, -119.3432825)          250.01999\nG (37.833654, -119.4510805)           255.37999\nH (38.0603395, -119.6666755)             255.48\nI (37.798171, -119.19955150              248.29\nJ (38.0391175, -119.3073495)          244.98999\nName: 14, dtype: object\nTime                            10/16/2017 6:00\nA (38.152231, -119.6666755)           255.45999\nB (38.279274, -119.6127765)              256.08\nC (38.50458, -119.62176)                 257.69\nD (37.862028, -119.6576925)              260.29\nE (37.89748, -119.2624335)               246.09\nF (37.8762105, -119.3432825)              249.4\nG (37.833654, -119.4510805)              254.62\nH (38.0603395, -119.6666755)             256.03\nI (37.798171, -119.19955150              248.39\nJ (38.0391175, -119.3073495)          246.37999\nName: 15, dtype: object\nTime                            10/17/2017 6:00\nA (38.152231, -119.6666755)           257.16998\nB (38.279274, -119.6127765)           257.63998\nC (38.50458, -119.62176)              261.00998\nD (37.862028, -119.6576925)           262.44998\nE (37.89748, -119.2624335)               249.87\nF (37.8762105, -119.3432825)          252.06999\nG (37.833654, -119.4510805)              254.84\nH (38.0603395, -119.6666755)             257.38\nI (37.798171, -119.19955150              252.18\nJ (38.0391175, -119.3073495)          250.01999\nName: 16, dtype: object\nTime                            10/18/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 17, dtype: object\nTime                            10/19/2017 6:00\nA (38.152231, -119.6666755)              252.36\nB (38.279274, -119.6127765)           253.29999\nC (38.50458, -119.62176)                 257.31\nD (37.862028, -119.6576925)              258.85\nE (37.89748, -119.2624335)            244.59999\nF (37.8762105, -119.3432825)             247.59\nG (37.833654, -119.4510805)           254.54999\nH (38.0603395, -119.6666755)             253.62\nI (37.798171, -119.19955150              246.47\nJ (38.0391175, -119.3073495)          246.01999\nName: 18, dtype: object\nTime                            10/20/2017 6:00\nA (38.152231, -119.6666755)               250.5\nB (38.279274, -119.6127765)              252.06\nC (38.50458, -119.62176)                  254.4\nD (37.862028, -119.6576925)              256.29\nE (37.89748, -119.2624335)               247.29\nF (37.8762105, -119.3432825)             250.47\nG (37.833654, -119.4510805)              254.95\nH (38.0603395, -119.6666755)             251.06\nI (37.798171, -119.19955150               250.5\nJ (38.0391175, -119.3073495)          248.15999\nName: 19, dtype: object\nTime                            10/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 20, dtype: object\nTime                            10/22/2017 6:00\nA (38.152231, -119.6666755)           255.43999\nB (38.279274, -119.6127765)              254.42\nC (38.50458, -119.62176)                 259.25\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)             255.12\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 21, dtype: object\nTime                            10/23/2017 6:00\nA (38.152231, -119.6666755)           257.66998\nB (38.279274, -119.6127765)              258.59\nC (38.50458, -119.62176)                 260.68\nD (37.862028, -119.6576925)              262.99\nE (37.89748, -119.2624335)               249.03\nF (37.8762105, -119.3432825)             252.68\nG (37.833654, -119.4510805)              258.36\nH (38.0603395, -119.6666755)          257.63998\nI (37.798171, -119.19955150              251.12\nJ (38.0391175, -119.3073495)              249.0\nName: 22, dtype: object\nTime                            10/24/2017 6:00\nA (38.152231, -119.6666755)              256.99\nB (38.279274, -119.6127765)              257.54\nC (38.50458, -119.62176)                 258.43\nD (37.862028, -119.6576925)              263.55\nE (37.89748, -119.2624335)            247.76999\nF (37.8762105, -119.3432825)             251.18\nG (37.833654, -119.4510805)               257.1\nH (38.0603395, -119.6666755)             257.79\nI (37.798171, -119.19955150              251.25\nJ (38.0391175, -119.3073495)             248.36\nName: 23, dtype: object\nTime                            10/25/2017 6:00\nA (38.152231, -119.6666755)              255.22\nB (38.279274, -119.6127765)              255.72\nC (38.50458, -119.62176)              259.00998\nD (37.862028, -119.6576925)              262.69\nE (37.89748, -119.2624335)               247.92\nF (37.8762105, -119.3432825)             251.83\nG (37.833654, -119.4510805)              256.34\nH (38.0603395, -119.6666755)          256.41998\nI (37.798171, -119.19955150           248.01999\nJ (38.0391175, -119.3073495)          250.65999\nName: 24, dtype: object\nTime                            10/26/2017 6:00\nA (38.152231, -119.6666755)              255.36\nB (38.279274, -119.6127765)              254.75\nC (38.50458, -119.62176)              258.91998\nD (37.862028, -119.6576925)              262.52\nE (37.89748, -119.2624335)            247.87999\nF (37.8762105, -119.3432825)             251.34\nG (37.833654, -119.4510805)              254.89\nH (38.0603395, -119.6666755)          256.22998\nI (37.798171, -119.19955150           247.95999\nJ (38.0391175, -119.3073495)          249.79999\nName: 25, dtype: object\nTime                            10/27/2017 6:00\nA (38.152231, -119.6666755)           254.68999\nB (38.279274, -119.6127765)              255.09\nC (38.50458, -119.62176)                 258.69\nD (37.862028, -119.6576925)              263.72\nE (37.89748, -119.2624335)               245.62\nF (37.8762105, -119.3432825)             250.15\nG (37.833654, -119.4510805)              257.02\nH (38.0603395, -119.6666755)             256.41\nI (37.798171, -119.19955150              248.17\nJ (38.0391175, -119.3073495)          248.31999\nName: 26, dtype: object\nTime                            10/28/2017 6:00\nA (38.152231, -119.6666755)              256.13\nB (38.279274, -119.6127765)              256.35\nC (38.50458, -119.62176)                 259.44\nD (37.862028, -119.6576925)              263.86\nE (37.89748, -119.2624335)            247.18999\nF (37.8762105, -119.3432825)             251.08\nG (37.833654, -119.4510805)              258.74\nH (38.0603395, -119.6666755)             256.93\nI (37.798171, -119.19955150           249.90999\nJ (38.0391175, -119.3073495)             247.28\nName: 27, dtype: object\nTime                            10/29/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 28, dtype: object\nTime                            10/30/2017 6:00\nA (38.152231, -119.6666755)              256.11\nB (38.279274, -119.6127765)              257.93\nC (38.50458, -119.62176)              261.38998\nD (37.862028, -119.6576925)              259.72\nE (37.89748, -119.2624335)            247.43999\nF (37.8762105, -119.3432825)             251.28\nG (37.833654, -119.4510805)              255.79\nH (38.0603395, -119.6666755)          255.81999\nI (37.798171, -119.19955150              249.97\nJ (38.0391175, -119.3073495)             246.33\nName: 29, dtype: object\nTime                            10/31/2017 6:00\nA (38.152231, -119.6666755)              252.62\nB (38.279274, -119.6127765)           253.31999\nC (38.50458, -119.62176)                 255.43\nD (37.862028, -119.6576925)              256.88\nE (37.89748, -119.2624335)            245.12999\nF (37.8762105, -119.3432825)             247.95\nG (37.833654, -119.4510805)               252.7\nH (38.0603395, -119.6666755)             252.93\nI (37.798171, -119.19955150           246.70999\nJ (38.0391175, -119.3073495)             246.01\nName: 30, dtype: object\nTime                            11/1/2017 6:00\nA (38.152231, -119.6666755)          252.76999\nB (38.279274, -119.6127765)             253.83\nC (38.50458, -119.62176)                256.04\nD (37.862028, -119.6576925)          257.88998\nE (37.89748, -119.2624335)              242.54\nF (37.8762105, -119.3432825)            244.75\nG (37.833654, -119.4510805)             252.54\nH (38.0603395, -119.6666755)            253.72\nI (37.798171, -119.19955150          247.48999\nJ (38.0391175, -119.3073495)         242.26999\nName: 31, dtype: object\nTime                            11/2/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 32, dtype: object\nTime                            11/3/2017 6:00\nA (38.152231, -119.6666755)             251.12\nB (38.279274, -119.6127765)             251.48\nC (38.50458, -119.62176)             255.37999\nD (37.862028, -119.6576925)             257.25\nE (37.89748, -119.2624335)              243.33\nF (37.8762105, -119.3432825)            247.25\nG (37.833654, -119.4510805)             252.22\nH (38.0603395, -119.6666755)         252.04999\nI (37.798171, -119.19955150             245.11\nJ (38.0391175, -119.3073495)         246.51999\nName: 33, dtype: object\nTime                            11/4/2017 6:00\nA (38.152231, -119.6666755)          252.26999\nB (38.279274, -119.6127765)             253.08\nC (38.50458, -119.62176)             256.19998\nD (37.862028, -119.6576925)             257.35\nE (37.89748, -119.2624335)              243.34\nF (37.8762105, -119.3432825)         246.37999\nG (37.833654, -119.4510805)             252.48\nH (38.0603395, -119.6666755)            252.83\nI (37.798171, -119.19955150          246.84999\nJ (38.0391175, -119.3073495)         245.20999\nName: 34, dtype: object\nTime                            11/5/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              237.04\nF (37.8762105, -119.3432825)            238.01\nG (37.833654, -119.4510805)             240.61\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             245.48\nJ (38.0391175, -119.3073495)            233.28\nName: 35, dtype: object\nTime                            11/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 36, dtype: object\nTime                            11/7/2017 6:00\nA (38.152231, -119.6666755)             247.76\nB (38.279274, -119.6127765)             248.95\nC (38.50458, -119.62176)                 250.9\nD (37.862028, -119.6576925)             252.73\nE (37.89748, -119.2624335)              240.93\nF (37.8762105, -119.3432825)             244.2\nG (37.833654, -119.4510805)             248.58\nH (38.0603395, -119.6666755)            247.93\nI (37.798171, -119.19955150          243.37999\nJ (38.0391175, -119.3073495)            239.97\nName: 37, dtype: object\nTime                            11/8/2017 6:00\nA (38.152231, -119.6666755)             250.84\nB (38.279274, -119.6127765)          251.68999\nC (38.50458, -119.62176)                254.92\nD (37.862028, -119.6576925)          255.98999\nE (37.89748, -119.2624335)              242.11\nF (37.8762105, -119.3432825)         245.79999\nG (37.833654, -119.4510805)             252.47\nH (38.0603395, -119.6666755)            250.92\nI (37.798171, -119.19955150              244.7\nJ (38.0391175, -119.3073495)            242.86\nName: 38, dtype: object\nTime                            11/9/2017 6:00\nA (38.152231, -119.6666755)          253.34999\nB (38.279274, -119.6127765)             254.17\nC (38.50458, -119.62176)             257.19998\nD (37.862028, -119.6576925)             258.83\nE (37.89748, -119.2624335)           247.93999\nF (37.8762105, -119.3432825)            248.22\nG (37.833654, -119.4510805)             251.83\nH (38.0603395, -119.6666755)             253.9\nI (37.798171, -119.19955150             249.59\nJ (38.0391175, -119.3073495)             247.7\nName: 39, dtype: object\nTime                            11/10/2017 6:00\nA (38.152231, -119.6666755)              250.61\nB (38.279274, -119.6127765)              251.12\nC (38.50458, -119.62176)                 254.92\nD (37.862028, -119.6576925)              256.31\nE (37.89748, -119.2624335)               242.43\nF (37.8762105, -119.3432825)              242.9\nG (37.833654, -119.4510805)           250.15999\nH (38.0603395, -119.6666755)             251.79\nI (37.798171, -119.19955150              246.26\nJ (38.0391175, -119.3073495)             243.03\nName: 40, dtype: object\nTime                            11/11/2017 6:00\nA (38.152231, -119.6666755)               246.5\nB (38.279274, -119.6127765)              246.33\nC (38.50458, -119.62176)              250.06999\nD (37.862028, -119.6576925)           255.31999\nE (37.89748, -119.2624335)            238.62999\nF (37.8762105, -119.3432825)          242.12999\nG (37.833654, -119.4510805)           249.81999\nH (38.0603395, -119.6666755)          247.59999\nI (37.798171, -119.19955150               241.7\nJ (38.0391175, -119.3073495)             240.06\nName: 41, dtype: object\nTime                            11/12/2017 6:00\nA (38.152231, -119.6666755)              250.92\nB (38.279274, -119.6127765)              252.59\nC (38.50458, -119.62176)              255.45999\nD (37.862028, -119.6576925)              258.66\nE (37.89748, -119.2624335)               244.75\nF (37.8762105, -119.3432825)             247.93\nG (37.833654, -119.4510805)              254.73\nH (38.0603395, -119.6666755)             251.79\nI (37.798171, -119.19955150              247.64\nJ (38.0391175, -119.3073495)             244.81\nName: 42, dtype: object\nTime                            11/13/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 43, dtype: object\nTime                            11/14/2017 6:00\nA (38.152231, -119.6666755)              250.61\nB (38.279274, -119.6127765)           253.26999\nC (38.50458, -119.62176)                  256.1\nD (37.862028, -119.6576925)           253.62999\nE (37.89748, -119.2624335)               245.37\nF (37.8762105, -119.3432825)          247.59999\nG (37.833654, -119.4510805)           251.12999\nH (38.0603395, -119.6666755)             250.08\nI (37.798171, -119.19955150              246.73\nJ (38.0391175, -119.3073495)             244.75\nName: 44, dtype: object\nTime                            11/15/2017 6:00\nA (38.152231, -119.6666755)           251.73999\nB (38.279274, -119.6127765)           253.40999\nC (38.50458, -119.62176)              256.00998\nD (37.862028, -119.6576925)              257.55\nE (37.89748, -119.2624335)            247.68999\nF (37.8762105, -119.3432825)             249.64\nG (37.833654, -119.4510805)              253.65\nH (38.0603395, -119.6666755)          252.73999\nI (37.798171, -119.19955150           249.73999\nJ (38.0391175, -119.3073495)             247.09\nName: 45, dtype: object\nTime                            11/16/2017 6:00\nA (38.152231, -119.6666755)              250.28\nB (38.279274, -119.6127765)              250.43\nC (38.50458, -119.62176)                 253.34\nD (37.862028, -119.6576925)           255.84999\nE (37.89748, -119.2624335)               247.28\nF (37.8762105, -119.3432825)          249.65999\nG (37.833654, -119.4510805)              253.95\nH (38.0603395, -119.6666755)             251.54\nI (37.798171, -119.19955150              250.08\nJ (38.0391175, -119.3073495)             247.51\nName: 46, dtype: object\nTime                            11/17/2017 6:00\nA (38.152231, -119.6666755)              246.45\nB (38.279274, -119.6127765)           246.51999\nC (38.50458, -119.62176)              250.09999\nD (37.862028, -119.6576925)              254.43\nE (37.89748, -119.2624335)               243.64\nF (37.8762105, -119.3432825)              247.9\nG (37.833654, -119.4510805)              249.95\nH (38.0603395, -119.6666755)             248.22\nI (37.798171, -119.19955150           245.43999\nJ (38.0391175, -119.3073495)          244.15999\nName: 47, dtype: object\nTime                            11/18/2017 6:00\nA (38.152231, -119.6666755)               240.2\nB (38.279274, -119.6127765)           238.37999\nC (38.50458, -119.62176)                 245.33\nD (37.862028, -119.6576925)              252.06\nE (37.89748, -119.2624335)            236.51999\nF (37.8762105, -119.3432825)             240.48\nG (37.833654, -119.4510805)              244.54\nH (38.0603395, -119.6666755)             241.78\nI (37.798171, -119.19955150           238.76999\nJ (38.0391175, -119.3073495)             237.09\nName: 48, dtype: object\nTime                            11/19/2017 6:00\nA (38.152231, -119.6666755)              239.37\nB (38.279274, -119.6127765)              239.86\nC (38.50458, -119.62176)                 247.62\nD (37.862028, -119.6576925)              252.37\nE (37.89748, -119.2624335)            237.01999\nF (37.8762105, -119.3432825)             239.89\nG (37.833654, -119.4510805)              245.39\nH (38.0603395, -119.6666755)             242.42\nI (37.798171, -119.19955150              240.23\nJ (38.0391175, -119.3073495)             235.48\nName: 49, dtype: object\nTime                            11/20/2017 6:00\nA (38.152231, -119.6666755)              245.98\nB (38.279274, -119.6127765)           247.18999\nC (38.50458, -119.62176)                 253.28\nD (37.862028, -119.6576925)               258.8\nE (37.89748, -119.2624335)                244.9\nF (37.8762105, -119.3432825)          248.59999\nG (37.833654, -119.4510805)           254.18999\nH (38.0603395, -119.6666755)             248.26\nI (37.798171, -119.19955150              248.01\nJ (38.0391175, -119.3073495)             242.86\nName: 50, dtype: object\nTime                            11/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 51, dtype: object\nTime                            11/22/2017 6:00\nA (38.152231, -119.6666755)              244.84\nB (38.279274, -119.6127765)              246.54\nC (38.50458, -119.62176)                 255.64\nD (37.862028, -119.6576925)              255.28\nE (37.89748, -119.2624335)               237.67\nF (37.8762105, -119.3432825)             239.58\nG (37.833654, -119.4510805)              244.31\nH (38.0603395, -119.6666755)             246.15\nI (37.798171, -119.19955150           241.59999\nJ (38.0391175, -119.3073495)          237.56999\nName: 52, dtype: object\nTime                            11/23/2017 6:00\nA (38.152231, -119.6666755)           242.45999\nB (38.279274, -119.6127765)               243.9\nC (38.50458, -119.62176)              254.59999\nD (37.862028, -119.6576925)              254.93\nE (37.89748, -119.2624335)            234.18999\nF (37.8762105, -119.3432825)             237.89\nG (37.833654, -119.4510805)              244.11\nH (38.0603395, -119.6666755)          244.62999\nI (37.798171, -119.19955150           238.29999\nJ (38.0391175, -119.3073495)             236.76\nName: 53, dtype: object\nTime                            11/24/2017 6:00\nA (38.152231, -119.6666755)           242.51999\nB (38.279274, -119.6127765)              243.61\nC (38.50458, -119.62176)                 251.39\nD (37.862028, -119.6576925)           254.15999\nE (37.89748, -119.2624335)            233.37999\nF (37.8762105, -119.3432825)             235.84\nG (37.833654, -119.4510805)              244.61\nH (38.0603395, -119.6666755)          244.48999\nI (37.798171, -119.19955150              238.12\nJ (38.0391175, -119.3073495)             234.15\nName: 54, dtype: object\nTime                            11/25/2017 6:00\nA (38.152231, -119.6666755)              243.26\nB (38.279274, -119.6127765)              242.62\nC (38.50458, -119.62176)                 250.72\nD (37.862028, -119.6576925)              257.53\nE (37.89748, -119.2624335)            233.23999\nF (37.8762105, -119.3432825)              238.2\nG (37.833654, -119.4510805)           249.34999\nH (38.0603395, -119.6666755)             246.98\nI (37.798171, -119.19955150              237.34\nJ (38.0391175, -119.3073495)             234.65\nName: 55, dtype: object\nTime                            11/26/2017 6:00\nA (38.152231, -119.6666755)              244.26\nB (38.279274, -119.6127765)              244.31\nC (38.50458, -119.62176)                 254.39\nD (37.862028, -119.6576925)           258.91998\nE (37.89748, -119.2624335)               236.93\nF (37.8762105, -119.3432825)              238.0\nG (37.833654, -119.4510805)              247.14\nH (38.0603395, -119.6666755)          247.06999\nI (37.798171, -119.19955150              241.98\nJ (38.0391175, -119.3073495)             238.17\nName: 56, dtype: object\nTime                            11/27/2017 6:00\nA (38.152231, -119.6666755)           244.31999\nB (38.279274, -119.6127765)           244.51999\nC (38.50458, -119.62176)                 251.09\nD (37.862028, -119.6576925)              252.11\nE (37.89748, -119.2624335)            238.26999\nF (37.8762105, -119.3432825)          241.15999\nG (37.833654, -119.4510805)           246.65999\nH (38.0603395, -119.6666755)          245.70999\nI (37.798171, -119.19955150              241.47\nJ (38.0391175, -119.3073495)             238.61\nName: 57, dtype: object\nTime                            11/28/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 58, dtype: object\nTime                            11/29/2017 6:00\nA (38.152231, -119.6666755)              236.54\nB (38.279274, -119.6127765)              238.01\nC (38.50458, -119.62176)                 246.93\nD (37.862028, -119.6576925)           246.56999\nE (37.89748, -119.2624335)               229.22\nF (37.8762105, -119.3432825)          232.59999\nG (37.833654, -119.4510805)           239.90999\nH (38.0603395, -119.6666755)          237.37999\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)             231.08\nName: 59, dtype: object\nTime                            11/30/2017 6:00\nA (38.152231, -119.6666755)              235.97\nB (38.279274, -119.6127765)              237.15\nC (38.50458, -119.62176)              245.81999\nD (37.862028, -119.6576925)              247.42\nE (37.89748, -119.2624335)            232.06999\nF (37.8762105, -119.3432825)          233.15999\nG (37.833654, -119.4510805)              239.03\nH (38.0603395, -119.6666755)          237.84999\nI (37.798171, -119.19955150              235.79\nJ (38.0391175, -119.3073495)          231.84999\nName: 60, dtype: object\nTime                            12/1/2017 6:00\nA (38.152231, -119.6666755)             237.48\nB (38.279274, -119.6127765)          234.90999\nC (38.50458, -119.62176)                245.12\nD (37.862028, -119.6576925)          250.20999\nE (37.89748, -119.2624335)              233.17\nF (37.8762105, -119.3432825)            236.31\nG (37.833654, -119.4510805)          241.76999\nH (38.0603395, -119.6666755)            238.89\nI (37.798171, -119.19955150          234.29999\nJ (38.0391175, -119.3073495)         234.54999\nName: 61, dtype: object\nTime                            12/2/2017 6:00\nA (38.152231, -119.6666755)             238.76\nB (38.279274, -119.6127765)          237.48999\nC (38.50458, -119.62176)             247.90999\nD (37.862028, -119.6576925)          252.51999\nE (37.89748, -119.2624335)           233.76999\nF (37.8762105, -119.3432825)         234.68999\nG (37.833654, -119.4510805)          239.70999\nH (38.0603395, -119.6666755)         240.79999\nI (37.798171, -119.19955150          238.18999\nJ (38.0391175, -119.3073495)            234.33\nName: 62, dtype: object\nTime                            12/3/2017 6:00\nA (38.152231, -119.6666755)             241.45\nB (38.279274, -119.6127765)          239.65999\nC (38.50458, -119.62176)             246.93999\nD (37.862028, -119.6576925)          252.40999\nE (37.89748, -119.2624335)           232.81999\nF (37.8762105, -119.3432825)         233.93999\nG (37.833654, -119.4510805)             243.89\nH (38.0603395, -119.6666755)         242.87999\nI (37.798171, -119.19955150          238.37999\nJ (38.0391175, -119.3073495)            234.45\nName: 63, dtype: object\nTime                            12/4/2017 6:00\nA (38.152231, -119.6666755)             231.39\nB (38.279274, -119.6127765)          230.09999\nC (38.50458, -119.62176)                 239.4\nD (37.862028, -119.6576925)             246.08\nE (37.89748, -119.2624335)              226.76\nF (37.8762105, -119.3432825)         229.40999\nG (37.833654, -119.4510805)          236.37999\nH (38.0603395, -119.6666755)            233.78\nI (37.798171, -119.19955150             227.89\nJ (38.0391175, -119.3073495)            226.75\nName: 64, dtype: object\nTime                            12/5/2017 6:00\nA (38.152231, -119.6666755)          236.18999\nB (38.279274, -119.6127765)             234.72\nC (38.50458, -119.62176)                241.17\nD (37.862028, -119.6576925)          251.48999\nE (37.89748, -119.2624335)           229.84999\nF (37.8762105, -119.3432825)            233.87\nG (37.833654, -119.4510805)          242.59999\nH (38.0603395, -119.6666755)         238.26999\nI (37.798171, -119.19955150             231.08\nJ (38.0391175, -119.3073495)            227.93\nName: 65, dtype: object\nTime                            12/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 66, dtype: object\nTime                            12/7/2017 6:00\nA (38.152231, -119.6666755)          235.18999\nB (38.279274, -119.6127765)          236.59999\nC (38.50458, -119.62176)                 244.7\nD (37.862028, -119.6576925)             248.34\nE (37.89748, -119.2624335)              231.23\nF (37.8762105, -119.3432825)            234.18\nG (37.833654, -119.4510805)          239.40999\nH (38.0603395, -119.6666755)            236.51\nI (37.798171, -119.19955150             234.36\nJ (38.0391175, -119.3073495)            228.76\nName: 67, dtype: object\nTime                            12/8/2017 6:00\nA (38.152231, -119.6666755)             236.06\nB (38.279274, -119.6127765)             236.08\nC (38.50458, -119.62176)             244.43999\nD (37.862028, -119.6576925)             250.37\nE (37.89748, -119.2624335)           231.34999\nF (37.8762105, -119.3432825)         233.56999\nG (37.833654, -119.4510805)             240.86\nH (38.0603395, -119.6666755)            238.65\nI (37.798171, -119.19955150             234.59\nJ (38.0391175, -119.3073495)         230.87999\nName: 68, dtype: object\nTime                            12/9/2017 6:00\nA (38.152231, -119.6666755)             236.18\nB (38.279274, -119.6127765)             233.83\nC (38.50458, -119.62176)                241.39\nD (37.862028, -119.6576925)          250.01999\nE (37.89748, -119.2624335)           229.26999\nF (37.8762105, -119.3432825)            234.53\nG (37.833654, -119.4510805)          241.15999\nH (38.0603395, -119.6666755)         238.59999\nI (37.798171, -119.19955150             231.98\nJ (38.0391175, -119.3073495)            230.83\nName: 69, dtype: object\nTime                            12/10/2017 6:00\nA (38.152231, -119.6666755)           236.37999\nB (38.279274, -119.6127765)           234.18999\nC (38.50458, -119.62176)              241.70999\nD (37.862028, -119.6576925)           253.20999\nE (37.89748, -119.2624335)               231.69\nF (37.8762105, -119.3432825)             236.18\nG (37.833654, -119.4510805)           241.62999\nH (38.0603395, -119.6666755)             239.34\nI (37.798171, -119.19955150              233.12\nJ (38.0391175, -119.3073495)          231.65999\nName: 70, dtype: object\nTime                            12/11/2017 6:00\nA (38.152231, -119.6666755)           239.68999\nB (38.279274, -119.6127765)              236.76\nC (38.50458, -119.62176)                 244.56\nD (37.862028, -119.6576925)              253.65\nE (37.89748, -119.2624335)               231.12\nF (37.8762105, -119.3432825)          235.23999\nG (37.833654, -119.4510805)           243.59999\nH (38.0603395, -119.6666755)          241.93999\nI (37.798171, -119.19955150              236.26\nJ (38.0391175, -119.3073495)          232.79999\nName: 71, dtype: object\nTime                            12/12/2017 6:00\nA (38.152231, -119.6666755)              238.65\nB (38.279274, -119.6127765)               238.5\nC (38.50458, -119.62176)                 247.04\nD (37.862028, -119.6576925)              255.15\nE (37.89748, -119.2624335)               233.26\nF (37.8762105, -119.3432825)             236.97\nG (37.833654, -119.4510805)           245.18999\nH (38.0603395, -119.6666755)          241.87999\nI (37.798171, -119.19955150           235.93999\nJ (38.0391175, -119.3073495)             233.28\nName: 72, dtype: object\nTime                            12/13/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 73, dtype: object\nTime                            12/14/2017 6:00\nA (38.152231, -119.6666755)              247.68\nB (38.279274, -119.6127765)           241.56999\nC (38.50458, -119.62176)              248.95999\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 74, dtype: object\nTime                            12/15/2017 6:00\nA (38.152231, -119.6666755)           238.90999\nB (38.279274, -119.6127765)              240.84\nC (38.50458, -119.62176)                 249.36\nD (37.862028, -119.6576925)              252.59\nE (37.89748, -119.2624335)               235.58\nF (37.8762105, -119.3432825)          238.43999\nG (37.833654, -119.4510805)              243.92\nH (38.0603395, -119.6666755)             241.22\nI (37.798171, -119.19955150           239.43999\nJ (38.0391175, -119.3073495)          234.90999\nName: 75, dtype: object\nTime                            12/16/2017 6:00\nA (38.152231, -119.6666755)              239.14\nB (38.279274, -119.6127765)              238.95\nC (38.50458, -119.62176)              248.51999\nD (37.862028, -119.6576925)           250.23999\nE (37.89748, -119.2624335)               238.84\nF (37.8762105, -119.3432825)             239.56\nG (37.833654, -119.4510805)              241.98\nH (38.0603395, -119.6666755)          241.40999\nI (37.798171, -119.19955150              240.45\nJ (38.0391175, -119.3073495)             239.31\nName: 76, dtype: object\nTime                            12/17/2017 6:00\nA (38.152231, -119.6666755)           238.56999\nB (38.279274, -119.6127765)              237.15\nC (38.50458, -119.62176)              241.87999\nD (37.862028, -119.6576925)              250.59\nE (37.89748, -119.2624335)            229.54999\nF (37.8762105, -119.3432825)             232.39\nG (37.833654, -119.4510805)           242.40999\nH (38.0603395, -119.6666755)             241.01\nI (37.798171, -119.19955150              233.39\nJ (38.0391175, -119.3073495)             228.44\nName: 77, dtype: object\nTime                            12/18/2017 6:00\nA (38.152231, -119.6666755)              240.45\nB (38.279274, -119.6127765)              238.34\nC (38.50458, -119.62176)                 244.81\nD (37.862028, -119.6576925)              253.04\nE (37.89748, -119.2624335)            231.37999\nF (37.8762105, -119.3432825)          236.59999\nG (37.833654, -119.4510805)               246.5\nH (38.0603395, -119.6666755)          242.56999\nI (37.798171, -119.19955150           234.76999\nJ (38.0391175, -119.3073495)          233.65999\nName: 78, dtype: object\nTime                            12/19/2017 6:00\nA (38.152231, -119.6666755)              238.34\nB (38.279274, -119.6127765)              239.31\nC (38.50458, -119.62176)                 248.06\nD (37.862028, -119.6576925)              251.93\nE (37.89748, -119.2624335)               234.04\nF (37.8762105, -119.3432825)             236.58\nG (37.833654, -119.4510805)           243.76999\nH (38.0603395, -119.6666755)             241.93\nI (37.798171, -119.19955150              236.81\nJ (38.0391175, -119.3073495)             234.39\nName: 79, dtype: object\nTime                            12/20/2017 6:00\nA (38.152231, -119.6666755)              241.03\nB (38.279274, -119.6127765)           240.70999\nC (38.50458, -119.62176)              249.09999\nD (37.862028, -119.6576925)              254.59\nE (37.89748, -119.2624335)               237.83\nF (37.8762105, -119.3432825)             240.75\nG (37.833654, -119.4510805)              247.23\nH (38.0603395, -119.6666755)          243.29999\nI (37.798171, -119.19955150              240.37\nJ (38.0391175, -119.3073495)          237.09999\nName: 80, dtype: object\nTime                            12/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 81, dtype: object\nTime                            12/22/2017 6:00\nA (38.152231, -119.6666755)           237.79999\nB (38.279274, -119.6127765)              239.42\nC (38.50458, -119.62176)              248.01999\nD (37.862028, -119.6576925)           247.68999\nE (37.89748, -119.2624335)               232.62\nF (37.8762105, -119.3432825)             235.31\nG (37.833654, -119.4510805)           240.09999\nH (38.0603395, -119.6666755)             239.87\nI (37.798171, -119.19955150              236.33\nJ (38.0391175, -119.3073495)          231.06999\nName: 82, dtype: object\nTime                            12/23/2017 6:00\nA (38.152231, -119.6666755)              240.22\nB (38.279274, -119.6127765)           242.06999\nC (38.50458, -119.62176)                 250.67\nD (37.862028, -119.6576925)              251.15\nE (37.89748, -119.2624335)                236.4\nF (37.8762105, -119.3432825)             237.93\nG (37.833654, -119.4510805)              243.29\nH (38.0603395, -119.6666755)             241.29\nI (37.798171, -119.19955150              238.95\nJ (38.0391175, -119.3073495)          237.29999\nName: 83, dtype: object\nTime                            12/24/2017 6:00\nA (38.152231, -119.6666755)              241.61\nB (38.279274, -119.6127765)           241.34999\nC (38.50458, -119.62176)              249.98999\nD (37.862028, -119.6576925)              254.78\nE (37.89748, -119.2624335)               239.25\nF (37.8762105, -119.3432825)          242.20999\nG (37.833654, -119.4510805)              245.76\nH (38.0603395, -119.6666755)          243.98999\nI (37.798171, -119.19955150              240.37\nJ (38.0391175, -119.3073495)          239.40999\nName: 84, dtype: object\nTime                            12/25/2017 6:00\nA (38.152231, -119.6666755)           244.09999\nB (38.279274, -119.6127765)              242.81\nC (38.50458, -119.62176)                  249.7\nD (37.862028, -119.6576925)           255.79999\nE (37.89748, -119.2624335)               239.15\nF (37.8762105, -119.3432825)             240.26\nG (37.833654, -119.4510805)              244.58\nH (38.0603395, -119.6666755)          246.56999\nI (37.798171, -119.19955150              242.08\nJ (38.0391175, -119.3073495)             240.22\nName: 85, dtype: object\nTime                            12/26/2017 6:00\nA (38.152231, -119.6666755)              240.29\nB (38.279274, -119.6127765)              238.42\nC (38.50458, -119.62176)                 246.92\nD (37.862028, -119.6576925)              253.67\nE (37.89748, -119.2624335)               233.79\nF (37.8762105, -119.3432825)             237.84\nG (37.833654, -119.4510805)              246.54\nH (38.0603395, -119.6666755)          243.06999\nI (37.798171, -119.19955150           236.93999\nJ (38.0391175, -119.3073495)          235.12999\nName: 86, dtype: object\nTime                            12/27/2017 6:00\nA (38.152231, -119.6666755)           239.87999\nB (38.279274, -119.6127765)              240.62\nC (38.50458, -119.62176)                 249.68\nD (37.862028, -119.6576925)              253.92\nE (37.89748, -119.2624335)            234.20999\nF (37.8762105, -119.3432825)             238.47\nG (37.833654, -119.4510805)           245.31999\nH (38.0603395, -119.6666755)          242.31999\nI (37.798171, -119.19955150           237.45999\nJ (38.0391175, -119.3073495)          234.98999\nName: 87, dtype: object\nTime                            12/28/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)               232.04\nF (37.8762105, -119.3432825)             234.56\nG (37.833654, -119.4510805)           238.93999\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150               238.9\nJ (38.0391175, -119.3073495)          229.79999\nName: 88, dtype: object\nTime                            12/29/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 89, dtype: object\nTime                            12/30/2017 6:00\nA (38.152231, -119.6666755)              240.98\nB (38.279274, -119.6127765)              242.12\nC (38.50458, -119.62176)                 251.34\nD (37.862028, -119.6576925)           252.70999\nE (37.89748, -119.2624335)               237.45\nF (37.8762105, -119.3432825)             239.18\nG (37.833654, -119.4510805)              243.86\nH (38.0603395, -119.6666755)             241.92\nI (37.798171, -119.19955150              240.47\nJ (38.0391175, -119.3073495)          236.48999\nName: 90, dtype: object\nTime                            12/31/2017 6:00\nA (38.152231, -119.6666755)              240.37\nB (38.279274, -119.6127765)              241.25\nC (38.50458, -119.62176)                 248.87\nD (37.862028, -119.6576925)              253.73\nE (37.89748, -119.2624335)            237.15999\nF (37.8762105, -119.3432825)          239.20999\nG (37.833654, -119.4510805)           242.56999\nH (38.0603395, -119.6666755)             243.17\nI (37.798171, -119.19955150              238.65\nJ (38.0391175, -119.3073495)          237.06999\nName: 91, dtype: object\nTime                            1/1/2018 6:00\nA (38.152231, -119.6666755)         241.26999\nB (38.279274, -119.6127765)            242.09\nC (38.50458, -119.62176)            248.93999\nD (37.862028, -119.6576925)         253.48999\nE (37.89748, -119.2624335)             236.67\nF (37.8762105, -119.3432825)        239.70999\nG (37.833654, -119.4510805)         247.09999\nH (38.0603395, -119.6666755)        244.20999\nI (37.798171, -119.19955150         237.48999\nJ (38.0391175, -119.3073495)        238.23999\nName: 92, dtype: object\nTime                            1/2/2018 6:00\nA (38.152231, -119.6666755)            241.31\nB (38.279274, -119.6127765)            239.89\nC (38.50458, -119.62176)               249.37\nD (37.862028, -119.6576925)            257.44\nE (37.89748, -119.2624335)              237.5\nF (37.8762105, -119.3432825)           240.73\nG (37.833654, -119.4510805)             243.2\nH (38.0603395, -119.6666755)        242.90999\nI (37.798171, -119.19955150            239.93\nJ (38.0391175, -119.3073495)        239.04999\nName: 93, dtype: object\nTime                            1/3/2018 6:00\nA (38.152231, -119.6666755)            242.48\nB (38.279274, -119.6127765)            242.58\nC (38.50458, -119.62176)                251.0\nD (37.862028, -119.6576925)            256.81\nE (37.89748, -119.2624335)          236.54999\nF (37.8762105, -119.3432825)           238.98\nG (37.833654, -119.4510805)         247.15999\nH (38.0603395, -119.6666755)        245.65999\nI (37.798171, -119.19955150            238.78\nJ (38.0391175, -119.3073495)           237.25\nName: 94, dtype: object\nTime                            1/4/2018 6:00\nA (38.152231, -119.6666755)            247.78\nB (38.279274, -119.6127765)         248.68999\nC (38.50458, -119.62176)               255.93\nD (37.862028, -119.6576925)            258.78\nE (37.89748, -119.2624335)          241.54999\nF (37.8762105, -119.3432825)           244.53\nG (37.833654, -119.4510805)            252.26\nH (38.0603395, -119.6666755)        249.68999\nI (37.798171, -119.19955150         244.65999\nJ (38.0391175, -119.3073495)        242.18999\nName: 95, dtype: object\nTime                            1/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 96, dtype: object\nTime                            1/6/2018 6:00\nA (38.152231, -119.6666755)            247.47\nB (38.279274, -119.6127765)            250.92\nC (38.50458, -119.62176)               256.43\nD (37.862028, -119.6576925)            250.73\nE (37.89748, -119.2624335)          239.68999\nF (37.8762105, -119.3432825)           241.11\nG (37.833654, -119.4510805)         244.73999\nH (38.0603395, -119.6666755)           247.39\nI (37.798171, -119.19955150            240.73\nJ (38.0391175, -119.3073495)        241.18999\nName: 97, dtype: object\nTime                            1/7/2018 6:00\nA (38.152231, -119.6666755)            239.73\nB (38.279274, -119.6127765)         240.87999\nC (38.50458, -119.62176)            247.40999\nD (37.862028, -119.6576925)            248.98\nE (37.89748, -119.2624335)             234.15\nF (37.8762105, -119.3432825)           236.15\nG (37.833654, -119.4510805)         240.37999\nH (38.0603395, -119.6666755)           240.18\nI (37.798171, -119.19955150            236.97\nJ (38.0391175, -119.3073495)        234.09999\nName: 98, dtype: object\nTime                            1/8/2018 6:00\nA (38.152231, -119.6666755)            246.51\nB (38.279274, -119.6127765)            248.01\nC (38.50458, -119.62176)               254.12\nD (37.862028, -119.6576925)            253.79\nE (37.89748, -119.2624335)             241.42\nF (37.8762105, -119.3432825)           241.93\nG (37.833654, -119.4510805)            245.14\nH (38.0603395, -119.6666755)           247.37\nI (37.798171, -119.19955150            243.14\nJ (38.0391175, -119.3073495)           242.97\nName: 99, dtype: object\nTime                            1/9/2018 6:00\nA (38.152231, -119.6666755)            248.72\nB (38.279274, -119.6127765)         251.09999\nC (38.50458, -119.62176)            255.51999\nD (37.862028, -119.6576925)            254.31\nE (37.89748, -119.2624335)          243.20999\nF (37.8762105, -119.3432825)        244.12999\nG (37.833654, -119.4510805)         249.45999\nH (38.0603395, -119.6666755)           249.79\nI (37.798171, -119.19955150            244.58\nJ (38.0391175, -119.3073495)           245.47\nName: 100, dtype: object\nTime                            1/10/2018 6:00\nA (38.152231, -119.6666755)             247.75\nB (38.279274, -119.6127765)             247.62\nC (38.50458, -119.62176)                 253.7\nD (37.862028, -119.6576925)             255.04\nE (37.89748, -119.2624335)           238.59999\nF (37.8762105, -119.3432825)             241.7\nG (37.833654, -119.4510805)             246.45\nH (38.0603395, -119.6666755)            249.67\nI (37.798171, -119.19955150             238.87\nJ (38.0391175, -119.3073495)             241.4\nName: 101, dtype: object\nTime                            1/11/2018 6:00\nA (38.152231, -119.6666755)          240.79999\nB (38.279274, -119.6127765)             241.93\nC (38.50458, -119.62176)             249.23999\nD (37.862028, -119.6576925)             251.67\nE (37.89748, -119.2624335)           234.84999\nF (37.8762105, -119.3432825)            236.62\nG (37.833654, -119.4510805)             243.39\nH (38.0603395, -119.6666755)         241.98999\nI (37.798171, -119.19955150          236.95999\nJ (38.0391175, -119.3073495)            236.26\nName: 102, dtype: object\nTime                            1/12/2018 6:00\nA (38.152231, -119.6666755)             234.83\nB (38.279274, -119.6127765)             239.97\nC (38.50458, -119.62176)                250.06\nD (37.862028, -119.6576925)          249.09999\nE (37.89748, -119.2624335)              235.42\nF (37.8762105, -119.3432825)            238.17\nG (37.833654, -119.4510805)             244.48\nH (38.0603395, -119.6666755)            235.92\nI (37.798171, -119.19955150             236.98\nJ (38.0391175, -119.3073495)         234.18999\nName: 103, dtype: object\nTime                            1/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 104, dtype: object\nTime                            1/14/2018 6:00\nA (38.152231, -119.6666755)             235.58\nB (38.279274, -119.6127765)             239.45\nC (38.50458, -119.62176)                248.81\nD (37.862028, -119.6576925)             248.23\nE (37.89748, -119.2624335)              234.03\nF (37.8762105, -119.3432825)            234.51\nG (37.833654, -119.4510805)             237.83\nH (38.0603395, -119.6666755)         236.84999\nI (37.798171, -119.19955150             236.81\nJ (38.0391175, -119.3073495)            234.01\nName: 105, dtype: object\nTime                            1/15/2018 6:00\nA (38.152231, -119.6666755)             237.06\nB (38.279274, -119.6127765)             239.97\nC (38.50458, -119.62176)                 249.9\nD (37.862028, -119.6576925)             247.93\nE (37.89748, -119.2624335)              234.28\nF (37.8762105, -119.3432825)            235.51\nG (37.833654, -119.4510805)             237.37\nH (38.0603395, -119.6666755)            237.48\nI (37.798171, -119.19955150             236.12\nJ (38.0391175, -119.3073495)            235.72\nName: 106, dtype: object\nTime                            1/16/2018 6:00\nA (38.152231, -119.6666755)             237.03\nB (38.279274, -119.6127765)          238.76999\nC (38.50458, -119.62176)                249.17\nD (37.862028, -119.6576925)          249.90999\nE (37.89748, -119.2624335)               235.9\nF (37.8762105, -119.3432825)            236.78\nG (37.833654, -119.4510805)          237.87999\nH (38.0603395, -119.6666755)            238.79\nI (37.798171, -119.19955150          238.18999\nJ (38.0391175, -119.3073495)         238.45999\nName: 107, dtype: object\nTime                            1/17/2018 6:00\nA (38.152231, -119.6666755)             239.29\nB (38.279274, -119.6127765)          239.76999\nC (38.50458, -119.62176)             248.48999\nD (37.862028, -119.6576925)             249.75\nE (37.89748, -119.2624335)               232.9\nF (37.8762105, -119.3432825)            232.23\nG (37.833654, -119.4510805)             241.15\nH (38.0603395, -119.6666755)         240.48999\nI (37.798171, -119.19955150              236.7\nJ (38.0391175, -119.3073495)            236.36\nName: 108, dtype: object\nTime                            1/18/2018 6:00\nA (38.152231, -119.6666755)             239.78\nB (38.279274, -119.6127765)          242.09999\nC (38.50458, -119.62176)                253.11\nD (37.862028, -119.6576925)             252.86\nE (37.89748, -119.2624335)              234.56\nF (37.8762105, -119.3432825)            235.73\nG (37.833654, -119.4510805)             243.09\nH (38.0603395, -119.6666755)            240.62\nI (37.798171, -119.19955150             237.65\nJ (38.0391175, -119.3073495)             238.4\nName: 109, dtype: object\nTime                            1/19/2018 6:00\nA (38.152231, -119.6666755)             242.67\nB (38.279274, -119.6127765)          242.51999\nC (38.50458, -119.62176)             249.45999\nD (37.862028, -119.6576925)             251.72\nE (37.89748, -119.2624335)           239.04999\nF (37.8762105, -119.3432825)            240.81\nG (37.833654, -119.4510805)             246.59\nH (38.0603395, -119.6666755)         244.45999\nI (37.798171, -119.19955150             243.08\nJ (38.0391175, -119.3073495)         238.45999\nName: 110, dtype: object\nTime                            1/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 111, dtype: object\nTime                            1/21/2018 6:00\nA (38.152231, -119.6666755)              230.2\nB (38.279274, -119.6127765)             231.93\nC (38.50458, -119.62176)                239.48\nD (37.862028, -119.6576925)             239.28\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)         230.95999\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 112, dtype: object\nTime                            1/22/2018 6:00\nA (38.152231, -119.6666755)             235.59\nB (38.279274, -119.6127765)          236.65999\nC (38.50458, -119.62176)                245.26\nD (37.862028, -119.6576925)             246.98\nE (37.89748, -119.2624335)           233.73999\nF (37.8762105, -119.3432825)         234.65999\nG (37.833654, -119.4510805)             238.58\nH (38.0603395, -119.6666755)            237.95\nI (37.798171, -119.19955150          235.93999\nJ (38.0391175, -119.3073495)            233.26\nName: 113, dtype: object\nTime                            1/23/2018 6:00\nA (38.152231, -119.6666755)             231.51\nB (38.279274, -119.6127765)             233.67\nC (38.50458, -119.62176)                240.72\nD (37.862028, -119.6576925)              243.9\nE (37.89748, -119.2624335)              228.98\nF (37.8762105, -119.3432825)            228.81\nG (37.833654, -119.4510805)          233.40999\nH (38.0603395, -119.6666755)            233.54\nI (37.798171, -119.19955150             232.22\nJ (38.0391175, -119.3073495)            228.87\nName: 114, dtype: object\nTime                            1/24/2018 6:00\nA (38.152231, -119.6666755)             231.06\nB (38.279274, -119.6127765)             232.25\nC (38.50458, -119.62176)             243.37999\nD (37.862028, -119.6576925)             247.26\nE (37.89748, -119.2624335)           229.65999\nF (37.8762105, -119.3432825)            232.51\nG (37.833654, -119.4510805)             235.81\nH (38.0603395, -119.6666755)            234.75\nI (37.798171, -119.19955150             231.75\nJ (38.0391175, -119.3073495)            231.87\nName: 115, dtype: object\nTime                            1/25/2018 6:00\nA (38.152231, -119.6666755)             233.64\nB (38.279274, -119.6127765)          233.12999\nC (38.50458, -119.62176)             241.81999\nD (37.862028, -119.6576925)          243.29999\nE (37.89748, -119.2624335)              229.83\nF (37.8762105, -119.3432825)         230.40999\nG (37.833654, -119.4510805)          236.20999\nH (38.0603395, -119.6666755)             234.5\nI (37.798171, -119.19955150          233.93999\nJ (38.0391175, -119.3073495)            232.23\nName: 116, dtype: object\nTime                            1/26/2018 6:00\nA (38.152231, -119.6666755)          229.81999\nB (38.279274, -119.6127765)          231.29999\nC (38.50458, -119.62176)             238.73999\nD (37.862028, -119.6576925)          242.43999\nE (37.89748, -119.2624335)              225.72\nF (37.8762105, -119.3432825)            227.15\nG (37.833654, -119.4510805)             234.15\nH (38.0603395, -119.6666755)         232.81999\nI (37.798171, -119.19955150          227.26999\nJ (38.0391175, -119.3073495)            226.89\nName: 117, dtype: object\nTime                            1/27/2018 6:00\nA (38.152231, -119.6666755)             238.43\nB (38.279274, -119.6127765)             240.39\nC (38.50458, -119.62176)             247.06999\nD (37.862028, -119.6576925)          249.79999\nE (37.89748, -119.2624335)           232.90999\nF (37.8762105, -119.3432825)             236.2\nG (37.833654, -119.4510805)             242.67\nH (38.0603395, -119.6666755)            239.58\nI (37.798171, -119.19955150          234.98999\nJ (38.0391175, -119.3073495)             232.2\nName: 118, dtype: object\nTime                            1/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 119, dtype: object\nTime                            1/29/2018 6:00\nA (38.152231, -119.6666755)             236.37\nB (38.279274, -119.6127765)          240.20999\nC (38.50458, -119.62176)                248.53\nD (37.862028, -119.6576925)             246.93\nE (37.89748, -119.2624335)              233.98\nF (37.8762105, -119.3432825)            235.39\nG (37.833654, -119.4510805)             238.75\nH (38.0603395, -119.6666755)         236.87999\nI (37.798171, -119.19955150          236.98999\nJ (38.0391175, -119.3073495)            233.53\nName: 120, dtype: object\nTime                            1/30/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 121, dtype: object\nTime                            1/31/2018 6:00\nA (38.152231, -119.6666755)             235.68\nB (38.279274, -119.6127765)             237.89\nC (38.50458, -119.62176)                246.29\nD (37.862028, -119.6576925)             247.03\nE (37.89748, -119.2624335)              231.09\nF (37.8762105, -119.3432825)         231.01999\nG (37.833654, -119.4510805)          237.84999\nH (38.0603395, -119.6666755)         236.48999\nI (37.798171, -119.19955150             235.65\nJ (38.0391175, -119.3073495)            231.93\nName: 122, dtype: object\nTime                            2/1/2018 6:00\nA (38.152231, -119.6666755)            237.62\nB (38.279274, -119.6127765)            238.87\nC (38.50458, -119.62176)               248.01\nD (37.862028, -119.6576925)         250.90999\nE (37.89748, -119.2624335)             233.12\nF (37.8762105, -119.3432825)        235.62999\nG (37.833654, -119.4510805)            244.11\nH (38.0603395, -119.6666755)        240.62999\nI (37.798171, -119.19955150             234.0\nJ (38.0391175, -119.3073495)        234.70999\nName: 123, dtype: object\nTime                            2/2/2018 6:00\nA (38.152231, -119.6666755)            238.78\nB (38.279274, -119.6127765)         239.04999\nC (38.50458, -119.62176)               247.61\nD (37.862028, -119.6576925)         251.81999\nE (37.89748, -119.2624335)              231.5\nF (37.8762105, -119.3432825)        234.90999\nG (37.833654, -119.4510805)         241.87999\nH (38.0603395, -119.6666755)           240.68\nI (37.798171, -119.19955150            235.81\nJ (38.0391175, -119.3073495)           235.76\nName: 124, dtype: object\nTime                            2/3/2018 6:00\nA (38.152231, -119.6666755)            237.86\nB (38.279274, -119.6127765)            238.79\nC (38.50458, -119.62176)               247.51\nD (37.862028, -119.6576925)            253.01\nE (37.89748, -119.2624335)             232.53\nF (37.8762105, -119.3432825)           235.53\nG (37.833654, -119.4510805)            242.25\nH (38.0603395, -119.6666755)           239.93\nI (37.798171, -119.19955150            233.65\nJ (38.0391175, -119.3073495)           233.72\nName: 125, dtype: object\nTime                            2/4/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 126, dtype: object\nTime                            2/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 127, dtype: object\nTime                            2/6/2018 6:00\nA (38.152231, -119.6666755)            232.68\nB (38.279274, -119.6127765)         236.23999\nC (38.50458, -119.62176)            246.48999\nD (37.862028, -119.6576925)            244.03\nE (37.89748, -119.2624335)          231.01999\nF (37.8762105, -119.3432825)           231.86\nG (37.833654, -119.4510805)         234.29999\nH (38.0603395, -119.6666755)        232.93999\nI (37.798171, -119.19955150            234.51\nJ (38.0391175, -119.3073495)           231.18\nName: 128, dtype: object\nTime                            2/7/2018 6:00\nA (38.152231, -119.6666755)            231.73\nB (38.279274, -119.6127765)            235.86\nC (38.50458, -119.62176)               245.28\nD (37.862028, -119.6576925)            243.75\nE (37.89748, -119.2624335)          227.87999\nF (37.8762105, -119.3432825)        229.76999\nG (37.833654, -119.4510805)            235.78\nH (38.0603395, -119.6666755)           232.39\nI (37.798171, -119.19955150         230.95999\nJ (38.0391175, -119.3073495)           230.68\nName: 129, dtype: object\nTime                            2/8/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 130, dtype: object\nTime                            2/9/2018 6:00\nA (38.152231, -119.6666755)            233.42\nB (38.279274, -119.6127765)            236.01\nC (38.50458, -119.62176)               246.73\nD (37.862028, -119.6576925)            247.43\nE (37.89748, -119.2624335)          226.65999\nF (37.8762105, -119.3432825)           229.53\nG (37.833654, -119.4510805)            238.31\nH (38.0603395, -119.6666755)           236.54\nI (37.798171, -119.19955150         228.65999\nJ (38.0391175, -119.3073495)           231.01\nName: 131, dtype: object\nTime                            2/10/2018 6:00\nA (38.152231, -119.6666755)             235.56\nB (38.279274, -119.6127765)          238.87999\nC (38.50458, -119.62176)             249.09999\nD (37.862028, -119.6576925)          247.37999\nE (37.89748, -119.2624335)              231.11\nF (37.8762105, -119.3432825)         232.43999\nG (37.833654, -119.4510805)          239.20999\nH (38.0603395, -119.6666755)            237.36\nI (37.798171, -119.19955150          234.04999\nJ (38.0391175, -119.3073495)         234.26999\nName: 132, dtype: object\nTime                            2/11/2018 6:00\nA (38.152231, -119.6666755)             232.76\nB (38.279274, -119.6127765)          235.20999\nC (38.50458, -119.62176)                 244.2\nD (37.862028, -119.6576925)          246.93999\nE (37.89748, -119.2624335)           226.12999\nF (37.8762105, -119.3432825)            229.06\nG (37.833654, -119.4510805)          236.43999\nH (38.0603395, -119.6666755)         234.43999\nI (37.798171, -119.19955150             228.92\nJ (38.0391175, -119.3073495)            227.53\nName: 133, dtype: object\nTime                            2/12/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 134, dtype: object\nTime                            2/13/2018 6:00\nA (38.152231, -119.6666755)             227.14\nB (38.279274, -119.6127765)             233.76\nC (38.50458, -119.62176)             243.29999\nD (37.862028, -119.6576925)             235.81\nE (37.89748, -119.2624335)              228.39\nF (37.8762105, -119.3432825)            227.92\nG (37.833654, -119.4510805)             228.92\nH (38.0603395, -119.6666755)            227.92\nI (37.798171, -119.19955150             230.56\nJ (38.0391175, -119.3073495)            229.33\nName: 135, dtype: object\nTime                            2/14/2018 6:00\nA (38.152231, -119.6666755)             225.73\nB (38.279274, -119.6127765)             231.87\nC (38.50458, -119.62176)             242.09999\nD (37.862028, -119.6576925)             237.48\nE (37.89748, -119.2624335)           225.01999\nF (37.8762105, -119.3432825)            224.58\nG (37.833654, -119.4510805)             226.92\nH (38.0603395, -119.6666755)            227.72\nI (37.798171, -119.19955150             226.73\nJ (38.0391175, -119.3073495)            227.68\nName: 136, dtype: object\nTime                            2/15/2018 6:00\nA (38.152231, -119.6666755)          225.09999\nB (38.279274, -119.6127765)             227.73\nC (38.50458, -119.62176)                238.58\nD (37.862028, -119.6576925)             237.54\nE (37.89748, -119.2624335)              221.03\nF (37.8762105, -119.3432825)            223.18\nG (37.833654, -119.4510805)          226.70999\nH (38.0603395, -119.6666755)         226.09999\nI (37.798171, -119.19955150              221.2\nJ (38.0391175, -119.3073495)            225.25\nName: 137, dtype: object\nTime                            2/16/2018 6:00\nA (38.152231, -119.6666755)             232.31\nB (38.279274, -119.6127765)          233.73999\nC (38.50458, -119.62176)                243.48\nD (37.862028, -119.6576925)          244.43999\nE (37.89748, -119.2624335)              228.28\nF (37.8762105, -119.3432825)            228.12\nG (37.833654, -119.4510805)             231.08\nH (38.0603395, -119.6666755)            232.89\nI (37.798171, -119.19955150             228.75\nJ (38.0391175, -119.3073495)         231.31999\nName: 138, dtype: object\nTime                            2/17/2018 6:00\nA (38.152231, -119.6666755)             232.31\nB (38.279274, -119.6127765)          233.59999\nC (38.50458, -119.62176)                244.37\nD (37.862028, -119.6576925)             245.54\nE (37.89748, -119.2624335)              226.17\nF (37.8762105, -119.3432825)            228.53\nG (37.833654, -119.4510805)             235.08\nH (38.0603395, -119.6666755)         233.81999\nI (37.798171, -119.19955150              227.5\nJ (38.0391175, -119.3073495)            228.19\nName: 139, dtype: object\nTime                            2/18/2018 6:00\nA (38.152231, -119.6666755)          233.90999\nB (38.279274, -119.6127765)             237.45\nC (38.50458, -119.62176)             247.43999\nD (37.862028, -119.6576925)             247.36\nE (37.89748, -119.2624335)           229.20999\nF (37.8762105, -119.3432825)            231.34\nG (37.833654, -119.4510805)          238.06999\nH (38.0603395, -119.6666755)            235.81\nI (37.798171, -119.19955150             231.98\nJ (38.0391175, -119.3073495)             231.7\nName: 140, dtype: object\nTime                            2/19/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)           221.15999\nF (37.8762105, -119.3432825)            221.89\nG (37.833654, -119.4510805)          224.04999\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             226.79\nJ (38.0391175, -119.3073495)            221.31\nName: 141, dtype: object\nTime                            2/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 142, dtype: object\nTime                            2/21/2018 6:00\nA (38.152231, -119.6666755)          224.59999\nB (38.279274, -119.6127765)          227.65999\nC (38.50458, -119.62176)                237.43\nD (37.862028, -119.6576925)             235.23\nE (37.89748, -119.2624335)              223.83\nF (37.8762105, -119.3432825)            223.29\nG (37.833654, -119.4510805)             225.86\nH (38.0603395, -119.6666755)             225.7\nI (37.798171, -119.19955150             225.94\nJ (38.0391175, -119.3073495)         223.95999\nName: 143, dtype: object\nTime                            2/22/2018 6:00\nA (38.152231, -119.6666755)             229.58\nB (38.279274, -119.6127765)          230.51999\nC (38.50458, -119.62176)             238.73999\nD (37.862028, -119.6576925)          239.59999\nE (37.89748, -119.2624335)              223.61\nF (37.8762105, -119.3432825)         224.98999\nG (37.833654, -119.4510805)             229.01\nH (38.0603395, -119.6666755)         231.12999\nI (37.798171, -119.19955150          224.84999\nJ (38.0391175, -119.3073495)         226.09999\nName: 144, dtype: object\nTime                            2/23/2018 6:00\nA (38.152231, -119.6666755)          225.15999\nB (38.279274, -119.6127765)             225.28\nC (38.50458, -119.62176)                 236.5\nD (37.862028, -119.6576925)          235.59999\nE (37.89748, -119.2624335)              220.37\nF (37.8762105, -119.3432825)            222.22\nG (37.833654, -119.4510805)          226.40999\nH (38.0603395, -119.6666755)            226.67\nI (37.798171, -119.19955150             220.84\nJ (38.0391175, -119.3073495)             222.7\nName: 145, dtype: object\nTime                            2/24/2018 6:00\nA (38.152231, -119.6666755)             226.34\nB (38.279274, -119.6127765)             228.04\nC (38.50458, -119.62176)                236.84\nD (37.862028, -119.6576925)              239.4\nE (37.89748, -119.2624335)              222.26\nF (37.8762105, -119.3432825)         225.15999\nG (37.833654, -119.4510805)             227.01\nH (38.0603395, -119.6666755)         228.34999\nI (37.798171, -119.19955150             224.04\nJ (38.0391175, -119.3073495)             225.2\nName: 146, dtype: object\nTime                            2/25/2018 6:00\nA (38.152231, -119.6666755)             230.12\nB (38.279274, -119.6127765)             229.53\nC (38.50458, -119.62176)                238.26\nD (37.862028, -119.6576925)             240.72\nE (37.89748, -119.2624335)           222.09999\nF (37.8762105, -119.3432825)         224.15999\nG (37.833654, -119.4510805)          231.98999\nH (38.0603395, -119.6666755)            231.65\nI (37.798171, -119.19955150             222.86\nJ (38.0391175, -119.3073495)             224.5\nName: 147, dtype: object\nTime                            2/26/2018 6:00\nA (38.152231, -119.6666755)             231.47\nB (38.279274, -119.6127765)             232.17\nC (38.50458, -119.62176)             240.90999\nD (37.862028, -119.6576925)             245.97\nE (37.89748, -119.2624335)              226.19\nF (37.8762105, -119.3432825)            229.44\nG (37.833654, -119.4510805)          235.54999\nH (38.0603395, -119.6666755)         233.37999\nI (37.798171, -119.19955150             229.18\nJ (38.0391175, -119.3073495)             228.2\nName: 148, dtype: object\nTime                            2/27/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 149, dtype: object\nTime                            2/28/2018 6:00\nA (38.152231, -119.6666755)          228.73999\nB (38.279274, -119.6127765)             232.53\nC (38.50458, -119.62176)                 239.2\nD (37.862028, -119.6576925)             237.22\nE (37.89748, -119.2624335)              225.04\nF (37.8762105, -119.3432825)            225.75\nG (37.833654, -119.4510805)             229.79\nH (38.0603395, -119.6666755)         229.04999\nI (37.798171, -119.19955150             225.29\nJ (38.0391175, -119.3073495)         227.23999\nName: 150, dtype: object\nTime                            3/1/2018 6:00\nA (38.152231, -119.6666755)         237.06999\nB (38.279274, -119.6127765)            236.45\nC (38.50458, -119.62176)               243.03\nD (37.862028, -119.6576925)            246.83\nE (37.89748, -119.2624335)             230.15\nF (37.8762105, -119.3432825)           231.33\nG (37.833654, -119.4510805)             236.5\nH (38.0603395, -119.6666755)        238.81999\nI (37.798171, -119.19955150            232.15\nJ (38.0391175, -119.3073495)           231.42\nName: 151, dtype: object\nTime                            3/2/2018 6:00\nA (38.152231, -119.6666755)            232.53\nB (38.279274, -119.6127765)            232.28\nC (38.50458, -119.62176)            238.37999\nD (37.862028, -119.6576925)            243.09\nE (37.89748, -119.2624335)             228.87\nF (37.8762105, -119.3432825)        229.29999\nG (37.833654, -119.4510805)         232.12999\nH (38.0603395, -119.6666755)           234.01\nI (37.798171, -119.19955150            230.83\nJ (38.0391175, -119.3073495)        229.79999\nName: 152, dtype: object\nTime                            3/3/2018 6:00\nA (38.152231, -119.6666755)         232.18999\nB (38.279274, -119.6127765)         231.81999\nC (38.50458, -119.62176)               238.03\nD (37.862028, -119.6576925)             242.4\nE (37.89748, -119.2624335)             224.98\nF (37.8762105, -119.3432825)           227.56\nG (37.833654, -119.4510805)            236.78\nH (38.0603395, -119.6666755)        233.48999\nI (37.798171, -119.19955150         227.79999\nJ (38.0391175, -119.3073495)        226.20999\nName: 153, dtype: object\nTime                            3/4/2018 6:00\nA (38.152231, -119.6666755)            228.56\nB (38.279274, -119.6127765)            228.17\nC (38.50458, -119.62176)            234.20999\nD (37.862028, -119.6576925)         240.15999\nE (37.89748, -119.2624335)             221.44\nF (37.8762105, -119.3432825)        224.95999\nG (37.833654, -119.4510805)            230.62\nH (38.0603395, -119.6666755)           230.51\nI (37.798171, -119.19955150            223.04\nJ (38.0391175, -119.3073495)           224.62\nName: 154, dtype: object\nTime                            3/5/2018 6:00\nA (38.152231, -119.6666755)             227.7\nB (38.279274, -119.6127765)            228.84\nC (38.50458, -119.62176)               232.43\nD (37.862028, -119.6576925)            237.45\nE (37.89748, -119.2624335)             222.61\nF (37.8762105, -119.3432825)        226.01999\nG (37.833654, -119.4510805)            231.12\nH (38.0603395, -119.6666755)           228.28\nI (37.798171, -119.19955150            224.22\nJ (38.0391175, -119.3073495)        224.51999\nName: 155, dtype: object\nTime                            3/6/2018 6:00\nA (38.152231, -119.6666755)            228.04\nB (38.279274, -119.6127765)            231.09\nC (38.50458, -119.62176)               234.26\nD (37.862028, -119.6576925)         238.98999\nE (37.89748, -119.2624335)             224.83\nF (37.8762105, -119.3432825)           227.97\nG (37.833654, -119.4510805)            234.87\nH (38.0603395, -119.6666755)        229.31999\nI (37.798171, -119.19955150             227.4\nJ (38.0391175, -119.3073495)           223.76\nName: 156, dtype: object\nTime                            3/7/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 157, dtype: object\nTime                            3/8/2018 6:00\nA (38.152231, -119.6666755)            232.84\nB (38.279274, -119.6127765)         234.59999\nC (38.50458, -119.62176)               241.28\nD (37.862028, -119.6576925)            239.58\nE (37.89748, -119.2624335)          229.26999\nF (37.8762105, -119.3432825)        230.76999\nG (37.833654, -119.4510805)         234.04999\nH (38.0603395, -119.6666755)           232.93\nI (37.798171, -119.19955150         230.95999\nJ (38.0391175, -119.3073495)        228.34999\nName: 158, dtype: object\nTime                            3/9/2018 6:00\nA (38.152231, -119.6666755)            230.18\nB (38.279274, -119.6127765)         231.98999\nC (38.50458, -119.62176)            237.65999\nD (37.862028, -119.6576925)            238.84\nE (37.89748, -119.2624335)             226.69\nF (37.8762105, -119.3432825)        227.40999\nG (37.833654, -119.4510805)         230.90999\nH (38.0603395, -119.6666755)        230.37999\nI (37.798171, -119.19955150         228.76999\nJ (38.0391175, -119.3073495)           227.09\nName: 159, dtype: object\nTime                            3/10/2018 6:00\nA (38.152231, -119.6666755)             231.33\nB (38.279274, -119.6127765)          231.06999\nC (38.50458, -119.62176)                239.06\nD (37.862028, -119.6576925)             241.42\nE (37.89748, -119.2624335)              229.86\nF (37.8762105, -119.3432825)         229.20999\nG (37.833654, -119.4510805)          232.51999\nH (38.0603395, -119.6666755)         232.90999\nI (37.798171, -119.19955150             231.53\nJ (38.0391175, -119.3073495)            230.42\nName: 160, dtype: object\nTime                            3/11/2018 6:00\nA (38.152231, -119.6666755)             228.84\nB (38.279274, -119.6127765)             229.06\nC (38.50458, -119.62176)                237.39\nD (37.862028, -119.6576925)             238.97\nE (37.89748, -119.2624335)              226.72\nF (37.8762105, -119.3432825)             227.0\nG (37.833654, -119.4510805)              232.7\nH (38.0603395, -119.6666755)            231.48\nI (37.798171, -119.19955150          228.70999\nJ (38.0391175, -119.3073495)         227.06999\nName: 161, dtype: object\nTime                            3/12/2018 6:00\nA (38.152231, -119.6666755)             229.94\nB (38.279274, -119.6127765)             228.79\nC (38.50458, -119.62176)             238.18999\nD (37.862028, -119.6576925)          244.31999\nE (37.89748, -119.2624335)           225.73999\nF (37.8762105, -119.3432825)            227.43\nG (37.833654, -119.4510805)          233.81999\nH (38.0603395, -119.6666755)         232.23999\nI (37.798171, -119.19955150             228.03\nJ (38.0391175, -119.3073495)            227.95\nName: 162, dtype: object\nTime                            3/13/2018 6:00\nA (38.152231, -119.6666755)             248.33\nB (38.279274, -119.6127765)          246.48999\nC (38.50458, -119.62176)                251.12\nD (37.862028, -119.6576925)             256.91\nE (37.89748, -119.2624335)              235.56\nF (37.8762105, -119.3432825)         240.04999\nG (37.833654, -119.4510805)          246.65999\nH (38.0603395, -119.6666755)            249.65\nI (37.798171, -119.19955150             239.22\nJ (38.0391175, -119.3073495)         238.34999\nName: 163, dtype: object\nTime                            3/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 164, dtype: object\nTime                            3/15/2018 6:00\nA (38.152231, -119.6666755)          234.81999\nB (38.279274, -119.6127765)             234.17\nC (38.50458, -119.62176)                240.09\nD (37.862028, -119.6576925)             241.26\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)          233.20999\nH (38.0603395, -119.6666755)            235.31\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)         227.09999\nName: 165, dtype: object\nTime                            3/16/2018 6:00\nA (38.152231, -119.6666755)          234.93999\nB (38.279274, -119.6127765)             232.54\nC (38.50458, -119.62176)                238.65\nD (37.862028, -119.6576925)             244.47\nE (37.89748, -119.2624335)              228.42\nF (37.8762105, -119.3432825)            230.14\nG (37.833654, -119.4510805)          234.73999\nH (38.0603395, -119.6666755)             236.5\nI (37.798171, -119.19955150             230.25\nJ (38.0391175, -119.3073495)             229.0\nName: 166, dtype: object\nTime                            3/17/2018 6:00\nA (38.152231, -119.6666755)             231.09\nB (38.279274, -119.6127765)             229.72\nC (38.50458, -119.62176)                233.65\nD (37.862028, -119.6576925)          239.79999\nE (37.89748, -119.2624335)              223.92\nF (37.8762105, -119.3432825)            225.97\nG (37.833654, -119.4510805)             231.97\nH (38.0603395, -119.6666755)            232.28\nI (37.798171, -119.19955150          225.29999\nJ (38.0391175, -119.3073495)            225.37\nName: 167, dtype: object\nTime                            3/18/2018 6:00\nA (38.152231, -119.6666755)             227.95\nB (38.279274, -119.6127765)              225.5\nC (38.50458, -119.62176)                230.95\nD (37.862028, -119.6576925)          238.98999\nE (37.89748, -119.2624335)              223.25\nF (37.8762105, -119.3432825)         225.54999\nG (37.833654, -119.4510805)              227.4\nH (38.0603395, -119.6666755)            229.89\nI (37.798171, -119.19955150             223.58\nJ (38.0391175, -119.3073495)         225.23999\nName: 168, dtype: object\nTime                            3/19/2018 6:00\nA (38.152231, -119.6666755)          228.34999\nB (38.279274, -119.6127765)             226.84\nC (38.50458, -119.62176)                234.04\nD (37.862028, -119.6576925)             238.76\nE (37.89748, -119.2624335)           224.29999\nF (37.8762105, -119.3432825)         225.59999\nG (37.833654, -119.4510805)             230.26\nH (38.0603395, -119.6666755)            230.15\nI (37.798171, -119.19955150             225.22\nJ (38.0391175, -119.3073495)            226.45\nName: 169, dtype: object\nTime                            3/20/2018 6:00\nA (38.152231, -119.6666755)          233.76999\nB (38.279274, -119.6127765)          230.26999\nC (38.50458, -119.62176)                238.31\nD (37.862028, -119.6576925)             246.42\nE (37.89748, -119.2624335)           228.65999\nF (37.8762105, -119.3432825)         231.51999\nG (37.833654, -119.4510805)          237.12999\nH (38.0603395, -119.6666755)            235.33\nI (37.798171, -119.19955150             229.31\nJ (38.0391175, -119.3073495)         229.87999\nName: 170, dtype: object\nTime                            3/21/2018 6:00\nA (38.152231, -119.6666755)          245.95999\nB (38.279274, -119.6127765)             243.37\nC (38.50458, -119.62176)             249.37999\nD (37.862028, -119.6576925)          256.50998\nE (37.89748, -119.2624335)              237.45\nF (37.8762105, -119.3432825)            240.28\nG (37.833654, -119.4510805)             247.58\nH (38.0603395, -119.6666755)         248.37999\nI (37.798171, -119.19955150             239.01\nJ (38.0391175, -119.3073495)            237.87\nName: 171, dtype: object\nTime                            3/22/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 172, dtype: object\nTime                            3/23/2018 6:00\nA (38.152231, -119.6666755)             238.58\nB (38.279274, -119.6127765)             237.51\nC (38.50458, -119.62176)                242.33\nD (37.862028, -119.6576925)             244.98\nE (37.89748, -119.2624335)              230.76\nF (37.8762105, -119.3432825)            233.18\nG (37.833654, -119.4510805)          238.06999\nH (38.0603395, -119.6666755)            239.31\nI (37.798171, -119.19955150             233.25\nJ (38.0391175, -119.3073495)            229.51\nName: 173, dtype: object\nTime                            3/24/2018 6:00\nA (38.152231, -119.6666755)             230.14\nB (38.279274, -119.6127765)             230.51\nC (38.50458, -119.62176)                236.68\nD (37.862028, -119.6576925)          237.37999\nE (37.89748, -119.2624335)              225.09\nF (37.8762105, -119.3432825)            226.83\nG (37.833654, -119.4510805)          231.09999\nH (38.0603395, -119.6666755)            231.01\nI (37.798171, -119.19955150             227.51\nJ (38.0391175, -119.3073495)            224.47\nName: 174, dtype: object\nTime                            3/25/2018 6:00\nA (38.152231, -119.6666755)             228.15\nB (38.279274, -119.6127765)             228.03\nC (38.50458, -119.62176)                232.56\nD (37.862028, -119.6576925)          236.20999\nE (37.89748, -119.2624335)              223.47\nF (37.8762105, -119.3432825)            226.53\nG (37.833654, -119.4510805)             232.86\nH (38.0603395, -119.6666755)            230.09\nI (37.798171, -119.19955150             225.62\nJ (38.0391175, -119.3073495)            222.89\nName: 175, dtype: object\nTime                            3/26/2018 6:00\nA (38.152231, -119.6666755)             225.06\nB (38.279274, -119.6127765)          224.56999\nC (38.50458, -119.62176)                230.26\nD (37.862028, -119.6576925)             234.98\nE (37.89748, -119.2624335)              219.72\nF (37.8762105, -119.3432825)         223.23999\nG (37.833654, -119.4510805)             229.72\nH (38.0603395, -119.6666755)         227.31999\nI (37.798171, -119.19955150             221.34\nJ (38.0391175, -119.3073495)            220.87\nName: 176, dtype: object\nTime                            3/27/2018 6:00\nA (38.152231, -119.6666755)          228.34999\nB (38.279274, -119.6127765)          225.76999\nC (38.50458, -119.62176)                233.84\nD (37.862028, -119.6576925)             242.09\nE (37.89748, -119.2624335)           225.23999\nF (37.8762105, -119.3432825)            228.29\nG (37.833654, -119.4510805)             233.97\nH (38.0603395, -119.6666755)            230.58\nI (37.798171, -119.19955150          226.01999\nJ (38.0391175, -119.3073495)            224.81\nName: 177, dtype: object\nTime                            3/28/2018 6:00\nA (38.152231, -119.6666755)             229.09\nB (38.279274, -119.6127765)             228.59\nC (38.50458, -119.62176)             237.87999\nD (37.862028, -119.6576925)          245.54999\nE (37.89748, -119.2624335)           226.26999\nF (37.8762105, -119.3432825)             228.9\nG (37.833654, -119.4510805)          236.76999\nH (38.0603395, -119.6666755)            232.45\nI (37.798171, -119.19955150             227.56\nJ (38.0391175, -119.3073495)            225.39\nName: 178, dtype: object\nTime                            3/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 179, dtype: object\nTime                            3/30/2018 6:00\nA (38.152231, -119.6666755)          232.87999\nB (38.279274, -119.6127765)          231.37999\nC (38.50458, -119.62176)             242.15999\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 180, dtype: object\nTime                            3/31/2018 6:00\nA (38.152231, -119.6666755)             226.98\nB (38.279274, -119.6127765)             227.94\nC (38.50458, -119.62176)                242.09\nD (37.862028, -119.6576925)             240.53\nE (37.89748, -119.2624335)           226.62999\nF (37.8762105, -119.3432825)            227.34\nG (37.833654, -119.4510805)             230.06\nH (38.0603395, -119.6666755)            228.68\nI (37.798171, -119.19955150             229.78\nJ (38.0391175, -119.3073495)            226.62\nName: 181, dtype: object\nTime                            4/1/2018 6:00\nA (38.152231, -119.6666755)             225.2\nB (38.279274, -119.6127765)         224.31999\nC (38.50458, -119.62176)               238.03\nD (37.862028, -119.6576925)         239.15999\nE (37.89748, -119.2624335)          225.48999\nF (37.8762105, -119.3432825)           226.58\nG (37.833654, -119.4510805)            228.45\nH (38.0603395, -119.6666755)           227.61\nI (37.798171, -119.19955150         226.84999\nJ (38.0391175, -119.3073495)           226.56\nName: 182, dtype: object\nTime                            4/2/2018 6:00\nA (38.152231, -119.6666755)         224.62999\nB (38.279274, -119.6127765)            226.37\nC (38.50458, -119.62176)               239.83\nD (37.862028, -119.6576925)         238.04999\nE (37.89748, -119.2624335)             221.03\nF (37.8762105, -119.3432825)        221.73999\nG (37.833654, -119.4510805)         228.15999\nH (38.0603395, -119.6666755)           227.26\nI (37.798171, -119.19955150            224.23\nJ (38.0391175, -119.3073495)        223.01999\nName: 183, dtype: object\nTime                            4/3/2018 6:00\nA (38.152231, -119.6666755)            222.17\nB (38.279274, -119.6127765)            222.23\nC (38.50458, -119.62176)            233.45999\nD (37.862028, -119.6576925)            241.15\nE (37.89748, -119.2624335)             217.44\nF (37.8762105, -119.3432825)        220.54999\nG (37.833654, -119.4510805)            230.08\nH (38.0603395, -119.6666755)            227.2\nI (37.798171, -119.19955150            219.73\nJ (38.0391175, -119.3073495)           222.26\nName: 184, dtype: object\nTime                            4/4/2018 6:00\nA (38.152231, -119.6666755)            225.51\nB (38.279274, -119.6127765)            223.95\nC (38.50458, -119.62176)            239.98999\nD (37.862028, -119.6576925)            243.48\nE (37.89748, -119.2624335)          223.54999\nF (37.8762105, -119.3432825)           224.79\nG (37.833654, -119.4510805)            230.36\nH (38.0603395, -119.6666755)           228.43\nI (37.798171, -119.19955150             223.9\nJ (38.0391175, -119.3073495)           225.81\nName: 185, dtype: object\nTime                            4/5/2018 6:00\nA (38.152231, -119.6666755)            239.76\nB (38.279274, -119.6127765)         240.04999\nC (38.50458, -119.62176)               251.81\nD (37.862028, -119.6576925)            253.42\nE (37.89748, -119.2624335)          233.20999\nF (37.8762105, -119.3432825)        235.20999\nG (37.833654, -119.4510805)         242.93999\nH (38.0603395, -119.6666755)           241.51\nI (37.798171, -119.19955150         234.26999\nJ (38.0391175, -119.3073495)        233.45999\nName: 186, dtype: object\nTime                            4/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 187, dtype: object\nTime                            4/7/2018 6:00\nA (38.152231, -119.6666755)            248.17\nB (38.279274, -119.6127765)            250.08\nC (38.50458, -119.62176)            254.23999\nD (37.862028, -119.6576925)         249.23999\nE (37.89748, -119.2624335)             246.68\nF (37.8762105, -119.3432825)           248.58\nG (37.833654, -119.4510805)         250.09999\nH (38.0603395, -119.6666755)        247.93999\nI (37.798171, -119.19955150         250.20999\nJ (38.0391175, -119.3073495)           245.76\nName: 188, dtype: object\nTime                            4/8/2018 6:00\nA (38.152231, -119.6666755)         221.34999\nB (38.279274, -119.6127765)            227.53\nC (38.50458, -119.62176)               242.95\nD (37.862028, -119.6576925)         235.31999\nE (37.89748, -119.2624335)             222.34\nF (37.8762105, -119.3432825)        220.76999\nG (37.833654, -119.4510805)            223.79\nH (38.0603395, -119.6666755)           222.47\nI (37.798171, -119.19955150            225.39\nJ (38.0391175, -119.3073495)           224.53\nName: 189, dtype: object\nTime                            4/9/2018 6:00\nA (38.152231, -119.6666755)         224.37999\nB (38.279274, -119.6127765)         227.95999\nC (38.50458, -119.62176)               243.48\nD (37.862028, -119.6576925)            244.22\nE (37.89748, -119.2624335)             224.11\nF (37.8762105, -119.3432825)        223.62999\nG (37.833654, -119.4510805)         226.29999\nH (38.0603395, -119.6666755)           228.09\nI (37.798171, -119.19955150            227.92\nJ (38.0391175, -119.3073495)           226.98\nName: 190, dtype: object\nTime                            4/10/2018 6:00\nA (38.152231, -119.6666755)             234.47\nB (38.279274, -119.6127765)             235.39\nC (38.50458, -119.62176)             249.62999\nD (37.862028, -119.6576925)              249.7\nE (37.89748, -119.2624335)              227.84\nF (37.8762105, -119.3432825)            226.65\nG (37.833654, -119.4510805)             234.78\nH (38.0603395, -119.6666755)         235.93999\nI (37.798171, -119.19955150             232.75\nJ (38.0391175, -119.3073495)            231.51\nName: 191, dtype: object\nTime                            4/11/2018 6:00\nA (38.152231, -119.6666755)             236.25\nB (38.279274, -119.6127765)             237.64\nC (38.50458, -119.62176)                248.76\nD (37.862028, -119.6576925)             249.26\nE (37.89748, -119.2624335)              225.03\nF (37.8762105, -119.3432825)            226.01\nG (37.833654, -119.4510805)             235.65\nH (38.0603395, -119.6666755)            237.93\nI (37.798171, -119.19955150              227.0\nJ (38.0391175, -119.3073495)            230.33\nName: 192, dtype: object\nTime                            4/12/2018 6:00\nA (38.152231, -119.6666755)             224.87\nB (38.279274, -119.6127765)             227.34\nC (38.50458, -119.62176)                240.48\nD (37.862028, -119.6576925)          241.45999\nE (37.89748, -119.2624335)              220.45\nF (37.8762105, -119.3432825)            221.19\nG (37.833654, -119.4510805)          228.29999\nH (38.0603395, -119.6666755)            228.36\nI (37.798171, -119.19955150          222.48999\nJ (38.0391175, -119.3073495)            221.89\nName: 193, dtype: object\nTime                            4/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              219.03\nF (37.8762105, -119.3432825)         219.34999\nG (37.833654, -119.4510805)             220.44\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             223.98\nJ (38.0391175, -119.3073495)         218.06999\nName: 194, dtype: object\nTime                            4/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 195, dtype: object\nTime                            4/15/2018 6:00\nA (38.152231, -119.6666755)          225.48999\nB (38.279274, -119.6127765)          233.62999\nC (38.50458, -119.62176)                 249.7\nD (37.862028, -119.6576925)             241.51\nE (37.89748, -119.2624335)              224.23\nF (37.8762105, -119.3432825)         222.48999\nG (37.833654, -119.4510805)             225.75\nH (38.0603395, -119.6666755)            227.03\nI (37.798171, -119.19955150             227.72\nJ (38.0391175, -119.3073495)            226.23\nName: 196, dtype: object\nTime                            4/16/2018 6:00\nA (38.152231, -119.6666755)          231.01999\nB (38.279274, -119.6127765)          234.68999\nC (38.50458, -119.62176)                244.65\nD (37.862028, -119.6576925)             241.62\nE (37.89748, -119.2624335)              224.76\nF (37.8762105, -119.3432825)            223.17\nG (37.833654, -119.4510805)          226.65999\nH (38.0603395, -119.6666755)         231.54999\nI (37.798171, -119.19955150             227.83\nJ (38.0391175, -119.3073495)         226.76999\nName: 197, dtype: object\nTime                            4/17/2018 6:00\nA (38.152231, -119.6666755)             224.06\nB (38.279274, -119.6127765)             227.29\nC (38.50458, -119.62176)                239.97\nD (37.862028, -119.6576925)          236.06999\nE (37.89748, -119.2624335)              220.19\nF (37.8762105, -119.3432825)            220.51\nG (37.833654, -119.4510805)             222.67\nH (38.0603395, -119.6666755)            227.09\nI (37.798171, -119.19955150             221.54\nJ (38.0391175, -119.3073495)            223.11\nName: 198, dtype: object\nTime                            4/18/2018 6:00\nA (38.152231, -119.6666755)             223.65\nB (38.279274, -119.6127765)             227.26\nC (38.50458, -119.62176)             242.84999\nD (37.862028, -119.6576925)             242.29\nE (37.89748, -119.2624335)              220.67\nF (37.8762105, -119.3432825)            219.86\nG (37.833654, -119.4510805)          221.56999\nH (38.0603395, -119.6666755)         225.26999\nI (37.798171, -119.19955150          225.09999\nJ (38.0391175, -119.3073495)            224.45\nName: 199, dtype: object\nTime                            4/19/2018 6:00\nA (38.152231, -119.6666755)             221.58\nB (38.279274, -119.6127765)          224.37999\nC (38.50458, -119.62176)                239.04\nD (37.862028, -119.6576925)             236.93\nE (37.89748, -119.2624335)              218.76\nF (37.8762105, -119.3432825)            220.09\nG (37.833654, -119.4510805)             225.51\nH (38.0603395, -119.6666755)            224.61\nI (37.798171, -119.19955150          219.76999\nJ (38.0391175, -119.3073495)            221.17\nName: 200, dtype: object\nTime                            4/20/2018 6:00\nA (38.152231, -119.6666755)          226.12999\nB (38.279274, -119.6127765)          227.65999\nC (38.50458, -119.62176)                 243.0\nD (37.862028, -119.6576925)             246.47\nE (37.89748, -119.2624335)              221.28\nF (37.8762105, -119.3432825)            223.83\nG (37.833654, -119.4510805)              231.4\nH (38.0603395, -119.6666755)            228.61\nI (37.798171, -119.19955150          225.12999\nJ (38.0391175, -119.3073495)            222.84\nName: 201, dtype: object\nTime                            4/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 202, dtype: object\nTime                            4/22/2018 6:00\nA (38.152231, -119.6666755)             237.83\nB (38.279274, -119.6127765)              246.0\nC (38.50458, -119.62176)                258.57\nD (37.862028, -119.6576925)             247.36\nE (37.89748, -119.2624335)              234.22\nF (37.8762105, -119.3432825)            233.65\nG (37.833654, -119.4510805)          236.59999\nH (38.0603395, -119.6666755)         238.20999\nI (37.798171, -119.19955150             233.89\nJ (38.0391175, -119.3073495)            238.56\nName: 203, dtype: object\nTime                            4/23/2018 6:00\nA (38.152231, -119.6666755)             232.67\nB (38.279274, -119.6127765)          238.54999\nC (38.50458, -119.62176)                252.28\nD (37.862028, -119.6576925)          244.34999\nE (37.89748, -119.2624335)              230.83\nF (37.8762105, -119.3432825)            229.51\nG (37.833654, -119.4510805)          231.56999\nH (38.0603395, -119.6666755)         233.34999\nI (37.798171, -119.19955150          233.06999\nJ (38.0391175, -119.3073495)            233.31\nName: 204, dtype: object\nTime                            4/24/2018 6:00\nA (38.152231, -119.6666755)          234.93999\nB (38.279274, -119.6127765)             238.22\nC (38.50458, -119.62176)                251.97\nD (37.862028, -119.6576925)          250.01999\nE (37.89748, -119.2624335)           230.12999\nF (37.8762105, -119.3432825)            227.86\nG (37.833654, -119.4510805)             236.09\nH (38.0603395, -119.6666755)            236.39\nI (37.798171, -119.19955150             234.65\nJ (38.0391175, -119.3073495)            232.18\nName: 205, dtype: object\nTime                            4/25/2018 6:00\nA (38.152231, -119.6666755)             236.01\nB (38.279274, -119.6127765)          238.87999\nC (38.50458, -119.62176)                252.61\nD (37.862028, -119.6576925)             248.48\nE (37.89748, -119.2624335)              229.04\nF (37.8762105, -119.3432825)         231.87999\nG (37.833654, -119.4510805)             239.14\nH (38.0603395, -119.6666755)         237.73999\nI (37.798171, -119.19955150          229.81999\nJ (38.0391175, -119.3073495)            233.53\nName: 206, dtype: object\nTime                            4/26/2018 6:00\nA (38.152231, -119.6666755)          240.68999\nB (38.279274, -119.6127765)             242.65\nC (38.50458, -119.62176)                255.68\nD (37.862028, -119.6576925)             256.43\nE (37.89748, -119.2624335)              234.56\nF (37.8762105, -119.3432825)            235.64\nG (37.833654, -119.4510805)             241.03\nH (38.0603395, -119.6666755)            242.62\nI (37.798171, -119.19955150             236.12\nJ (38.0391175, -119.3073495)            238.79\nName: 207, dtype: object\nTime                            4/27/2018 6:00\nA (38.152231, -119.6666755)             253.51\nB (38.279274, -119.6127765)              254.4\nC (38.50458, -119.62176)                260.62\nD (37.862028, -119.6576925)              258.8\nE (37.89748, -119.2624335)              238.92\nF (37.8762105, -119.3432825)             240.9\nG (37.833654, -119.4510805)             247.62\nH (38.0603395, -119.6666755)            253.83\nI (37.798171, -119.19955150             241.78\nJ (38.0391175, -119.3073495)            242.68\nName: 208, dtype: object\nTime                            4/28/2018 6:00\nA (38.152231, -119.6666755)             230.42\nB (38.279274, -119.6127765)             238.12\nC (38.50458, -119.62176)             252.59999\nD (37.862028, -119.6576925)             249.79\nE (37.89748, -119.2624335)              230.03\nF (37.8762105, -119.3432825)            233.61\nG (37.833654, -119.4510805)             240.01\nH (38.0603395, -119.6666755)            230.86\nI (37.798171, -119.19955150             229.81\nJ (38.0391175, -119.3073495)         230.15999\nName: 209, dtype: object\nTime                            4/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 210, dtype: object\nTime                            4/30/2018 6:00\nA (38.152231, -119.6666755)          227.81999\nB (38.279274, -119.6127765)             235.65\nC (38.50458, -119.62176)                251.43\nD (37.862028, -119.6576925)             240.84\nE (37.89748, -119.2624335)              227.23\nF (37.8762105, -119.3432825)         224.95999\nG (37.833654, -119.4510805)             228.86\nH (38.0603395, -119.6666755)            227.81\nI (37.798171, -119.19955150          229.34999\nJ (38.0391175, -119.3073495)         229.98999\nName: 211, dtype: object\nTime                            5/1/2018 6:00\nA (38.152231, -119.6666755)            231.59\nB (38.279274, -119.6127765)            236.11\nC (38.50458, -119.62176)            251.26999\nD (37.862028, -119.6576925)            245.75\nE (37.89748, -119.2624335)             231.93\nF (37.8762105, -119.3432825)        230.76999\nG (37.833654, -119.4510805)         234.76999\nH (38.0603395, -119.6666755)           233.23\nI (37.798171, -119.19955150            234.97\nJ (38.0391175, -119.3073495)           233.76\nName: 212, dtype: object\nTime                            5/2/2018 6:00\nA (38.152231, -119.6666755)            230.48\nB (38.279274, -119.6127765)            236.33\nC (38.50458, -119.62176)            251.43999\nD (37.862028, -119.6576925)            245.26\nE (37.89748, -119.2624335)             225.62\nF (37.8762105, -119.3432825)            225.0\nG (37.833654, -119.4510805)            230.93\nH (38.0603395, -119.6666755)           233.42\nI (37.798171, -119.19955150         230.37999\nJ (38.0391175, -119.3073495)           230.97\nName: 213, dtype: object\nTime                            5/3/2018 6:00\nA (38.152231, -119.6666755)            234.28\nB (38.279274, -119.6127765)            238.58\nC (38.50458, -119.62176)            250.68999\nD (37.862028, -119.6576925)         247.98999\nE (37.89748, -119.2624335)          225.79999\nF (37.8762105, -119.3432825)           226.97\nG (37.833654, -119.4510805)         238.15999\nH (38.0603395, -119.6666755)        235.79999\nI (37.798171, -119.19955150         227.56999\nJ (38.0391175, -119.3073495)        231.95999\nName: 214, dtype: object\nTime                            5/4/2018 6:00\nA (38.152231, -119.6666755)            239.76\nB (38.279274, -119.6127765)         242.68999\nC (38.50458, -119.62176)            254.76999\nD (37.862028, -119.6576925)         255.93999\nE (37.89748, -119.2624335)             230.29\nF (37.8762105, -119.3432825)           234.14\nG (37.833654, -119.4510805)            242.42\nH (38.0603395, -119.6666755)           242.26\nI (37.798171, -119.19955150         234.95999\nJ (38.0391175, -119.3073495)        236.01999\nName: 215, dtype: object\nTime                            5/5/2018 6:00\nA (38.152231, -119.6666755)         248.56999\nB (38.279274, -119.6127765)         252.01999\nC (38.50458, -119.62176)               261.15\nD (37.862028, -119.6576925)            260.66\nE (37.89748, -119.2624335)          236.84999\nF (37.8762105, -119.3432825)           238.22\nG (37.833654, -119.4510805)         249.43999\nH (38.0603395, -119.6666755)        249.37999\nI (37.798171, -119.19955150            237.53\nJ (38.0391175, -119.3073495)        241.09999\nName: 216, dtype: object\nTime                            5/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 217, dtype: object\nTime                            5/7/2018 6:00\nA (38.152231, -119.6666755)         250.54999\nB (38.279274, -119.6127765)         255.48999\nC (38.50458, -119.62176)               264.72\nD (37.862028, -119.6576925)            256.54\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)            247.42\nH (38.0603395, -119.6666755)        250.51999\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)        241.06999\nName: 218, dtype: object\nTime                            5/8/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 219, dtype: object\nTime                            5/9/2018 6:00\nA (38.152231, -119.6666755)            255.47\nB (38.279274, -119.6127765)            258.32\nC (38.50458, -119.62176)            264.66998\nD (37.862028, -119.6576925)         261.16998\nE (37.89748, -119.2624335)             247.01\nF (37.8762105, -119.3432825)           249.72\nG (37.833654, -119.4510805)         254.59999\nH (38.0603395, -119.6666755)           255.75\nI (37.798171, -119.19955150            249.17\nJ (38.0391175, -119.3073495)        250.34999\nName: 220, dtype: object\nTime                            5/10/2018 6:00\nA (38.152231, -119.6666755)             251.29\nB (38.279274, -119.6127765)             253.65\nC (38.50458, -119.62176)                261.72\nD (37.862028, -119.6576925)             259.37\nE (37.89748, -119.2624335)              243.01\nF (37.8762105, -119.3432825)            243.31\nG (37.833654, -119.4510805)             246.61\nH (38.0603395, -119.6666755)            250.73\nI (37.798171, -119.19955150             245.87\nJ (38.0391175, -119.3073495)         246.79999\nName: 221, dtype: object\nTime                            5/11/2018 6:00\nA (38.152231, -119.6666755)             250.37\nB (38.279274, -119.6127765)          252.73999\nC (38.50458, -119.62176)             259.00998\nD (37.862028, -119.6576925)             257.33\nE (37.89748, -119.2624335)              237.67\nF (37.8762105, -119.3432825)         240.45999\nG (37.833654, -119.4510805)             249.54\nH (38.0603395, -119.6666755)         251.43999\nI (37.798171, -119.19955150             240.59\nJ (38.0391175, -119.3073495)         244.29999\nName: 222, dtype: object\nTime                            5/12/2018 6:00\nA (38.152231, -119.6666755)          241.68999\nB (38.279274, -119.6127765)              245.2\nC (38.50458, -119.62176)                 254.9\nD (37.862028, -119.6576925)             255.58\nE (37.89748, -119.2624335)           233.98999\nF (37.8762105, -119.3432825)         236.01999\nG (37.833654, -119.4510805)          244.81999\nH (38.0603395, -119.6666755)            244.03\nI (37.798171, -119.19955150             236.51\nJ (38.0391175, -119.3073495)            238.61\nName: 223, dtype: object\nTime                            5/13/2018 6:00\nA (38.152231, -119.6666755)          252.37999\nB (38.279274, -119.6127765)             253.62\nC (38.50458, -119.62176)                259.59\nD (37.862028, -119.6576925)             260.04\nE (37.89748, -119.2624335)              244.84\nF (37.8762105, -119.3432825)         246.81999\nG (37.833654, -119.4510805)          252.59999\nH (38.0603395, -119.6666755)         252.59999\nI (37.798171, -119.19955150             244.95\nJ (38.0391175, -119.3073495)         246.26999\nName: 224, dtype: object\nTime                            5/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 225, dtype: object\nTime                            5/15/2018 6:00\nA (38.152231, -119.6666755)             252.76\nB (38.279274, -119.6127765)             258.03\nC (38.50458, -119.62176)                265.36\nD (37.862028, -119.6576925)             257.66\nE (37.89748, -119.2624335)               249.4\nF (37.8762105, -119.3432825)            251.03\nG (37.833654, -119.4510805)             253.65\nH (38.0603395, -119.6666755)         252.76999\nI (37.798171, -119.19955150             251.18\nJ (38.0391175, -119.3073495)            249.86\nName: 226, dtype: object\nTime                            5/16/2018 6:00\nA (38.152231, -119.6666755)             248.62\nB (38.279274, -119.6127765)             252.79\nC (38.50458, -119.62176)                261.82\nD (37.862028, -119.6576925)             254.04\nE (37.89748, -119.2624335)              241.79\nF (37.8762105, -119.3432825)            240.84\nG (37.833654, -119.4510805)          244.54999\nH (38.0603395, -119.6666755)            248.22\nI (37.798171, -119.19955150             243.68\nJ (38.0391175, -119.3073495)         245.37999\nName: 227, dtype: object\nTime                            5/17/2018 6:00\nA (38.152231, -119.6666755)             244.01\nB (38.279274, -119.6127765)             248.25\nC (38.50458, -119.62176)             257.88998\nD (37.862028, -119.6576925)          251.06999\nE (37.89748, -119.2624335)           233.43999\nF (37.8762105, -119.3432825)         236.20999\nG (37.833654, -119.4510805)              243.2\nH (38.0603395, -119.6666755)         243.54999\nI (37.798171, -119.19955150             234.59\nJ (38.0391175, -119.3073495)            239.04\nName: 228, dtype: object\nTime                            5/18/2018 6:00\nA (38.152231, -119.6666755)              252.2\nB (38.279274, -119.6127765)          255.51999\nC (38.50458, -119.62176)                262.84\nD (37.862028, -119.6576925)          260.44998\nE (37.89748, -119.2624335)              245.22\nF (37.8762105, -119.3432825)            246.33\nG (37.833654, -119.4510805)             246.86\nH (38.0603395, -119.6666755)            252.43\nI (37.798171, -119.19955150             246.14\nJ (38.0391175, -119.3073495)         250.54999\nName: 229, dtype: object\nTime                            5/19/2018 6:00\nA (38.152231, -119.6666755)          248.51999\nB (38.279274, -119.6127765)          252.68999\nC (38.50458, -119.62176)             261.91998\nD (37.862028, -119.6576925)          259.75998\nE (37.89748, -119.2624335)           247.04999\nF (37.8762105, -119.3432825)            248.78\nG (37.833654, -119.4510805)          253.81999\nH (38.0603395, -119.6666755)            250.03\nI (37.798171, -119.19955150             248.59\nJ (38.0391175, -119.3073495)         249.79999\nName: 230, dtype: object\nTime                            5/20/2018 6:00\nA (38.152231, -119.6666755)             251.01\nB (38.279274, -119.6127765)             254.31\nC (38.50458, -119.62176)             262.25998\nD (37.862028, -119.6576925)              260.1\nE (37.89748, -119.2624335)              240.47\nF (37.8762105, -119.3432825)            242.29\nG (37.833654, -119.4510805)             250.26\nH (38.0603395, -119.6666755)         251.87999\nI (37.798171, -119.19955150             241.78\nJ (38.0391175, -119.3073495)             244.9\nName: 231, dtype: object\nTime                            5/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 232, dtype: object\nTime                            5/22/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                266.34\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 233, dtype: object\nTime                            5/23/2018 6:00\nA (38.152231, -119.6666755)          257.97998\nB (38.279274, -119.6127765)          261.16998\nC (38.50458, -119.62176)                265.72\nD (37.862028, -119.6576925)          259.94998\nE (37.89748, -119.2624335)              252.18\nF (37.8762105, -119.3432825)            253.92\nG (37.833654, -119.4510805)             256.55\nH (38.0603395, -119.6666755)            257.83\nI (37.798171, -119.19955150             253.31\nJ (38.0391175, -119.3073495)         252.98999\nName: 234, dtype: object\nTime                            5/24/2018 6:00\nA (38.152231, -119.6666755)             258.07\nB (38.279274, -119.6127765)          261.13998\nC (38.50458, -119.62176)                266.81\nD (37.862028, -119.6576925)             260.74\nE (37.89748, -119.2624335)           250.26999\nF (37.8762105, -119.3432825)             252.0\nG (37.833654, -119.4510805)          254.15999\nH (38.0603395, -119.6666755)            257.33\nI (37.798171, -119.19955150             249.92\nJ (38.0391175, -119.3073495)            253.39\nName: 235, dtype: object\nTime                            5/25/2018 6:00\nA (38.152231, -119.6666755)             255.76\nB (38.279274, -119.6127765)              258.1\nC (38.50458, -119.62176)             260.19998\nD (37.862028, -119.6576925)             258.08\nE (37.89748, -119.2624335)           244.65999\nF (37.8762105, -119.3432825)         247.29999\nG (37.833654, -119.4510805)          254.23999\nH (38.0603395, -119.6666755)            255.83\nI (37.798171, -119.19955150             246.37\nJ (38.0391175, -119.3073495)         248.51999\nName: 236, dtype: object\nTime                            5/26/2018 6:00\nA (38.152231, -119.6666755)             248.84\nB (38.279274, -119.6127765)          252.26999\nC (38.50458, -119.62176)                260.07\nD (37.862028, -119.6576925)             256.56\nE (37.89748, -119.2624335)              241.03\nF (37.8762105, -119.3432825)         242.56999\nG (37.833654, -119.4510805)          244.51999\nH (38.0603395, -119.6666755)            248.61\nI (37.798171, -119.19955150             242.28\nJ (38.0391175, -119.3073495)             246.5\nName: 237, dtype: object\nTime                            5/27/2018 6:00\nA (38.152231, -119.6666755)          253.04999\nB (38.279274, -119.6127765)          255.81999\nC (38.50458, -119.62176)                262.16\nD (37.862028, -119.6576925)             260.59\nE (37.89748, -119.2624335)           242.56999\nF (37.8762105, -119.3432825)            245.68\nG (37.833654, -119.4510805)             251.79\nH (38.0603395, -119.6666755)            253.25\nI (37.798171, -119.19955150             245.34\nJ (38.0391175, -119.3073495)         248.93999\nName: 238, dtype: object\nTime                            5/28/2018 6:00\nA (38.152231, -119.6666755)          256.72998\nB (38.279274, -119.6127765)             259.86\nC (38.50458, -119.62176)                265.53\nD (37.862028, -119.6576925)             264.37\nE (37.89748, -119.2624335)           247.23999\nF (37.8762105, -119.3432825)            250.39\nG (37.833654, -119.4510805)             256.58\nH (38.0603395, -119.6666755)            258.13\nI (37.798171, -119.19955150          250.09999\nJ (38.0391175, -119.3073495)         250.01999\nName: 239, dtype: object\nTime                            5/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 240, dtype: object\nTime                            5/30/2018 6:00\nA (38.152231, -119.6666755)          260.50998\nB (38.279274, -119.6127765)             265.27\nC (38.50458, -119.62176)                270.56\nD (37.862028, -119.6576925)             263.03\nE (37.89748, -119.2624335)              252.84\nF (37.8762105, -119.3432825)            254.68\nG (37.833654, -119.4510805)             257.88\nH (38.0603395, -119.6666755)            259.75\nI (37.798171, -119.19955150          255.37999\nJ (38.0391175, -119.3073495)            254.28\nName: 241, dtype: object\nTime                            5/31/2018 6:00\nA (38.152231, -119.6666755)             256.66\nB (38.279274, -119.6127765)             260.12\nC (38.50458, -119.62176)                265.31\nD (37.862028, -119.6576925)             260.43\nE (37.89748, -119.2624335)              249.04\nF (37.8762105, -119.3432825)            250.86\nG (37.833654, -119.4510805)             254.79\nH (38.0603395, -119.6666755)            256.15\nI (37.798171, -119.19955150              250.2\nJ (38.0391175, -119.3073495)            251.43\nName: 242, dtype: object\nTime                            6/1/2018 6:00\nA (38.152231, -119.6666755)            256.37\nB (38.279274, -119.6127765)         259.16998\nC (38.50458, -119.62176)               263.81\nD (37.862028, -119.6576925)            261.25\nE (37.89748, -119.2624335)             248.95\nF (37.8762105, -119.3432825)        250.31999\nG (37.833654, -119.4510805)         251.98999\nH (38.0603395, -119.6666755)        256.41998\nI (37.798171, -119.19955150            249.64\nJ (38.0391175, -119.3073495)           252.45\nName: 243, dtype: object\nTime                            6/2/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 244, dtype: object\nTime                            6/3/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 245, dtype: object\nTime                            6/4/2018 6:00\nA (38.152231, -119.6666755)            261.05\nB (38.279274, -119.6127765)            263.11\nC (38.50458, -119.62176)                269.1\nD (37.862028, -119.6576925)         267.69998\nE (37.89748, -119.2624335)             250.37\nF (37.8762105, -119.3432825)           253.81\nG (37.833654, -119.4510805)            261.02\nH (38.0603395, -119.6666755)           261.41\nI (37.798171, -119.19955150         253.84999\nJ (38.0391175, -119.3073495)        253.29999\nName: 246, dtype: object\nTime                            6/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)             246.86\nF (37.8762105, -119.3432825)           239.28\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150            255.86\nJ (38.0391175, -119.3073495)              NaN\nName: 247, dtype: object\nTime                            6/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 248, dtype: object\nTime                            6/7/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 249, dtype: object\nTime                            6/8/2018 6:00\nA (38.152231, -119.6666755)         260.41998\nB (38.279274, -119.6127765)         263.97998\nC (38.50458, -119.62176)               267.22\nD (37.862028, -119.6576925)            264.74\nE (37.89748, -119.2624335)             254.65\nF (37.8762105, -119.3432825)        256.38998\nG (37.833654, -119.4510805)            259.33\nH (38.0603395, -119.6666755)            260.1\nI (37.798171, -119.19955150            256.08\nJ (38.0391175, -119.3073495)            256.1\nName: 250, dtype: object\nTime                            6/9/2018 6:00\nA (38.152231, -119.6666755)            260.56\nB (38.279274, -119.6127765)            263.04\nC (38.50458, -119.62176)                267.9\nD (37.862028, -119.6576925)            264.85\nE (37.89748, -119.2624335)              253.7\nF (37.8762105, -119.3432825)           256.32\nG (37.833654, -119.4510805)            257.24\nH (38.0603395, -119.6666755)           260.82\nI (37.798171, -119.19955150            254.29\nJ (38.0391175, -119.3073495)        258.38998\nName: 251, dtype: object\nTime                            6/10/2018 6:00\nA (38.152231, -119.6666755)          256.63998\nB (38.279274, -119.6127765)             259.22\nC (38.50458, -119.62176)                263.21\nD (37.862028, -119.6576925)              261.3\nE (37.89748, -119.2624335)              247.08\nF (37.8762105, -119.3432825)             248.5\nG (37.833654, -119.4510805)             255.31\nH (38.0603395, -119.6666755)         257.50998\nI (37.798171, -119.19955150             250.04\nJ (38.0391175, -119.3073495)         249.76999\nName: 252, dtype: object\nTime                            6/11/2018 6:00\nA (38.152231, -119.6666755)              259.6\nB (38.279274, -119.6127765)             263.05\nC (38.50458, -119.62176)                267.85\nD (37.862028, -119.6576925)              265.8\nE (37.89748, -119.2624335)              251.65\nF (37.8762105, -119.3432825)            254.22\nG (37.833654, -119.4510805)             259.56\nH (38.0603395, -119.6666755)            259.71\nI (37.798171, -119.19955150              253.9\nJ (38.0391175, -119.3073495)            255.29\nName: 253, dtype: object\nTime                            6/12/2018 6:00\nA (38.152231, -119.6666755)             262.65\nB (38.279274, -119.6127765)             265.66\nC (38.50458, -119.62176)                269.19\nD (37.862028, -119.6576925)          269.94998\nE (37.89748, -119.2624335)              255.06\nF (37.8762105, -119.3432825)            259.15\nG (37.833654, -119.4510805)          265.13998\nH (38.0603395, -119.6666755)             263.0\nI (37.798171, -119.19955150             258.43\nJ (38.0391175, -119.3073495)            257.18\nName: 254, dtype: object\nTime                            6/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 255, dtype: object\nTime                            6/14/2018 6:00\nA (38.152231, -119.6666755)             264.57\nB (38.279274, -119.6127765)             268.58\nC (38.50458, -119.62176)             272.50998\nD (37.862028, -119.6576925)             265.86\nE (37.89748, -119.2624335)              257.03\nF (37.8762105, -119.3432825)            258.53\nG (37.833654, -119.4510805)             261.86\nH (38.0603395, -119.6666755)            262.99\nI (37.798171, -119.19955150             257.43\nJ (38.0391175, -119.3073495)         258.41998\nName: 256, dtype: object\nTime                            6/15/2018 6:00\nA (38.152231, -119.6666755)             263.24\nB (38.279274, -119.6127765)             266.54\nC (38.50458, -119.62176)                270.35\nD (37.862028, -119.6576925)              265.3\nE (37.89748, -119.2624335)              255.68\nF (37.8762105, -119.3432825)            258.16\nG (37.833654, -119.4510805)             261.54\nH (38.0603395, -119.6666755)            262.61\nI (37.798171, -119.19955150          255.84999\nJ (38.0391175, -119.3073495)            258.31\nName: 257, dtype: object\nTime                            6/16/2018 6:00\nA (38.152231, -119.6666755)          261.50998\nB (38.279274, -119.6127765)             264.72\nC (38.50458, -119.62176)             267.66998\nD (37.862028, -119.6576925)          264.91998\nE (37.89748, -119.2624335)              251.62\nF (37.8762105, -119.3432825)            254.26\nG (37.833654, -119.4510805)             261.29\nH (38.0603395, -119.6666755)            260.97\nI (37.798171, -119.19955150             254.78\nJ (38.0391175, -119.3073495)            254.67\nName: 258, dtype: object\nTime                            6/17/2018 6:00\nA (38.152231, -119.6666755)             257.65\nB (38.279274, -119.6127765)             260.25\nC (38.50458, -119.62176)                 265.0\nD (37.862028, -119.6576925)             261.53\nE (37.89748, -119.2624335)              250.17\nF (37.8762105, -119.3432825)         253.37999\nG (37.833654, -119.4510805)             256.22\nH (38.0603395, -119.6666755)            257.44\nI (37.798171, -119.19955150             250.18\nJ (38.0391175, -119.3073495)         255.31999\nName: 259, dtype: object\nTime                            6/18/2018 6:00\nA (38.152231, -119.6666755)             259.36\nB (38.279274, -119.6127765)             260.61\nC (38.50458, -119.62176)             264.97998\nD (37.862028, -119.6576925)             264.54\nE (37.89748, -119.2624335)           251.43999\nF (37.8762105, -119.3432825)            253.08\nG (37.833654, -119.4510805)             258.83\nH (38.0603395, -119.6666755)            259.87\nI (37.798171, -119.19955150             254.25\nJ (38.0391175, -119.3073495)            253.64\nName: 260, dtype: object\nTime                            6/19/2018 6:00\nA (38.152231, -119.6666755)          261.22998\nB (38.279274, -119.6127765)             263.57\nC (38.50458, -119.62176)                267.21\nD (37.862028, -119.6576925)              266.4\nE (37.89748, -119.2624335)           252.62999\nF (37.8762105, -119.3432825)            256.25\nG (37.833654, -119.4510805)          261.72998\nH (38.0603395, -119.6666755)            261.44\nI (37.798171, -119.19955150          254.31999\nJ (38.0391175, -119.3073495)         255.90999\nName: 261, dtype: object\nTime                            6/20/2018 6:00\nA (38.152231, -119.6666755)             263.28\nB (38.279274, -119.6127765)             266.75\nC (38.50458, -119.62176)                 271.0\nD (37.862028, -119.6576925)          268.19998\nE (37.89748, -119.2624335)              256.68\nF (37.8762105, -119.3432825)            259.43\nG (37.833654, -119.4510805)             264.29\nH (38.0603395, -119.6666755)         264.63998\nI (37.798171, -119.19955150             260.77\nJ (38.0391175, -119.3073495)         256.69998\nName: 262, dtype: object\nTime                            6/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 263, dtype: object\nTime                            6/22/2018 6:00\nA (38.152231, -119.6666755)             264.88\nB (38.279274, -119.6127765)             268.63\nC (38.50458, -119.62176)             271.50998\nD (37.862028, -119.6576925)             266.97\nE (37.89748, -119.2624335)              255.64\nF (37.8762105, -119.3432825)            259.43\nG (37.833654, -119.4510805)          263.25998\nH (38.0603395, -119.6666755)            264.11\nI (37.798171, -119.19955150          257.94998\nJ (38.0391175, -119.3073495)            257.53\nName: 264, dtype: object\nTime                            6/23/2018 6:00\nA (38.152231, -119.6666755)             264.47\nB (38.279274, -119.6127765)             267.34\nC (38.50458, -119.62176)                270.83\nD (37.862028, -119.6576925)             267.19\nE (37.89748, -119.2624335)              257.21\nF (37.8762105, -119.3432825)            258.99\nG (37.833654, -119.4510805)             263.87\nH (38.0603395, -119.6666755)             263.9\nI (37.798171, -119.19955150          258.38998\nJ (38.0391175, -119.3073495)            258.21\nName: 265, dtype: object\nTime                            6/24/2018 6:00\nA (38.152231, -119.6666755)             265.84\nB (38.279274, -119.6127765)             267.49\nC (38.50458, -119.62176)                269.36\nD (37.862028, -119.6576925)              269.0\nE (37.89748, -119.2624335)           254.06999\nF (37.8762105, -119.3432825)            257.28\nG (37.833654, -119.4510805)             265.71\nH (38.0603395, -119.6666755)         265.63998\nI (37.798171, -119.19955150             257.61\nJ (38.0391175, -119.3073495)         257.38998\nName: 266, dtype: object\nTime                            6/25/2018 6:00\nA (38.152231, -119.6666755)              264.6\nB (38.279274, -119.6127765)             267.71\nC (38.50458, -119.62176)             271.63998\nD (37.862028, -119.6576925)             269.12\nE (37.89748, -119.2624335)           255.06999\nF (37.8762105, -119.3432825)            259.44\nG (37.833654, -119.4510805)             265.41\nH (38.0603395, -119.6666755)            264.86\nI (37.798171, -119.19955150              256.0\nJ (38.0391175, -119.3073495)         260.50998\nName: 267, dtype: object\nTime                            6/26/2018 6:00\nA (38.152231, -119.6666755)             264.69\nB (38.279274, -119.6127765)             267.16\nC (38.50458, -119.62176)             271.13998\nD (37.862028, -119.6576925)             270.53\nE (37.89748, -119.2624335)              256.38\nF (37.8762105, -119.3432825)            259.05\nG (37.833654, -119.4510805)             264.36\nH (38.0603395, -119.6666755)            264.91\nI (37.798171, -119.19955150             258.88\nJ (38.0391175, -119.3073495)            259.37\nName: 268, dtype: object\nTime                            6/27/2018 6:00\nA (38.152231, -119.6666755)             264.94\nB (38.279274, -119.6127765)             267.91\nC (38.50458, -119.62176)                271.27\nD (37.862028, -119.6576925)          270.47998\nE (37.89748, -119.2624335)              256.71\nF (37.8762105, -119.3432825)         259.94998\nG (37.833654, -119.4510805)             265.71\nH (38.0603395, -119.6666755)            265.02\nI (37.798171, -119.19955150             260.55\nJ (38.0391175, -119.3073495)            259.79\nName: 269, dtype: object\nTime                            6/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 270, dtype: object\nTime                            6/29/2018 6:00\nA (38.152231, -119.6666755)             263.55\nB (38.279274, -119.6127765)             266.18\nC (38.50458, -119.62176)                269.28\nD (37.862028, -119.6576925)             264.57\nE (37.89748, -119.2624335)           259.00998\nF (37.8762105, -119.3432825)            258.88\nG (37.833654, -119.4510805)             260.75\nH (38.0603395, -119.6666755)         262.47998\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)            261.37\nName: 271, dtype: object\nTime                            6/30/2018 6:00\nA (38.152231, -119.6666755)          264.88998\nB (38.279274, -119.6127765)             267.49\nC (38.50458, -119.62176)                270.27\nD (37.862028, -119.6576925)          268.13998\nE (37.89748, -119.2624335)           258.00998\nF (37.8762105, -119.3432825)            260.41\nG (37.833654, -119.4510805)          264.16998\nH (38.0603395, -119.6666755)            264.71\nI (37.798171, -119.19955150          259.19998\nJ (38.0391175, -119.3073495)            258.22\nName: 272, dtype: object\nTime                            7/1/2018 6:00\nA (38.152231, -119.6666755)            266.59\nB (38.279274, -119.6127765)            268.25\nC (38.50458, -119.62176)                271.1\nD (37.862028, -119.6576925)            269.52\nE (37.89748, -119.2624335)          258.91998\nF (37.8762105, -119.3432825)            261.9\nG (37.833654, -119.4510805)            265.09\nH (38.0603395, -119.6666755)           266.19\nI (37.798171, -119.19955150            256.85\nJ (38.0391175, -119.3073495)           261.33\nName: 273, dtype: object\nTime                            7/2/2018 6:00\nA (38.152231, -119.6666755)            267.18\nB (38.279274, -119.6127765)            269.34\nC (38.50458, -119.62176)               272.33\nD (37.862028, -119.6576925)            269.72\nE (37.89748, -119.2624335)          256.66998\nF (37.8762105, -119.3432825)        257.22998\nG (37.833654, -119.4510805)            263.91\nH (38.0603395, -119.6666755)           267.19\nI (37.798171, -119.19955150            260.63\nJ (38.0391175, -119.3073495)           258.21\nName: 274, dtype: object\nTime                            7/3/2018 6:00\nA (38.152231, -119.6666755)            263.86\nB (38.279274, -119.6127765)            266.62\nC (38.50458, -119.62176)               269.68\nD (37.862028, -119.6576925)            268.09\nE (37.89748, -119.2624335)             252.22\nF (37.8762105, -119.3432825)           256.05\nG (37.833654, -119.4510805)            264.19\nH (38.0603395, -119.6666755)        264.19998\nI (37.798171, -119.19955150            254.58\nJ (38.0391175, -119.3073495)           257.33\nName: 275, dtype: object\nTime                            7/4/2018 6:00\nA (38.152231, -119.6666755)            263.06\nB (38.279274, -119.6127765)            265.71\nC (38.50458, -119.62176)               269.27\nD (37.862028, -119.6576925)         268.22998\nE (37.89748, -119.2624335)             253.09\nF (37.8762105, -119.3432825)           256.97\nG (37.833654, -119.4510805)            263.79\nH (38.0603395, -119.6666755)           263.04\nI (37.798171, -119.19955150            255.79\nJ (38.0391175, -119.3073495)           257.41\nName: 276, dtype: object\nTime                            7/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 277, dtype: object\nTime                            7/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 278, dtype: object\nTime                            7/7/2018 6:00\nA (38.152231, -119.6666755)            268.91\nB (38.279274, -119.6127765)            272.56\nC (38.50458, -119.62176)            275.50998\nD (37.862028, -119.6576925)         270.44998\nE (37.89748, -119.2624335)              260.6\nF (37.8762105, -119.3432825)           263.27\nG (37.833654, -119.4510805)            266.94\nH (38.0603395, -119.6666755)           267.94\nI (37.798171, -119.19955150         262.13998\nJ (38.0391175, -119.3073495)           261.57\nName: 279, dtype: object\nTime                            7/8/2018 6:00\nA (38.152231, -119.6666755)         269.00998\nB (38.279274, -119.6127765)            271.57\nC (38.50458, -119.62176)               274.34\nD (37.862028, -119.6576925)            271.24\nE (37.89748, -119.2624335)          259.94998\nF (37.8762105, -119.3432825)           262.75\nG (37.833654, -119.4510805)            267.33\nH (38.0603395, -119.6666755)           268.65\nI (37.798171, -119.19955150            260.93\nJ (38.0391175, -119.3073495)           261.79\nName: 280, dtype: object\nTime                            7/9/2018 6:00\nA (38.152231, -119.6666755)            267.61\nB (38.279274, -119.6127765)             269.8\nC (38.50458, -119.62176)               273.05\nD (37.862028, -119.6576925)            270.15\nE (37.89748, -119.2624335)          260.47998\nF (37.8762105, -119.3432825)        263.22998\nG (37.833654, -119.4510805)            265.57\nH (38.0603395, -119.6666755)           267.52\nI (37.798171, -119.19955150         259.47998\nJ (38.0391175, -119.3073495)           264.96\nName: 281, dtype: object\nTime                            7/10/2018 6:00\nA (38.152231, -119.6666755)             267.04\nB (38.279274, -119.6127765)             269.25\nC (38.50458, -119.62176)                272.11\nD (37.862028, -119.6576925)             271.02\nE (37.89748, -119.2624335)              259.72\nF (37.8762105, -119.3432825)            261.12\nG (37.833654, -119.4510805)             264.28\nH (38.0603395, -119.6666755)         266.97998\nI (37.798171, -119.19955150             260.99\nJ (38.0391175, -119.3073495)         262.50998\nName: 282, dtype: object\nTime                            7/11/2018 6:00\nA (38.152231, -119.6666755)             266.49\nB (38.279274, -119.6127765)              269.0\nC (38.50458, -119.62176)                272.53\nD (37.862028, -119.6576925)             270.22\nE (37.89748, -119.2624335)               257.4\nF (37.8762105, -119.3432825)         260.38998\nG (37.833654, -119.4510805)             265.86\nH (38.0603395, -119.6666755)            266.29\nI (37.798171, -119.19955150             259.78\nJ (38.0391175, -119.3073495)            261.56\nName: 283, dtype: object\nTime                            7/12/2018 6:00\nA (38.152231, -119.6666755)             267.37\nB (38.279274, -119.6127765)             270.11\nC (38.50458, -119.62176)             273.63998\nD (37.862028, -119.6576925)             272.06\nE (37.89748, -119.2624335)           259.16998\nF (37.8762105, -119.3432825)         262.00998\nG (37.833654, -119.4510805)             268.38\nH (38.0603395, -119.6666755)            267.53\nI (37.798171, -119.19955150             261.29\nJ (38.0391175, -119.3073495)            262.27\nName: 284, dtype: object\nTime                            7/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 285, dtype: object\nTime                            7/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 286, dtype: object\nTime                            7/15/2018 6:00\nA (38.152231, -119.6666755)          269.69998\nB (38.279274, -119.6127765)             273.16\nC (38.50458, -119.62176)                276.56\nD (37.862028, -119.6576925)             271.16\nE (37.89748, -119.2624335)              262.83\nF (37.8762105, -119.3432825)         265.19998\nG (37.833654, -119.4510805)             267.97\nH (38.0603395, -119.6666755)         269.16998\nI (37.798171, -119.19955150             264.08\nJ (38.0391175, -119.3073495)         264.13998\nName: 287, dtype: object\nTime                            7/16/2018 6:00\nA (38.152231, -119.6666755)              269.4\nB (38.279274, -119.6127765)             272.29\nC (38.50458, -119.62176)                273.69\nD (37.862028, -119.6576925)             271.43\nE (37.89748, -119.2624335)               261.5\nF (37.8762105, -119.3432825)            262.78\nG (37.833654, -119.4510805)             266.71\nH (38.0603395, -119.6666755)             268.9\nI (37.798171, -119.19955150             263.65\nJ (38.0391175, -119.3073495)            263.36\nName: 288, dtype: object\nTime                            7/17/2018 6:00\nA (38.152231, -119.6666755)             268.27\nB (38.279274, -119.6127765)          270.63998\nC (38.50458, -119.62176)                 273.1\nD (37.862028, -119.6576925)             269.97\nE (37.89748, -119.2624335)              259.81\nF (37.8762105, -119.3432825)             264.1\nG (37.833654, -119.4510805)             268.29\nH (38.0603395, -119.6666755)            268.22\nI (37.798171, -119.19955150          258.66998\nJ (38.0391175, -119.3073495)            263.91\nName: 289, dtype: object\nTime                            7/18/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 290, dtype: object\nTime                            7/19/2018 6:00\nA (38.152231, -119.6666755)             267.47\nB (38.279274, -119.6127765)          269.50998\nC (38.50458, -119.62176)                272.71\nD (37.862028, -119.6576925)             271.87\nE (37.89748, -119.2624335)               258.3\nF (37.8762105, -119.3432825)            260.99\nG (37.833654, -119.4510805)             269.24\nH (38.0603395, -119.6666755)         267.50998\nI (37.798171, -119.19955150             260.93\nJ (38.0391175, -119.3073495)            263.03\nName: 291, dtype: object\nTime                            7/20/2018 6:00\nA (38.152231, -119.6666755)             268.16\nB (38.279274, -119.6127765)             270.85\nC (38.50458, -119.62176)                273.77\nD (37.862028, -119.6576925)             273.07\nE (37.89748, -119.2624335)           260.50998\nF (37.8762105, -119.3432825)            263.21\nG (37.833654, -119.4510805)             269.27\nH (38.0603395, -119.6666755)            268.37\nI (37.798171, -119.19955150             263.49\nJ (38.0391175, -119.3073495)            261.58\nName: 292, dtype: object\nTime                            7/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 293, dtype: object\nTime                            7/22/2018 6:00\nA (38.152231, -119.6666755)             269.41\nB (38.279274, -119.6127765)             271.07\nC (38.50458, -119.62176)                274.18\nD (37.862028, -119.6576925)          270.13998\nE (37.89748, -119.2624335)              259.54\nF (37.8762105, -119.3432825)            262.52\nG (37.833654, -119.4510805)          266.63998\nH (38.0603395, -119.6666755)            269.08\nI (37.798171, -119.19955150             261.71\nJ (38.0391175, -119.3073495)             260.4\nName: 294, dtype: object\nTime                            7/23/2018 6:00\nA (38.152231, -119.6666755)             267.78\nB (38.279274, -119.6127765)          269.50998\nC (38.50458, -119.62176)                272.66\nD (37.862028, -119.6576925)              271.1\nE (37.89748, -119.2624335)           259.50998\nF (37.8762105, -119.3432825)            262.52\nG (37.833654, -119.4510805)             266.38\nH (38.0603395, -119.6666755)            267.94\nI (37.798171, -119.19955150          260.72998\nJ (38.0391175, -119.3073495)         261.63998\nName: 295, dtype: object\nTime                            7/24/2018 6:00\nA (38.152231, -119.6666755)             268.83\nB (38.279274, -119.6127765)             270.61\nC (38.50458, -119.62176)                272.94\nD (37.862028, -119.6576925)          271.72998\nE (37.89748, -119.2624335)              258.71\nF (37.8762105, -119.3432825)            260.25\nG (37.833654, -119.4510805)          266.50998\nH (38.0603395, -119.6666755)            268.78\nI (37.798171, -119.19955150             261.99\nJ (38.0391175, -119.3073495)            260.49\nName: 296, dtype: object\nTime                            7/25/2018 6:00\nA (38.152231, -119.6666755)             267.12\nB (38.279274, -119.6127765)             269.63\nC (38.50458, -119.62176)                271.63\nD (37.862028, -119.6576925)          269.72998\nE (37.89748, -119.2624335)           256.72998\nF (37.8762105, -119.3432825)             259.4\nG (37.833654, -119.4510805)             266.66\nH (38.0603395, -119.6666755)         266.88998\nI (37.798171, -119.19955150             259.55\nJ (38.0391175, -119.3073495)             260.0\nName: 297, dtype: object\nTime                            7/26/2018 6:00\nA (38.152231, -119.6666755)             268.12\nB (38.279274, -119.6127765)          270.16998\nC (38.50458, -119.62176)                273.02\nD (37.862028, -119.6576925)             271.58\nE (37.89748, -119.2624335)              259.06\nF (37.8762105, -119.3432825)            262.29\nG (37.833654, -119.4510805)             268.53\nH (38.0603395, -119.6666755)            267.59\nI (37.798171, -119.19955150          260.00998\nJ (38.0391175, -119.3073495)         263.41998\nName: 298, dtype: object\nTime                            7/27/2018 6:00\nA (38.152231, -119.6666755)             267.37\nB (38.279274, -119.6127765)             269.35\nC (38.50458, -119.62176)                272.85\nD (37.862028, -119.6576925)          271.97998\nE (37.89748, -119.2624335)              258.22\nF (37.8762105, -119.3432825)            260.96\nG (37.833654, -119.4510805)             267.09\nH (38.0603395, -119.6666755)            267.11\nI (37.798171, -119.19955150             260.77\nJ (38.0391175, -119.3073495)            261.31\nName: 299, dtype: object\nTime                            7/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              252.92\nF (37.8762105, -119.3432825)            255.17\nG (37.833654, -119.4510805)             262.03\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             261.72\nJ (38.0391175, -119.3073495)            253.37\nName: 300, dtype: object\nTime                            7/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 301, dtype: object\nTime                            7/30/2018 6:00\nA (38.152231, -119.6666755)             269.59\nB (38.279274, -119.6127765)             272.61\nC (38.50458, -119.62176)                274.62\nD (37.862028, -119.6576925)             270.93\nE (37.89748, -119.2624335)           262.22998\nF (37.8762105, -119.3432825)            264.88\nG (37.833654, -119.4510805)              268.3\nH (38.0603395, -119.6666755)            268.71\nI (37.798171, -119.19955150             263.41\nJ (38.0391175, -119.3073495)            262.79\nName: 302, dtype: object\nTime                            7/31/2018 6:00\nA (38.152231, -119.6666755)             269.21\nB (38.279274, -119.6127765)             271.83\nC (38.50458, -119.62176)             274.72998\nD (37.862028, -119.6576925)             271.43\nE (37.89748, -119.2624335)               260.1\nF (37.8762105, -119.3432825)             263.5\nG (37.833654, -119.4510805)             269.52\nH (38.0603395, -119.6666755)         269.19998\nI (37.798171, -119.19955150             260.94\nJ (38.0391175, -119.3073495)            262.87\nName: 303, dtype: object\nTime                            8/1/2018 6:00\nA (38.152231, -119.6666755)            269.43\nB (38.279274, -119.6127765)            271.53\nC (38.50458, -119.62176)               273.65\nD (37.862028, -119.6576925)            271.55\nE (37.89748, -119.2624335)             262.85\nF (37.8762105, -119.3432825)           262.63\nG (37.833654, -119.4510805)            265.66\nH (38.0603395, -119.6666755)           268.99\nI (37.798171, -119.19955150            263.09\nJ (38.0391175, -119.3073495)           263.34\nName: 304, dtype: object\nTime                            8/2/2018 6:00\nA (38.152231, -119.6666755)            267.19\nB (38.279274, -119.6127765)            269.16\nC (38.50458, -119.62176)            271.75998\nD (37.862028, -119.6576925)            270.15\nE (37.89748, -119.2624335)          259.75998\nF (37.8762105, -119.3432825)            259.5\nG (37.833654, -119.4510805)            264.93\nH (38.0603395, -119.6666755)           266.59\nI (37.798171, -119.19955150            262.12\nJ (38.0391175, -119.3073495)           261.03\nName: 305, dtype: object\nTime                            8/3/2018 6:00\nA (38.152231, -119.6666755)            264.15\nB (38.279274, -119.6127765)            266.85\nC (38.50458, -119.62176)                269.9\nD (37.862028, -119.6576925)            268.77\nE (37.89748, -119.2624335)             255.65\nF (37.8762105, -119.3432825)           259.18\nG (37.833654, -119.4510805)             264.6\nH (38.0603395, -119.6666755)           264.08\nI (37.798171, -119.19955150            257.03\nJ (38.0391175, -119.3073495)           259.53\nName: 306, dtype: object\nTime                            8/4/2018 6:00\nA (38.152231, -119.6666755)         264.41998\nB (38.279274, -119.6127765)            267.24\nC (38.50458, -119.62176)               270.15\nD (37.862028, -119.6576925)             269.9\nE (37.89748, -119.2624335)          256.44998\nF (37.8762105, -119.3432825)           259.44\nG (37.833654, -119.4510805)            265.79\nH (38.0603395, -119.6666755)        264.38998\nI (37.798171, -119.19955150            258.57\nJ (38.0391175, -119.3073495)           259.13\nName: 307, dtype: object\nTime                            8/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 308, dtype: object\nTime                            8/6/2018 6:00\nA (38.152231, -119.6666755)            265.74\nB (38.279274, -119.6127765)         267.75998\nC (38.50458, -119.62176)               270.44\nD (37.862028, -119.6576925)            266.91\nE (37.89748, -119.2624335)             258.11\nF (37.8762105, -119.3432825)           260.31\nG (37.833654, -119.4510805)            263.75\nH (38.0603395, -119.6666755)           265.18\nI (37.798171, -119.19955150             259.3\nJ (38.0391175, -119.3073495)            257.4\nName: 309, dtype: object\nTime                            8/7/2018 6:00\nA (38.152231, -119.6666755)             266.5\nB (38.279274, -119.6127765)            268.03\nC (38.50458, -119.62176)               270.85\nD (37.862028, -119.6576925)            269.09\nE (37.89748, -119.2624335)          257.91998\nF (37.8762105, -119.3432825)           260.61\nG (37.833654, -119.4510805)            265.77\nH (38.0603395, -119.6666755)        265.97998\nI (37.798171, -119.19955150            259.12\nJ (38.0391175, -119.3073495)        259.69998\nName: 310, dtype: object\nTime                            8/8/2018 6:00\nA (38.152231, -119.6666755)            265.62\nB (38.279274, -119.6127765)            268.28\nC (38.50458, -119.62176)               271.11\nD (37.862028, -119.6576925)            268.63\nE (37.89748, -119.2624335)             257.36\nF (37.8762105, -119.3432825)           262.03\nG (37.833654, -119.4510805)         267.13998\nH (38.0603395, -119.6666755)           264.94\nI (37.798171, -119.19955150            256.85\nJ (38.0391175, -119.3073495)        259.66998\nName: 311, dtype: object\nTime                            8/9/2018 6:00\nA (38.152231, -119.6666755)            266.46\nB (38.279274, -119.6127765)         268.19998\nC (38.50458, -119.62176)                270.5\nD (37.862028, -119.6576925)            269.65\nE (37.89748, -119.2624335)             259.54\nF (37.8762105, -119.3432825)           263.49\nG (37.833654, -119.4510805)            265.05\nH (38.0603395, -119.6666755)           266.49\nI (37.798171, -119.19955150            257.63\nJ (38.0391175, -119.3073495)           264.19\nName: 312, dtype: object\nTime                            8/10/2018 6:00\nA (38.152231, -119.6666755)          266.47998\nB (38.279274, -119.6127765)          268.47998\nC (38.50458, -119.62176)             271.13998\nD (37.862028, -119.6576925)             269.54\nE (37.89748, -119.2624335)              258.33\nF (37.8762105, -119.3432825)            260.88\nG (37.833654, -119.4510805)             264.52\nH (38.0603395, -119.6666755)            266.22\nI (37.798171, -119.19955150             259.09\nJ (38.0391175, -119.3073495)            261.79\nName: 313, dtype: object\nTime                            8/11/2018 6:00\nA (38.152231, -119.6666755)          265.16998\nB (38.279274, -119.6127765)             266.97\nC (38.50458, -119.62176)                270.63\nD (37.862028, -119.6576925)             269.71\nE (37.89748, -119.2624335)              257.22\nF (37.8762105, -119.3432825)            259.36\nG (37.833654, -119.4510805)              265.3\nH (38.0603395, -119.6666755)             265.6\nI (37.798171, -119.19955150             258.88\nJ (38.0391175, -119.3073495)         259.13998\nName: 314, dtype: object\nTime                            8/12/2018 6:00\nA (38.152231, -119.6666755)             265.32\nB (38.279274, -119.6127765)             267.96\nC (38.50458, -119.62176)                270.59\nD (37.862028, -119.6576925)             269.74\nE (37.89748, -119.2624335)              258.62\nF (37.8762105, -119.3432825)            261.21\nG (37.833654, -119.4510805)              266.3\nH (38.0603395, -119.6666755)         265.25998\nI (37.798171, -119.19955150             261.09\nJ (38.0391175, -119.3073495)         259.41998\nName: 315, dtype: object\nTime                            8/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 316, dtype: object\nTime                            8/14/2018 6:00\nA (38.152231, -119.6666755)             267.68\nB (38.279274, -119.6127765)          269.91998\nC (38.50458, -119.62176)                273.07\nD (37.862028, -119.6576925)             269.37\nE (37.89748, -119.2624335)              260.82\nF (37.8762105, -119.3432825)            263.15\nG (37.833654, -119.4510805)             266.33\nH (38.0603395, -119.6666755)         267.13998\nI (37.798171, -119.19955150             262.04\nJ (38.0391175, -119.3073495)            260.53\nName: 317, dtype: object\nTime                            8/15/2018 6:00\nA (38.152231, -119.6666755)          266.66998\nB (38.279274, -119.6127765)             268.53\nC (38.50458, -119.62176)                271.56\nD (37.862028, -119.6576925)             268.34\nE (37.89748, -119.2624335)              258.94\nF (37.8762105, -119.3432825)            262.47\nG (37.833654, -119.4510805)             265.55\nH (38.0603395, -119.6666755)            266.08\nI (37.798171, -119.19955150             259.53\nJ (38.0391175, -119.3073495)            260.74\nName: 318, dtype: object\nTime                            8/16/2018 6:00\nA (38.152231, -119.6666755)          266.00998\nB (38.279274, -119.6127765)          267.97998\nC (38.50458, -119.62176)                269.54\nD (37.862028, -119.6576925)          267.88998\nE (37.89748, -119.2624335)           254.20999\nF (37.8762105, -119.3432825)            258.46\nG (37.833654, -119.4510805)             265.27\nH (38.0603395, -119.6666755)            265.58\nI (37.798171, -119.19955150             257.53\nJ (38.0391175, -119.3073495)            257.61\nName: 319, dtype: object\nTime                            8/17/2018 6:00\nA (38.152231, -119.6666755)             264.69\nB (38.279274, -119.6127765)          266.72998\nC (38.50458, -119.62176)                269.15\nD (37.862028, -119.6576925)             268.46\nE (37.89748, -119.2624335)              254.64\nF (37.8762105, -119.3432825)            258.24\nG (37.833654, -119.4510805)             266.94\nH (38.0603395, -119.6666755)            265.15\nI (37.798171, -119.19955150             256.19\nJ (38.0391175, -119.3073495)            258.82\nName: 320, dtype: object\nTime                            8/18/2018 6:00\nA (38.152231, -119.6666755)             264.12\nB (38.279274, -119.6127765)             266.18\nC (38.50458, -119.62176)                 269.1\nD (37.862028, -119.6576925)              269.0\nE (37.89748, -119.2624335)           256.19998\nF (37.8762105, -119.3432825)            259.37\nG (37.833654, -119.4510805)             263.79\nH (38.0603395, -119.6666755)         263.88998\nI (37.798171, -119.19955150             257.11\nJ (38.0391175, -119.3073495)            260.09\nName: 321, dtype: object\nTime                            8/19/2018 6:00\nA (38.152231, -119.6666755)             265.27\nB (38.279274, -119.6127765)             267.13\nC (38.50458, -119.62176)                270.33\nD (37.862028, -119.6576925)             270.24\nE (37.89748, -119.2624335)           256.66998\nF (37.8762105, -119.3432825)             259.5\nG (37.833654, -119.4510805)          266.38998\nH (38.0603395, -119.6666755)         265.44998\nI (37.798171, -119.19955150          259.00998\nJ (38.0391175, -119.3073495)            258.61\nName: 322, dtype: object\nTime                            8/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 323, dtype: object\nTime                            8/21/2018 6:00\nA (38.152231, -119.6666755)             263.71\nB (38.279274, -119.6127765)             266.06\nC (38.50458, -119.62176)                270.08\nD (37.862028, -119.6576925)             265.63\nE (37.89748, -119.2624335)              259.08\nF (37.8762105, -119.3432825)             260.0\nG (37.833654, -119.4510805)             261.66\nH (38.0603395, -119.6666755)            263.18\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)            260.28\nName: 324, dtype: object\nTime                            8/22/2018 6:00\nA (38.152231, -119.6666755)             262.04\nB (38.279274, -119.6127765)          264.94998\nC (38.50458, -119.62176)             267.50998\nD (37.862028, -119.6576925)             264.81\nE (37.89748, -119.2624335)              255.15\nF (37.8762105, -119.3432825)            258.03\nG (37.833654, -119.4510805)             262.15\nH (38.0603395, -119.6666755)         261.47998\nI (37.798171, -119.19955150             257.04\nJ (38.0391175, -119.3073495)            256.31\nName: 325, dtype: object\nTime                            8/23/2018 6:00\nA (38.152231, -119.6666755)             263.07\nB (38.279274, -119.6127765)             264.34\nC (38.50458, -119.62176)                267.63\nD (37.862028, -119.6576925)              265.8\nE (37.89748, -119.2624335)              256.02\nF (37.8762105, -119.3432825)            258.91\nG (37.833654, -119.4510805)          261.94998\nH (38.0603395, -119.6666755)            262.82\nI (37.798171, -119.19955150             254.39\nJ (38.0391175, -119.3073495)            257.94\nName: 326, dtype: object\nTime                            8/24/2018 6:00\nA (38.152231, -119.6666755)             261.86\nB (38.279274, -119.6127765)             263.79\nC (38.50458, -119.62176)                266.56\nD (37.862028, -119.6576925)             265.02\nE (37.89748, -119.2624335)              254.76\nF (37.8762105, -119.3432825)            255.64\nG (37.833654, -119.4510805)             259.77\nH (38.0603395, -119.6666755)         262.19998\nI (37.798171, -119.19955150             256.29\nJ (38.0391175, -119.3073495)         255.26999\nName: 327, dtype: object\nTime                            8/25/2018 6:00\nA (38.152231, -119.6666755)             260.05\nB (38.279274, -119.6127765)             261.81\nC (38.50458, -119.62176)                264.41\nD (37.862028, -119.6576925)             264.15\nE (37.89748, -119.2624335)              248.97\nF (37.8762105, -119.3432825)            252.22\nG (37.833654, -119.4510805)          259.94998\nH (38.0603395, -119.6666755)            260.41\nI (37.798171, -119.19955150          252.29999\nJ (38.0391175, -119.3073495)            252.92\nName: 328, dtype: object\nTime                            8/26/2018 6:00\nA (38.152231, -119.6666755)             260.16\nB (38.279274, -119.6127765)          262.50998\nC (38.50458, -119.62176)                264.87\nD (37.862028, -119.6576925)             264.96\nE (37.89748, -119.2624335)           251.18999\nF (37.8762105, -119.3432825)         254.18999\nG (37.833654, -119.4510805)             260.93\nH (38.0603395, -119.6666755)             260.6\nI (37.798171, -119.19955150              254.5\nJ (38.0391175, -119.3073495)         253.81999\nName: 329, dtype: object\nTime                            8/27/2018 6:00\nA (38.152231, -119.6666755)             261.08\nB (38.279274, -119.6127765)             263.61\nC (38.50458, -119.62176)                266.33\nD (37.862028, -119.6576925)             265.82\nE (37.89748, -119.2624335)              253.04\nF (37.8762105, -119.3432825)            256.78\nG (37.833654, -119.4510805)          261.44998\nH (38.0603395, -119.6666755)            261.46\nI (37.798171, -119.19955150             256.13\nJ (38.0391175, -119.3073495)         254.68999\nName: 330, dtype: object\nTime                            8/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 331, dtype: object\nTime                            8/29/2018 6:00\nA (38.152231, -119.6666755)          263.69998\nB (38.279274, -119.6127765)          266.47998\nC (38.50458, -119.62176)                270.09\nD (37.862028, -119.6576925)              265.8\nE (37.89748, -119.2624335)              255.97\nF (37.8762105, -119.3432825)             259.4\nG (37.833654, -119.4510805)          262.91998\nH (38.0603395, -119.6666755)         263.41998\nI (37.798171, -119.19955150             258.86\nJ (38.0391175, -119.3073495)            255.72\nName: 332, dtype: object\nTime                            8/30/2018 6:00\nA (38.152231, -119.6666755)             261.27\nB (38.279274, -119.6127765)             263.41\nC (38.50458, -119.62176)             265.63998\nD (37.862028, -119.6576925)             264.33\nE (37.89748, -119.2624335)              252.76\nF (37.8762105, -119.3432825)            255.31\nG (37.833654, -119.4510805)             260.29\nH (38.0603395, -119.6666755)         261.38998\nI (37.798171, -119.19955150          253.68999\nJ (38.0391175, -119.3073495)            254.31\nName: 333, dtype: object\nTime                            8/31/2018 6:00\nA (38.152231, -119.6666755)          258.97998\nB (38.279274, -119.6127765)             260.03\nC (38.50458, -119.62176)                 263.4\nD (37.862028, -119.6576925)             262.35\nE (37.89748, -119.2624335)           252.01999\nF (37.8762105, -119.3432825)            255.34\nG (37.833654, -119.4510805)          258.63998\nH (38.0603395, -119.6666755)            258.72\nI (37.798171, -119.19955150          251.48999\nJ (38.0391175, -119.3073495)            253.98\nName: 334, dtype: object\nTime                            9/1/2018 6:00\nA (38.152231, -119.6666755)            260.99\nB (38.279274, -119.6127765)         261.63998\nC (38.50458, -119.62176)               264.71\nD (37.862028, -119.6576925)            265.94\nE (37.89748, -119.2624335)             254.37\nF (37.8762105, -119.3432825)        257.19998\nG (37.833654, -119.4510805)            259.68\nH (38.0603395, -119.6666755)           261.04\nI (37.798171, -119.19955150            254.89\nJ (38.0391175, -119.3073495)        257.44998\nName: 335, dtype: object\nTime                            9/2/2018 6:00\nA (38.152231, -119.6666755)            261.21\nB (38.279274, -119.6127765)         262.63998\nC (38.50458, -119.62176)            266.16998\nD (37.862028, -119.6576925)            267.07\nE (37.89748, -119.2624335)             252.37\nF (37.8762105, -119.3432825)           255.68\nG (37.833654, -119.4510805)            262.77\nH (38.0603395, -119.6666755)        261.50998\nI (37.798171, -119.19955150         255.51999\nJ (38.0391175, -119.3073495)            255.2\nName: 336, dtype: object\nTime                            9/3/2018 6:00\nA (38.152231, -119.6666755)            262.31\nB (38.279274, -119.6127765)            263.02\nC (38.50458, -119.62176)                266.9\nD (37.862028, -119.6576925)            268.18\nE (37.89748, -119.2624335)             255.53\nF (37.8762105, -119.3432825)           258.54\nG (37.833654, -119.4510805)            265.25\nH (38.0603395, -119.6666755)           263.19\nI (37.798171, -119.19955150            256.47\nJ (38.0391175, -119.3073495)        256.25998\nName: 337, dtype: object\nTime                            9/4/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 338, dtype: object\nTime                            9/5/2018 6:00\nA (38.152231, -119.6666755)         264.94998\nB (38.279274, -119.6127765)            265.06\nC (38.50458, -119.62176)               270.52\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)           265.31\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 339, dtype: object\nTime                            9/6/2018 6:00\nA (38.152231, -119.6666755)            263.37\nB (38.279274, -119.6127765)            265.86\nC (38.50458, -119.62176)                269.0\nD (37.862028, -119.6576925)            268.19\nE (37.89748, -119.2624335)             255.65\nF (37.8762105, -119.3432825)           259.02\nG (37.833654, -119.4510805)            262.87\nH (38.0603395, -119.6666755)           263.59\nI (37.798171, -119.19955150         257.44998\nJ (38.0391175, -119.3073495)           256.27\nName: 340, dtype: object\nTime                            9/7/2018 6:00\nA (38.152231, -119.6666755)            264.18\nB (38.279274, -119.6127765)            266.06\nC (38.50458, -119.62176)               268.15\nD (37.862028, -119.6576925)         267.66998\nE (37.89748, -119.2624335)             256.56\nF (37.8762105, -119.3432825)            259.0\nG (37.833654, -119.4510805)            262.16\nH (38.0603395, -119.6666755)           264.15\nI (37.798171, -119.19955150             258.0\nJ (38.0391175, -119.3073495)           258.13\nName: 341, dtype: object\nTime                            9/8/2018 6:00\nA (38.152231, -119.6666755)            262.49\nB (38.279274, -119.6127765)            264.18\nC (38.50458, -119.62176)               267.18\nD (37.862028, -119.6576925)             265.5\nE (37.89748, -119.2624335)          251.48999\nF (37.8762105, -119.3432825)           256.28\nG (37.833654, -119.4510805)            262.91\nH (38.0603395, -119.6666755)           262.54\nI (37.798171, -119.19955150            253.45\nJ (38.0391175, -119.3073495)        254.87999\nName: 342, dtype: object\nTime                            9/9/2018 6:00\nA (38.152231, -119.6666755)            259.31\nB (38.279274, -119.6127765)         260.25998\nC (38.50458, -119.62176)                263.4\nD (37.862028, -119.6576925)            264.49\nE (37.89748, -119.2624335)          250.37999\nF (37.8762105, -119.3432825)           254.26\nG (37.833654, -119.4510805)            258.61\nH (38.0603395, -119.6666755)           258.94\nI (37.798171, -119.19955150         250.76999\nJ (38.0391175, -119.3073495)        253.23999\nName: 343, dtype: object\nTime                            9/10/2018 6:00\nA (38.152231, -119.6666755)             259.43\nB (38.279274, -119.6127765)              261.1\nC (38.50458, -119.62176)                 264.1\nD (37.862028, -119.6576925)             264.37\nE (37.89748, -119.2624335)           248.93999\nF (37.8762105, -119.3432825)             252.7\nG (37.833654, -119.4510805)             260.68\nH (38.0603395, -119.6666755)            259.71\nI (37.798171, -119.19955150             252.06\nJ (38.0391175, -119.3073495)            252.39\nName: 344, dtype: object\nTime                            9/11/2018 6:00\nA (38.152231, -119.6666755)             258.78\nB (38.279274, -119.6127765)             260.62\nC (38.50458, -119.62176)                263.69\nD (37.862028, -119.6576925)             264.65\nE (37.89748, -119.2624335)              250.47\nF (37.8762105, -119.3432825)            253.98\nG (37.833654, -119.4510805)             260.47\nH (38.0603395, -119.6666755)         259.25998\nI (37.798171, -119.19955150             253.39\nJ (38.0391175, -119.3073495)            251.22\nName: 345, dtype: object\nTime                            9/12/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 346, dtype: object\nTime                            9/13/2018 6:00\nA (38.152231, -119.6666755)             258.91\nB (38.279274, -119.6127765)             261.19\nC (38.50458, -119.62176)                 264.4\nD (37.862028, -119.6576925)          261.75998\nE (37.89748, -119.2624335)           250.95999\nF (37.8762105, -119.3432825)            253.95\nG (37.833654, -119.4510805)             257.53\nH (38.0603395, -119.6666755)         258.38998\nI (37.798171, -119.19955150             253.09\nJ (38.0391175, -119.3073495)            250.33\nName: 347, dtype: object\nTime                            9/14/2018 6:00\nA (38.152231, -119.6666755)             257.82\nB (38.279274, -119.6127765)             259.83\nC (38.50458, -119.62176)                262.27\nD (37.862028, -119.6576925)             261.16\nE (37.89748, -119.2624335)              250.75\nF (37.8762105, -119.3432825)            252.73\nG (37.833654, -119.4510805)             257.56\nH (38.0603395, -119.6666755)         257.66998\nI (37.798171, -119.19955150             251.84\nJ (38.0391175, -119.3073495)         251.09999\nName: 348, dtype: object\nTime                            9/15/2018 6:00\nA (38.152231, -119.6666755)              259.8\nB (38.279274, -119.6127765)             261.11\nC (38.50458, -119.62176)                263.65\nD (37.862028, -119.6576925)             263.16\nE (37.89748, -119.2624335)              253.62\nF (37.8762105, -119.3432825)             254.4\nG (37.833654, -119.4510805)              257.0\nH (38.0603395, -119.6666755)         259.88998\nI (37.798171, -119.19955150             254.58\nJ (38.0391175, -119.3073495)            253.98\nName: 349, dtype: object\nTime                            9/16/2018 6:00\nA (38.152231, -119.6666755)             256.24\nB (38.279274, -119.6127765)             257.91\nC (38.50458, -119.62176)                260.12\nD (37.862028, -119.6576925)          259.91998\nE (37.89748, -119.2624335)              244.48\nF (37.8762105, -119.3432825)         247.65999\nG (37.833654, -119.4510805)             255.78\nH (38.0603395, -119.6666755)            256.25\nI (37.798171, -119.19955150          249.34999\nJ (38.0391175, -119.3073495)            246.68\nName: 350, dtype: object\nTime                            9/17/2018 6:00\nA (38.152231, -119.6666755)              255.9\nB (38.279274, -119.6127765)             256.93\nC (38.50458, -119.62176)                260.41\nD (37.862028, -119.6576925)             260.75\nE (37.89748, -119.2624335)           246.23999\nF (37.8762105, -119.3432825)            250.06\nG (37.833654, -119.4510805)             256.38\nH (38.0603395, -119.6666755)            256.04\nI (37.798171, -119.19955150             248.61\nJ (38.0391175, -119.3073495)         249.87999\nName: 351, dtype: object\nTime                            9/18/2018 6:00\nA (38.152231, -119.6666755)          255.87999\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                260.35\nD (37.862028, -119.6576925)          261.19998\nE (37.89748, -119.2624335)              246.86\nF (37.8762105, -119.3432825)         250.48999\nG (37.833654, -119.4510805)          257.44998\nH (38.0603395, -119.6666755)            256.03\nI (37.798171, -119.19955150              249.0\nJ (38.0391175, -119.3073495)            249.83\nName: 352, dtype: object\nTime                            9/19/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)           236.01999\nF (37.8762105, -119.3432825)         244.06999\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             250.83\nJ (38.0391175, -119.3073495)            232.73\nName: 353, dtype: object\nTime                            9/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 354, dtype: object\nTime                            9/21/2018 6:00\nA (38.152231, -119.6666755)             260.37\nB (38.279274, -119.6127765)             262.32\nC (38.50458, -119.62176)                264.22\nD (37.862028, -119.6576925)             262.87\nE (37.89748, -119.2624335)               251.5\nF (37.8762105, -119.3432825)         254.76999\nG (37.833654, -119.4510805)             258.78\nH (38.0603395, -119.6666755)            259.83\nI (37.798171, -119.19955150          253.70999\nJ (38.0391175, -119.3073495)         251.70999\nName: 355, dtype: object\nTime                            9/22/2018 6:00\nA (38.152231, -119.6666755)             258.46\nB (38.279274, -119.6127765)          260.97998\nC (38.50458, -119.62176)                262.99\nD (37.862028, -119.6576925)             262.28\nE (37.89748, -119.2624335)           250.23999\nF (37.8762105, -119.3432825)            253.28\nG (37.833654, -119.4510805)             257.22\nH (38.0603395, -119.6666755)            258.43\nI (37.798171, -119.19955150             252.01\nJ (38.0391175, -119.3073495)         252.15999\nName: 356, dtype: object\nTime                            9/23/2018 6:00\nA (38.152231, -119.6666755)          258.72998\nB (38.279274, -119.6127765)             259.35\nC (38.50458, -119.62176)                262.74\nD (37.862028, -119.6576925)             263.06\nE (37.89748, -119.2624335)           251.70999\nF (37.8762105, -119.3432825)            255.06\nG (37.833654, -119.4510805)             256.08\nH (38.0603395, -119.6666755)            259.15\nI (37.798171, -119.19955150          251.48999\nJ (38.0391175, -119.3073495)            255.45\nName: 357, dtype: object\nTime                            9/24/2018 6:00\nA (38.152231, -119.6666755)          257.13998\nB (38.279274, -119.6127765)             257.56\nC (38.50458, -119.62176)                260.66\nD (37.862028, -119.6576925)          261.22998\nE (37.89748, -119.2624335)              247.48\nF (37.8762105, -119.3432825)            248.53\nG (37.833654, -119.4510805)             255.36\nH (38.0603395, -119.6666755)            257.46\nI (37.798171, -119.19955150          249.54999\nJ (38.0391175, -119.3073495)         248.45999\nName: 358, dtype: object\nTime                            9/25/2018 6:00\nA (38.152231, -119.6666755)             256.36\nB (38.279274, -119.6127765)          257.00998\nC (38.50458, -119.62176)                259.55\nD (37.862028, -119.6576925)             263.61\nE (37.89748, -119.2624335)              247.53\nF (37.8762105, -119.3432825)            252.42\nG (37.833654, -119.4510805)             257.21\nH (38.0603395, -119.6666755)            257.47\nI (37.798171, -119.19955150             248.78\nJ (38.0391175, -119.3073495)            250.34\nName: 359, dtype: object\nTime                            9/26/2018 6:00\nA (38.152231, -119.6666755)             257.65\nB (38.279274, -119.6127765)          258.88998\nC (38.50458, -119.62176)                260.77\nD (37.862028, -119.6576925)             265.22\nE (37.89748, -119.2624335)              248.58\nF (37.8762105, -119.3432825)         252.26999\nG (37.833654, -119.4510805)          258.50998\nH (38.0603395, -119.6666755)            258.22\nI (37.798171, -119.19955150             251.23\nJ (38.0391175, -119.3073495)            249.79\nName: 360, dtype: object\nTime                            9/27/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 361, dtype: object\nTime                            9/28/2018 6:00\nA (38.152231, -119.6666755)             260.49\nB (38.279274, -119.6127765)             262.97\nC (38.50458, -119.62176)                266.09\nD (37.862028, -119.6576925)          262.97998\nE (37.89748, -119.2624335)              253.01\nF (37.8762105, -119.3432825)            255.48\nG (37.833654, -119.4510805)             259.61\nH (38.0603395, -119.6666755)            260.33\nI (37.798171, -119.19955150          254.70999\nJ (38.0391175, -119.3073495)         252.51999\nName: 362, dtype: object\nTime                            9/29/2018 6:00\nA (38.152231, -119.6666755)             259.52\nB (38.279274, -119.6127765)             261.13\nC (38.50458, -119.62176)                263.65\nD (37.862028, -119.6576925)              263.1\nE (37.89748, -119.2624335)              252.01\nF (37.8762105, -119.3432825)            254.34\nG (37.833654, -119.4510805)             258.03\nH (38.0603395, -119.6666755)            259.63\nI (37.798171, -119.19955150          253.34999\nJ (38.0391175, -119.3073495)         252.98999\nName: 363, dtype: object\n             date        lat           lon     pmv\n0  10/1/2017 6:00  38.152231  -119.6666755  256.03\n1  10/1/2017 6:00  38.279274  -119.6127765  257.35\n2  10/1/2017 6:00   38.50458    -119.62176  259.46\n3  10/1/2017 6:00  37.862028  -119.6576925  259.87\n4  10/1/2017 6:00   37.89748  -119.2624335  247.56\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/tD6x9eyL4KSi/data_merge_hackweek.py\", line 71, in <module>\n    print(pmv_new_df.shape())\n          ^^^^^^^^^^^^^^^^^^\nTypeError: 'tuple' object is not callable\n",
  "history_begin_time" : 1698188622799,
  "history_end_time" : 1698188626255,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JPTeafljGmJ2",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  print(row)\n  \n  for property_name, value in row.items():\n    print(\"property_name: \", property_name)\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/2/2017 6:00\nA (38.152231, -119.6666755)             249.79\nB (38.279274, -119.6127765)             250.26\nC (38.50458, -119.62176)                253.73\nD (37.862028, -119.6576925)          256.16998\nE (37.89748, -119.2624335)           243.37999\nF (37.8762105, -119.3432825)            247.51\nG (37.833654, -119.4510805)              251.0\nH (38.0603395, -119.6666755)             251.2\nI (37.798171, -119.19955150             242.98\nJ (38.0391175, -119.3073495)         246.26999\nName: 1, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/3/2017 6:00\nA (38.152231, -119.6666755)             249.61\nB (38.279274, -119.6127765)             248.98\nC (38.50458, -119.62176)                252.51\nD (37.862028, -119.6576925)          255.79999\nE (37.89748, -119.2624335)              242.95\nF (37.8762105, -119.3432825)         244.23999\nG (37.833654, -119.4510805)             248.87\nH (38.0603395, -119.6666755)            250.31\nI (37.798171, -119.19955150             242.78\nJ (38.0391175, -119.3073495)             243.7\nName: 2, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/4/2017 6:00\nA (38.152231, -119.6666755)             249.28\nB (38.279274, -119.6127765)          249.40999\nC (38.50458, -119.62176)                252.68\nD (37.862028, -119.6576925)              255.2\nE (37.89748, -119.2624335)              239.87\nF (37.8762105, -119.3432825)            242.56\nG (37.833654, -119.4510805)             249.95\nH (38.0603395, -119.6666755)         249.95999\nI (37.798171, -119.19955150          242.20999\nJ (38.0391175, -119.3073495)         242.09999\nName: 3, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/5/2017 6:00\nA (38.152231, -119.6666755)             250.11\nB (38.279274, -119.6127765)          251.12999\nC (38.50458, -119.62176)             254.76999\nD (37.862028, -119.6576925)             258.03\nE (37.89748, -119.2624335)           241.40999\nF (37.8762105, -119.3432825)         244.70999\nG (37.833654, -119.4510805)             252.42\nH (38.0603395, -119.6666755)         251.29999\nI (37.798171, -119.19955150             245.09\nJ (38.0391175, -119.3073495)         243.06999\nName: 4, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 5, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/7/2017 6:00\nA (38.152231, -119.6666755)          256.16998\nB (38.279274, -119.6127765)              258.3\nC (38.50458, -119.62176)                261.61\nD (37.862028, -119.6576925)             258.69\nE (37.89748, -119.2624335)              252.11\nF (37.8762105, -119.3432825)            252.48\nG (37.833654, -119.4510805)             254.45\nH (38.0603395, -119.6666755)         255.95999\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)             252.4\nName: 6, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/8/2017 6:00\nA (38.152231, -119.6666755)             255.84\nB (38.279274, -119.6127765)             257.32\nC (38.50458, -119.62176)                260.16\nD (37.862028, -119.6576925)          259.66998\nE (37.89748, -119.2624335)              248.25\nF (37.8762105, -119.3432825)            250.84\nG (37.833654, -119.4510805)          255.84999\nH (38.0603395, -119.6666755)            255.72\nI (37.798171, -119.19955150             250.14\nJ (38.0391175, -119.3073495)            248.34\nName: 7, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/9/2017 6:00\nA (38.152231, -119.6666755)          253.45999\nB (38.279274, -119.6127765)          252.51999\nC (38.50458, -119.62176)                253.48\nD (37.862028, -119.6576925)             260.43\nE (37.89748, -119.2624335)              245.37\nF (37.8762105, -119.3432825)         248.04999\nG (37.833654, -119.4510805)             252.81\nH (38.0603395, -119.6666755)            254.29\nI (37.798171, -119.19955150          247.12999\nJ (38.0391175, -119.3073495)             245.4\nName: 8, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/10/2017 6:00\nA (38.152231, -119.6666755)              252.25\nB (38.279274, -119.6127765)              253.53\nC (38.50458, -119.62176)                 255.42\nD (37.862028, -119.6576925)              257.82\nE (37.89748, -119.2624335)               241.45\nF (37.8762105, -119.3432825)          245.81999\nG (37.833654, -119.4510805)           254.76999\nH (38.0603395, -119.6666755)             253.28\nI (37.798171, -119.19955150              243.92\nJ (38.0391175, -119.3073495)             243.51\nName: 9, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/11/2017 6:00\nA (38.152231, -119.6666755)              252.04\nB (38.279274, -119.6127765)              253.58\nC (38.50458, -119.62176)              257.69998\nD (37.862028, -119.6576925)              258.77\nE (37.89748, -119.2624335)            244.37999\nF (37.8762105, -119.3432825)             248.17\nG (37.833654, -119.4510805)           252.37999\nH (38.0603395, -119.6666755)             253.06\nI (37.798171, -119.19955150              245.86\nJ (38.0391175, -119.3073495)             247.12\nName: 10, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/12/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 11, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/13/2017 6:00\nA (38.152231, -119.6666755)              251.18\nB (38.279274, -119.6127765)           251.81999\nC (38.50458, -119.62176)              254.79999\nD (37.862028, -119.6576925)           257.22998\nE (37.89748, -119.2624335)               241.64\nF (37.8762105, -119.3432825)          244.65999\nG (37.833654, -119.4510805)           252.01999\nH (38.0603395, -119.6666755)          251.84999\nI (37.798171, -119.19955150           244.51999\nJ (38.0391175, -119.3073495)             242.09\nName: 12, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/14/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 13, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/15/2017 6:00\nA (38.152231, -119.6666755)              255.17\nB (38.279274, -119.6127765)           256.94998\nC (38.50458, -119.62176)                 258.22\nD (37.862028, -119.6576925)              259.59\nE (37.89748, -119.2624335)               245.95\nF (37.8762105, -119.3432825)          250.01999\nG (37.833654, -119.4510805)           255.37999\nH (38.0603395, -119.6666755)             255.48\nI (37.798171, -119.19955150              248.29\nJ (38.0391175, -119.3073495)          244.98999\nName: 14, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/16/2017 6:00\nA (38.152231, -119.6666755)           255.45999\nB (38.279274, -119.6127765)              256.08\nC (38.50458, -119.62176)                 257.69\nD (37.862028, -119.6576925)              260.29\nE (37.89748, -119.2624335)               246.09\nF (37.8762105, -119.3432825)              249.4\nG (37.833654, -119.4510805)              254.62\nH (38.0603395, -119.6666755)             256.03\nI (37.798171, -119.19955150              248.39\nJ (38.0391175, -119.3073495)          246.37999\nName: 15, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/17/2017 6:00\nA (38.152231, -119.6666755)           257.16998\nB (38.279274, -119.6127765)           257.63998\nC (38.50458, -119.62176)              261.00998\nD (37.862028, -119.6576925)           262.44998\nE (37.89748, -119.2624335)               249.87\nF (37.8762105, -119.3432825)          252.06999\nG (37.833654, -119.4510805)              254.84\nH (38.0603395, -119.6666755)             257.38\nI (37.798171, -119.19955150              252.18\nJ (38.0391175, -119.3073495)          250.01999\nName: 16, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/18/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 17, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/19/2017 6:00\nA (38.152231, -119.6666755)              252.36\nB (38.279274, -119.6127765)           253.29999\nC (38.50458, -119.62176)                 257.31\nD (37.862028, -119.6576925)              258.85\nE (37.89748, -119.2624335)            244.59999\nF (37.8762105, -119.3432825)             247.59\nG (37.833654, -119.4510805)           254.54999\nH (38.0603395, -119.6666755)             253.62\nI (37.798171, -119.19955150              246.47\nJ (38.0391175, -119.3073495)          246.01999\nName: 18, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/20/2017 6:00\nA (38.152231, -119.6666755)               250.5\nB (38.279274, -119.6127765)              252.06\nC (38.50458, -119.62176)                  254.4\nD (37.862028, -119.6576925)              256.29\nE (37.89748, -119.2624335)               247.29\nF (37.8762105, -119.3432825)             250.47\nG (37.833654, -119.4510805)              254.95\nH (38.0603395, -119.6666755)             251.06\nI (37.798171, -119.19955150               250.5\nJ (38.0391175, -119.3073495)          248.15999\nName: 19, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 20, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/22/2017 6:00\nA (38.152231, -119.6666755)           255.43999\nB (38.279274, -119.6127765)              254.42\nC (38.50458, -119.62176)                 259.25\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)             255.12\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 21, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/23/2017 6:00\nA (38.152231, -119.6666755)           257.66998\nB (38.279274, -119.6127765)              258.59\nC (38.50458, -119.62176)                 260.68\nD (37.862028, -119.6576925)              262.99\nE (37.89748, -119.2624335)               249.03\nF (37.8762105, -119.3432825)             252.68\nG (37.833654, -119.4510805)              258.36\nH (38.0603395, -119.6666755)          257.63998\nI (37.798171, -119.19955150              251.12\nJ (38.0391175, -119.3073495)              249.0\nName: 22, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/24/2017 6:00\nA (38.152231, -119.6666755)              256.99\nB (38.279274, -119.6127765)              257.54\nC (38.50458, -119.62176)                 258.43\nD (37.862028, -119.6576925)              263.55\nE (37.89748, -119.2624335)            247.76999\nF (37.8762105, -119.3432825)             251.18\nG (37.833654, -119.4510805)               257.1\nH (38.0603395, -119.6666755)             257.79\nI (37.798171, -119.19955150              251.25\nJ (38.0391175, -119.3073495)             248.36\nName: 23, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/25/2017 6:00\nA (38.152231, -119.6666755)              255.22\nB (38.279274, -119.6127765)              255.72\nC (38.50458, -119.62176)              259.00998\nD (37.862028, -119.6576925)              262.69\nE (37.89748, -119.2624335)               247.92\nF (37.8762105, -119.3432825)             251.83\nG (37.833654, -119.4510805)              256.34\nH (38.0603395, -119.6666755)          256.41998\nI (37.798171, -119.19955150           248.01999\nJ (38.0391175, -119.3073495)          250.65999\nName: 24, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/26/2017 6:00\nA (38.152231, -119.6666755)              255.36\nB (38.279274, -119.6127765)              254.75\nC (38.50458, -119.62176)              258.91998\nD (37.862028, -119.6576925)              262.52\nE (37.89748, -119.2624335)            247.87999\nF (37.8762105, -119.3432825)             251.34\nG (37.833654, -119.4510805)              254.89\nH (38.0603395, -119.6666755)          256.22998\nI (37.798171, -119.19955150           247.95999\nJ (38.0391175, -119.3073495)          249.79999\nName: 25, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/27/2017 6:00\nA (38.152231, -119.6666755)           254.68999\nB (38.279274, -119.6127765)              255.09\nC (38.50458, -119.62176)                 258.69\nD (37.862028, -119.6576925)              263.72\nE (37.89748, -119.2624335)               245.62\nF (37.8762105, -119.3432825)             250.15\nG (37.833654, -119.4510805)              257.02\nH (38.0603395, -119.6666755)             256.41\nI (37.798171, -119.19955150              248.17\nJ (38.0391175, -119.3073495)          248.31999\nName: 26, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/28/2017 6:00\nA (38.152231, -119.6666755)              256.13\nB (38.279274, -119.6127765)              256.35\nC (38.50458, -119.62176)                 259.44\nD (37.862028, -119.6576925)              263.86\nE (37.89748, -119.2624335)            247.18999\nF (37.8762105, -119.3432825)             251.08\nG (37.833654, -119.4510805)              258.74\nH (38.0603395, -119.6666755)             256.93\nI (37.798171, -119.19955150           249.90999\nJ (38.0391175, -119.3073495)             247.28\nName: 27, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/29/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 28, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/30/2017 6:00\nA (38.152231, -119.6666755)              256.11\nB (38.279274, -119.6127765)              257.93\nC (38.50458, -119.62176)              261.38998\nD (37.862028, -119.6576925)              259.72\nE (37.89748, -119.2624335)            247.43999\nF (37.8762105, -119.3432825)             251.28\nG (37.833654, -119.4510805)              255.79\nH (38.0603395, -119.6666755)          255.81999\nI (37.798171, -119.19955150              249.97\nJ (38.0391175, -119.3073495)             246.33\nName: 29, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            10/31/2017 6:00\nA (38.152231, -119.6666755)              252.62\nB (38.279274, -119.6127765)           253.31999\nC (38.50458, -119.62176)                 255.43\nD (37.862028, -119.6576925)              256.88\nE (37.89748, -119.2624335)            245.12999\nF (37.8762105, -119.3432825)             247.95\nG (37.833654, -119.4510805)               252.7\nH (38.0603395, -119.6666755)             252.93\nI (37.798171, -119.19955150           246.70999\nJ (38.0391175, -119.3073495)             246.01\nName: 30, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/1/2017 6:00\nA (38.152231, -119.6666755)          252.76999\nB (38.279274, -119.6127765)             253.83\nC (38.50458, -119.62176)                256.04\nD (37.862028, -119.6576925)          257.88998\nE (37.89748, -119.2624335)              242.54\nF (37.8762105, -119.3432825)            244.75\nG (37.833654, -119.4510805)             252.54\nH (38.0603395, -119.6666755)            253.72\nI (37.798171, -119.19955150          247.48999\nJ (38.0391175, -119.3073495)         242.26999\nName: 31, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/2/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 32, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/3/2017 6:00\nA (38.152231, -119.6666755)             251.12\nB (38.279274, -119.6127765)             251.48\nC (38.50458, -119.62176)             255.37999\nD (37.862028, -119.6576925)             257.25\nE (37.89748, -119.2624335)              243.33\nF (37.8762105, -119.3432825)            247.25\nG (37.833654, -119.4510805)             252.22\nH (38.0603395, -119.6666755)         252.04999\nI (37.798171, -119.19955150             245.11\nJ (38.0391175, -119.3073495)         246.51999\nName: 33, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/4/2017 6:00\nA (38.152231, -119.6666755)          252.26999\nB (38.279274, -119.6127765)             253.08\nC (38.50458, -119.62176)             256.19998\nD (37.862028, -119.6576925)             257.35\nE (37.89748, -119.2624335)              243.34\nF (37.8762105, -119.3432825)         246.37999\nG (37.833654, -119.4510805)             252.48\nH (38.0603395, -119.6666755)            252.83\nI (37.798171, -119.19955150          246.84999\nJ (38.0391175, -119.3073495)         245.20999\nName: 34, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/5/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              237.04\nF (37.8762105, -119.3432825)            238.01\nG (37.833654, -119.4510805)             240.61\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             245.48\nJ (38.0391175, -119.3073495)            233.28\nName: 35, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 36, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/7/2017 6:00\nA (38.152231, -119.6666755)             247.76\nB (38.279274, -119.6127765)             248.95\nC (38.50458, -119.62176)                 250.9\nD (37.862028, -119.6576925)             252.73\nE (37.89748, -119.2624335)              240.93\nF (37.8762105, -119.3432825)             244.2\nG (37.833654, -119.4510805)             248.58\nH (38.0603395, -119.6666755)            247.93\nI (37.798171, -119.19955150          243.37999\nJ (38.0391175, -119.3073495)            239.97\nName: 37, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/8/2017 6:00\nA (38.152231, -119.6666755)             250.84\nB (38.279274, -119.6127765)          251.68999\nC (38.50458, -119.62176)                254.92\nD (37.862028, -119.6576925)          255.98999\nE (37.89748, -119.2624335)              242.11\nF (37.8762105, -119.3432825)         245.79999\nG (37.833654, -119.4510805)             252.47\nH (38.0603395, -119.6666755)            250.92\nI (37.798171, -119.19955150              244.7\nJ (38.0391175, -119.3073495)            242.86\nName: 38, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/9/2017 6:00\nA (38.152231, -119.6666755)          253.34999\nB (38.279274, -119.6127765)             254.17\nC (38.50458, -119.62176)             257.19998\nD (37.862028, -119.6576925)             258.83\nE (37.89748, -119.2624335)           247.93999\nF (37.8762105, -119.3432825)            248.22\nG (37.833654, -119.4510805)             251.83\nH (38.0603395, -119.6666755)             253.9\nI (37.798171, -119.19955150             249.59\nJ (38.0391175, -119.3073495)             247.7\nName: 39, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/10/2017 6:00\nA (38.152231, -119.6666755)              250.61\nB (38.279274, -119.6127765)              251.12\nC (38.50458, -119.62176)                 254.92\nD (37.862028, -119.6576925)              256.31\nE (37.89748, -119.2624335)               242.43\nF (37.8762105, -119.3432825)              242.9\nG (37.833654, -119.4510805)           250.15999\nH (38.0603395, -119.6666755)             251.79\nI (37.798171, -119.19955150              246.26\nJ (38.0391175, -119.3073495)             243.03\nName: 40, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/11/2017 6:00\nA (38.152231, -119.6666755)               246.5\nB (38.279274, -119.6127765)              246.33\nC (38.50458, -119.62176)              250.06999\nD (37.862028, -119.6576925)           255.31999\nE (37.89748, -119.2624335)            238.62999\nF (37.8762105, -119.3432825)          242.12999\nG (37.833654, -119.4510805)           249.81999\nH (38.0603395, -119.6666755)          247.59999\nI (37.798171, -119.19955150               241.7\nJ (38.0391175, -119.3073495)             240.06\nName: 41, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/12/2017 6:00\nA (38.152231, -119.6666755)              250.92\nB (38.279274, -119.6127765)              252.59\nC (38.50458, -119.62176)              255.45999\nD (37.862028, -119.6576925)              258.66\nE (37.89748, -119.2624335)               244.75\nF (37.8762105, -119.3432825)             247.93\nG (37.833654, -119.4510805)              254.73\nH (38.0603395, -119.6666755)             251.79\nI (37.798171, -119.19955150              247.64\nJ (38.0391175, -119.3073495)             244.81\nName: 42, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/13/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 43, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/14/2017 6:00\nA (38.152231, -119.6666755)              250.61\nB (38.279274, -119.6127765)           253.26999\nC (38.50458, -119.62176)                  256.1\nD (37.862028, -119.6576925)           253.62999\nE (37.89748, -119.2624335)               245.37\nF (37.8762105, -119.3432825)          247.59999\nG (37.833654, -119.4510805)           251.12999\nH (38.0603395, -119.6666755)             250.08\nI (37.798171, -119.19955150              246.73\nJ (38.0391175, -119.3073495)             244.75\nName: 44, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/15/2017 6:00\nA (38.152231, -119.6666755)           251.73999\nB (38.279274, -119.6127765)           253.40999\nC (38.50458, -119.62176)              256.00998\nD (37.862028, -119.6576925)              257.55\nE (37.89748, -119.2624335)            247.68999\nF (37.8762105, -119.3432825)             249.64\nG (37.833654, -119.4510805)              253.65\nH (38.0603395, -119.6666755)          252.73999\nI (37.798171, -119.19955150           249.73999\nJ (38.0391175, -119.3073495)             247.09\nName: 45, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/16/2017 6:00\nA (38.152231, -119.6666755)              250.28\nB (38.279274, -119.6127765)              250.43\nC (38.50458, -119.62176)                 253.34\nD (37.862028, -119.6576925)           255.84999\nE (37.89748, -119.2624335)               247.28\nF (37.8762105, -119.3432825)          249.65999\nG (37.833654, -119.4510805)              253.95\nH (38.0603395, -119.6666755)             251.54\nI (37.798171, -119.19955150              250.08\nJ (38.0391175, -119.3073495)             247.51\nName: 46, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/17/2017 6:00\nA (38.152231, -119.6666755)              246.45\nB (38.279274, -119.6127765)           246.51999\nC (38.50458, -119.62176)              250.09999\nD (37.862028, -119.6576925)              254.43\nE (37.89748, -119.2624335)               243.64\nF (37.8762105, -119.3432825)              247.9\nG (37.833654, -119.4510805)              249.95\nH (38.0603395, -119.6666755)             248.22\nI (37.798171, -119.19955150           245.43999\nJ (38.0391175, -119.3073495)          244.15999\nName: 47, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/18/2017 6:00\nA (38.152231, -119.6666755)               240.2\nB (38.279274, -119.6127765)           238.37999\nC (38.50458, -119.62176)                 245.33\nD (37.862028, -119.6576925)              252.06\nE (37.89748, -119.2624335)            236.51999\nF (37.8762105, -119.3432825)             240.48\nG (37.833654, -119.4510805)              244.54\nH (38.0603395, -119.6666755)             241.78\nI (37.798171, -119.19955150           238.76999\nJ (38.0391175, -119.3073495)             237.09\nName: 48, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/19/2017 6:00\nA (38.152231, -119.6666755)              239.37\nB (38.279274, -119.6127765)              239.86\nC (38.50458, -119.62176)                 247.62\nD (37.862028, -119.6576925)              252.37\nE (37.89748, -119.2624335)            237.01999\nF (37.8762105, -119.3432825)             239.89\nG (37.833654, -119.4510805)              245.39\nH (38.0603395, -119.6666755)             242.42\nI (37.798171, -119.19955150              240.23\nJ (38.0391175, -119.3073495)             235.48\nName: 49, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/20/2017 6:00\nA (38.152231, -119.6666755)              245.98\nB (38.279274, -119.6127765)           247.18999\nC (38.50458, -119.62176)                 253.28\nD (37.862028, -119.6576925)               258.8\nE (37.89748, -119.2624335)                244.9\nF (37.8762105, -119.3432825)          248.59999\nG (37.833654, -119.4510805)           254.18999\nH (38.0603395, -119.6666755)             248.26\nI (37.798171, -119.19955150              248.01\nJ (38.0391175, -119.3073495)             242.86\nName: 50, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 51, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/22/2017 6:00\nA (38.152231, -119.6666755)              244.84\nB (38.279274, -119.6127765)              246.54\nC (38.50458, -119.62176)                 255.64\nD (37.862028, -119.6576925)              255.28\nE (37.89748, -119.2624335)               237.67\nF (37.8762105, -119.3432825)             239.58\nG (37.833654, -119.4510805)              244.31\nH (38.0603395, -119.6666755)             246.15\nI (37.798171, -119.19955150           241.59999\nJ (38.0391175, -119.3073495)          237.56999\nName: 52, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/23/2017 6:00\nA (38.152231, -119.6666755)           242.45999\nB (38.279274, -119.6127765)               243.9\nC (38.50458, -119.62176)              254.59999\nD (37.862028, -119.6576925)              254.93\nE (37.89748, -119.2624335)            234.18999\nF (37.8762105, -119.3432825)             237.89\nG (37.833654, -119.4510805)              244.11\nH (38.0603395, -119.6666755)          244.62999\nI (37.798171, -119.19955150           238.29999\nJ (38.0391175, -119.3073495)             236.76\nName: 53, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/24/2017 6:00\nA (38.152231, -119.6666755)           242.51999\nB (38.279274, -119.6127765)              243.61\nC (38.50458, -119.62176)                 251.39\nD (37.862028, -119.6576925)           254.15999\nE (37.89748, -119.2624335)            233.37999\nF (37.8762105, -119.3432825)             235.84\nG (37.833654, -119.4510805)              244.61\nH (38.0603395, -119.6666755)          244.48999\nI (37.798171, -119.19955150              238.12\nJ (38.0391175, -119.3073495)             234.15\nName: 54, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/25/2017 6:00\nA (38.152231, -119.6666755)              243.26\nB (38.279274, -119.6127765)              242.62\nC (38.50458, -119.62176)                 250.72\nD (37.862028, -119.6576925)              257.53\nE (37.89748, -119.2624335)            233.23999\nF (37.8762105, -119.3432825)              238.2\nG (37.833654, -119.4510805)           249.34999\nH (38.0603395, -119.6666755)             246.98\nI (37.798171, -119.19955150              237.34\nJ (38.0391175, -119.3073495)             234.65\nName: 55, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/26/2017 6:00\nA (38.152231, -119.6666755)              244.26\nB (38.279274, -119.6127765)              244.31\nC (38.50458, -119.62176)                 254.39\nD (37.862028, -119.6576925)           258.91998\nE (37.89748, -119.2624335)               236.93\nF (37.8762105, -119.3432825)              238.0\nG (37.833654, -119.4510805)              247.14\nH (38.0603395, -119.6666755)          247.06999\nI (37.798171, -119.19955150              241.98\nJ (38.0391175, -119.3073495)             238.17\nName: 56, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/27/2017 6:00\nA (38.152231, -119.6666755)           244.31999\nB (38.279274, -119.6127765)           244.51999\nC (38.50458, -119.62176)                 251.09\nD (37.862028, -119.6576925)              252.11\nE (37.89748, -119.2624335)            238.26999\nF (37.8762105, -119.3432825)          241.15999\nG (37.833654, -119.4510805)           246.65999\nH (38.0603395, -119.6666755)          245.70999\nI (37.798171, -119.19955150              241.47\nJ (38.0391175, -119.3073495)             238.61\nName: 57, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/28/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 58, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/29/2017 6:00\nA (38.152231, -119.6666755)              236.54\nB (38.279274, -119.6127765)              238.01\nC (38.50458, -119.62176)                 246.93\nD (37.862028, -119.6576925)           246.56999\nE (37.89748, -119.2624335)               229.22\nF (37.8762105, -119.3432825)          232.59999\nG (37.833654, -119.4510805)           239.90999\nH (38.0603395, -119.6666755)          237.37999\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)             231.08\nName: 59, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            11/30/2017 6:00\nA (38.152231, -119.6666755)              235.97\nB (38.279274, -119.6127765)              237.15\nC (38.50458, -119.62176)              245.81999\nD (37.862028, -119.6576925)              247.42\nE (37.89748, -119.2624335)            232.06999\nF (37.8762105, -119.3432825)          233.15999\nG (37.833654, -119.4510805)              239.03\nH (38.0603395, -119.6666755)          237.84999\nI (37.798171, -119.19955150              235.79\nJ (38.0391175, -119.3073495)          231.84999\nName: 60, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/1/2017 6:00\nA (38.152231, -119.6666755)             237.48\nB (38.279274, -119.6127765)          234.90999\nC (38.50458, -119.62176)                245.12\nD (37.862028, -119.6576925)          250.20999\nE (37.89748, -119.2624335)              233.17\nF (37.8762105, -119.3432825)            236.31\nG (37.833654, -119.4510805)          241.76999\nH (38.0603395, -119.6666755)            238.89\nI (37.798171, -119.19955150          234.29999\nJ (38.0391175, -119.3073495)         234.54999\nName: 61, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/2/2017 6:00\nA (38.152231, -119.6666755)             238.76\nB (38.279274, -119.6127765)          237.48999\nC (38.50458, -119.62176)             247.90999\nD (37.862028, -119.6576925)          252.51999\nE (37.89748, -119.2624335)           233.76999\nF (37.8762105, -119.3432825)         234.68999\nG (37.833654, -119.4510805)          239.70999\nH (38.0603395, -119.6666755)         240.79999\nI (37.798171, -119.19955150          238.18999\nJ (38.0391175, -119.3073495)            234.33\nName: 62, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/3/2017 6:00\nA (38.152231, -119.6666755)             241.45\nB (38.279274, -119.6127765)          239.65999\nC (38.50458, -119.62176)             246.93999\nD (37.862028, -119.6576925)          252.40999\nE (37.89748, -119.2624335)           232.81999\nF (37.8762105, -119.3432825)         233.93999\nG (37.833654, -119.4510805)             243.89\nH (38.0603395, -119.6666755)         242.87999\nI (37.798171, -119.19955150          238.37999\nJ (38.0391175, -119.3073495)            234.45\nName: 63, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/4/2017 6:00\nA (38.152231, -119.6666755)             231.39\nB (38.279274, -119.6127765)          230.09999\nC (38.50458, -119.62176)                 239.4\nD (37.862028, -119.6576925)             246.08\nE (37.89748, -119.2624335)              226.76\nF (37.8762105, -119.3432825)         229.40999\nG (37.833654, -119.4510805)          236.37999\nH (38.0603395, -119.6666755)            233.78\nI (37.798171, -119.19955150             227.89\nJ (38.0391175, -119.3073495)            226.75\nName: 64, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/5/2017 6:00\nA (38.152231, -119.6666755)          236.18999\nB (38.279274, -119.6127765)             234.72\nC (38.50458, -119.62176)                241.17\nD (37.862028, -119.6576925)          251.48999\nE (37.89748, -119.2624335)           229.84999\nF (37.8762105, -119.3432825)            233.87\nG (37.833654, -119.4510805)          242.59999\nH (38.0603395, -119.6666755)         238.26999\nI (37.798171, -119.19955150             231.08\nJ (38.0391175, -119.3073495)            227.93\nName: 65, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/6/2017 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 66, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/7/2017 6:00\nA (38.152231, -119.6666755)          235.18999\nB (38.279274, -119.6127765)          236.59999\nC (38.50458, -119.62176)                 244.7\nD (37.862028, -119.6576925)             248.34\nE (37.89748, -119.2624335)              231.23\nF (37.8762105, -119.3432825)            234.18\nG (37.833654, -119.4510805)          239.40999\nH (38.0603395, -119.6666755)            236.51\nI (37.798171, -119.19955150             234.36\nJ (38.0391175, -119.3073495)            228.76\nName: 67, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/8/2017 6:00\nA (38.152231, -119.6666755)             236.06\nB (38.279274, -119.6127765)             236.08\nC (38.50458, -119.62176)             244.43999\nD (37.862028, -119.6576925)             250.37\nE (37.89748, -119.2624335)           231.34999\nF (37.8762105, -119.3432825)         233.56999\nG (37.833654, -119.4510805)             240.86\nH (38.0603395, -119.6666755)            238.65\nI (37.798171, -119.19955150             234.59\nJ (38.0391175, -119.3073495)         230.87999\nName: 68, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/9/2017 6:00\nA (38.152231, -119.6666755)             236.18\nB (38.279274, -119.6127765)             233.83\nC (38.50458, -119.62176)                241.39\nD (37.862028, -119.6576925)          250.01999\nE (37.89748, -119.2624335)           229.26999\nF (37.8762105, -119.3432825)            234.53\nG (37.833654, -119.4510805)          241.15999\nH (38.0603395, -119.6666755)         238.59999\nI (37.798171, -119.19955150             231.98\nJ (38.0391175, -119.3073495)            230.83\nName: 69, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/10/2017 6:00\nA (38.152231, -119.6666755)           236.37999\nB (38.279274, -119.6127765)           234.18999\nC (38.50458, -119.62176)              241.70999\nD (37.862028, -119.6576925)           253.20999\nE (37.89748, -119.2624335)               231.69\nF (37.8762105, -119.3432825)             236.18\nG (37.833654, -119.4510805)           241.62999\nH (38.0603395, -119.6666755)             239.34\nI (37.798171, -119.19955150              233.12\nJ (38.0391175, -119.3073495)          231.65999\nName: 70, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/11/2017 6:00\nA (38.152231, -119.6666755)           239.68999\nB (38.279274, -119.6127765)              236.76\nC (38.50458, -119.62176)                 244.56\nD (37.862028, -119.6576925)              253.65\nE (37.89748, -119.2624335)               231.12\nF (37.8762105, -119.3432825)          235.23999\nG (37.833654, -119.4510805)           243.59999\nH (38.0603395, -119.6666755)          241.93999\nI (37.798171, -119.19955150              236.26\nJ (38.0391175, -119.3073495)          232.79999\nName: 71, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/12/2017 6:00\nA (38.152231, -119.6666755)              238.65\nB (38.279274, -119.6127765)               238.5\nC (38.50458, -119.62176)                 247.04\nD (37.862028, -119.6576925)              255.15\nE (37.89748, -119.2624335)               233.26\nF (37.8762105, -119.3432825)             236.97\nG (37.833654, -119.4510805)           245.18999\nH (38.0603395, -119.6666755)          241.87999\nI (37.798171, -119.19955150           235.93999\nJ (38.0391175, -119.3073495)             233.28\nName: 72, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/13/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 73, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/14/2017 6:00\nA (38.152231, -119.6666755)              247.68\nB (38.279274, -119.6127765)           241.56999\nC (38.50458, -119.62176)              248.95999\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 74, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/15/2017 6:00\nA (38.152231, -119.6666755)           238.90999\nB (38.279274, -119.6127765)              240.84\nC (38.50458, -119.62176)                 249.36\nD (37.862028, -119.6576925)              252.59\nE (37.89748, -119.2624335)               235.58\nF (37.8762105, -119.3432825)          238.43999\nG (37.833654, -119.4510805)              243.92\nH (38.0603395, -119.6666755)             241.22\nI (37.798171, -119.19955150           239.43999\nJ (38.0391175, -119.3073495)          234.90999\nName: 75, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/16/2017 6:00\nA (38.152231, -119.6666755)              239.14\nB (38.279274, -119.6127765)              238.95\nC (38.50458, -119.62176)              248.51999\nD (37.862028, -119.6576925)           250.23999\nE (37.89748, -119.2624335)               238.84\nF (37.8762105, -119.3432825)             239.56\nG (37.833654, -119.4510805)              241.98\nH (38.0603395, -119.6666755)          241.40999\nI (37.798171, -119.19955150              240.45\nJ (38.0391175, -119.3073495)             239.31\nName: 76, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/17/2017 6:00\nA (38.152231, -119.6666755)           238.56999\nB (38.279274, -119.6127765)              237.15\nC (38.50458, -119.62176)              241.87999\nD (37.862028, -119.6576925)              250.59\nE (37.89748, -119.2624335)            229.54999\nF (37.8762105, -119.3432825)             232.39\nG (37.833654, -119.4510805)           242.40999\nH (38.0603395, -119.6666755)             241.01\nI (37.798171, -119.19955150              233.39\nJ (38.0391175, -119.3073495)             228.44\nName: 77, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/18/2017 6:00\nA (38.152231, -119.6666755)              240.45\nB (38.279274, -119.6127765)              238.34\nC (38.50458, -119.62176)                 244.81\nD (37.862028, -119.6576925)              253.04\nE (37.89748, -119.2624335)            231.37999\nF (37.8762105, -119.3432825)          236.59999\nG (37.833654, -119.4510805)               246.5\nH (38.0603395, -119.6666755)          242.56999\nI (37.798171, -119.19955150           234.76999\nJ (38.0391175, -119.3073495)          233.65999\nName: 78, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/19/2017 6:00\nA (38.152231, -119.6666755)              238.34\nB (38.279274, -119.6127765)              239.31\nC (38.50458, -119.62176)                 248.06\nD (37.862028, -119.6576925)              251.93\nE (37.89748, -119.2624335)               234.04\nF (37.8762105, -119.3432825)             236.58\nG (37.833654, -119.4510805)           243.76999\nH (38.0603395, -119.6666755)             241.93\nI (37.798171, -119.19955150              236.81\nJ (38.0391175, -119.3073495)             234.39\nName: 79, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/20/2017 6:00\nA (38.152231, -119.6666755)              241.03\nB (38.279274, -119.6127765)           240.70999\nC (38.50458, -119.62176)              249.09999\nD (37.862028, -119.6576925)              254.59\nE (37.89748, -119.2624335)               237.83\nF (37.8762105, -119.3432825)             240.75\nG (37.833654, -119.4510805)              247.23\nH (38.0603395, -119.6666755)          243.29999\nI (37.798171, -119.19955150              240.37\nJ (38.0391175, -119.3073495)          237.09999\nName: 80, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/21/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 81, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/22/2017 6:00\nA (38.152231, -119.6666755)           237.79999\nB (38.279274, -119.6127765)              239.42\nC (38.50458, -119.62176)              248.01999\nD (37.862028, -119.6576925)           247.68999\nE (37.89748, -119.2624335)               232.62\nF (37.8762105, -119.3432825)             235.31\nG (37.833654, -119.4510805)           240.09999\nH (38.0603395, -119.6666755)             239.87\nI (37.798171, -119.19955150              236.33\nJ (38.0391175, -119.3073495)          231.06999\nName: 82, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/23/2017 6:00\nA (38.152231, -119.6666755)              240.22\nB (38.279274, -119.6127765)           242.06999\nC (38.50458, -119.62176)                 250.67\nD (37.862028, -119.6576925)              251.15\nE (37.89748, -119.2624335)                236.4\nF (37.8762105, -119.3432825)             237.93\nG (37.833654, -119.4510805)              243.29\nH (38.0603395, -119.6666755)             241.29\nI (37.798171, -119.19955150              238.95\nJ (38.0391175, -119.3073495)          237.29999\nName: 83, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/24/2017 6:00\nA (38.152231, -119.6666755)              241.61\nB (38.279274, -119.6127765)           241.34999\nC (38.50458, -119.62176)              249.98999\nD (37.862028, -119.6576925)              254.78\nE (37.89748, -119.2624335)               239.25\nF (37.8762105, -119.3432825)          242.20999\nG (37.833654, -119.4510805)              245.76\nH (38.0603395, -119.6666755)          243.98999\nI (37.798171, -119.19955150              240.37\nJ (38.0391175, -119.3073495)          239.40999\nName: 84, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/25/2017 6:00\nA (38.152231, -119.6666755)           244.09999\nB (38.279274, -119.6127765)              242.81\nC (38.50458, -119.62176)                  249.7\nD (37.862028, -119.6576925)           255.79999\nE (37.89748, -119.2624335)               239.15\nF (37.8762105, -119.3432825)             240.26\nG (37.833654, -119.4510805)              244.58\nH (38.0603395, -119.6666755)          246.56999\nI (37.798171, -119.19955150              242.08\nJ (38.0391175, -119.3073495)             240.22\nName: 85, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/26/2017 6:00\nA (38.152231, -119.6666755)              240.29\nB (38.279274, -119.6127765)              238.42\nC (38.50458, -119.62176)                 246.92\nD (37.862028, -119.6576925)              253.67\nE (37.89748, -119.2624335)               233.79\nF (37.8762105, -119.3432825)             237.84\nG (37.833654, -119.4510805)              246.54\nH (38.0603395, -119.6666755)          243.06999\nI (37.798171, -119.19955150           236.93999\nJ (38.0391175, -119.3073495)          235.12999\nName: 86, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/27/2017 6:00\nA (38.152231, -119.6666755)           239.87999\nB (38.279274, -119.6127765)              240.62\nC (38.50458, -119.62176)                 249.68\nD (37.862028, -119.6576925)              253.92\nE (37.89748, -119.2624335)            234.20999\nF (37.8762105, -119.3432825)             238.47\nG (37.833654, -119.4510805)           245.31999\nH (38.0603395, -119.6666755)          242.31999\nI (37.798171, -119.19955150           237.45999\nJ (38.0391175, -119.3073495)          234.98999\nName: 87, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/28/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)               232.04\nF (37.8762105, -119.3432825)             234.56\nG (37.833654, -119.4510805)           238.93999\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150               238.9\nJ (38.0391175, -119.3073495)          229.79999\nName: 88, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/29/2017 6:00\nA (38.152231, -119.6666755)                 NaN\nB (38.279274, -119.6127765)                 NaN\nC (38.50458, -119.62176)                    NaN\nD (37.862028, -119.6576925)                 NaN\nE (37.89748, -119.2624335)                  NaN\nF (37.8762105, -119.3432825)                NaN\nG (37.833654, -119.4510805)                 NaN\nH (38.0603395, -119.6666755)                NaN\nI (37.798171, -119.19955150                 NaN\nJ (38.0391175, -119.3073495)                NaN\nName: 89, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/30/2017 6:00\nA (38.152231, -119.6666755)              240.98\nB (38.279274, -119.6127765)              242.12\nC (38.50458, -119.62176)                 251.34\nD (37.862028, -119.6576925)           252.70999\nE (37.89748, -119.2624335)               237.45\nF (37.8762105, -119.3432825)             239.18\nG (37.833654, -119.4510805)              243.86\nH (38.0603395, -119.6666755)             241.92\nI (37.798171, -119.19955150              240.47\nJ (38.0391175, -119.3073495)          236.48999\nName: 90, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            12/31/2017 6:00\nA (38.152231, -119.6666755)              240.37\nB (38.279274, -119.6127765)              241.25\nC (38.50458, -119.62176)                 248.87\nD (37.862028, -119.6576925)              253.73\nE (37.89748, -119.2624335)            237.15999\nF (37.8762105, -119.3432825)          239.20999\nG (37.833654, -119.4510805)           242.56999\nH (38.0603395, -119.6666755)             243.17\nI (37.798171, -119.19955150              238.65\nJ (38.0391175, -119.3073495)          237.06999\nName: 91, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/1/2018 6:00\nA (38.152231, -119.6666755)         241.26999\nB (38.279274, -119.6127765)            242.09\nC (38.50458, -119.62176)            248.93999\nD (37.862028, -119.6576925)         253.48999\nE (37.89748, -119.2624335)             236.67\nF (37.8762105, -119.3432825)        239.70999\nG (37.833654, -119.4510805)         247.09999\nH (38.0603395, -119.6666755)        244.20999\nI (37.798171, -119.19955150         237.48999\nJ (38.0391175, -119.3073495)        238.23999\nName: 92, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/2/2018 6:00\nA (38.152231, -119.6666755)            241.31\nB (38.279274, -119.6127765)            239.89\nC (38.50458, -119.62176)               249.37\nD (37.862028, -119.6576925)            257.44\nE (37.89748, -119.2624335)              237.5\nF (37.8762105, -119.3432825)           240.73\nG (37.833654, -119.4510805)             243.2\nH (38.0603395, -119.6666755)        242.90999\nI (37.798171, -119.19955150            239.93\nJ (38.0391175, -119.3073495)        239.04999\nName: 93, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/3/2018 6:00\nA (38.152231, -119.6666755)            242.48\nB (38.279274, -119.6127765)            242.58\nC (38.50458, -119.62176)                251.0\nD (37.862028, -119.6576925)            256.81\nE (37.89748, -119.2624335)          236.54999\nF (37.8762105, -119.3432825)           238.98\nG (37.833654, -119.4510805)         247.15999\nH (38.0603395, -119.6666755)        245.65999\nI (37.798171, -119.19955150            238.78\nJ (38.0391175, -119.3073495)           237.25\nName: 94, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/4/2018 6:00\nA (38.152231, -119.6666755)            247.78\nB (38.279274, -119.6127765)         248.68999\nC (38.50458, -119.62176)               255.93\nD (37.862028, -119.6576925)            258.78\nE (37.89748, -119.2624335)          241.54999\nF (37.8762105, -119.3432825)           244.53\nG (37.833654, -119.4510805)            252.26\nH (38.0603395, -119.6666755)        249.68999\nI (37.798171, -119.19955150         244.65999\nJ (38.0391175, -119.3073495)        242.18999\nName: 95, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 96, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/6/2018 6:00\nA (38.152231, -119.6666755)            247.47\nB (38.279274, -119.6127765)            250.92\nC (38.50458, -119.62176)               256.43\nD (37.862028, -119.6576925)            250.73\nE (37.89748, -119.2624335)          239.68999\nF (37.8762105, -119.3432825)           241.11\nG (37.833654, -119.4510805)         244.73999\nH (38.0603395, -119.6666755)           247.39\nI (37.798171, -119.19955150            240.73\nJ (38.0391175, -119.3073495)        241.18999\nName: 97, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/7/2018 6:00\nA (38.152231, -119.6666755)            239.73\nB (38.279274, -119.6127765)         240.87999\nC (38.50458, -119.62176)            247.40999\nD (37.862028, -119.6576925)            248.98\nE (37.89748, -119.2624335)             234.15\nF (37.8762105, -119.3432825)           236.15\nG (37.833654, -119.4510805)         240.37999\nH (38.0603395, -119.6666755)           240.18\nI (37.798171, -119.19955150            236.97\nJ (38.0391175, -119.3073495)        234.09999\nName: 98, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/8/2018 6:00\nA (38.152231, -119.6666755)            246.51\nB (38.279274, -119.6127765)            248.01\nC (38.50458, -119.62176)               254.12\nD (37.862028, -119.6576925)            253.79\nE (37.89748, -119.2624335)             241.42\nF (37.8762105, -119.3432825)           241.93\nG (37.833654, -119.4510805)            245.14\nH (38.0603395, -119.6666755)           247.37\nI (37.798171, -119.19955150            243.14\nJ (38.0391175, -119.3073495)           242.97\nName: 99, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/9/2018 6:00\nA (38.152231, -119.6666755)            248.72\nB (38.279274, -119.6127765)         251.09999\nC (38.50458, -119.62176)            255.51999\nD (37.862028, -119.6576925)            254.31\nE (37.89748, -119.2624335)          243.20999\nF (37.8762105, -119.3432825)        244.12999\nG (37.833654, -119.4510805)         249.45999\nH (38.0603395, -119.6666755)           249.79\nI (37.798171, -119.19955150            244.58\nJ (38.0391175, -119.3073495)           245.47\nName: 100, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/10/2018 6:00\nA (38.152231, -119.6666755)             247.75\nB (38.279274, -119.6127765)             247.62\nC (38.50458, -119.62176)                 253.7\nD (37.862028, -119.6576925)             255.04\nE (37.89748, -119.2624335)           238.59999\nF (37.8762105, -119.3432825)             241.7\nG (37.833654, -119.4510805)             246.45\nH (38.0603395, -119.6666755)            249.67\nI (37.798171, -119.19955150             238.87\nJ (38.0391175, -119.3073495)             241.4\nName: 101, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/11/2018 6:00\nA (38.152231, -119.6666755)          240.79999\nB (38.279274, -119.6127765)             241.93\nC (38.50458, -119.62176)             249.23999\nD (37.862028, -119.6576925)             251.67\nE (37.89748, -119.2624335)           234.84999\nF (37.8762105, -119.3432825)            236.62\nG (37.833654, -119.4510805)             243.39\nH (38.0603395, -119.6666755)         241.98999\nI (37.798171, -119.19955150          236.95999\nJ (38.0391175, -119.3073495)            236.26\nName: 102, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/12/2018 6:00\nA (38.152231, -119.6666755)             234.83\nB (38.279274, -119.6127765)             239.97\nC (38.50458, -119.62176)                250.06\nD (37.862028, -119.6576925)          249.09999\nE (37.89748, -119.2624335)              235.42\nF (37.8762105, -119.3432825)            238.17\nG (37.833654, -119.4510805)             244.48\nH (38.0603395, -119.6666755)            235.92\nI (37.798171, -119.19955150             236.98\nJ (38.0391175, -119.3073495)         234.18999\nName: 103, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 104, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/14/2018 6:00\nA (38.152231, -119.6666755)             235.58\nB (38.279274, -119.6127765)             239.45\nC (38.50458, -119.62176)                248.81\nD (37.862028, -119.6576925)             248.23\nE (37.89748, -119.2624335)              234.03\nF (37.8762105, -119.3432825)            234.51\nG (37.833654, -119.4510805)             237.83\nH (38.0603395, -119.6666755)         236.84999\nI (37.798171, -119.19955150             236.81\nJ (38.0391175, -119.3073495)            234.01\nName: 105, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/15/2018 6:00\nA (38.152231, -119.6666755)             237.06\nB (38.279274, -119.6127765)             239.97\nC (38.50458, -119.62176)                 249.9\nD (37.862028, -119.6576925)             247.93\nE (37.89748, -119.2624335)              234.28\nF (37.8762105, -119.3432825)            235.51\nG (37.833654, -119.4510805)             237.37\nH (38.0603395, -119.6666755)            237.48\nI (37.798171, -119.19955150             236.12\nJ (38.0391175, -119.3073495)            235.72\nName: 106, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/16/2018 6:00\nA (38.152231, -119.6666755)             237.03\nB (38.279274, -119.6127765)          238.76999\nC (38.50458, -119.62176)                249.17\nD (37.862028, -119.6576925)          249.90999\nE (37.89748, -119.2624335)               235.9\nF (37.8762105, -119.3432825)            236.78\nG (37.833654, -119.4510805)          237.87999\nH (38.0603395, -119.6666755)            238.79\nI (37.798171, -119.19955150          238.18999\nJ (38.0391175, -119.3073495)         238.45999\nName: 107, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/17/2018 6:00\nA (38.152231, -119.6666755)             239.29\nB (38.279274, -119.6127765)          239.76999\nC (38.50458, -119.62176)             248.48999\nD (37.862028, -119.6576925)             249.75\nE (37.89748, -119.2624335)               232.9\nF (37.8762105, -119.3432825)            232.23\nG (37.833654, -119.4510805)             241.15\nH (38.0603395, -119.6666755)         240.48999\nI (37.798171, -119.19955150              236.7\nJ (38.0391175, -119.3073495)            236.36\nName: 108, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/18/2018 6:00\nA (38.152231, -119.6666755)             239.78\nB (38.279274, -119.6127765)          242.09999\nC (38.50458, -119.62176)                253.11\nD (37.862028, -119.6576925)             252.86\nE (37.89748, -119.2624335)              234.56\nF (37.8762105, -119.3432825)            235.73\nG (37.833654, -119.4510805)             243.09\nH (38.0603395, -119.6666755)            240.62\nI (37.798171, -119.19955150             237.65\nJ (38.0391175, -119.3073495)             238.4\nName: 109, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/19/2018 6:00\nA (38.152231, -119.6666755)             242.67\nB (38.279274, -119.6127765)          242.51999\nC (38.50458, -119.62176)             249.45999\nD (37.862028, -119.6576925)             251.72\nE (37.89748, -119.2624335)           239.04999\nF (37.8762105, -119.3432825)            240.81\nG (37.833654, -119.4510805)             246.59\nH (38.0603395, -119.6666755)         244.45999\nI (37.798171, -119.19955150             243.08\nJ (38.0391175, -119.3073495)         238.45999\nName: 110, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 111, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/21/2018 6:00\nA (38.152231, -119.6666755)              230.2\nB (38.279274, -119.6127765)             231.93\nC (38.50458, -119.62176)                239.48\nD (37.862028, -119.6576925)             239.28\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)         230.95999\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 112, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/22/2018 6:00\nA (38.152231, -119.6666755)             235.59\nB (38.279274, -119.6127765)          236.65999\nC (38.50458, -119.62176)                245.26\nD (37.862028, -119.6576925)             246.98\nE (37.89748, -119.2624335)           233.73999\nF (37.8762105, -119.3432825)         234.65999\nG (37.833654, -119.4510805)             238.58\nH (38.0603395, -119.6666755)            237.95\nI (37.798171, -119.19955150          235.93999\nJ (38.0391175, -119.3073495)            233.26\nName: 113, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/23/2018 6:00\nA (38.152231, -119.6666755)             231.51\nB (38.279274, -119.6127765)             233.67\nC (38.50458, -119.62176)                240.72\nD (37.862028, -119.6576925)              243.9\nE (37.89748, -119.2624335)              228.98\nF (37.8762105, -119.3432825)            228.81\nG (37.833654, -119.4510805)          233.40999\nH (38.0603395, -119.6666755)            233.54\nI (37.798171, -119.19955150             232.22\nJ (38.0391175, -119.3073495)            228.87\nName: 114, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/24/2018 6:00\nA (38.152231, -119.6666755)             231.06\nB (38.279274, -119.6127765)             232.25\nC (38.50458, -119.62176)             243.37999\nD (37.862028, -119.6576925)             247.26\nE (37.89748, -119.2624335)           229.65999\nF (37.8762105, -119.3432825)            232.51\nG (37.833654, -119.4510805)             235.81\nH (38.0603395, -119.6666755)            234.75\nI (37.798171, -119.19955150             231.75\nJ (38.0391175, -119.3073495)            231.87\nName: 115, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/25/2018 6:00\nA (38.152231, -119.6666755)             233.64\nB (38.279274, -119.6127765)          233.12999\nC (38.50458, -119.62176)             241.81999\nD (37.862028, -119.6576925)          243.29999\nE (37.89748, -119.2624335)              229.83\nF (37.8762105, -119.3432825)         230.40999\nG (37.833654, -119.4510805)          236.20999\nH (38.0603395, -119.6666755)             234.5\nI (37.798171, -119.19955150          233.93999\nJ (38.0391175, -119.3073495)            232.23\nName: 116, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/26/2018 6:00\nA (38.152231, -119.6666755)          229.81999\nB (38.279274, -119.6127765)          231.29999\nC (38.50458, -119.62176)             238.73999\nD (37.862028, -119.6576925)          242.43999\nE (37.89748, -119.2624335)              225.72\nF (37.8762105, -119.3432825)            227.15\nG (37.833654, -119.4510805)             234.15\nH (38.0603395, -119.6666755)         232.81999\nI (37.798171, -119.19955150          227.26999\nJ (38.0391175, -119.3073495)            226.89\nName: 117, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/27/2018 6:00\nA (38.152231, -119.6666755)             238.43\nB (38.279274, -119.6127765)             240.39\nC (38.50458, -119.62176)             247.06999\nD (37.862028, -119.6576925)          249.79999\nE (37.89748, -119.2624335)           232.90999\nF (37.8762105, -119.3432825)             236.2\nG (37.833654, -119.4510805)             242.67\nH (38.0603395, -119.6666755)            239.58\nI (37.798171, -119.19955150          234.98999\nJ (38.0391175, -119.3073495)             232.2\nName: 118, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/28/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 119, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/29/2018 6:00\nA (38.152231, -119.6666755)             236.37\nB (38.279274, -119.6127765)          240.20999\nC (38.50458, -119.62176)                248.53\nD (37.862028, -119.6576925)             246.93\nE (37.89748, -119.2624335)              233.98\nF (37.8762105, -119.3432825)            235.39\nG (37.833654, -119.4510805)             238.75\nH (38.0603395, -119.6666755)         236.87999\nI (37.798171, -119.19955150          236.98999\nJ (38.0391175, -119.3073495)            233.53\nName: 120, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/30/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 121, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            1/31/2018 6:00\nA (38.152231, -119.6666755)             235.68\nB (38.279274, -119.6127765)             237.89\nC (38.50458, -119.62176)                246.29\nD (37.862028, -119.6576925)             247.03\nE (37.89748, -119.2624335)              231.09\nF (37.8762105, -119.3432825)         231.01999\nG (37.833654, -119.4510805)          237.84999\nH (38.0603395, -119.6666755)         236.48999\nI (37.798171, -119.19955150             235.65\nJ (38.0391175, -119.3073495)            231.93\nName: 122, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/1/2018 6:00\nA (38.152231, -119.6666755)            237.62\nB (38.279274, -119.6127765)            238.87\nC (38.50458, -119.62176)               248.01\nD (37.862028, -119.6576925)         250.90999\nE (37.89748, -119.2624335)             233.12\nF (37.8762105, -119.3432825)        235.62999\nG (37.833654, -119.4510805)            244.11\nH (38.0603395, -119.6666755)        240.62999\nI (37.798171, -119.19955150             234.0\nJ (38.0391175, -119.3073495)        234.70999\nName: 123, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/2/2018 6:00\nA (38.152231, -119.6666755)            238.78\nB (38.279274, -119.6127765)         239.04999\nC (38.50458, -119.62176)               247.61\nD (37.862028, -119.6576925)         251.81999\nE (37.89748, -119.2624335)              231.5\nF (37.8762105, -119.3432825)        234.90999\nG (37.833654, -119.4510805)         241.87999\nH (38.0603395, -119.6666755)           240.68\nI (37.798171, -119.19955150            235.81\nJ (38.0391175, -119.3073495)           235.76\nName: 124, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/3/2018 6:00\nA (38.152231, -119.6666755)            237.86\nB (38.279274, -119.6127765)            238.79\nC (38.50458, -119.62176)               247.51\nD (37.862028, -119.6576925)            253.01\nE (37.89748, -119.2624335)             232.53\nF (37.8762105, -119.3432825)           235.53\nG (37.833654, -119.4510805)            242.25\nH (38.0603395, -119.6666755)           239.93\nI (37.798171, -119.19955150            233.65\nJ (38.0391175, -119.3073495)           233.72\nName: 125, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/4/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 126, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/5/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 127, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/6/2018 6:00\nA (38.152231, -119.6666755)            232.68\nB (38.279274, -119.6127765)         236.23999\nC (38.50458, -119.62176)            246.48999\nD (37.862028, -119.6576925)            244.03\nE (37.89748, -119.2624335)          231.01999\nF (37.8762105, -119.3432825)           231.86\nG (37.833654, -119.4510805)         234.29999\nH (38.0603395, -119.6666755)        232.93999\nI (37.798171, -119.19955150            234.51\nJ (38.0391175, -119.3073495)           231.18\nName: 128, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/7/2018 6:00\nA (38.152231, -119.6666755)            231.73\nB (38.279274, -119.6127765)            235.86\nC (38.50458, -119.62176)               245.28\nD (37.862028, -119.6576925)            243.75\nE (37.89748, -119.2624335)          227.87999\nF (37.8762105, -119.3432825)        229.76999\nG (37.833654, -119.4510805)            235.78\nH (38.0603395, -119.6666755)           232.39\nI (37.798171, -119.19955150         230.95999\nJ (38.0391175, -119.3073495)           230.68\nName: 129, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/8/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 130, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/9/2018 6:00\nA (38.152231, -119.6666755)            233.42\nB (38.279274, -119.6127765)            236.01\nC (38.50458, -119.62176)               246.73\nD (37.862028, -119.6576925)            247.43\nE (37.89748, -119.2624335)          226.65999\nF (37.8762105, -119.3432825)           229.53\nG (37.833654, -119.4510805)            238.31\nH (38.0603395, -119.6666755)           236.54\nI (37.798171, -119.19955150         228.65999\nJ (38.0391175, -119.3073495)           231.01\nName: 131, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/10/2018 6:00\nA (38.152231, -119.6666755)             235.56\nB (38.279274, -119.6127765)          238.87999\nC (38.50458, -119.62176)             249.09999\nD (37.862028, -119.6576925)          247.37999\nE (37.89748, -119.2624335)              231.11\nF (37.8762105, -119.3432825)         232.43999\nG (37.833654, -119.4510805)          239.20999\nH (38.0603395, -119.6666755)            237.36\nI (37.798171, -119.19955150          234.04999\nJ (38.0391175, -119.3073495)         234.26999\nName: 132, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/11/2018 6:00\nA (38.152231, -119.6666755)             232.76\nB (38.279274, -119.6127765)          235.20999\nC (38.50458, -119.62176)                 244.2\nD (37.862028, -119.6576925)          246.93999\nE (37.89748, -119.2624335)           226.12999\nF (37.8762105, -119.3432825)            229.06\nG (37.833654, -119.4510805)          236.43999\nH (38.0603395, -119.6666755)         234.43999\nI (37.798171, -119.19955150             228.92\nJ (38.0391175, -119.3073495)            227.53\nName: 133, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/12/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 134, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/13/2018 6:00\nA (38.152231, -119.6666755)             227.14\nB (38.279274, -119.6127765)             233.76\nC (38.50458, -119.62176)             243.29999\nD (37.862028, -119.6576925)             235.81\nE (37.89748, -119.2624335)              228.39\nF (37.8762105, -119.3432825)            227.92\nG (37.833654, -119.4510805)             228.92\nH (38.0603395, -119.6666755)            227.92\nI (37.798171, -119.19955150             230.56\nJ (38.0391175, -119.3073495)            229.33\nName: 135, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/14/2018 6:00\nA (38.152231, -119.6666755)             225.73\nB (38.279274, -119.6127765)             231.87\nC (38.50458, -119.62176)             242.09999\nD (37.862028, -119.6576925)             237.48\nE (37.89748, -119.2624335)           225.01999\nF (37.8762105, -119.3432825)            224.58\nG (37.833654, -119.4510805)             226.92\nH (38.0603395, -119.6666755)            227.72\nI (37.798171, -119.19955150             226.73\nJ (38.0391175, -119.3073495)            227.68\nName: 136, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/15/2018 6:00\nA (38.152231, -119.6666755)          225.09999\nB (38.279274, -119.6127765)             227.73\nC (38.50458, -119.62176)                238.58\nD (37.862028, -119.6576925)             237.54\nE (37.89748, -119.2624335)              221.03\nF (37.8762105, -119.3432825)            223.18\nG (37.833654, -119.4510805)          226.70999\nH (38.0603395, -119.6666755)         226.09999\nI (37.798171, -119.19955150              221.2\nJ (38.0391175, -119.3073495)            225.25\nName: 137, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/16/2018 6:00\nA (38.152231, -119.6666755)             232.31\nB (38.279274, -119.6127765)          233.73999\nC (38.50458, -119.62176)                243.48\nD (37.862028, -119.6576925)          244.43999\nE (37.89748, -119.2624335)              228.28\nF (37.8762105, -119.3432825)            228.12\nG (37.833654, -119.4510805)             231.08\nH (38.0603395, -119.6666755)            232.89\nI (37.798171, -119.19955150             228.75\nJ (38.0391175, -119.3073495)         231.31999\nName: 138, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/17/2018 6:00\nA (38.152231, -119.6666755)             232.31\nB (38.279274, -119.6127765)          233.59999\nC (38.50458, -119.62176)                244.37\nD (37.862028, -119.6576925)             245.54\nE (37.89748, -119.2624335)              226.17\nF (37.8762105, -119.3432825)            228.53\nG (37.833654, -119.4510805)             235.08\nH (38.0603395, -119.6666755)         233.81999\nI (37.798171, -119.19955150              227.5\nJ (38.0391175, -119.3073495)            228.19\nName: 139, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/18/2018 6:00\nA (38.152231, -119.6666755)          233.90999\nB (38.279274, -119.6127765)             237.45\nC (38.50458, -119.62176)             247.43999\nD (37.862028, -119.6576925)             247.36\nE (37.89748, -119.2624335)           229.20999\nF (37.8762105, -119.3432825)            231.34\nG (37.833654, -119.4510805)          238.06999\nH (38.0603395, -119.6666755)            235.81\nI (37.798171, -119.19955150             231.98\nJ (38.0391175, -119.3073495)             231.7\nName: 140, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/19/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)           221.15999\nF (37.8762105, -119.3432825)            221.89\nG (37.833654, -119.4510805)          224.04999\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             226.79\nJ (38.0391175, -119.3073495)            221.31\nName: 141, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/20/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 142, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/21/2018 6:00\nA (38.152231, -119.6666755)          224.59999\nB (38.279274, -119.6127765)          227.65999\nC (38.50458, -119.62176)                237.43\nD (37.862028, -119.6576925)             235.23\nE (37.89748, -119.2624335)              223.83\nF (37.8762105, -119.3432825)            223.29\nG (37.833654, -119.4510805)             225.86\nH (38.0603395, -119.6666755)             225.7\nI (37.798171, -119.19955150             225.94\nJ (38.0391175, -119.3073495)         223.95999\nName: 143, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/22/2018 6:00\nA (38.152231, -119.6666755)             229.58\nB (38.279274, -119.6127765)          230.51999\nC (38.50458, -119.62176)             238.73999\nD (37.862028, -119.6576925)          239.59999\nE (37.89748, -119.2624335)              223.61\nF (37.8762105, -119.3432825)         224.98999\nG (37.833654, -119.4510805)             229.01\nH (38.0603395, -119.6666755)         231.12999\nI (37.798171, -119.19955150          224.84999\nJ (38.0391175, -119.3073495)         226.09999\nName: 144, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/23/2018 6:00\nA (38.152231, -119.6666755)          225.15999\nB (38.279274, -119.6127765)             225.28\nC (38.50458, -119.62176)                 236.5\nD (37.862028, -119.6576925)          235.59999\nE (37.89748, -119.2624335)              220.37\nF (37.8762105, -119.3432825)            222.22\nG (37.833654, -119.4510805)          226.40999\nH (38.0603395, -119.6666755)            226.67\nI (37.798171, -119.19955150             220.84\nJ (38.0391175, -119.3073495)             222.7\nName: 145, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/24/2018 6:00\nA (38.152231, -119.6666755)             226.34\nB (38.279274, -119.6127765)             228.04\nC (38.50458, -119.62176)                236.84\nD (37.862028, -119.6576925)              239.4\nE (37.89748, -119.2624335)              222.26\nF (37.8762105, -119.3432825)         225.15999\nG (37.833654, -119.4510805)             227.01\nH (38.0603395, -119.6666755)         228.34999\nI (37.798171, -119.19955150             224.04\nJ (38.0391175, -119.3073495)             225.2\nName: 146, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/25/2018 6:00\nA (38.152231, -119.6666755)             230.12\nB (38.279274, -119.6127765)             229.53\nC (38.50458, -119.62176)                238.26\nD (37.862028, -119.6576925)             240.72\nE (37.89748, -119.2624335)           222.09999\nF (37.8762105, -119.3432825)         224.15999\nG (37.833654, -119.4510805)          231.98999\nH (38.0603395, -119.6666755)            231.65\nI (37.798171, -119.19955150             222.86\nJ (38.0391175, -119.3073495)             224.5\nName: 147, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/26/2018 6:00\nA (38.152231, -119.6666755)             231.47\nB (38.279274, -119.6127765)             232.17\nC (38.50458, -119.62176)             240.90999\nD (37.862028, -119.6576925)             245.97\nE (37.89748, -119.2624335)              226.19\nF (37.8762105, -119.3432825)            229.44\nG (37.833654, -119.4510805)          235.54999\nH (38.0603395, -119.6666755)         233.37999\nI (37.798171, -119.19955150             229.18\nJ (38.0391175, -119.3073495)             228.2\nName: 148, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/27/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 149, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            2/28/2018 6:00\nA (38.152231, -119.6666755)          228.73999\nB (38.279274, -119.6127765)             232.53\nC (38.50458, -119.62176)                 239.2\nD (37.862028, -119.6576925)             237.22\nE (37.89748, -119.2624335)              225.04\nF (37.8762105, -119.3432825)            225.75\nG (37.833654, -119.4510805)             229.79\nH (38.0603395, -119.6666755)         229.04999\nI (37.798171, -119.19955150             225.29\nJ (38.0391175, -119.3073495)         227.23999\nName: 150, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/1/2018 6:00\nA (38.152231, -119.6666755)         237.06999\nB (38.279274, -119.6127765)            236.45\nC (38.50458, -119.62176)               243.03\nD (37.862028, -119.6576925)            246.83\nE (37.89748, -119.2624335)             230.15\nF (37.8762105, -119.3432825)           231.33\nG (37.833654, -119.4510805)             236.5\nH (38.0603395, -119.6666755)        238.81999\nI (37.798171, -119.19955150            232.15\nJ (38.0391175, -119.3073495)           231.42\nName: 151, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/2/2018 6:00\nA (38.152231, -119.6666755)            232.53\nB (38.279274, -119.6127765)            232.28\nC (38.50458, -119.62176)            238.37999\nD (37.862028, -119.6576925)            243.09\nE (37.89748, -119.2624335)             228.87\nF (37.8762105, -119.3432825)        229.29999\nG (37.833654, -119.4510805)         232.12999\nH (38.0603395, -119.6666755)           234.01\nI (37.798171, -119.19955150            230.83\nJ (38.0391175, -119.3073495)        229.79999\nName: 152, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/3/2018 6:00\nA (38.152231, -119.6666755)         232.18999\nB (38.279274, -119.6127765)         231.81999\nC (38.50458, -119.62176)               238.03\nD (37.862028, -119.6576925)             242.4\nE (37.89748, -119.2624335)             224.98\nF (37.8762105, -119.3432825)           227.56\nG (37.833654, -119.4510805)            236.78\nH (38.0603395, -119.6666755)        233.48999\nI (37.798171, -119.19955150         227.79999\nJ (38.0391175, -119.3073495)        226.20999\nName: 153, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/4/2018 6:00\nA (38.152231, -119.6666755)            228.56\nB (38.279274, -119.6127765)            228.17\nC (38.50458, -119.62176)            234.20999\nD (37.862028, -119.6576925)         240.15999\nE (37.89748, -119.2624335)             221.44\nF (37.8762105, -119.3432825)        224.95999\nG (37.833654, -119.4510805)            230.62\nH (38.0603395, -119.6666755)           230.51\nI (37.798171, -119.19955150            223.04\nJ (38.0391175, -119.3073495)           224.62\nName: 154, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/5/2018 6:00\nA (38.152231, -119.6666755)             227.7\nB (38.279274, -119.6127765)            228.84\nC (38.50458, -119.62176)               232.43\nD (37.862028, -119.6576925)            237.45\nE (37.89748, -119.2624335)             222.61\nF (37.8762105, -119.3432825)        226.01999\nG (37.833654, -119.4510805)            231.12\nH (38.0603395, -119.6666755)           228.28\nI (37.798171, -119.19955150            224.22\nJ (38.0391175, -119.3073495)        224.51999\nName: 155, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/6/2018 6:00\nA (38.152231, -119.6666755)            228.04\nB (38.279274, -119.6127765)            231.09\nC (38.50458, -119.62176)               234.26\nD (37.862028, -119.6576925)         238.98999\nE (37.89748, -119.2624335)             224.83\nF (37.8762105, -119.3432825)           227.97\nG (37.833654, -119.4510805)            234.87\nH (38.0603395, -119.6666755)        229.31999\nI (37.798171, -119.19955150             227.4\nJ (38.0391175, -119.3073495)           223.76\nName: 156, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/7/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 157, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/8/2018 6:00\nA (38.152231, -119.6666755)            232.84\nB (38.279274, -119.6127765)         234.59999\nC (38.50458, -119.62176)               241.28\nD (37.862028, -119.6576925)            239.58\nE (37.89748, -119.2624335)          229.26999\nF (37.8762105, -119.3432825)        230.76999\nG (37.833654, -119.4510805)         234.04999\nH (38.0603395, -119.6666755)           232.93\nI (37.798171, -119.19955150         230.95999\nJ (38.0391175, -119.3073495)        228.34999\nName: 158, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/9/2018 6:00\nA (38.152231, -119.6666755)            230.18\nB (38.279274, -119.6127765)         231.98999\nC (38.50458, -119.62176)            237.65999\nD (37.862028, -119.6576925)            238.84\nE (37.89748, -119.2624335)             226.69\nF (37.8762105, -119.3432825)        227.40999\nG (37.833654, -119.4510805)         230.90999\nH (38.0603395, -119.6666755)        230.37999\nI (37.798171, -119.19955150         228.76999\nJ (38.0391175, -119.3073495)           227.09\nName: 159, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/10/2018 6:00\nA (38.152231, -119.6666755)             231.33\nB (38.279274, -119.6127765)          231.06999\nC (38.50458, -119.62176)                239.06\nD (37.862028, -119.6576925)             241.42\nE (37.89748, -119.2624335)              229.86\nF (37.8762105, -119.3432825)         229.20999\nG (37.833654, -119.4510805)          232.51999\nH (38.0603395, -119.6666755)         232.90999\nI (37.798171, -119.19955150             231.53\nJ (38.0391175, -119.3073495)            230.42\nName: 160, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/11/2018 6:00\nA (38.152231, -119.6666755)             228.84\nB (38.279274, -119.6127765)             229.06\nC (38.50458, -119.62176)                237.39\nD (37.862028, -119.6576925)             238.97\nE (37.89748, -119.2624335)              226.72\nF (37.8762105, -119.3432825)             227.0\nG (37.833654, -119.4510805)              232.7\nH (38.0603395, -119.6666755)            231.48\nI (37.798171, -119.19955150          228.70999\nJ (38.0391175, -119.3073495)         227.06999\nName: 161, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/12/2018 6:00\nA (38.152231, -119.6666755)             229.94\nB (38.279274, -119.6127765)             228.79\nC (38.50458, -119.62176)             238.18999\nD (37.862028, -119.6576925)          244.31999\nE (37.89748, -119.2624335)           225.73999\nF (37.8762105, -119.3432825)            227.43\nG (37.833654, -119.4510805)          233.81999\nH (38.0603395, -119.6666755)         232.23999\nI (37.798171, -119.19955150             228.03\nJ (38.0391175, -119.3073495)            227.95\nName: 162, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/13/2018 6:00\nA (38.152231, -119.6666755)             248.33\nB (38.279274, -119.6127765)          246.48999\nC (38.50458, -119.62176)                251.12\nD (37.862028, -119.6576925)             256.91\nE (37.89748, -119.2624335)              235.56\nF (37.8762105, -119.3432825)         240.04999\nG (37.833654, -119.4510805)          246.65999\nH (38.0603395, -119.6666755)            249.65\nI (37.798171, -119.19955150             239.22\nJ (38.0391175, -119.3073495)         238.34999\nName: 163, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 164, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/15/2018 6:00\nA (38.152231, -119.6666755)          234.81999\nB (38.279274, -119.6127765)             234.17\nC (38.50458, -119.62176)                240.09\nD (37.862028, -119.6576925)             241.26\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)          233.20999\nH (38.0603395, -119.6666755)            235.31\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)         227.09999\nName: 165, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/16/2018 6:00\nA (38.152231, -119.6666755)          234.93999\nB (38.279274, -119.6127765)             232.54\nC (38.50458, -119.62176)                238.65\nD (37.862028, -119.6576925)             244.47\nE (37.89748, -119.2624335)              228.42\nF (37.8762105, -119.3432825)            230.14\nG (37.833654, -119.4510805)          234.73999\nH (38.0603395, -119.6666755)             236.5\nI (37.798171, -119.19955150             230.25\nJ (38.0391175, -119.3073495)             229.0\nName: 166, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/17/2018 6:00\nA (38.152231, -119.6666755)             231.09\nB (38.279274, -119.6127765)             229.72\nC (38.50458, -119.62176)                233.65\nD (37.862028, -119.6576925)          239.79999\nE (37.89748, -119.2624335)              223.92\nF (37.8762105, -119.3432825)            225.97\nG (37.833654, -119.4510805)             231.97\nH (38.0603395, -119.6666755)            232.28\nI (37.798171, -119.19955150          225.29999\nJ (38.0391175, -119.3073495)            225.37\nName: 167, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/18/2018 6:00\nA (38.152231, -119.6666755)             227.95\nB (38.279274, -119.6127765)              225.5\nC (38.50458, -119.62176)                230.95\nD (37.862028, -119.6576925)          238.98999\nE (37.89748, -119.2624335)              223.25\nF (37.8762105, -119.3432825)         225.54999\nG (37.833654, -119.4510805)              227.4\nH (38.0603395, -119.6666755)            229.89\nI (37.798171, -119.19955150             223.58\nJ (38.0391175, -119.3073495)         225.23999\nName: 168, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/19/2018 6:00\nA (38.152231, -119.6666755)          228.34999\nB (38.279274, -119.6127765)             226.84\nC (38.50458, -119.62176)                234.04\nD (37.862028, -119.6576925)             238.76\nE (37.89748, -119.2624335)           224.29999\nF (37.8762105, -119.3432825)         225.59999\nG (37.833654, -119.4510805)             230.26\nH (38.0603395, -119.6666755)            230.15\nI (37.798171, -119.19955150             225.22\nJ (38.0391175, -119.3073495)            226.45\nName: 169, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/20/2018 6:00\nA (38.152231, -119.6666755)          233.76999\nB (38.279274, -119.6127765)          230.26999\nC (38.50458, -119.62176)                238.31\nD (37.862028, -119.6576925)             246.42\nE (37.89748, -119.2624335)           228.65999\nF (37.8762105, -119.3432825)         231.51999\nG (37.833654, -119.4510805)          237.12999\nH (38.0603395, -119.6666755)            235.33\nI (37.798171, -119.19955150             229.31\nJ (38.0391175, -119.3073495)         229.87999\nName: 170, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/21/2018 6:00\nA (38.152231, -119.6666755)          245.95999\nB (38.279274, -119.6127765)             243.37\nC (38.50458, -119.62176)             249.37999\nD (37.862028, -119.6576925)          256.50998\nE (37.89748, -119.2624335)              237.45\nF (37.8762105, -119.3432825)            240.28\nG (37.833654, -119.4510805)             247.58\nH (38.0603395, -119.6666755)         248.37999\nI (37.798171, -119.19955150             239.01\nJ (38.0391175, -119.3073495)            237.87\nName: 171, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/22/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 172, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/23/2018 6:00\nA (38.152231, -119.6666755)             238.58\nB (38.279274, -119.6127765)             237.51\nC (38.50458, -119.62176)                242.33\nD (37.862028, -119.6576925)             244.98\nE (37.89748, -119.2624335)              230.76\nF (37.8762105, -119.3432825)            233.18\nG (37.833654, -119.4510805)          238.06999\nH (38.0603395, -119.6666755)            239.31\nI (37.798171, -119.19955150             233.25\nJ (38.0391175, -119.3073495)            229.51\nName: 173, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/24/2018 6:00\nA (38.152231, -119.6666755)             230.14\nB (38.279274, -119.6127765)             230.51\nC (38.50458, -119.62176)                236.68\nD (37.862028, -119.6576925)          237.37999\nE (37.89748, -119.2624335)              225.09\nF (37.8762105, -119.3432825)            226.83\nG (37.833654, -119.4510805)          231.09999\nH (38.0603395, -119.6666755)            231.01\nI (37.798171, -119.19955150             227.51\nJ (38.0391175, -119.3073495)            224.47\nName: 174, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/25/2018 6:00\nA (38.152231, -119.6666755)             228.15\nB (38.279274, -119.6127765)             228.03\nC (38.50458, -119.62176)                232.56\nD (37.862028, -119.6576925)          236.20999\nE (37.89748, -119.2624335)              223.47\nF (37.8762105, -119.3432825)            226.53\nG (37.833654, -119.4510805)             232.86\nH (38.0603395, -119.6666755)            230.09\nI (37.798171, -119.19955150             225.62\nJ (38.0391175, -119.3073495)            222.89\nName: 175, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/26/2018 6:00\nA (38.152231, -119.6666755)             225.06\nB (38.279274, -119.6127765)          224.56999\nC (38.50458, -119.62176)                230.26\nD (37.862028, -119.6576925)             234.98\nE (37.89748, -119.2624335)              219.72\nF (37.8762105, -119.3432825)         223.23999\nG (37.833654, -119.4510805)             229.72\nH (38.0603395, -119.6666755)         227.31999\nI (37.798171, -119.19955150             221.34\nJ (38.0391175, -119.3073495)            220.87\nName: 176, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/27/2018 6:00\nA (38.152231, -119.6666755)          228.34999\nB (38.279274, -119.6127765)          225.76999\nC (38.50458, -119.62176)                233.84\nD (37.862028, -119.6576925)             242.09\nE (37.89748, -119.2624335)           225.23999\nF (37.8762105, -119.3432825)            228.29\nG (37.833654, -119.4510805)             233.97\nH (38.0603395, -119.6666755)            230.58\nI (37.798171, -119.19955150          226.01999\nJ (38.0391175, -119.3073495)            224.81\nName: 177, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/28/2018 6:00\nA (38.152231, -119.6666755)             229.09\nB (38.279274, -119.6127765)             228.59\nC (38.50458, -119.62176)             237.87999\nD (37.862028, -119.6576925)          245.54999\nE (37.89748, -119.2624335)           226.26999\nF (37.8762105, -119.3432825)             228.9\nG (37.833654, -119.4510805)          236.76999\nH (38.0603395, -119.6666755)            232.45\nI (37.798171, -119.19955150             227.56\nJ (38.0391175, -119.3073495)            225.39\nName: 178, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/29/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 179, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/30/2018 6:00\nA (38.152231, -119.6666755)          232.87999\nB (38.279274, -119.6127765)          231.37999\nC (38.50458, -119.62176)             242.15999\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 180, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            3/31/2018 6:00\nA (38.152231, -119.6666755)             226.98\nB (38.279274, -119.6127765)             227.94\nC (38.50458, -119.62176)                242.09\nD (37.862028, -119.6576925)             240.53\nE (37.89748, -119.2624335)           226.62999\nF (37.8762105, -119.3432825)            227.34\nG (37.833654, -119.4510805)             230.06\nH (38.0603395, -119.6666755)            228.68\nI (37.798171, -119.19955150             229.78\nJ (38.0391175, -119.3073495)            226.62\nName: 181, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/1/2018 6:00\nA (38.152231, -119.6666755)             225.2\nB (38.279274, -119.6127765)         224.31999\nC (38.50458, -119.62176)               238.03\nD (37.862028, -119.6576925)         239.15999\nE (37.89748, -119.2624335)          225.48999\nF (37.8762105, -119.3432825)           226.58\nG (37.833654, -119.4510805)            228.45\nH (38.0603395, -119.6666755)           227.61\nI (37.798171, -119.19955150         226.84999\nJ (38.0391175, -119.3073495)           226.56\nName: 182, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/2/2018 6:00\nA (38.152231, -119.6666755)         224.62999\nB (38.279274, -119.6127765)            226.37\nC (38.50458, -119.62176)               239.83\nD (37.862028, -119.6576925)         238.04999\nE (37.89748, -119.2624335)             221.03\nF (37.8762105, -119.3432825)        221.73999\nG (37.833654, -119.4510805)         228.15999\nH (38.0603395, -119.6666755)           227.26\nI (37.798171, -119.19955150            224.23\nJ (38.0391175, -119.3073495)        223.01999\nName: 183, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/3/2018 6:00\nA (38.152231, -119.6666755)            222.17\nB (38.279274, -119.6127765)            222.23\nC (38.50458, -119.62176)            233.45999\nD (37.862028, -119.6576925)            241.15\nE (37.89748, -119.2624335)             217.44\nF (37.8762105, -119.3432825)        220.54999\nG (37.833654, -119.4510805)            230.08\nH (38.0603395, -119.6666755)            227.2\nI (37.798171, -119.19955150            219.73\nJ (38.0391175, -119.3073495)           222.26\nName: 184, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/4/2018 6:00\nA (38.152231, -119.6666755)            225.51\nB (38.279274, -119.6127765)            223.95\nC (38.50458, -119.62176)            239.98999\nD (37.862028, -119.6576925)            243.48\nE (37.89748, -119.2624335)          223.54999\nF (37.8762105, -119.3432825)           224.79\nG (37.833654, -119.4510805)            230.36\nH (38.0603395, -119.6666755)           228.43\nI (37.798171, -119.19955150             223.9\nJ (38.0391175, -119.3073495)           225.81\nName: 185, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/5/2018 6:00\nA (38.152231, -119.6666755)            239.76\nB (38.279274, -119.6127765)         240.04999\nC (38.50458, -119.62176)               251.81\nD (37.862028, -119.6576925)            253.42\nE (37.89748, -119.2624335)          233.20999\nF (37.8762105, -119.3432825)        235.20999\nG (37.833654, -119.4510805)         242.93999\nH (38.0603395, -119.6666755)           241.51\nI (37.798171, -119.19955150         234.26999\nJ (38.0391175, -119.3073495)        233.45999\nName: 186, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/6/2018 6:00\nA (38.152231, -119.6666755)               NaN\nB (38.279274, -119.6127765)               NaN\nC (38.50458, -119.62176)                  NaN\nD (37.862028, -119.6576925)               NaN\nE (37.89748, -119.2624335)                NaN\nF (37.8762105, -119.3432825)              NaN\nG (37.833654, -119.4510805)               NaN\nH (38.0603395, -119.6666755)              NaN\nI (37.798171, -119.19955150               NaN\nJ (38.0391175, -119.3073495)              NaN\nName: 187, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/7/2018 6:00\nA (38.152231, -119.6666755)            248.17\nB (38.279274, -119.6127765)            250.08\nC (38.50458, -119.62176)            254.23999\nD (37.862028, -119.6576925)         249.23999\nE (37.89748, -119.2624335)             246.68\nF (37.8762105, -119.3432825)           248.58\nG (37.833654, -119.4510805)         250.09999\nH (38.0603395, -119.6666755)        247.93999\nI (37.798171, -119.19955150         250.20999\nJ (38.0391175, -119.3073495)           245.76\nName: 188, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/8/2018 6:00\nA (38.152231, -119.6666755)         221.34999\nB (38.279274, -119.6127765)            227.53\nC (38.50458, -119.62176)               242.95\nD (37.862028, -119.6576925)         235.31999\nE (37.89748, -119.2624335)             222.34\nF (37.8762105, -119.3432825)        220.76999\nG (37.833654, -119.4510805)            223.79\nH (38.0603395, -119.6666755)           222.47\nI (37.798171, -119.19955150            225.39\nJ (38.0391175, -119.3073495)           224.53\nName: 189, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/9/2018 6:00\nA (38.152231, -119.6666755)         224.37999\nB (38.279274, -119.6127765)         227.95999\nC (38.50458, -119.62176)               243.48\nD (37.862028, -119.6576925)            244.22\nE (37.89748, -119.2624335)             224.11\nF (37.8762105, -119.3432825)        223.62999\nG (37.833654, -119.4510805)         226.29999\nH (38.0603395, -119.6666755)           228.09\nI (37.798171, -119.19955150            227.92\nJ (38.0391175, -119.3073495)           226.98\nName: 190, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/10/2018 6:00\nA (38.152231, -119.6666755)             234.47\nB (38.279274, -119.6127765)             235.39\nC (38.50458, -119.62176)             249.62999\nD (37.862028, -119.6576925)              249.7\nE (37.89748, -119.2624335)              227.84\nF (37.8762105, -119.3432825)            226.65\nG (37.833654, -119.4510805)             234.78\nH (38.0603395, -119.6666755)         235.93999\nI (37.798171, -119.19955150             232.75\nJ (38.0391175, -119.3073495)            231.51\nName: 191, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/11/2018 6:00\nA (38.152231, -119.6666755)             236.25\nB (38.279274, -119.6127765)             237.64\nC (38.50458, -119.62176)                248.76\nD (37.862028, -119.6576925)             249.26\nE (37.89748, -119.2624335)              225.03\nF (37.8762105, -119.3432825)            226.01\nG (37.833654, -119.4510805)             235.65\nH (38.0603395, -119.6666755)            237.93\nI (37.798171, -119.19955150              227.0\nJ (38.0391175, -119.3073495)            230.33\nName: 192, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/12/2018 6:00\nA (38.152231, -119.6666755)             224.87\nB (38.279274, -119.6127765)             227.34\nC (38.50458, -119.62176)                240.48\nD (37.862028, -119.6576925)          241.45999\nE (37.89748, -119.2624335)              220.45\nF (37.8762105, -119.3432825)            221.19\nG (37.833654, -119.4510805)          228.29999\nH (38.0603395, -119.6666755)            228.36\nI (37.798171, -119.19955150          222.48999\nJ (38.0391175, -119.3073495)            221.89\nName: 193, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/13/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)              219.03\nF (37.8762105, -119.3432825)         219.34999\nG (37.833654, -119.4510805)             220.44\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150             223.98\nJ (38.0391175, -119.3073495)         218.06999\nName: 194, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/14/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 195, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/15/2018 6:00\nA (38.152231, -119.6666755)          225.48999\nB (38.279274, -119.6127765)          233.62999\nC (38.50458, -119.62176)                 249.7\nD (37.862028, -119.6576925)             241.51\nE (37.89748, -119.2624335)              224.23\nF (37.8762105, -119.3432825)         222.48999\nG (37.833654, -119.4510805)             225.75\nH (38.0603395, -119.6666755)            227.03\nI (37.798171, -119.19955150             227.72\nJ (38.0391175, -119.3073495)            226.23\nName: 196, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/16/2018 6:00\nA (38.152231, -119.6666755)          231.01999\nB (38.279274, -119.6127765)          234.68999\nC (38.50458, -119.62176)                244.65\nD (37.862028, -119.6576925)             241.62\nE (37.89748, -119.2624335)              224.76\nF (37.8762105, -119.3432825)            223.17\nG (37.833654, -119.4510805)          226.65999\nH (38.0603395, -119.6666755)         231.54999\nI (37.798171, -119.19955150             227.83\nJ (38.0391175, -119.3073495)         226.76999\nName: 197, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/17/2018 6:00\nA (38.152231, -119.6666755)             224.06\nB (38.279274, -119.6127765)             227.29\nC (38.50458, -119.62176)                239.97\nD (37.862028, -119.6576925)          236.06999\nE (37.89748, -119.2624335)              220.19\nF (37.8762105, -119.3432825)            220.51\nG (37.833654, -119.4510805)             222.67\nH (38.0603395, -119.6666755)            227.09\nI (37.798171, -119.19955150             221.54\nJ (38.0391175, -119.3073495)            223.11\nName: 198, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/18/2018 6:00\nA (38.152231, -119.6666755)             223.65\nB (38.279274, -119.6127765)             227.26\nC (38.50458, -119.62176)             242.84999\nD (37.862028, -119.6576925)             242.29\nE (37.89748, -119.2624335)              220.67\nF (37.8762105, -119.3432825)            219.86\nG (37.833654, -119.4510805)          221.56999\nH (38.0603395, -119.6666755)         225.26999\nI (37.798171, -119.19955150          225.09999\nJ (38.0391175, -119.3073495)            224.45\nName: 199, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/19/2018 6:00\nA (38.152231, -119.6666755)             221.58\nB (38.279274, -119.6127765)          224.37999\nC (38.50458, -119.62176)                239.04\nD (37.862028, -119.6576925)             236.93\nE (37.89748, -119.2624335)              218.76\nF (37.8762105, -119.3432825)            220.09\nG (37.833654, -119.4510805)             225.51\nH (38.0603395, -119.6666755)            224.61\nI (37.798171, -119.19955150          219.76999\nJ (38.0391175, -119.3073495)            221.17\nName: 200, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/20/2018 6:00\nA (38.152231, -119.6666755)          226.12999\nB (38.279274, -119.6127765)          227.65999\nC (38.50458, -119.62176)                 243.0\nD (37.862028, -119.6576925)             246.47\nE (37.89748, -119.2624335)              221.28\nF (37.8762105, -119.3432825)            223.83\nG (37.833654, -119.4510805)              231.4\nH (38.0603395, -119.6666755)            228.61\nI (37.798171, -119.19955150          225.12999\nJ (38.0391175, -119.3073495)            222.84\nName: 201, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/21/2018 6:00\nA (38.152231, -119.6666755)                NaN\nB (38.279274, -119.6127765)                NaN\nC (38.50458, -119.62176)                   NaN\nD (37.862028, -119.6576925)                NaN\nE (37.89748, -119.2624335)                 NaN\nF (37.8762105, -119.3432825)               NaN\nG (37.833654, -119.4510805)                NaN\nH (38.0603395, -119.6666755)               NaN\nI (37.798171, -119.19955150                NaN\nJ (38.0391175, -119.3073495)               NaN\nName: 202, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/22/2018 6:00\nA (38.152231, -119.6666755)             237.83\nB (38.279274, -119.6127765)              246.0\nC (38.50458, -119.62176)                258.57\nD (37.862028, -119.6576925)             247.36\nE (37.89748, -119.2624335)              234.22\nF (37.8762105, -119.3432825)            233.65\nG (37.833654, -119.4510805)          236.59999\nH (38.0603395, -119.6666755)         238.20999\nI (37.798171, -119.19955150             233.89\nJ (38.0391175, -119.3073495)            238.56\nName: 203, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/23/2018 6:00\nA (38.152231, -119.6666755)             232.67\nB (38.279274, -119.6127765)          238.54999\nC (38.50458, -119.62176)                252.28\nD (37.862028, -119.6576925)          244.34999\nE (37.89748, -119.2624335)              230.83\nF (37.8762105, -119.3432825)            229.51\nG (37.833654, -119.4510805)          231.56999\nH (38.0603395, -119.6666755)         233.34999\nI (37.798171, -119.19955150          233.06999\nJ (38.0391175, -119.3073495)            233.31\nName: 204, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/24/2018 6:00\nA (38.152231, -119.6666755)          234.93999\nB (38.279274, -119.6127765)             238.22\nC (38.50458, -119.62176)                251.97\nD (37.862028, -119.6576925)          250.01999\nE (37.89748, -119.2624335)           230.12999\nF (37.8762105, -119.3432825)            227.86\nG (37.833654, -119.4510805)             236.09\nH (38.0603395, -119.6666755)            236.39\nI (37.798171, -119.19955150             234.65\nJ (38.0391175, -119.3073495)            232.18\nName: 205, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/25/2018 6:00\nA (38.152231, -119.6666755)             236.01\nB (38.279274, -119.6127765)          238.87999\nC (38.50458, -119.62176)                252.61\nD (37.862028, -119.6576925)             248.48\nE (37.89748, -119.2624335)              229.04\nF (37.8762105, -119.3432825)         231.87999\nG (37.833654, -119.4510805)             239.14\nH (38.0603395, -119.6666755)         237.73999\nI (37.798171, -119.19955150          229.81999\nJ (38.0391175, -119.3073495)            233.53\nName: 206, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/26/2018 6:00\nA (38.152231, -119.6666755)          240.68999\nB (38.279274, -119.6127765)             242.65\nC (38.50458, -119.62176)                255.68\nD (37.862028, -119.6576925)             256.43\nE (37.89748, -119.2624335)              234.56\nF (37.8762105, -119.3432825)            235.64\nG (37.833654, -119.4510805)             241.03\nH (38.0603395, -119.6666755)            242.62\nI (37.798171, -119.19955150             236.12\nJ (38.0391175, -119.3073495)            238.79\nName: 207, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\nTime                            4/27/2018 6:00\nA (38.152231, -119.6666755)             253.51\nB (38.279274, -119.6127765)              254.4\nC (38.50458, -119.62176)                260.62\nD (37.862028, -119.6576925)              258.8\nE (37.89748, -119.2624335)              238.92\nF (37.8762105, -119.3432825)             240.9\nG (37.833654, -119.4510805)             247.62\nH (38.0603395, -119.6666755)            253.83\nI (37.798171, -119.19955150             241.78\nJ (38.0391175, -119.3073495)            242.68\nName: 208, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nLatitude: 37.798171\nLongitude: \nproperty_name:  J (38.0391175, -119.3073495)\nLatitude: 38.0391175\nLongitude: -119.3073495\n\nStream closed",
  "history_begin_time" : 1698188518725,
  "history_end_time" : 1698188527528,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "xMx07rL1LVvc",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  print(row)\n  \n  for property_name, value in row.items():\n    print(\"property_name: \", property_name)\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nproperty_name:  Time\nproperty_name:  A (38.152231, -119.6666755)\nLatitude: 38.152231\nLongitude: -119.6666755\nproperty_name:  B (38.279274, -119.6127765)\nLatitude: 38.279274\nLongitude: -119.6127765\nproperty_name:  C (38.50458, -119.62176)\nLatitude: 38.50458\nLongitude: -119.62176\nproperty_name:  D (37.862028, -119.6576925)\nLatitude: 37.862028\nLongitude: -119.6576925\nproperty_name:  E (37.89748, -119.2624335)\nLatitude: 37.89748\nLongitude: -119.2624335\nproperty_name:  F (37.8762105, -119.3432825)\nLatitude: 37.8762105\nLongitude: -119.3432825\nproperty_name:  G (37.833654, -119.4510805)\nLatitude: 37.833654\nLongitude: -119.4510805\nproperty_name:  H (38.0603395, -119.6666755)\nLatitude: 38.0603395\nLongitude: -119.6666755\nproperty_name:  I (37.798171, -119.19955150\nError: Latitude and longitude not found in the input string.\n",
  "history_begin_time" : 1698188393753,
  "history_end_time" : 1698188394255,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "AI7pp4DQqPzX",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  global pmv_new_df\n  print(row)\n  \n  for property_name, value in row.items():\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nLatitude: 38.152231\nLongitude: -119.6666755\nLatitude: 38.279274\nLongitude: -119.6127765\nLatitude: 38.50458\nLongitude: -119.62176\nLatitude: 37.862028\nLongitude: -119.6576925\nLatitude: 37.89748\nLongitude: -119.2624335\nLatitude: 37.8762105\nLongitude: -119.3432825\nLatitude: 37.833654\nLongitude: -119.4510805\nLatitude: 38.0603395\nLongitude: -119.6666755\nError: Latitude and longitude not found in the input string.\n",
  "history_begin_time" : 1698188363972,
  "history_end_time" : 1698188364471,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "B3N07JGWisQA",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\ncolumn_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\npmv_new_df = pd.DataFrame(columns=column_names)\n\ndef adjust_column_to_rows(row):\n  print(row)\n  \n  for property_name, value in row.items():\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nLatitude: 38.152231\nLongitude: -119.6666755\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/B3N07JGWisQA/data_merge_hackweek.py\", line 54, in <module>\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/B3N07JGWisQA/data_merge_hackweek.py\", line 47, in adjust_column_to_rows\n    pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n                            ^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'pmv_new_df' where it is not associated with a value\n",
  "history_begin_time" : 1698188187346,
  "history_end_time" : 1698188187848,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "qcnNFGtlq9iy",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\npmv_new_df = pd.DataFrame(columns=[\"date\", \"lat\", \"lon\", \"pmv\"])\n\ndef adjust_column_to_rows(row):\n  print(row)\n  \n  for property_name, value in row.items():\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = [row[\"Time\"], lat, lon, column_data]\n\n        # Create a new DataFrame with the new row\n        new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n        # Concatenate the new row DataFrame with the original DataFrame\n        pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nLatitude: 38.152231\nLongitude: -119.6666755\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/qcnNFGtlq9iy/data_merge_hackweek.py\", line 53, in <module>\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/qcnNFGtlq9iy/data_merge_hackweek.py\", line 43, in adjust_column_to_rows\n    new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n                                                      ^^^^^^^^^^^^\nNameError: name 'column_names' is not defined. Did you mean: 'column_data'?\n",
  "history_begin_time" : 1698188170354,
  "history_end_time" : 1698188170874,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "nXvQ3ImnHmti",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\npmv_new_df = pd.DataFrame(columns=[\"date\", \"lat\", \"lon\", \"pmv\"])\n\ndef adjust_column_to_rows(row):\n  print(row)\n  \n  for property_name, value in row.items():\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = {\n            'date': row[\"Time\"],\n            'lat': lat,\n            'lon': lon,\n            'pmv': column_data\n        }\n        pmv_new_df.append(new_row_data, ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nLatitude: 38.152231\nLongitude: -119.6666755\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/nXvQ3ImnHmti/data_merge_hackweek.py\", line 53, in <module>\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/nXvQ3ImnHmti/data_merge_hackweek.py\", line 46, in adjust_column_to_rows\n    pmv_new_df.append(new_row_data, ignore_index=True)\n    ^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'DataFrame' object has no attribute 'append'. Did you mean: '_append'?\n",
  "history_begin_time" : 1698188058857,
  "history_end_time" : 1698188059351,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "j9Y3YUrkVtUp",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\npmv_new_df = pd.DataFrame(columns=[\"date\", \"lat\", \"lon\", \"pmv\"])\n\ndef adjust_column_to_rows(row):\n  print(row)\n  \n  for property_name, value in row.items():\n    if property_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[property_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = {\n            'date': row[\"Time\"],\n            'lat': lat,\n            'lon': lon,\n            'pmv': column_data\n        }\n        pmv_new_df = pmv_new_df.append(new_row_data, ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nLatitude: 38.152231\nLongitude: -119.6666755\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/j9Y3YUrkVtUp/data_merge_hackweek.py\", line 53, in <module>\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/j9Y3YUrkVtUp/data_merge_hackweek.py\", line 46, in adjust_column_to_rows\n    pmv_new_df = pmv_new_df.append(new_row_data, ignore_index=True)\n                 ^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'pmv_new_df' where it is not associated with a value\n",
  "history_begin_time" : 1698188032858,
  "history_end_time" : 1698188033349,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "VnOFvFbWl0mO",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\npmv_new_df = pd.DataFrame(columns=[\"date\", \"lat\", \"lon\", \"pmv\"])\n\ndef adjust_column_to_rows(row):\n  print(row)\n  \n  for column_name in row.columns:\n    if column_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[column_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', column_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = {\n            'date': row[\"Time\"],\n            'lat': lat,\n            'lon': lon,\n            'pmv': column_data\n        }\n        pmv_new_df = pmv_new_df.append(new_row_data, ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTime                            10/1/2017 6:00\nA (38.152231, -119.6666755)             256.03\nB (38.279274, -119.6127765)             257.35\nC (38.50458, -119.62176)                259.46\nD (37.862028, -119.6576925)             259.87\nE (37.89748, -119.2624335)              247.56\nF (37.8762105, -119.3432825)         250.18999\nG (37.833654, -119.4510805)          254.59999\nH (38.0603395, -119.6666755)         255.98999\nI (37.798171, -119.19955150             248.56\nJ (38.0391175, -119.3073495)             247.7\nName: 0, dtype: object\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/VnOFvFbWl0mO/data_merge_hackweek.py\", line 53, in <module>\n    pmv_old_df.apply(adjust_column_to_rows, axis=1)\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 9423, in apply\n    return op.apply().__finalize__(self, method=\"apply\")\n           ^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 678, in apply\n    return self.apply_standard()\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 798, in apply_standard\n    results, res_index = self.apply_series_generator()\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py\", line 814, in apply_series_generator\n    results[i] = self.f(v)\n                 ^^^^^^^^^\n  File \"/home/ubuntu/gw-workspace/VnOFvFbWl0mO/data_merge_hackweek.py\", line 28, in adjust_column_to_rows\n    for column_name in row.columns:\n                       ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py\", line 5989, in __getattr__\n    return object.__getattribute__(self, name)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAttributeError: 'Series' object has no attribute 'columns'\n",
  "history_begin_time" : 1698187929631,
  "history_end_time" : 1698187930141,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "JMHCCupi9t7b",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\n\n\n# list all the csvs to merge here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\npmw_training = f\"{work_dir}/PMV_training.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\"\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nfsca_testing = f\"{work_dir}/fSCA_testingCells/fSCA_testingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_snowclassifcations.csv\"\n\n\ndf1 = pd.read_csv(landcover_training)\n\n# convert pmv data into the same format\npmv_new_df = pd.DataFrame(columns=[\"date\", \"lat\", \"lon\", \"pmv\"])\n\ndef adjust_column_to_rows(row):\n  print(row)\n  \n  for column_name in row.columns:\n    if column_name == \"Time\":\n      \tcontinue\n    \n    column_data = row[column_name]  # Access the column data\n    match = re.search(r'\\((.*?),\\s(.*?)\\)', column_name)\n\n    if match:\n        lat = match.group(1)\n        lon = match.group(2)\n        print(\"Latitude:\", lat)\n        print(\"Longitude:\", lon)\n        new_row_data = {\n            'date': row[\"Time\"],\n            'lat': lat,\n            'lon': lon,\n            'pmv': column_data\n        }\n        pmv_new_df = pmv_new_df.append(new_row_data, ignore_index=True)\n        \n    else:\n        print(\"Error: Latitude and longitude not found in the input string.\")\n        exit(1)\n\npmv_old_df = pd.read_csv(pmw_training)\npmv_old_df.apply(adjust_column_to_rows, axis=1)\n\nprint(pmv_new_df.head())\n\n\n# Read the first CSV file\n# df2 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# # Read the second CSV file\n# df2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\n# print(df1.head())\n# print(df2.head())\n\n# # Merge the DataFrames using the common columns as keys\n# merged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# # Print the merged DataFrame\n# print(merged_df)\n# merged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/JMHCCupi9t7b/data_merge_hackweek.py\", line 52, in <module>\n    pmv_old_df = pd.read_csv(pmw_training)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n    self.handles = get_handle(\n                   ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/common.py\", line 859, in get_handle\n    handle = open(\n             ^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/gridmet_test_run/PMV_training.csv'\n",
  "history_begin_time" : 1698187908737,
  "history_end_time" : 1698187909227,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "K8HOVPdEmxCU",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\n\n# Read the first CSV file\ndf1 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# Read the second CSV file\ndf2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\nprint(df1.head())\nprint(df2.head())\n\n# Merge the DataFrames using the common columns as keys\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# Print the merged DataFrame\nprint(merged_df)\nmerged_df.to_csv(f\"{work_dir}/hackweek_lc_merged.csv\", index=False)\n\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\n         date        lat         lon  ...     aspect  eastness  northness\n0  2020-12-01  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n1  2020-12-06  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n2  2021-12-30  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n3  2021-11-28  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n4  2021-10-17  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n[5 rows x 21 columns]\n         date        lat         lon  lc_code\n0  2017-01-01  38.152231 -119.666675      152\n1  2017-01-02  38.152231 -119.666675      152\n2  2017-01-03  38.152231 -119.666675      152\n3  2017-01-04  38.152231 -119.666675      152\n4  2017-01-05  38.152231 -119.666675      152\n            date        lat         lon  ...  eastness  northness  lc_code\n0     2019-11-22  38.152231 -119.666675  ...  0.750545   0.346330      152\n1     2019-08-30  38.152231 -119.666675  ...  0.750545   0.346330      152\n2     2019-07-06  38.152231 -119.666675  ...  0.750545   0.346330      152\n3     2019-05-14  38.152231 -119.666675  ...  0.750545   0.346330      152\n4     2019-05-24  38.152231 -119.666675  ...  0.750545   0.346330      152\n...          ...        ...         ...  ...       ...        ...      ...\n7660  2019-04-02  38.039118 -119.307350  ...  0.683913   0.525018      152\n7661  2019-06-13  38.039118 -119.307350  ...  0.683913   0.525018      152\n7662  2019-07-23  38.039118 -119.307350  ...  0.683913   0.525018      152\n7663  2019-10-15  38.039118 -119.307350  ...  0.683913   0.525018      152\n7664  2019-11-09  38.039118 -119.307350  ...  0.683913   0.525018      152\n[7665 rows x 22 columns]\n",
  "history_begin_time" : 1698184712328,
  "history_end_time" : 1698184712952,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "IVuNHXY2taVq",
  "history_input" : "# This process is to merge the land cover data into the training.csv\n\nimport pandas as pd\nfrom snowcast_utils import work_dir\n\n# Read the first CSV file\ndf1 = pd.read_csv(f\"{work_dir}/final_merged_data_3yrs_cleaned_v3_hackweek_subset.csv\")\n\n# Read the second CSV file\ndf2 = pd.read_csv(f\"{work_dir}/lc_data_train.csv\")\n\nprint(df1.head())\nprint(df2.head())\n\n# Merge the DataFrames using the common columns as keys\nmerged_df = pd.merge(df1, df2, on=['date', 'lat', 'lon'], how='inner')\n\n# 'how' parameter specifies the type of merge (inner, outer, left, or right)\n\n# Print the merged DataFrame\nprint(merged_df)\n\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\n         date        lat         lon  ...     aspect  eastness  northness\n0  2020-12-01  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n1  2020-12-06  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n2  2021-12-30  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n3  2021-11-28  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n4  2021-10-17  38.152231 -119.666675  ...  68.846054  0.750545    0.34633\n[5 rows x 21 columns]\n         date        lat         lon  lc_code\n0  2017-01-01  38.152231 -119.666675      152\n1  2017-01-02  38.152231 -119.666675      152\n2  2017-01-03  38.152231 -119.666675      152\n3  2017-01-04  38.152231 -119.666675      152\n4  2017-01-05  38.152231 -119.666675      152\n            date        lat         lon  ...  eastness  northness  lc_code\n0     2019-11-22  38.152231 -119.666675  ...  0.750545   0.346330      152\n1     2019-08-30  38.152231 -119.666675  ...  0.750545   0.346330      152\n2     2019-07-06  38.152231 -119.666675  ...  0.750545   0.346330      152\n3     2019-05-14  38.152231 -119.666675  ...  0.750545   0.346330      152\n4     2019-05-24  38.152231 -119.666675  ...  0.750545   0.346330      152\n...          ...        ...         ...  ...       ...        ...      ...\n7660  2019-04-02  38.039118 -119.307350  ...  0.683913   0.525018      152\n7661  2019-06-13  38.039118 -119.307350  ...  0.683913   0.525018      152\n7662  2019-07-23  38.039118 -119.307350  ...  0.683913   0.525018      152\n7663  2019-10-15  38.039118 -119.307350  ...  0.683913   0.525018      152\n7664  2019-11-09  38.039118 -119.307350  ...  0.683913   0.525018      152\n[7665 rows x 22 columns]\n",
  "history_begin_time" : 1698184537260,
  "history_end_time" : 1698184537982,
  "history_notes" : null,
  "history_process" : "83d2yv",
  "host_id" : null,
  "indicator" : "Done"
},]
