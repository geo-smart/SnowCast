[{
  "history_id" : "q3w03vcs5bn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719806410971,
  "history_end_time" : 1719806410971,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hokix9gdncv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719720012510,
  "history_end_time" : 1719720012510,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k4wlm5e5g5s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719633609568,
  "history_end_time" : 1719633609568,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1d1kcsy0udd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719547210304,
  "history_end_time" : 1719547210304,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "drua6g5prd3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719460810914,
  "history_end_time" : 1719460810914,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w5n8yartvxb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719374414003,
  "history_end_time" : 1719374414003,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c4ib1gregm3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719288011434,
  "history_end_time" : 1719288011434,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x0vw9qlcgkv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719201615575,
  "history_end_time" : 1719201615575,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7jvw7lj8i0d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719115212088,
  "history_end_time" : 1719115212088,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2y1anqh4pok",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719061831239,
  "history_end_time" : 1719061831239,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bi2uf9w663j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1719028810255,
  "history_end_time" : 1719028810255,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ew6hkkruk2f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718992708832,
  "history_end_time" : 1719004828846,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ei9umbw9qk8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718942410559,
  "history_end_time" : 1718942410559,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "59ybti7c87a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718856011884,
  "history_end_time" : 1718856011884,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nkjcfllfep8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718850066041,
  "history_end_time" : 1718850066041,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7tc9ugqofug",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718821279696,
  "history_end_time" : 1718821279696,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9vrbfb8ow3u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718813272273,
  "history_end_time" : 1718813272273,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pzxtk773200",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718778405627,
  "history_end_time" : 1718778405627,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ir6zu7mv9jk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718775386196,
  "history_end_time" : 1718775386196,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xukio121zd0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718771671480,
  "history_end_time" : 1718771671480,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1ld49l6lwzk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718769611599,
  "history_end_time" : 1718769611599,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b2as9qts646",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718683212878,
  "history_end_time" : 1718683212878,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lh4d2rk1psj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718596812825,
  "history_end_time" : 1718596812825,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5ah17mc4199",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718510413598,
  "history_end_time" : 1718510413598,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s6wwfgxxcok",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718459775281,
  "history_end_time" : 1718459775281,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5jgzchyyune",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718424009928,
  "history_end_time" : 1718424009928,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "najhkfvqd89",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718373285032,
  "history_end_time" : 1718373285032,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vdi815nxq95",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718337613408,
  "history_end_time" : 1718337613408,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cjrr8yep3s5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718323441969,
  "history_end_time" : 1718323441969,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zfv55v4vxsm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718302153583,
  "history_end_time" : 1718302153583,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uttoexp3dqq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718251217169,
  "history_end_time" : 1718251217169,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8yosvpwhigf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718178289082,
  "history_end_time" : 1718178289082,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pm90la7i9wg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718174667298,
  "history_end_time" : 1718174667298,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tobp6yn2zib",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718164810543,
  "history_end_time" : 1718164810543,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tghl5nhzqmu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718098293581,
  "history_end_time" : 1718098293581,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x4niwp6an49",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718089421304,
  "history_end_time" : 1718089421304,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ova9w237oy9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718082201004,
  "history_end_time" : 1718082201004,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z9wpj0itvbr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718078411458,
  "history_end_time" : 1718078411458,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wn66wht3zvw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1718053277657,
  "history_end_time" : 1718053277657,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m6vy1jzh6sh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717992010302,
  "history_end_time" : 1717992010302,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d7os7ax4eey",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717985376094,
  "history_end_time" : 1717985376094,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lznkmpwoh8u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717917246814,
  "history_end_time" : 1717917246814,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8kl9bnpnqvh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717912483923,
  "history_end_time" : 1717912483923,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6qqqyhb28y2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717905610342,
  "history_end_time" : 1717905610342,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lbi9phd7nlh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717819210323,
  "history_end_time" : 1717819210323,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h5a2lgis0rq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717732810450,
  "history_end_time" : 1717732810450,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d800xgwkvqx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717646410416,
  "history_end_time" : 1717646410416,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k6p26rqwnj5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717599091204,
  "history_end_time" : 1717599091204,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "r4dg5tpe0ra",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717579163826,
  "history_end_time" : 1717579163826,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3pdackn7l2b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717560009965,
  "history_end_time" : 1717579485518,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8aa9d6p2uln",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717473609691,
  "history_end_time" : 1717473609691,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zx67gdhx72o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717387237209,
  "history_end_time" : 1717387237209,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rlk3x8kkh3y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717300810193,
  "history_end_time" : 1717579483771,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z7ti9k2pj4i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717214410298,
  "history_end_time" : 1717579482959,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "31nlj3hck7t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717128010222,
  "history_end_time" : 1717579482055,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hjsa4oy29fa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1717041609989,
  "history_end_time" : 1717041609989,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pkijjmfxb3x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716955209984,
  "history_end_time" : 1716955209984,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0fyghlcn3w7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716868810836,
  "history_end_time" : 1716868810836,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xmqeiv6xhyz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716817415350,
  "history_end_time" : 1716817415350,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "erbj909urz5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716797560660,
  "history_end_time" : 1716797560660,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qda0bfkppmv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716782411233,
  "history_end_time" : 1716782411233,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8m4rvgwkh77",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716777609621,
  "history_end_time" : 1716777609621,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ah8cemrslwr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716774915364,
  "history_end_time" : 1716774915364,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9k0qpp0lxb9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716754065201,
  "history_end_time" : 1716754065201,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nxq6gc1m3xo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716751217913,
  "history_end_time" : 1716751217913,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qthbzxmfick",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716750881086,
  "history_end_time" : 1716751199718,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2eoelubtngw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716746640546,
  "history_end_time" : 1716746640546,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p8dz8hk5vhz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716696010789,
  "history_end_time" : 1716696010789,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o23lyarbzz1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716618219071,
  "history_end_time" : 1716618219071,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ay983nqyok6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716609611181,
  "history_end_time" : 1716609611181,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rvvbn79lii8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716563756595,
  "history_end_time" : 1716563756595,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b4zn11y2qjv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716524704910,
  "history_end_time" : 1716524704910,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4mhzemssxo8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716523211045,
  "history_end_time" : 1716523211045,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cc91dti3fy3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716521650261,
  "history_end_time" : 1716521650261,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5spei4ufngn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716519590219,
  "history_end_time" : 1716519590219,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l4favkyrd0t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716513818353,
  "history_end_time" : 1716513818353,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0xbqocwaju3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716471261970,
  "history_end_time" : 1716471261970,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6ia6vtbbbax",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716436814241,
  "history_end_time" : 1716799400442,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1tsehxomqss",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716422698093,
  "history_end_time" : 1716422827951,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d2qkyf6v8c6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716422518614,
  "history_end_time" : 1716422518614,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e9fxegddozx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716418167066,
  "history_end_time" : 1716418167066,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v36afr0nht2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716350423628,
  "history_end_time" : 1716418192224,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gqkmxklsmat",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716264025356,
  "history_end_time" : 1716418192346,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zzi1c6ly5bl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716215991445,
  "history_end_time" : 1716220261661,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ay6dch3062l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716184844742,
  "history_end_time" : 1716216036782,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ce8hdyi8kcg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716177613893,
  "history_end_time" : 1716418192110,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j27wd3sqjwn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716091214113,
  "history_end_time" : 1716418191185,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qzhjv8d3ora",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716062449371,
  "history_end_time" : 1716124523832,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "iqk7tau93cj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716043610000,
  "history_end_time" : 1716057482688,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xdc9untup98",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716035693368,
  "history_end_time" : 1716036846665,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kx0s09v61bw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1716004810001,
  "history_end_time" : 1716418194184,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4895288r8sw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715982779451,
  "history_end_time" : 1716035691978,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7eyja4gwyxg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715980750513,
  "history_end_time" : 1715982778747,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "56pv3q7gjm8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715972396945,
  "history_end_time" : 1715980749625,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4koka3q9l6g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715967265190,
  "history_end_time" : 1715972396083,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "baj6q3935wi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715960695208,
  "history_end_time" : 1715967264440,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "i9gvbae5k2c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715957532895,
  "history_end_time" : 1715960694160,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "egwjge3bpon",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715952722407,
  "history_end_time" : 1715957517343,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "azfrfqtq4lk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715939821483,
  "history_end_time" : 1716418196286,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w71vh55b3nn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715937541753,
  "history_end_time" : 1715937541753,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1c4p6ivjlyo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715937326837,
  "history_end_time" : 1715937540116,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7wikonwbxrz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715927549716,
  "history_end_time" : 1715927549716,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "31nt7jp2k06",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715918413412,
  "history_end_time" : 1716418198301,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "g7jti814jp3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715832031146,
  "history_end_time" : 1716418197889,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "inl0txptju9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715799528287,
  "history_end_time" : 1716418199555,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ycwxolt2cv7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715798138031,
  "history_end_time" : 1715799518110,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qjb41bw6u7e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715780310599,
  "history_end_time" : 1715780310599,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f4n223yig6b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715755079591,
  "history_end_time" : 1715780309810,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h1vblu7cbmr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715745613763,
  "history_end_time" : 1716418200335,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "m686qotrll6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715670775314,
  "history_end_time" : 1715670775314,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tba05b0mmrb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715668379781,
  "history_end_time" : 1715668379781,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xtelwwfrcys",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715660606906,
  "history_end_time" : 1715660606906,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2pwl6l2b4n3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715659214551,
  "history_end_time" : 1716418201746,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lkrrfv1ltc5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715657268425,
  "history_end_time" : 1715657268425,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rb9de8l0yw1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715654494344,
  "history_end_time" : 1715654494344,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xa0een53fd9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715651069898,
  "history_end_time" : 1715654493202,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "js2ne69og91",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715635694105,
  "history_end_time" : 1715635694105,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ixrz3y2nmm6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715631991574,
  "history_end_time" : 1715631991574,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c4t9m1an1mi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715593112778,
  "history_end_time" : 1715593112778,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lioh0n1v21d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715588598054,
  "history_end_time" : 1715588598054,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "14mx1h8twyu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715580981995,
  "history_end_time" : 1715588593042,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qkyws350tmb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715577232566,
  "history_end_time" : 1715577232566,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9roxeer9wdv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715572814124,
  "history_end_time" : 1716418204353,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dkpa13zqpge",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715569180322,
  "history_end_time" : 1715569180322,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7opouma2dxu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715566644979,
  "history_end_time" : 1715569179351,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "11s4d0v66j6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715564159015,
  "history_end_time" : 1715566643898,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8f0n3sp2xgy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715553269472,
  "history_end_time" : 1715553269472,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9uiwp4ljgwa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715551757512,
  "history_end_time" : 1716418204470,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oi48t81ygtb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715550036517,
  "history_end_time" : 1715551587890,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0zdg7uf854a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715548411124,
  "history_end_time" : 1715551556733,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pizuovynm6k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715526734843,
  "history_end_time" : 1715548400465,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x242yybld4n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715486409575,
  "history_end_time" : 1715486409575,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "k49zf6pxi5p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715400010186,
  "history_end_time" : 1715549424582,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3ehh54ma56b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715313617346,
  "history_end_time" : 1715549423037,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lxurzfexcj8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715227215158,
  "history_end_time" : 1715549422501,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "shsfxnggg8q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715140820129,
  "history_end_time" : 1715549421791,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "27tw1hhz4id",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715054415066,
  "history_end_time" : 1715549421335,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "onh8lluzmfz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715045815906,
  "history_end_time" : 1715045815906,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zu3szkusj07",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1715045635657,
  "history_end_time" : 1715045800483,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kiwasyx3qoc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714998835643,
  "history_end_time" : 1714998835643,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "28x3a6quwv6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714977336015,
  "history_end_time" : 1714977336015,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3q31ql5ccdf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714968024310,
  "history_end_time" : 1715549419198,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yx8lgmv4coo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714956083592,
  "history_end_time" : 1714956344016,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "vbx6zc",
  "indicator" : "Stopped"
},{
  "history_id" : "d9us3v6jdbc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714944536727,
  "history_end_time" : 1714944767580,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "vbx6zc",
  "indicator" : "Stopped"
},{
  "history_id" : "icn2z92ncsb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714943546225,
  "history_end_time" : 1715549417791,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pd26o5tkmcr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714897025213,
  "history_end_time" : 1714897025213,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oddfsnvuzag",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714894044193,
  "history_end_time" : 1714895139379,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0510ee3j3ty",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714881609878,
  "history_end_time" : 1715549416949,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "og4vovfcrmp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714409076791,
  "history_end_time" : 1714409076791,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "i85dpmfzmlf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714407283848,
  "history_end_time" : 1714407283848,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "t5dclspd5eu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714404800129,
  "history_end_time" : 1714404800129,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vjsihu8jqhk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714363210275,
  "history_end_time" : 1715549415942,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rk7sh24mms4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714362032715,
  "history_end_time" : 1714362032715,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "p34lm6suep0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714355595256,
  "history_end_time" : 1714355595256,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ne1mnjju8k2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714351396910,
  "history_end_time" : 1714351396910,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bx9veidc0uc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714348539294,
  "history_end_time" : 1714348539294,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vd919vnaloa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714345973261,
  "history_end_time" : 1714345973261,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dqq67g59lhi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714343583903,
  "history_end_time" : 1714343583903,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0yn1uuk5bwq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714340841828,
  "history_end_time" : 1714340841828,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cznuycsulic",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714332798196,
  "history_end_time" : 1714332798196,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jgb23kqbxku",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714328816230,
  "history_end_time" : 1714328816230,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "07mlc5u12u5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714323529112,
  "history_end_time" : 1714323529112,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uuxtwtuo7xx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714320354974,
  "history_end_time" : 1714320354974,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rb62nvaeemu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714315863136,
  "history_end_time" : 1714315863136,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yooxs7883lt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714283543376,
  "history_end_time" : 1714315862069,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gmp0gy3vpmi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714281870994,
  "history_end_time" : 1714283515040,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ur3y286tacj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714276838463,
  "history_end_time" : 1714315876916,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e7m0acnbx6i",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1714274662764,
  "history_end_time" : 1714281869552,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w8h5rf6v2q2",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2024-04-28\ntest start date:  2024-04-09\ntest end date:  2024-04-12\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              32.525307\nstd         6.921275  ...              25.414461\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              38.500000\n75%        43.024000  ...              50.700000\nmax        49.000000  ...             100.000000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1714268520614,
  "history_end_time" : 1714272892784,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "wb8j2dfx581",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2024-04-24\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              32.525307\nstd         6.921275  ...              25.414461\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              38.500000\n75%        43.024000  ...              50.700000\nmax        49.000000  ...             100.000000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1713982507904,
  "history_end_time" : 1713982546308,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "oti173ssmwm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711771209903,
  "history_end_time" : 1711771209903,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vh4wa6z3so8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711684813924,
  "history_end_time" : 1714282476122,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ou3od0alsmt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711598415348,
  "history_end_time" : 1714282474715,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "68yf74fp7nz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592280534,
  "history_end_time" : 1711592280534,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ky1nc81kdm3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711592081946,
  "history_end_time" : 1711592081946,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "7tbmwja8gb1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711512016654,
  "history_end_time" : 1711512016654,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8setu31py92",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711425614417,
  "history_end_time" : 1714282483204,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uns2g0p5ms1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711339212955,
  "history_end_time" : 1714282483869,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cp2u97y9ccn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711252813736,
  "history_end_time" : 1711252813736,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "aise9gqig2v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711166414337,
  "history_end_time" : 1711166414337,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xpt48kk94tu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1711080009323,
  "history_end_time" : 1711080009323,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dn0oawto8tj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710993609969,
  "history_end_time" : 1714282484655,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "536td7deikb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710907209404,
  "history_end_time" : 1714282485302,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vht6bco64sw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710820809072,
  "history_end_time" : 1714282486309,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "54leky06n97",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710734409795,
  "history_end_time" : 1714282486976,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u62emnlevih",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710690214210,
  "history_end_time" : 1710690214210,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "8haamqb0d9k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710648009493,
  "history_end_time" : 1714282488982,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ba6set4ll9d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710561608961,
  "history_end_time" : 1714282489556,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4e9ezlndaez",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710475209569,
  "history_end_time" : 1714282490330,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "9sotuv66crn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710388809553,
  "history_end_time" : 1714282490851,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "twdu0ykz36m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710302409644,
  "history_end_time" : 1714282491524,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "on0nmcz7uej",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710216009721,
  "history_end_time" : 1714282492142,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "p1nyy4690dw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710172046541,
  "history_end_time" : 1710172046541,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "vr64fw7qik6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710129609388,
  "history_end_time" : 1714282493754,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7a4ywppvlnv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710080234077,
  "history_end_time" : 1710080234077,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "sdd8ro2lvlt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1710043209029,
  "history_end_time" : 1714282495098,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lr3xsjr2gw8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709998716298,
  "history_end_time" : 1709998716298,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "joslkw0gg6j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709956809306,
  "history_end_time" : 1714282495738,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h31e7jxo11q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709924987603,
  "history_end_time" : 1709924987603,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mfz38wt9pho",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709870410311,
  "history_end_time" : 1714282496860,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hlqi0rnhrqb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709845595332,
  "history_end_time" : 1709845595332,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pjh7nz8qjzx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709844621979,
  "history_end_time" : 1709844621979,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nylb3wva3ay",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709842923405,
  "history_end_time" : 1709842923405,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "in81jjd9moa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709827652624,
  "history_end_time" : 1709844621186,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cceg5kdkb8j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709826138717,
  "history_end_time" : 1709826138717,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gyod046mfrx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709797240855,
  "history_end_time" : 1709797240855,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mq8gpgjt6ed",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709791538005,
  "history_end_time" : 1709791538005,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jczfsxlb6e9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709784009783,
  "history_end_time" : 1714282504746,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q1z3qkhb7ml",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709778056676,
  "history_end_time" : 1709778056676,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ldvpq261d27",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709774843628,
  "history_end_time" : 1709774843628,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w3pphbfrjq4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709765244688,
  "history_end_time" : 1709774842951,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lui24nxcsd7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709763117641,
  "history_end_time" : 1709765243450,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7s4sb6dd8d7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751527396,
  "history_end_time" : 1709751527396,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xa9qk1dkeml",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709751443200,
  "history_end_time" : 1709751495340,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "06uxtuo8q1e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709697609605,
  "history_end_time" : 1714282506691,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l5z1tlo8mjm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709611209021,
  "history_end_time" : 1714282507225,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "51jwm6euyro",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709524809208,
  "history_end_time" : 1714282507737,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zupglqsgx92",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709438408990,
  "history_end_time" : 1709438408990,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xagsjv69iyf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709352009379,
  "history_end_time" : 1709352009379,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nb5rkc117al",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709265609752,
  "history_end_time" : 1709265609752,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "y27clus287z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709179209711,
  "history_end_time" : 1709179209711,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e8zyb46esng",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709137312989,
  "history_end_time" : 1709137312989,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "o2hvphw8s8c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709092809303,
  "history_end_time" : 1709092809303,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2mbxwuis1vy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709085684225,
  "history_end_time" : 1709085684225,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "dng20mzabvh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709078942167,
  "history_end_time" : 1709085672814,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "ot149xdgof8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038894873,
  "history_end_time" : 1709038894873,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "uixa1llcupw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709038872997,
  "history_end_time" : 1709038879074,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "nokp4lvomas",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1709006408917,
  "history_end_time" : 1709006408917,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hnyrl9x53kd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708971826227,
  "history_end_time" : 1708971826227,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "515fylkbjw5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708958410898,
  "history_end_time" : 1708958410898,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "rtboz0ddyrm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708954624229,
  "history_end_time" : 1708954624229,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "t7z1oo3qpnt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708920009736,
  "history_end_time" : 1708920009736,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g62y9enjt91",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708833609426,
  "history_end_time" : 1708833609426,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "l2xn6hii1q9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708747209175,
  "history_end_time" : 1708747209175,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "c7athrsl1r7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708660809302,
  "history_end_time" : 1708660809302,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gwzyddmjyvq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708574409975,
  "history_end_time" : 1708574409975,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xtce3zp1qjs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708488010477,
  "history_end_time" : 1708488010477,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ezp7r3udtbb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708401609209,
  "history_end_time" : 1708401609209,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oqnznx8zfon",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708352215563,
  "history_end_time" : 1708352215563,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "5hjevfv37cz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708348193305,
  "history_end_time" : 1708352214877,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "nndlbcey87n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708315209359,
  "history_end_time" : 1708315209359,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pw0iftg2ve3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708312689977,
  "history_end_time" : 1708312689977,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "ht9gsu1z629",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708305309945,
  "history_end_time" : 1708312689139,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "0owan51qypk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708242679329,
  "history_end_time" : 1708242679329,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "4z03ts9z9w1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708240733541,
  "history_end_time" : 1708240733541,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "066lsq6ynqa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708238769960,
  "history_end_time" : 1708238769960,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "fwohpf2bjlp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708237144904,
  "history_end_time" : 1708237144904,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "dlvh7ash1fj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708235187255,
  "history_end_time" : 1708235187255,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "av1g6dlarer",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708233874704,
  "history_end_time" : 1708233874704,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "b742uhg1lku",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708228809480,
  "history_end_time" : 1708228809480,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2ihn2k9cqvv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708227613832,
  "history_end_time" : 1708227613832,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "rpwezbpj2xs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708142409684,
  "history_end_time" : 1708142409684,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wrr2dfa8vtv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1708056009494,
  "history_end_time" : 1708056009494,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m4cd0az4496",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707969609319,
  "history_end_time" : 1707969609319,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zejlwe5lvfp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707883209102,
  "history_end_time" : 1707883209102,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dktex8je0zl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707796809608,
  "history_end_time" : 1707796809608,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mqgz7qfhz7j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707750669781,
  "history_end_time" : 1707750669781,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "z37a7x1nwdn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707710409659,
  "history_end_time" : 1707710409659,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qqu40kbz0ck",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707624009718,
  "history_end_time" : 1707624009718,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8wzj1qugtij",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707537608884,
  "history_end_time" : 1707537608884,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wkprovfxrea",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707491746725,
  "history_end_time" : 1707491746725,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "1sv2ynvfa94",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707484657959,
  "history_end_time" : 1707484657959,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "wij6srlf8ar",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707451209943,
  "history_end_time" : 1707451209943,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e36wf7rcqmy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707434644207,
  "history_end_time" : 1707434644207,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "o2b1o4ldut3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707432571825,
  "history_end_time" : 1707432571825,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Skipped"
},{
  "history_id" : "kxi4ty0ok5p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707431129263,
  "history_end_time" : 1707432053960,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "8drjlv",
  "indicator" : "Stopped"
},{
  "history_id" : "1ldegehgfyz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707418188662,
  "history_end_time" : 1707418188662,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "gh6yjdjrdc4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707413610363,
  "history_end_time" : 1707413610363,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "mahjd7",
  "indicator" : "Skipped"
},{
  "history_id" : "3c73f7ryrmk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707364808970,
  "history_end_time" : 1707364808970,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9gdkggkrs92",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707278409197,
  "history_end_time" : 1707278409197,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "z9767v7utnu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192718897,
  "history_end_time" : 1707192718897,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ere9mlkd3cd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707192009386,
  "history_end_time" : 1707448888656,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "vm8tkqlm5w3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707189399140,
  "history_end_time" : 1707189399140,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3x9505tns7k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707105609414,
  "history_end_time" : 1707750639196,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pfc98vyn5x2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1707019209429,
  "history_end_time" : 1707750639665,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l25263uh4gi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706932809309,
  "history_end_time" : 1707750640209,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "tz0qwt5jh1t",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706846409782,
  "history_end_time" : 1707750640643,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7u8xh399mo5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706760010054,
  "history_end_time" : 1707750643300,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3295fj6wu56",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706673609582,
  "history_end_time" : 1707750643805,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "bg6qvoe7lg5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706587209506,
  "history_end_time" : 1707750644988,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "275qpreeuim",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706500809095,
  "history_end_time" : 1707750645665,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2jwrbvvcdxz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706414409424,
  "history_end_time" : 1707750646110,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uunev1noy6y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706366105916,
  "history_end_time" : 1706366105916,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cxi422p65zd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706364888470,
  "history_end_time" : 1706364888470,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kqmsac9o63z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706328009616,
  "history_end_time" : 1707750646909,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qez6vkv4me7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706280497978,
  "history_end_time" : 1706280497978,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zbeob28wkc3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706244881245,
  "history_end_time" : 1706244881245,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qfdhkkn27zo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706241609835,
  "history_end_time" : 1706244810658,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ayveonydj99",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706155209704,
  "history_end_time" : 1706244801319,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qxbipscnq3q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1706068809212,
  "history_end_time" : 1706244800345,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mc3kk6nph0d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705982409101,
  "history_end_time" : 1706244799979,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "br6zvtu1tqd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896759298,
  "history_end_time" : 1706244799015,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oexsqzodvgc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705896009900,
  "history_end_time" : 1706244798535,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e7wsy3b0iyv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705849064146,
  "history_end_time" : 1706244798022,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "veciq24vkoi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705809609041,
  "history_end_time" : 1705849649605,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ttaiybvomce",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705793527199,
  "history_end_time" : 1705849647064,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pjgaudpvyu8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705790835105,
  "history_end_time" : 1705790835105,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uxkir99r170",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705770628210,
  "history_end_time" : 1705849642250,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ap8otj9sph3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705762760988,
  "history_end_time" : 1705849640895,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6bi0g0xfbn6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705723209621,
  "history_end_time" : 1705789738453,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gv7bl8e673x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705636808928,
  "history_end_time" : 1705770636236,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "k5q17ya1mmc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705550409409,
  "history_end_time" : 1705770635553,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "aykkl57lg9s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705464008983,
  "history_end_time" : 1705770635064,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "u1ww40di2k1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705422422532,
  "history_end_time" : 1705422422532,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "d30hr331djq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705377609560,
  "history_end_time" : 1705770633058,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "re8wu5qzesj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705291209173,
  "history_end_time" : 1705770632179,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mdv4c6e0e9l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705278850594,
  "history_end_time" : 1705278850594,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "76wwln5bydx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705270952800,
  "history_end_time" : 1705270952800,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lva4qtd0gsg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705204809638,
  "history_end_time" : 1705789662010,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8c2bd8ris6z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705169057234,
  "history_end_time" : 1705169057234,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qonabn2zu85",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705118409765,
  "history_end_time" : 1705789660779,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1wuhseh4m03",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705072448176,
  "history_end_time" : 1705072448176,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "g6ekhib2e78",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1705032009624,
  "history_end_time" : 1705789659550,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cwh1bk9ke5k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704979918380,
  "history_end_time" : 1704979918380,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4erdskqbbfm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704945609762,
  "history_end_time" : 1705789658819,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "e8lcf1yjtpi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704918977815,
  "history_end_time" : 1704918977815,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "calyr8qq2xw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704908919842,
  "history_end_time" : 1704908919842,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "buyy7wgwnit",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704859207707,
  "history_end_time" : 1705789668264,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j2iksrsbgyp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704775840761,
  "history_end_time" : 1704775840761,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h4onkcvcl27",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704772806920,
  "history_end_time" : 1705789667294,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cyx5ixeifau",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704726161283,
  "history_end_time" : 1704727049032,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ypt61vybige",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704686408500,
  "history_end_time" : 1705789666651,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "1i362744ni3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704644803736,
  "history_end_time" : 1704644803736,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "fejke5komr1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704600008332,
  "history_end_time" : 1705789665925,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ujw1uk535yn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704566156635,
  "history_end_time" : 1704566156635,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m5g89emnaqs",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704565587392,
  "history_end_time" : 1704565587392,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uyon49gq64o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704564424168,
  "history_end_time" : 1704564424168,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cul0kztkpj8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704562992189,
  "history_end_time" : 1704562992189,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e125eme6rjc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561889807,
  "history_end_time" : 1704561889807,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b7jpcw9wzib",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704561861181,
  "history_end_time" : 1704561887041,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rjr6omyzsud",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555479217,
  "history_end_time" : 1704555479217,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "06eozbvazqr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704555028208,
  "history_end_time" : 1704555028208,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vlam8vmmewq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704553241778,
  "history_end_time" : 1704553241778,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jdh9chgaxwn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704552254642,
  "history_end_time" : 1704552254642,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "1r6vn2zgjr2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704513607487,
  "history_end_time" : 1705789671134,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "506k4am4dk5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704427207643,
  "history_end_time" : 1705789671874,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "y0i68f03tqg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704340807673,
  "history_end_time" : 1705789673117,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "gbgnoa5sxvy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704330109327,
  "history_end_time" : 1704330109327,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "e85zeacnu4e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704329364877,
  "history_end_time" : 1704329364877,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3orb0z7kjby",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704254407654,
  "history_end_time" : 1705789675686,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4zp8v9jpx0f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704208947966,
  "history_end_time" : 1704208947966,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "lmahhg4xwd9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704207352028,
  "history_end_time" : 1704207352028,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "992eh977ev9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704205859384,
  "history_end_time" : 1704205859384,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3xi88cwkfe5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704168007506,
  "history_end_time" : 1705789676831,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4z2frydtlvh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1704081607614,
  "history_end_time" : 1705789677624,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5clrwjof57q",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703995208545,
  "history_end_time" : 1705789678819,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "lma8usydr90",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703962871415,
  "history_end_time" : 1703962871415,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7zochwvbskm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703960265453,
  "history_end_time" : 1703960265453,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ppb9yvyf38d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703959737856,
  "history_end_time" : 1703959737856,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jop7ib4ie82",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703958611600,
  "history_end_time" : 1703958611600,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hg9mb40j29n",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703955838236,
  "history_end_time" : 1703955838236,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kudk0v9qge6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703954150387,
  "history_end_time" : 1703954150387,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "114y4t9kmqf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915768081,
  "history_end_time" : 1703915768081,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pv6nh9kb3tx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703915283497,
  "history_end_time" : 1703915283497,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rma30dbkjmr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703914476653,
  "history_end_time" : 1703914476653,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5r6569c3kg8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703912302186,
  "history_end_time" : 1703912302186,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dhbhcs3bmv3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703908807271,
  "history_end_time" : 1705789681237,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5oqhis1ief5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703906215390,
  "history_end_time" : 1703906215390,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3vpnyto0055",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703900919154,
  "history_end_time" : 1703900919154,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "in8vh822tmo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703899837777,
  "history_end_time" : 1703899837777,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "a0bmmmy6jo0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703897422961,
  "history_end_time" : 1703897422961,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "do5c09m0vvk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703896125594,
  "history_end_time" : 1703896125594,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5kkua28nlca",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703890276001,
  "history_end_time" : 1703890276001,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "kj38lwi4d7w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703886800819,
  "history_end_time" : 1703886800819,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7maiwo7b7n3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703885997777,
  "history_end_time" : 1703885997777,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vl7j3ku61km",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703880194726,
  "history_end_time" : 1703880194726,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rsiumafebpq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703872753044,
  "history_end_time" : 1703872753044,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "chkql9unyrb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703869828245,
  "history_end_time" : 1703869828245,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dbhvcly1jqk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703868616942,
  "history_end_time" : 1703868616942,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "zrii44t2grv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703867114057,
  "history_end_time" : 1703867114057,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "xh8m6fo8556",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703864885457,
  "history_end_time" : 1703864885457,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rymgcpdygsf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703862637379,
  "history_end_time" : 1703862637379,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rnhqdrsuo3v",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703827227358,
  "history_end_time" : 1703827227358,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "olymunj5d5r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703822411598,
  "history_end_time" : 1703822411598,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "nx7i6mpe75d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786924644,
  "history_end_time" : 1703789718829,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7qziitp5ubw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703786053498,
  "history_end_time" : 1703786917618,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zt6dhfjp1m5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703778395404,
  "history_end_time" : 1703778395404,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gd1gbaniulv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703739034568,
  "history_end_time" : 1703739034568,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "3l1l266egq7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703738754616,
  "history_end_time" : 1703792459280,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "733waddefkh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703736166918,
  "history_end_time" : 1703737316889,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "xn7vo3ckd25",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703694763588,
  "history_end_time" : 1703694763588,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "95q6zn7mtz8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703659541209,
  "history_end_time" : 1703659541209,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5fjhoxgg57b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703658144714,
  "history_end_time" : 1703658144714,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dlpg14q2ngr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703650855782,
  "history_end_time" : 1703650855782,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4g2axs52uj4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703646751544,
  "history_end_time" : 1703650812447,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dosgo83578p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703642120904,
  "history_end_time" : 1703646749628,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "227blmjfqml",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703641988955,
  "history_end_time" : 1703642074636,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nxs21x1jhaz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703629665516,
  "history_end_time" : 1703629665516,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w2rer7hsh9z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703626687995,
  "history_end_time" : 1703627783061,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "naal9o5zkvi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703625782110,
  "history_end_time" : 1703625782110,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "wdu58i393dk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1703624784030,
  "history_end_time" : 1703624784030,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6epeeegpnjt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702875592853,
  "history_end_time" : 1702875592853,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "099bcaa9dxf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702871264385,
  "history_end_time" : 1702871264385,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qlqsrpuzoc2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702867996404,
  "history_end_time" : 1702867996404,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qooej9web0s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866593402,
  "history_end_time" : 1702866593402,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jjhy0zu8yqb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702866137644,
  "history_end_time" : 1702866137644,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0qpm4j8s4yx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702657305642,
  "history_end_time" : 1702657305642,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ij39qrqdcz2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633223055,
  "history_end_time" : 1702633223055,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "skz27vay9ja",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702633156939,
  "history_end_time" : 1702633163904,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "0khg24i8w4z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702274520893,
  "history_end_time" : 1702274520893,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cj6k5h0y34y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702257109210,
  "history_end_time" : 1702257109210,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "guwkhnu4k1h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702253506550,
  "history_end_time" : 1702253506550,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "51u42ik4zk2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702047800951,
  "history_end_time" : 1702047800951,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ybjsl605hu7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1702046671883,
  "history_end_time" : 1702047789489,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "33nrovyocnq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701838624078,
  "history_end_time" : 1701838624078,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mje1hp5s78o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272631488,
  "history_end_time" : 1701272875116,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zpuugj528w5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701272152710,
  "history_end_time" : 1701272363360,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ih89efk20y5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701269761354,
  "history_end_time" : 1701269761354,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yfqw971gvks",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701245472029,
  "history_end_time" : 1701245472029,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "561eg0dm3dz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701234300626,
  "history_end_time" : 1701234300626,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gexlukv6ruq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701232375289,
  "history_end_time" : 1701234158047,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "4321o9rrbn5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701231048677,
  "history_end_time" : 1701231048677,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "697skzy9pya",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230933473,
  "history_end_time" : 1701230952353,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "sxf2bemmg3x",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230796351,
  "history_end_time" : 1701230932260,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d9pdlt3gro5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701230384897,
  "history_end_time" : 1701230384897,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6gksgcshdot",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701229983517,
  "history_end_time" : 1701229983517,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "awq0nwnnz1f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228899380,
  "history_end_time" : 1701228899380,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "cat5mqkicco",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228374813,
  "history_end_time" : 1701228374813,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8p40akx346h",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228236351,
  "history_end_time" : 1701228236351,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4rxpgrv0d1w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228118518,
  "history_end_time" : 1701228118518,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tcg1hdi5tzg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701228056540,
  "history_end_time" : 1701228056540,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qhyvh21qxxr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701227912539,
  "history_end_time" : 1701227912539,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "o35q8ykphoj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1701013937419,
  "history_end_time" : 1701015920042,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "b96h7szxg3d",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1700974688736,
  "history_end_time" : 1700974688736,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2kb6no5n324",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-25\ntest start date:  2022-11-05\ntest end date:  2023-10-07\n/home/chetana\n2022275\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              10.967866\nstd         6.921275  ...               8.376990\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              13.400000\n75%        43.024000  ...              17.000000\nmax        49.000000  ...              40.200000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n",
  "history_begin_time" : 1700885714192,
  "history_end_time" : 1700885737804,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1j02qyxyott",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700471602375,
  "history_end_time" : 1700471604269,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4o4w61a3et6",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700468948827,
  "history_end_time" : 1700468950602,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0jerxwgx5vl",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              32.525307\nstd         6.921275  ...              25.414461\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              38.500000\n75%        43.024000  ...              50.700000\nmax        49.000000  ...             100.000000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1700448600599,
  "history_end_time" : 1700448623890,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "uiq0swal87r",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-20\ntest start date:  2022-10-15\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              32.525307\nstd         6.921275  ...              25.414461\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              38.500000\n75%        43.024000  ...              50.700000\nmax        49.000000  ...             100.000000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n",
  "history_begin_time" : 1700448576132,
  "history_end_time" : 1700448600537,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "a32v7anxm0i",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700230077999,
  "history_end_time" : 1700230079443,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "p9d0fhg0g7n",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700229023093,
  "history_end_time" : 1700229024523,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "om5fsjbjc7p",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700210224378,
  "history_end_time" : 1700210225817,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "5cb4u0czn8f",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700209793483,
  "history_end_time" : 1700209794944,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "5tt26yzj3zv",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700209738681,
  "history_end_time" : 1700209740132,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "qijxb1qaghu",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2022-10-04\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...               8.954971\nstd         6.921275  ...               6.906217\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              10.800000\n75%        43.024000  ...              14.000000\nmax        49.000000  ...              30.700000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1700202007890,
  "history_end_time" : 1700202027915,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "hax5uipp5rp",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-17\ntest start date:  2023-10-04\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             219.445843\nstd         6.921275  ...             166.411147\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             271.000000\n75%        43.024000  ...             351.100000\nmax        49.000000  ...             730.700000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1700201129715,
  "history_end_time" : 1700201149401,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "rajpmifo1uo",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700145681239,
  "history_end_time" : 1700145682667,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "qv62b98qpth",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700143307334,
  "history_end_time" : 1700143308770,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0wrnba91nqe",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700141628664,
  "history_end_time" : 1700141630090,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "8fybj3q0912",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700134142694,
  "history_end_time" : 1700134144139,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "0cre54qq2a4",
  "history_input" : null,
  "history_output" : "Authentication Failed. Wrong Password.",
  "history_begin_time" : 1700133797358,
  "history_end_time" : 1700133798785,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "dvctxvcf7de",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2023-03-10\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             219.445843\nstd         6.921275  ...             166.411147\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             271.000000\n75%        43.024000  ...             351.100000\nmax        49.000000  ...             730.700000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699999649730,
  "history_end_time" : 1699999670126,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mj4mk7ay0s9",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2023-01-10\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             219.445843\nstd         6.921275  ...             166.411147\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             271.000000\n75%        43.024000  ...             351.100000\nmax        49.000000  ...             730.700000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699985493789,
  "history_end_time" : 1699985520555,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "kl75sxo14hl",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2022-12-25\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             219.445843\nstd         6.921275  ...             166.411147\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             271.000000\n75%        43.024000  ...             351.100000\nmax        49.000000  ...             730.700000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699943636297,
  "history_end_time" : 1699943655823,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "p9dl99f56cy",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2022-11-15\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             116.826322\nstd         6.921275  ...              86.660669\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             149.200000\n75%        43.024000  ...             185.700000\nmax        49.000000  ...             347.800000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699939911821,
  "history_end_time" : 1699939932897,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "pxx1c9yog3e",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-14\ntest start date:  2022-12-17\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             199.093315\nstd         6.921275  ...             149.914472\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             248.800000\n75%        43.024000  ...             317.700000\nmax        49.000000  ...             646.300000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699938923231,
  "history_end_time" : 1699938942850,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ZZdo7nmQJS5G",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-13\ntest start date:  2022-12-15\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 28 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...             195.576836\nstd         6.921275  ...             147.108686\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...             244.500000\n75%        43.024000  ...             312.300000\nmax        49.000000  ...             620.900000\n[8 rows x 28 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699836753210,
  "history_end_time" : 1699836773240,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WPi7Qxwecdxd",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 18 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              34.491123\nstd         6.921275  ...              26.677463\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              41.400000\n75%        43.024000  ...              53.600000\nmax        49.000000  ...             103.300000\n[8 rows x 18 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\nError: SWE is not in testing csv\n4  -  Flag\nError: Flag is not in testing csv\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\nError: air_temperature_tmmn is not in testing csv\n7  -  potential_evapotranspiration\nError: potential_evapotranspiration is not in testing csv\n8  -  mean_vapor_pressure_deficit\nError: mean_vapor_pressure_deficit is not in testing csv\n9  -  relative_humidity_rmax\nError: relative_humidity_rmax is not in testing csv\n10  -  relative_humidity_rmin\nError: relative_humidity_rmin is not in testing csv\n11  -  precipitation_amount\nError: precipitation_amount is not in testing csv\n12  -  air_temperature_tmmx\nError: air_temperature_tmmx is not in testing csv\n13  -  wind_speed\nError: wind_speed is not in testing csv\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699805345514,
  "history_end_time" : 1699805362943,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "APX5uf3nppey",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 18 columns]\nte_df describe:                   lat  ...  cumulative_wind_speed\ncount  462204.000000  ...          462204.000000\nmean       37.030000  ...              34.491123\nstd         6.921275  ...              26.677463\nmin        25.060000  ...               0.000000\n25%        31.036000  ...               0.000000\n50%        37.030000  ...              41.400000\n75%        43.024000  ...              53.600000\nmax        49.000000  ...             103.300000\n[8 rows x 18 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\nError: SWE is not in testing csv\n4  -  Flag\nError: Flag is not in testing csv\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\nError: air_temperature_tmmn is not in testing csv\n7  -  potential_evapotranspiration\nError: potential_evapotranspiration is not in testing csv\n8  -  mean_vapor_pressure_deficit\nError: mean_vapor_pressure_deficit is not in testing csv\n9  -  relative_humidity_rmax\nError: relative_humidity_rmax is not in testing csv\n10  -  relative_humidity_rmin\nError: relative_humidity_rmin is not in testing csv\n11  -  precipitation_amount\nError: precipitation_amount is not in testing csv\n12  -  air_temperature_tmmx\nError: air_temperature_tmmx is not in testing csv\n13  -  wind_speed\nError: wind_speed is not in testing csv\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699805252198,
  "history_end_time" : 1699805269539,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "AU3YsIoTwHeS",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 18 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\nError: SWE is not in testing csv\n4  -  Flag\nError: Flag is not in testing csv\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\nError: air_temperature_tmmn is not in testing csv\n7  -  potential_evapotranspiration\nError: potential_evapotranspiration is not in testing csv\n8  -  mean_vapor_pressure_deficit\nError: mean_vapor_pressure_deficit is not in testing csv\n9  -  relative_humidity_rmax\nError: relative_humidity_rmax is not in testing csv\n10  -  relative_humidity_rmin\nError: relative_humidity_rmin is not in testing csv\n11  -  precipitation_amount\nError: precipitation_amount is not in testing csv\n12  -  air_temperature_tmmx\nError: air_temperature_tmmx is not in testing csv\n13  -  wind_speed\nError: wind_speed is not in testing csv\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\nError: water_year is not in testing csv\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699804992497,
  "history_end_time" : 1699805009562,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "iwqubPti9qr0",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\nError: date is not in testing csv\n1  -  lat\n2  -  lon\n3  -  SWE\nError: SWE is not in testing csv\n4  -  Flag\nError: Flag is not in testing csv\n5  -  swe_value\nError: swe_value is not in testing csv\n6  -  air_temperature_tmmn\nError: air_temperature_tmmn is not in testing csv\n7  -  potential_evapotranspiration\nError: potential_evapotranspiration is not in testing csv\n8  -  mean_vapor_pressure_deficit\nError: mean_vapor_pressure_deficit is not in testing csv\n9  -  relative_humidity_rmax\nError: relative_humidity_rmax is not in testing csv\n10  -  relative_humidity_rmin\nError: relative_humidity_rmin is not in testing csv\n11  -  precipitation_amount\nError: precipitation_amount is not in testing csv\n12  -  air_temperature_tmmx\nError: air_temperature_tmmx is not in testing csv\n13  -  wind_speed\nError: wind_speed is not in testing csv\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699802993600,
  "history_end_time" : 1699803013944,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8mnqOn62mxfD",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  ...  cumulative_wind_speed\n0       44158  ...             208.000003\n1       44120  ...              72.700001\n2       44093  ...             977.200008\n3       44803  ...             811.300009\n4       43739  ...               6.800000\n...       ...  ...                    ...\n767195  43787  ...             113.000001\n767196  44026  ...             817.500007\n767197  44072  ...            1014.600011\n767198  44444  ...             918.400012\n767199  44354  ...             720.800010\n[766500 rows x 31 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness', 'water_year', 'cumulative_SWE',\n       'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  31\n0  -  date\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n20  -  water_year\n21  -  cumulative_SWE\n22  -  cumulative_Flag\n23  -  cumulative_air_temperature_tmmn\n24  -  cumulative_potential_evapotranspiration\n25  -  cumulative_mean_vapor_pressure_deficit\n26  -  cumulative_relative_humidity_rmax\n27  -  cumulative_relative_humidity_rmin\n28  -  cumulative_precipitation_amount\n29  -  cumulative_air_temperature_tmmx\n30  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699802663368,
  "history_end_time" : 1699802681235,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cmtJoEEf11Sp",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    766500.000000\nmean         62.437693\nstd          16.505208\nmin           4.277402\n25%          52.134558\n50%          67.681070\n75%          75.046610\nmax          83.685555\nName: slope, dtype: float64\nGet SWE statistics\ncount    766500.000000\nmean          2.661918\nstd           3.989610\nmin           0.000000\n25%           0.000000\n50%           0.000000\n75%           5.900000\nmax          14.300000\nName: swe_value, dtype: float64\nTraining DataFrame:           date  level_0  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       43737      819  ...                            299.2               2.200000\n1       43737      820  ...                            594.5               5.800000\n2       43737      821  ...                            875.5               8.100000\n3       43738      822  ...                           1173.3              10.600000\n4       43738      823  ...                           1472.7              13.200000\n...       ...      ...  ...                              ...                    ...\n767195  44831  1022419  ...                         304936.7            3771.500036\n767196  44831  1022420  ...                         305472.3            3780.300036\n767197  44832  1022421  ...                         306015.1            3786.900036\n767198  44832  1022422  ...                         305743.7            3783.600036\n767199  44832  1022423  ...                         306286.5            3790.200036\n[766500 rows x 103 columns]\nTesting DataFrame:          Latitude  Longitude  rmax  ...  cumulative_AMSR_Flag        date  water_year\n0          49.00   -125.000   NaN  ...                  3870  2022-10-16        2023\n1          49.00   -124.964   NaN  ...                  3870  2022-10-16        2023\n2          49.00   -124.928   NaN  ...                  3870  2022-10-16        2023\n3          49.00   -124.892   NaN  ...                  3870  2022-10-16        2023\n4          49.00   -124.856   NaN  ...                  3870  2022-10-16        2023\n...          ...        ...   ...  ...                   ...         ...         ...\n462199     25.06   -100.196   NaN  ...                  4047  2022-10-16        2023\n462200     25.06   -100.160   NaN  ...                  4047  2022-10-16        2023\n462201     25.06   -100.124   NaN  ...                  4047  2022-10-16        2023\n462202     25.06   -100.088   NaN  ...                  4047  2022-10-16        2023\n462203     25.06   -100.052   NaN  ...                  4047  2022-10-16        2023\n[462204 rows x 32 columns]\nTraining columns:  Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       ...\n       'cumulative_SWE', 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object', length=103)\nTesting columns:  Index(['Latitude', 'Longitude', 'rmax', 'cumulative_rmax', 'tmmn',\n       'cumulative_tmmn', 'vs', 'cumulative_vs', 'rmin', 'cumulative_rmin',\n       'tmmx', 'cumulative_tmmx', 'pr', 'cumulative_pr', 'vpd',\n       'cumulative_vpd', 'etr', 'cumulative_etr', 'x', 'y', 'Elevation',\n       'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness', 'AMSR_SWE',\n       'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag', 'date',\n       'water_year'],\n      dtype='object')\nlength:  103\n0  -  date\n1  -  level_0\n2  -  index\n3  -  lat\n4  -  lon\n5  -  SWE\n6  -  Flag\n7  -  swe_value\n8  -  air_temperature_tmmn\n9  -  potential_evapotranspiration\n10  -  mean_vapor_pressure_deficit\n11  -  relative_humidity_rmax\n12  -  relative_humidity_rmin\n13  -  precipitation_amount\n14  -  air_temperature_tmmx\n15  -  wind_speed\n16  -  elevation\n17  -  slope\n18  -  curvature\n19  -  aspect\n20  -  eastness\n21  -  northness\n22  -  SWE_1\n23  -  Flag_1\n24  -  air_temperature_tmmn_1\n25  -  potential_evapotranspiration_1\n26  -  mean_vapor_pressure_deficit_1\n27  -  relative_humidity_rmax_1\n28  -  relative_humidity_rmin_1\n29  -  precipitation_amount_1\n30  -  air_temperature_tmmx_1\n31  -  wind_speed_1\n32  -  SWE_2\n33  -  Flag_2\n34  -  air_temperature_tmmn_2\n35  -  potential_evapotranspiration_2\n36  -  mean_vapor_pressure_deficit_2\n37  -  relative_humidity_rmax_2\n38  -  relative_humidity_rmin_2\n39  -  precipitation_amount_2\n40  -  air_temperature_tmmx_2\n41  -  wind_speed_2\n42  -  SWE_3\n43  -  Flag_3\n44  -  air_temperature_tmmn_3\n45  -  potential_evapotranspiration_3\n46  -  mean_vapor_pressure_deficit_3\n47  -  relative_humidity_rmax_3\n48  -  relative_humidity_rmin_3\n49  -  precipitation_amount_3\n50  -  air_temperature_tmmx_3\n51  -  wind_speed_3\n52  -  SWE_4\n53  -  Flag_4\n54  -  air_temperature_tmmn_4\n55  -  potential_evapotranspiration_4\n56  -  mean_vapor_pressure_deficit_4\n57  -  relative_humidity_rmax_4\n58  -  relative_humidity_rmin_4\n59  -  precipitation_amount_4\n60  -  air_temperature_tmmx_4\n61  -  wind_speed_4\n62  -  SWE_5\n63  -  Flag_5\n64  -  air_temperature_tmmn_5\n65  -  potential_evapotranspiration_5\n66  -  mean_vapor_pressure_deficit_5\n67  -  relative_humidity_rmax_5\n68  -  relative_humidity_rmin_5\n69  -  precipitation_amount_5\n70  -  air_temperature_tmmx_5\n71  -  wind_speed_5\n72  -  SWE_6\n73  -  Flag_6\n74  -  air_temperature_tmmn_6\n75  -  potential_evapotranspiration_6\n76  -  mean_vapor_pressure_deficit_6\n77  -  relative_humidity_rmax_6\n78  -  relative_humidity_rmin_6\n79  -  precipitation_amount_6\n80  -  air_temperature_tmmx_6\n81  -  wind_speed_6\n82  -  SWE_7\n83  -  Flag_7\n84  -  air_temperature_tmmn_7\n85  -  potential_evapotranspiration_7\n86  -  mean_vapor_pressure_deficit_7\n87  -  relative_humidity_rmax_7\n88  -  relative_humidity_rmin_7\n89  -  precipitation_amount_7\n90  -  air_temperature_tmmx_7\n91  -  wind_speed_7\n92  -  water_year\n93  -  cumulative_SWE\n94  -  cumulative_Flag\n95  -  cumulative_air_temperature_tmmn\n96  -  cumulative_potential_evapotranspiration\n97  -  cumulative_mean_vapor_pressure_deficit\n98  -  cumulative_relative_humidity_rmax\n99  -  cumulative_relative_humidity_rmin\n100  -  cumulative_precipitation_amount\n101  -  cumulative_air_temperature_tmmx\n102  -  cumulative_wind_speed\n",
  "history_begin_time" : 1699799626077,
  "history_end_time" : 1699799680829,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "D0l4rBR2QBoI",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:            lat      lon  ...  cumulative_air_temperature_tmmx  cumulative_wind_speed\n0       49.00 -125.000  ...                              0.0                    0.0\n1       49.00 -124.964  ...                              0.0                    0.0\n2       49.00 -124.928  ...                              0.0                    0.0\n3       49.00 -124.892  ...                              0.0                    0.0\n4       49.00 -124.856  ...                              0.0                    0.0\n...       ...      ...  ...                              ...                    ...\n462199  25.06 -100.196  ...                              0.0                    0.0\n462200  25.06 -100.160  ...                              0.0                    0.0\n462201  25.06 -100.124  ...                              0.0                    0.0\n462202  25.06 -100.088  ...                              0.0                    0.0\n462203  25.06 -100.052  ...                              0.0                    0.0\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'elevation', 'slope', 'curvature', 'aspect', 'eastness',\n       'northness', 'water_year', 'cumulative_SWE', 'cumulative_Flag',\n       'cumulative_air_temperature_tmmn',\n       'cumulative_potential_evapotranspiration',\n       'cumulative_mean_vapor_pressure_deficit',\n       'cumulative_relative_humidity_rmax',\n       'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n       'cumulative_air_temperature_tmmx', 'cumulative_wind_speed'],\n      dtype='object')\nlength:  20\n0  -  date\n1  -  lat\n2  -  lon\n3  -  SWE\n4  -  Flag\n5  -  swe_value\n6  -  air_temperature_tmmn\n7  -  potential_evapotranspiration\n8  -  mean_vapor_pressure_deficit\n9  -  relative_humidity_rmax\n10  -  relative_humidity_rmin\n11  -  precipitation_amount\n12  -  air_temperature_tmmx\n13  -  wind_speed\n14  -  elevation\n15  -  slope\n16  -  curvature\n17  -  aspect\n18  -  eastness\n19  -  northness\n",
  "history_begin_time" : 1699799526237,
  "history_end_time" : 1699799542201,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ZJx9F99qjfja",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-11-12\ntest start date:  2022-10-16\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ZJx9F99qjfja/train_test_pattern_compare.py\", line 104, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/ZJx9F99qjfja/train_test_pattern_compare.py\", line 60, in compare\n    te_df = te_df.drop('date', axis=1)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 5399, in drop\n    return super().drop(\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 331, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4505, in drop\n    obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\", line 4546, in _drop_axis\n    new_axis = axis.drop(labels, errors=errors)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 6934, in drop\n    raise KeyError(f\"{list(labels[mask])} not found in axis\")\nKeyError: \"['date'] not found in axis\"\n",
  "history_begin_time" : 1699798904418,
  "history_end_time" : 1699798915567,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ee1q4p3yqo0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699684154076,
  "history_end_time" : 1705789690246,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ak99yxw30kk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1699681071375,
  "history_end_time" : 1699681071375,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "te949ipdxb6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762678709,
  "history_end_time" : 1698762678709,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b1hf0xwzccg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1698762637994,
  "history_end_time" : 1698762637994,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pbcvx40kawa",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2018-01-01\ntest end date:  2023-10-25\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698276643768,
  "history_end_time" : 1698276651773,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "npcmftobxrw",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-03-02\ntest end date:  2023-10-25\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n",
  "history_begin_time" : 1698252394659,
  "history_end_time" : 1698252402063,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "k1gg7oj6f4i",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698251445340,
  "history_end_time" : 1698251452710,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gclmz5addlj",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-26\ntest end date:  2023-10-25\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n",
  "history_begin_time" : 1698228372721,
  "history_end_time" : 1698228388733,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "m8jdfqevuqn",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-25\ntest start date:  2023-05-29\ntest end date:  2023-10-25\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n",
  "history_begin_time" : 1698227952868,
  "history_end_time" : 1698227960246,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "mfpg5x5qrxo",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-29\ntest end date:  2023-10-24\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n",
  "history_begin_time" : 1698163871539,
  "history_end_time" : 1698163879581,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "cs6jschoywy",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-28\ntest end date:  2023-10-24\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n",
  "history_begin_time" : 1698163581503,
  "history_end_time" : 1698163589493,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "u8ze07lfral",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-26\ntest end date:  2023-10-24\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698163258837,
  "history_end_time" : 1698163266660,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "aSNjZJnih1kR",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-25\ntest end date:  2023-10-24\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698162786931,
  "history_end_time" : 1698162794492,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "plna5ayv9ju",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-26\ntest end date:  2023-10-24\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n",
  "history_begin_time" : 1698161169226,
  "history_end_time" : 1698161184221,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "hijtutezhgm",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-25\ntest end date:  2023-10-24\n/home/ubuntu\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698157860884,
  "history_end_time" : 1698157868936,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "y0dwk8fdt1d",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-24\ntest start date:  2023-05-17\ntest end date:  2023-10-24\n/home/ubuntu\nTraceback (most recent call last):\n  File \"/home/ubuntu/gw-workspace/y0dwk8fdt1d/train_test_pattern_compare.py\", line 104, in <module>\n    compare()\n  File \"/home/ubuntu/gw-workspace/y0dwk8fdt1d/train_test_pattern_compare.py\", line 55, in compare\n    tr_df = pd.read_csv(training_data_path)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 912, in read_csv\n    return _read(filepath_or_buffer, kwds)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 577, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1407, in __init__\n    self._engine = self._make_engine(f, self.engine)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py\", line 1661, in _make_engine\n    self.handles = get_handle(\n                   ^^^^^^^^^^^\n  File \"/home/ubuntu/anaconda3/lib/python3.11/site-packages/pandas/io/common.py\", line 859, in get_handle\n    handle = open(\n             ^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/gridmet_test_run/final_merged_data_3yrs_cleaned_v3.csv'\n",
  "history_begin_time" : 1698152115683,
  "history_end_time" : 1698152116463,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "b9ozo437xmh",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-23\ntest start date:  2023-05-18\ntest end date:  2023-10-23\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698095669222,
  "history_end_time" : 1698095682527,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "vu51jsulgs3",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-23\ntest start date:  2023-05-18\ntest end date:  2023-10-23\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1698075773683,
  "history_end_time" : 1698075789861,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "so1qeo8pn1n",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-17\ntest end date:  2023-10-15\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697349812591,
  "history_end_time" : 1697349828089,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "6ucjwmqemu9",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-05-16\ntest end date:  2023-10-15\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697349107347,
  "history_end_time" : 1697349122258,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2UcJqmburUDM",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697347593592,
  "history_end_time" : 1697347607472,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "Rqo3YX8QfOe0",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-15\ntest start date:  2023-03-16\ntest end date:  2023-10-15\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697347375119,
  "history_end_time" : 1697347389311,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "g3gp03zrl7k",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697190045837,
  "history_end_time" : 1697190061477,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "aps001eqsja",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-03-16\ntest end date:  2023-10-13\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697188816738,
  "history_end_time" : 1697188832324,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "oldjezy33nw",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-07-15\ntest end date:  2023-10-13\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697188196179,
  "history_end_time" : 1697188212218,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "gghh126ep90",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-03-15\ntest end date:  2023-10-13\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697187706665,
  "history_end_time" : 1697187721953,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "SAae2sjU2FWu",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-13\ntest start date:  2023-09-16\ntest end date:  2023-10-13\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1697187212947,
  "history_end_time" : 1697187230552,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pswh8wk8m8n",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-09-16\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1696864244643,
  "history_end_time" : 1696864260901,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "q5fxa4gfnio",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-09-15\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1696862736766,
  "history_end_time" : 1696862760730,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "b9qkezs10y2",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-06-15\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1696832531719,
  "history_end_time" : 1696832547662,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "scgn1h9tkx9",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-12\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  252   252  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  252   252  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  252   252  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  252   252  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  252   252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1696832106859,
  "history_end_time" : 1696832122501,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "aYUH9nZvHuoZ",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\n17  -  eastness\n18  -  northness\n",
  "history_begin_time" : 1696831208728,
  "history_end_time" : 1696831223207,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "GbreSw24JB93",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nlength:  19\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/GbreSw24JB93/train_test_pattern_compare.py\", line 104, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/GbreSw24JB93/train_test_pattern_compare.py\", line 85, in compare\n    axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\nIndexError: index 16 is out of bounds for axis 0 with size 16\n",
  "history_begin_time" : 1696831164684,
  "history_end_time" : 1696831172137,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "2ANroc8JqvoD",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n0  -  lat\n1  -  lon\n2  -  SWE\n3  -  Flag\n4  -  swe_value\n5  -  air_temperature_tmmn\n6  -  potential_evapotranspiration\n7  -  mean_vapor_pressure_deficit\n8  -  relative_humidity_rmax\n9  -  relative_humidity_rmin\n10  -  precipitation_amount\n11  -  air_temperature_tmmx\n12  -  wind_speed\n13  -  elevation\n14  -  slope\n15  -  curvature\n16  -  aspect\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/2ANroc8JqvoD/train_test_pattern_compare.py\", line 104, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/2ANroc8JqvoD/train_test_pattern_compare.py\", line 85, in compare\n    axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\nIndexError: index 16 is out of bounds for axis 0 with size 16\n",
  "history_begin_time" : 1696831062130,
  "history_end_time" : 1696831069920,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "hG5SiwIBWxQK",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:            lat      lon  SWE  Flag  ...  curvature  aspect  eastness  northness\n0       49.00 -125.000    0   241  ...        NaN     NaN       NaN        NaN\n1       49.00 -124.964    0   241  ...        NaN     NaN       NaN        NaN\n2       49.00 -124.928    0   241  ...        NaN     NaN       NaN        NaN\n3       49.00 -124.892    0   241  ...        NaN     NaN       NaN        NaN\n4       49.00 -124.856    0   241  ...        NaN     NaN       NaN        NaN\n...       ...      ...  ...   ...  ...        ...     ...       ...        ...\n462199  25.06 -100.196  255   255  ...        NaN     NaN       NaN        NaN\n462200  25.06 -100.160  255   255  ...        NaN     NaN       NaN        NaN\n462201  25.06 -100.124  255   255  ...        NaN     NaN       NaN        NaN\n462202  25.06 -100.088  255   255  ...        NaN     NaN       NaN        NaN\n462203  25.06 -100.052  255   255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 18 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/hG5SiwIBWxQK/train_test_pattern_compare.py\", line 103, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/hG5SiwIBWxQK/train_test_pattern_compare.py\", line 84, in compare\n    axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\nIndexError: index 16 is out of bounds for axis 0 with size 16\n",
  "history_begin_time" : 1696830955501,
  "history_end_time" : 1696830963272,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "h9fZqJYHdb7E",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    data = data.drop('date', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:                 lat         lon  SWE  ...      aspect  eastness  northness\n0        37.192360 -118.939041  255  ...   24.605782  0.394541   0.737872\n1        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n2        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n3        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n4        37.192360 -118.939041    0  ...   24.605782  0.394541   0.737872\n...            ...         ...  ...  ...         ...       ...        ...\n1022695  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022696  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022697  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022698  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n1022699  36.682572 -118.427001    0  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 19 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44965  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44965  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44965  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44965  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44965  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44965  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44965  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44965  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44965  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44965  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['lat', 'lon', 'SWE', 'Flag', 'swe_value', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/h9fZqJYHdb7E/train_test_pattern_compare.py\", line 100, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/h9fZqJYHdb7E/train_test_pattern_compare.py\", line 81, in compare\n    axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\nIndexError: index 16 is out of bounds for axis 0 with size 16\n",
  "history_begin_time" : 1696830895255,
  "history_end_time" : 1696830902995,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "90tdcmqmi13",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    data = data.drop('date', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_trainin():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \n#compare()\ncalculate_feature_colleration_in_trainin\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-11\ntest end date:  2023-10-09\n/home/chetana\n",
  "history_begin_time" : 1696830416824,
  "history_end_time" : 1696830421948,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "MZRu0GfMtAEE",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_trainin():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \n#compare()\ncalculate_feature_colleration_in_trainin\n",
  "history_output" : "today date = 2023-10-09\ntest start date:  2023-02-10\ntest end date:  2023-10-09\n/home/chetana\n",
  "history_begin_time" : 1696829981850,
  "history_end_time" : 1696829984648,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "cd49lqdnpj4",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_trainin():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \n#compare()\ncalculate_feature_colleration_in_trainin\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-02-10\ntest end date:  2023-10-08\n/home/chetana\n",
  "history_begin_time" : 1696787740924,
  "history_end_time" : 1696787743771,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "08i6kva4t5r",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_trainin():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \n#compare()\ncalculate_feature_colleration_in_trainin\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\n",
  "history_begin_time" : 1696786915899,
  "history_end_time" : 1696786918764,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1D8SEUm3FkYX",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-20\ntest end date:  2023-10-08\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696773734963,
  "history_end_time" : 1696773749392,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "zu6vpafaq3w",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "",
  "history_begin_time" : 1696772041987,
  "history_end_time" : 1696772045819,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "oao7Ig40H5bj",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-19\ntest end date:  2023-10-08\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696732253020,
  "history_end_time" : 1696732272359,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "B8tvnSAJdnJB",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "",
  "history_begin_time" : 1696732219755,
  "history_end_time" : 1696732234946,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "R0wonJChx8FH",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-08\ntest start date:  2023-01-19\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n",
  "history_begin_time" : 1696731985335,
  "history_end_time" : 1696732002409,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "8ieQGbc4FTh0",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "Running",
  "history_begin_time" : 1696731977244,
  "history_end_time" : 1696732234921,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Stopped"
},{
  "history_id" : "bah3XDSdlens",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols)\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696690820777,
  "history_end_time" : 1696690835009,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "X3rqpZodeCou",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696690683503,
  "history_end_time" : 1696690697124,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "tviPZFFGnlo3",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    num_rows = int(num_cols**0.5)  # Square grid\n    num_cols = int(num_cols / num_rows) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'Comparison of {col} (Train vs Test)')\n        \n    \n    axs.flatten()[-2].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696690411402,
  "history_end_time" : 1696690424095,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "iRZAcGMha3in",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    num_rows = int(num_cols**0.5)  # Square grid\n    num_cols = int(num_cols / num_rows) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'Comparison of {col} (Train vs Test)')\n        \n    \n    axs.flatten()[-2].legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696690166975,
  "history_end_time" : 1696690179165,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pY015grabomn",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    num_rows = int(num_cols**0.5)  # Square grid\n    num_cols = int(num_cols / num_rows) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].title(f'Comparison of {col} (Train vs Test)')\n        axs[i].legend()\n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/pY015grabomn/train_test_pattern_compare.py\", line 91, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/pY015grabomn/train_test_pattern_compare.py\", line 84, in compare\n    axs[i].title(f'Comparison of {col} (Train vs Test)')\nTypeError: 'Text' object is not callable\n",
  "history_begin_time" : 1696689684034,
  "history_end_time" : 1696689689051,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "W7xEWBz0al5h",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    num_rows = int(num_cols**0.5)  # Square grid\n    num_cols = int(num_cols / num_rows) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(num_rows, num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        axs[i].figure(figsize=(8, 4))\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].title(f'Comparison of {col} (Train vs Test)')\n        axs[i].legend()\n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n    plt.close()\n\ncompare()\n",
  "history_output" : "today date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/W7xEWBz0al5h/train_test_pattern_compare.py\", line 92, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/W7xEWBz0al5h/train_test_pattern_compare.py\", line 80, in compare\n    axs[i].figure(figsize=(8, 4))\nTypeError: 'Figure' object is not callable\n",
  "history_begin_time" : 1696689617746,
  "history_end_time" : 1696689622570,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "L8WBM5vLmMeM",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n\n    # Iterate over columns and create subplots\n    for col in tr_df.columns:\n        plt.figure(figsize=(8, 4))\n\n        plt.subplot(1, 2, 1)  # Subplot for the first data frame\n        plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n\n        plt.subplot(1, 2, 2)  # Subplot for the second data frame\n        if col in te_df.columns:\n            plt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n            plt.title(f'Distribution of {col} (Test)')\n\n        plt.title(f'Comparison of {col} (Train vs Test)')\n\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n        plt.close()\n\ncompare()\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-07\ntest start date:  2023-01-19\ntest end date:  2023-10-07\n/home/chetana\nGet slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nGet SWE statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\nTraining DataFrame:            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\nTesting DataFrame:           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\nTraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\nTesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696685397019,
  "history_end_time" : 1696685412090,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ec8T3W0ejbzY",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get slope statistics\")\n  print(data[\"slope\"].describe())\n  \n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (Train)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      if col in te_df.columns:\n        \n      \tplt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      \tplt.title(f'Distribution of {col} (Test)')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-06\ntest start date:  2023-01-19\ntest end date:  2023-10-06\n/home/chetana\nget slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44943  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44943  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44943  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44943  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44943  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44943  25.06 -100.196  252  ...        NaN     NaN       NaN        NaN\n462200  44943  25.06 -100.160  252  ...        NaN     NaN       NaN        NaN\n462201  44943  25.06 -100.124  252  ...        NaN     NaN       NaN        NaN\n462202  44943  25.06 -100.088  252  ...        NaN     NaN       NaN        NaN\n462203  44943  25.06 -100.052  252  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696604412377,
  "history_end_time" : 1696604423212,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "pmae1xyo217",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get slope statistics\")\n  print(data[\"slope\"].describe())\n  \n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (Train)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      if col in te_df.columns:\n        \n      \tplt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      \tplt.title(f'Distribution of {col} (Test)')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "",
  "history_begin_time" : 1696603192258,
  "history_end_time" : 1696603198896,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "eTJmEUJUoBBF",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get slope statistics\")\n  print(data[\"slope\"].describe())\n  \n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (Train)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      if col in te_df.columns:\n        \n      \tplt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      \tplt.title(f'Distribution of {col} (Test)')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\nget slope statistics\ncount    1.008700e+06\nmean     6.243769e+01\nstd      1.650521e+01\nmin      4.277402e+00\n25%      5.213456e+01\n50%      6.768107e+01\n75%      7.504661e+01\nmax      8.368555e+01\nName: slope, dtype: float64\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696601426552,
  "history_end_time" : 1696601438945,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oulaf3BlFNJ1",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (Train)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      if col in te_df.columns:\n        \n      \tplt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      \tplt.title(f'Distribution of {col} (Test)')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696601335131,
  "history_end_time" : 1696601347240,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "HlJDh2DfW1Ji",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (Train)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      if col in te_df.columns:\n        \n      \tplt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      \tplt.title(f'Distribution of {col} (Test)')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-06\ntest start date:  2023-01-18\ntest end date:  2023-10-06\n/home/chetana\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696600652999,
  "history_end_time" : 1696600667972,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "wdMiCiFe0tFR",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (Train)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      if col in te_df.columns:\n        \n      \tplt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      \tplt.title(f'Distribution of {col} (Test)')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\ntoday date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696434037280,
  "history_end_time" : 1696434047984,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "dneAhUZ0w34T",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (DataFrame 1)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      plt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      plt.title(f'Train vs Test - {col} ')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "today date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3802, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 165, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'swe_value'\nThe above exception was the direct cause of the following exception:\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/dneAhUZ0w34T/train_test_pattern_compare.py\", line 75, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/dneAhUZ0w34T/train_test_pattern_compare.py\", line 66, in compare\n    plt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\", line 3807, in __getitem__\n    indexer = self.columns.get_loc(key)\n  File \"/home/chetana/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\", line 3804, in get_loc\n    raise KeyError(key) from err\nKeyError: 'swe_value'\n",
  "history_begin_time" : 1696433835753,
  "history_end_time" : 1696433841928,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "s4fgWEZpTHZQ",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (DataFrame 1)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      plt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      plt.title(f'Train vs Test - {col} ')\n\n      plt.legend()\n      plt.tight_layout()\n      plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "today date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\nget swe statistics\ncount    1.008700e+06\nmean     3.333796e+00\nstd      5.245389e+00\nmin      0.000000e+00\n25%      0.000000e+00\n50%      0.000000e+00\n75%      6.300000e+00\nmax      2.260000e+01\nName: swe_value, dtype: float64\ntraining_df :            date        lat         lon  ...      aspect  eastness  northness\n0        44158  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        44120  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        44093  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        44803  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        43739  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...        ...        ...         ...  ...         ...       ...        ...\n1022695  43787  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  44026  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  44072  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  44444  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  44354  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1008700 rows x 20 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/s4fgWEZpTHZQ/train_test_pattern_compare.py\", line 74, in <module>\n    compare()\n  File \"/home/chetana/gw-workspace/s4fgWEZpTHZQ/train_test_pattern_compare.py\", line 58, in compare\n    plt.figure(figsize=(8, 4))\nNameError: name 'plt' is not defined\n",
  "history_begin_time" : 1696433815842,
  "history_end_time" : 1696433820613,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "UXrpTKUfTyHJ",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\n\ndef clean_train_df(data):\n  data['date'] = pd.to_datetime(data['date'])\n  reference_date = pd.to_datetime('1900-01-01')\n  data['date'] = (data['date'] - reference_date).dt.days\n  data.replace('--', pd.NA, inplace=True)\n  data.fillna(-999, inplace=True)\n  #data.dropna(inplace=True)\n\n  #         for column in data.columns:\n  #           data[column] = pd.to_numeric(data[column], errors='coerce')\n  #           print(data[column].describe())\n\n  # remove all the rows that has swe_value is -999\n  data = data[(data['swe_value'] != -999)]\n\n  print(\"get swe statistics\")\n  print(data[\"swe_value\"].describe())\n\n  #data = data.dropna(subset=['swe_value'])\n  data = data.drop('Unnamed: 0', axis=1)\n  #data = data[data['swe_value'] != 0]\n  return data\n\n\ndef compare():\n  new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  te_df = pd.read_csv(new_testing_data_path)\n\n  print(\"training_df : \", tr_df)\n  print(\"testing_df : \", te_df)\n\n  print(\"training columns: \", tr_df.columns)\n  print(\"testing columns: \", te_df.columns)\n\n  var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n  if not os.path.exists(var_comparison_plot_path):\n      os.makedirs(var_comparison_plot_path)\n\n  # Iterate over columns and create subplots\n  for col in tr_df.columns:\n      plt.figure(figsize=(8, 4))\n\n      plt.subplot(1, 2, 1)  # Subplot for the first data frame\n      plt.hist(tr_df[col], bins=10, alpha=0.5, color='blue', label='Train')\n      plt.title(f'Distribution of {col} (DataFrame 1)')\n\n      plt.subplot(1, 2, 2)  # Subplot for the second data frame\n      plt.hist(te_df[col], bins=10, alpha=0.5, color='red', label='Test')\n      plt.title(f'Train vs Test - {col} ')\n\n      plt.legend()\n      plt.tight_layout()\n       plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\n      plt.close()\n\n      \ncompare()\n\n",
  "history_output" : "  File \"/home/chetana/gw-workspace/UXrpTKUfTyHJ/train_test_pattern_compare.py\", line 70\n    plt.savefig(f'{var_comparison_plot_path}/{col}_comparison.png')\nIndentationError: unexpected indent\n",
  "history_begin_time" : 1696433798986,
  "history_end_time" : 1696433799041,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "n5IhhNQrFBle",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\n\nnew_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\ntraining_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\ntr_df = pd.read_csv(training_data_path)\nte_df = pd.read_csv(new_testing_data_path)\n\nprint(\"training_df : \", tr_df)\nprint(\"testing_df : \", te_df)\n\nprint(\"training columns: \", tr_df.columns)\nprint(\"testing columns: \", te_df.columns)\n\n\n",
  "history_output" : "today date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\ntraining_df :                 date        lat         lon  ...      aspect  eastness  northness\n0        2020-11-25  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        2020-10-18  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        2020-09-21  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        2022-09-01  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        2019-10-03  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...             ...        ...         ...  ...         ...       ...        ...\n1022695  2019-11-20  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  2020-07-16  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  2020-08-31  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  2021-09-07  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  2021-06-09  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1022700 rows x 21 columns]\ntesting_df :           date    lat      lon  SWE  ...  curvature  aspect  eastness  northness\n0       44942  49.00 -125.000    0  ...        NaN     NaN       NaN        NaN\n1       44942  49.00 -124.964    0  ...        NaN     NaN       NaN        NaN\n2       44942  49.00 -124.928    0  ...        NaN     NaN       NaN        NaN\n3       44942  49.00 -124.892    0  ...        NaN     NaN       NaN        NaN\n4       44942  49.00 -124.856    0  ...        NaN     NaN       NaN        NaN\n...       ...    ...      ...  ...  ...        ...     ...       ...        ...\n462199  44942  25.06 -100.196  255  ...        NaN     NaN       NaN        NaN\n462200  44942  25.06 -100.160  255  ...        NaN     NaN       NaN        NaN\n462201  44942  25.06 -100.124  255  ...        NaN     NaN       NaN        NaN\n462202  44942  25.06 -100.088  255  ...        NaN     NaN       NaN        NaN\n462203  44942  25.06 -100.052  255  ...        NaN     NaN       NaN        NaN\n[462204 rows x 19 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'Unnamed: 0',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n       'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n       'relative_humidity_rmax', 'relative_humidity_rmin',\n       'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n       'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness'],\n      dtype='object')\n",
  "history_begin_time" : 1696433347156,
  "history_end_time" : 1696433351209,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "loBgGHlUZwQS",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\n\nnew_testing_data_path = f'{work_dir}/testing_all_ready.csv'\ntraining_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\ntr_df = pd.read_csv(training_data_path)\nte_df = pd.read_csv(new_testing_data_path)\n\nprint(\"training_df : \", tr_df)\nprint(\"testing_df : \", te_df)\n\nprint(\"training columns: \", tr_df.columns)\nprint(\"testing columns: \", te_df.columns)\n\n\n",
  "history_output" : "today date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\ntraining_df :                 date        lat         lon  ...      aspect  eastness  northness\n0        2020-11-25  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        2020-10-18  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        2020-09-21  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        2022-09-01  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        2019-10-03  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...             ...        ...         ...  ...         ...       ...        ...\n1022695  2019-11-20  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  2020-07-16  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  2020-08-31  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  2021-09-07  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  2021-06-09  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1022700 rows x 21 columns]\ntesting_df :          Latitude  Longitude rmin  ... AMSR_SWE AMSR_Flag        date\n0          49.00   -125.000   --  ...        0       241  2023-01-18\n1          49.00   -124.964   --  ...        0       241  2023-01-18\n2          49.00   -124.928   --  ...        0       241  2023-01-18\n3          49.00   -124.892   --  ...        0       241  2023-01-18\n4          49.00   -124.856   --  ...        0       241  2023-01-18\n...          ...        ...  ...  ...      ...       ...         ...\n462199     25.06   -100.196   --  ...      255       255  2023-01-18\n462200     25.06   -100.160   --  ...      255       255  2023-01-18\n462201     25.06   -100.124   --  ...      255       255  2023-01-18\n462202     25.06   -100.088   --  ...      255       255  2023-01-18\n462203     25.06   -100.052   --  ...      255       255  2023-01-18\n[462204 rows x 21 columns]\ntraining columns:  Index(['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'Unnamed: 0',\n       'air_temperature_tmmn', 'potential_evapotranspiration',\n       'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n       'relative_humidity_rmin', 'precipitation_amount',\n       'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n       'aspect', 'eastness', 'northness'],\n      dtype='object')\ntesting columns:  Index(['Latitude', 'Longitude', 'rmin', 'tmmx', 'vs', 'etr', 'pr', 'rmax',\n       'vpd', 'tmmn', 'x', 'y', 'Elevation', 'Slope', 'Aspect', 'Curvature',\n       'Northness', 'Eastness', 'AMSR_SWE', 'AMSR_Flag', 'date'],\n      dtype='object')\n",
  "history_begin_time" : 1696433289827,
  "history_end_time" : 1696433293930,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ovThrGfmX8cU",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\n\nnew_testing_data_path = f'{work_dir}/testing_all_ready.csv'\ntraining_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\n\ntraining_df = pd.read_csv(training_data_path)\ntesting_df = pd.read_csv(new_testing_data_path)\n\nprint(\"training_df : \", training_df)\nprint(\"testing_df : \", testing_df)\n\n\n",
  "history_output" : "today date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\ntraining_df :                 date        lat         lon  ...      aspect  eastness  northness\n0        2020-11-25  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n1        2020-10-18  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n2        2020-09-21  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n3        2022-09-01  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n4        2019-10-03  37.192360 -118.939041  ...   24.605782  0.394541   0.737872\n...             ...        ...         ...  ...         ...       ...        ...\n1022695  2019-11-20  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022696  2020-07-16  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022697  2020-08-31  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022698  2021-09-07  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n1022699  2021-06-09  36.682572 -118.427001  ...  194.287310 -0.241950  -0.769692\n[1022700 rows x 21 columns]\ntesting_df :          Latitude  Longitude rmin  ... AMSR_SWE AMSR_Flag        date\n0          49.00   -125.000   --  ...        0       241  2023-01-18\n1          49.00   -124.964   --  ...        0       241  2023-01-18\n2          49.00   -124.928   --  ...        0       241  2023-01-18\n3          49.00   -124.892   --  ...        0       241  2023-01-18\n4          49.00   -124.856   --  ...        0       241  2023-01-18\n...          ...        ...  ...  ...      ...       ...         ...\n462199     25.06   -100.196   --  ...      255       255  2023-01-18\n462200     25.06   -100.160   --  ...      255       255  2023-01-18\n462201     25.06   -100.124   --  ...      255       255  2023-01-18\n462202     25.06   -100.088   --  ...      255       255  2023-01-18\n462203     25.06   -100.052   --  ...      255       255  2023-01-18\n[462204 rows x 21 columns]\n",
  "history_begin_time" : 1696433241526,
  "history_end_time" : 1696433245864,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "BKusCny9g9OI",
  "history_input" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# this process only analyzes data, we don't touch model at all here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir\nimport os\nimport pandas as pd\n\nnew_testing_data_path = f'{work_dir}/testing_all_ready.csv'\ntraining_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n\ntraining_df = pd.read_csv(training_data_path)\ntesting_df = pd.read_csv(new_testing_data_path)\n\nprint(\"training_df : \", training_df)\nprint(\"testing_df : \", testing_df)\n\n\n",
  "history_output" : "today date = 2023-10-04\ntest start date:  2023-01-18\ntest end date:  2023-10-04\n/home/chetana\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/BKusCny9g9OI/train_test_pattern_compare.py\", line 15, in <module>\n    training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\nNameError: name 'working_dir' is not defined\n",
  "history_begin_time" : 1696433229336,
  "history_end_time" : 1696433230630,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "978v4pbj3q0",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1699806085202,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "t5i5i5s4r1u",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1700204245688,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "424dem85kgs",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1714265111662,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "6q7ef0msrab",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1714265250941,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dmjswu2uycd",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1714274352449,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "anm7br2aqvu",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1700462913697,
  "history_notes" : null,
  "history_process" : "9c573m",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
